{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home # Explore our guides and examples for using SVL Simulator (formerly \"LGSVL Simulator\"). Get started here . Visit our website here: https://www.svlsimulator.com Table of Contents # Release Notes # Release features Release notes Contents of this release Limitations and license notice Previous releases Installation Guide # System requirements Installation procedure Building from source Debugging Unity scripts Running Linux GPU applications on Windows Getting Started # Introduction Conventions Run a simulation Tutorials # Modular testing with the Apollo AD stack Deep learning lane following model Creating a simple ROS2-based AD stack Viewing and subscribing to ground truth obstacles Running a basic Robotics Simulation with ROS 2 Robot simulation with ROS 2 Navigation Stack Mapping a simulation environment in ROS 2 Local Automation (CI/CD) Tutorial Running SVL Simulator on AWS User Interface # Web User Interface Store Library Clusters Simulations Test Results Simulation User Interface Simulator main menu Simulation menu Sensor visualizers Bridge connection UI Configuration file and command line parameters Keyboard shortcuts System Under Test # Introduction Messages Simulator messages The lgsvl_msgs package Sensor message publishing ROS (Autoware.AI) ROS 2 Autoware.Auto Setting up ROS 2 bridge Apollo Latest Apollo Apollo 5.0 Running Simulations # Runtime templates Running simulator Offline mode Developer mode Local Automation (CI/CD) Creating Scenarios # Random traffic Visual scenario editor Python API Python API # Python API guide Python API quickstart guide Dreamview API Simulation Content # Sharing assets Building content Maps Creating a new map Map annotation Road network generation Adding destinations to a map Point cloud import Point cloud rendering Point cloud export Lane-line detector Lighting for indoor enviroments Vehicles Creating a new ego vehicle Vehicle dynamics Sensors List of sensors Lane-line sensor Differential Drive Control Sensor SDF Importing SDF world files Plugins # Sensor plugins Bridge plugins Controllable plugins NPC plugins Pedestrian plugins Developer Debug mode Digital Twin # Digital Twin Lite example map Distributed Simulation # Introduction Running distributed simulation Third Party Integration # OpenAI Gym Support # Troubleshooting Unity help Frequently asked questions Contributing","title":"Home"},{"location":"#table-of-contents","text":"","title":"Table of Contents"},{"location":"#release-notes","text":"Release features Release notes Contents of this release Limitations and license notice Previous releases","title":"Release Notes"},{"location":"#installation-guide","text":"System requirements Installation procedure Building from source Debugging Unity scripts Running Linux GPU applications on Windows","title":"Installation Guide"},{"location":"#getting-started","text":"Introduction Conventions Run a simulation","title":"Getting Started"},{"location":"#tutorials","text":"Modular testing with the Apollo AD stack Deep learning lane following model Creating a simple ROS2-based AD stack Viewing and subscribing to ground truth obstacles Running a basic Robotics Simulation with ROS 2 Robot simulation with ROS 2 Navigation Stack Mapping a simulation environment in ROS 2 Local Automation (CI/CD) Tutorial Running SVL Simulator on AWS","title":"Tutorials"},{"location":"#user-interface","text":"Web User Interface Store Library Clusters Simulations Test Results Simulation User Interface Simulator main menu Simulation menu Sensor visualizers Bridge connection UI Configuration file and command line parameters Keyboard shortcuts","title":"User Interface"},{"location":"#system-under-test","text":"Introduction Messages Simulator messages The lgsvl_msgs package Sensor message publishing ROS (Autoware.AI) ROS 2 Autoware.Auto Setting up ROS 2 bridge Apollo Latest Apollo Apollo 5.0","title":"System Under Test"},{"location":"#running-simulations","text":"Runtime templates Running simulator Offline mode Developer mode Local Automation (CI/CD)","title":"Running Simulations"},{"location":"#creating-scenarios","text":"Random traffic Visual scenario editor Python API","title":"Creating Scenarios"},{"location":"#python-api","text":"Python API guide Python API quickstart guide Dreamview API","title":"Python API"},{"location":"#simulation-content","text":"Sharing assets Building content Maps Creating a new map Map annotation Road network generation Adding destinations to a map Point cloud import Point cloud rendering Point cloud export Lane-line detector Lighting for indoor enviroments Vehicles Creating a new ego vehicle Vehicle dynamics Sensors List of sensors Lane-line sensor Differential Drive Control Sensor SDF Importing SDF world files","title":"Simulation Content"},{"location":"#plugins","text":"Sensor plugins Bridge plugins Controllable plugins NPC plugins Pedestrian plugins Developer Debug mode","title":"Plugins"},{"location":"#digital-twin","text":"Digital Twin Lite example map","title":"Digital Twin"},{"location":"#distributed-simulation","text":"Introduction Running distributed simulation","title":"Distributed Simulation"},{"location":"#third-party-integration","text":"OpenAI Gym","title":"Third Party Integration"},{"location":"#support","text":"Troubleshooting Unity help Frequently asked questions Contributing","title":"Support"},{"location":"archive/","text":"Documentation Archives # You can access the documentation for previous releases of SVL Simulator here. 2021.2.2 2021.2.1 2021.2 2021.1.1 2021.1 2020.06 2020.05 2020.03 2020.01 2019.12 and older: Please refer to the Docs directory in the simulator repository on Github for the relevant release version.","title":"Previous releases"},{"location":"archive/running-simulations/python-vse-runner/","text":"TestCaseRunner # Video # Introduction # TestCaseRunner is a tool for the SVL Simulator which executes test cases created with the Visual Scenario Editor (as well as other Python tests). These tests are executed by the TestCaseRunner using the LGSVL Python API . Note : This tool is only supported on Linux. Package overview top # Python Runner is delivered as part of the svlsimulator-linux64-{release-version}.zip archive with the following directory layout: svlsimulator-linux64-{release-version} \u251c\u2500\u2500 TestCaseRunner \u2502 \u2514\u2500\u2500 scenarioRunner \u2502 \u2514\u2500\u2500 docker \u2502 \u2514\u2500\u2500 scenario_runner.sh The docker directory contains a docker image saved as a tarball. scenario_runner.sh along with the above docker image is used for Python API / Visual scenario editor a.k.a VSE \" runtime templates in web UI to run the scenarios automatically. Visual Scenario runtime template is a way to upload and run VSE test cases directly from web UI. scenario_runner.sh is the Python Runner script that executes VSE test cases in the simulator through the simulator's Python API. Also, if you want to run the VSE scenario manually using \"API only\" runtime template, then you can use this file. Python Runner workflow top # Setup simulator in web UI Launch simulator in API mode Run the Python runner script Start the ego vehicle Requirements # Python Runner has these dependencies: SVL Simulator -- the release tag of the simulator should match the release tag of Python Runner to ensure compatibility. Apollo 5.0 lgsvl fork Docker and Docker-compose Downloading and launching the SVL Simulator top # The Simulator release can be downloaded as a prebuilt binary from the github releases page . Download and extract the zip archive at the desired location. In the unzipped directory, run the executable file named simulator to launch the SVL Simulator. The main window of the simulator will open. Click on the \"Link to Cloud\" or \"Open Browser\" button to launch the simulator Web UI in a web browser. The Web UI controls all aspects of the simulator. You will need to create an account to login to the Web UI. Once logged in you will be able to link your simulator into a cluster, select maps, configure vehicles, and create simulations to run in the simulator. For more information on initial setup of the simulator, refer to the Installing Simuator and Link to Cloud sections of the Running SVL Simulator documentation for an illustrated step-by-step walk through of this process. Configure API Only mode in Web UI (if needed) top # From the simulator Web UI, you can find any needed maps and vehicles in the Store and add each of them to your library. Or, if you want to use a private map to create VSE tests, you will need to upload your map's asset bundle (refer to Adding a map for how to upload your own maps). Similarly, you can also use your own private vehicle (refer to Adding a vehicle for how to upload your own vehicles). Note for any Apollo compatible vehicle you use, you need to have ApolloControl sensor included in order to correctly start simulation. Refer to Apollo Control sensor configuration for more information. Refer to the Store for information on how to add the \"BorregasAve\" and \"SanFrancisco\" maps and \"Lincoln2017MKZ (Apollo 5.0)\" vehicle. Refer to API Only Simulation for an illustrated step-by-step walk through on creating an API Only simulation. Start SVL Simulator in API Only mode top # In the Simulations tab in the Web UI locate the API Only simulation, you need to create a simulation with API Only if you don't have it (for steps refer to Runtime Template: API Only ). Then click the red \"Run Simulation\" button under the API Only simulation to start the simulation. Launching Apollo alongside the Simulator # Installing (and Building) Apollo 5.0 top # Please follow the instructions on the SVL Simulator documentation website to for Running Apollo 5.0 with SVL Simulator if you have not already done so. This will involve installing Docker CE, Nvidia Docker and the LGSVL Docker image, then cloning the lgsvl fork of the Apollo 5.0 sources, and finally building Apollo and the bridge. Start Apollo 5.0 top # Open a terminal, change directory to the apollo-5.0 project (lgsvl fork: https://github.com/lgsvl/apollo-5.0), and type: ./docker/dev_start.sh to start apollo container ./docker/dev_into.sh to enter the container Make sure you have built Apollo with GPU enabled options; for more details, refer to Running Apollo 5.0 with SVL Simulator . Make sure you have HD map files and vehicle calibration files for your VSE test case in apollo repository. bootstrap.sh && bridge.sh & to start the Apollo Dreamview Web UI and cyber_bridge. Open browser, enter address localhost:8888 to view Dreamview. Using the Python Runner for VSE tests # Python Runner top # Navigate to the unzipped directory of the SVL Simulator Linux package in a terminal window. The Python Runner script can be run from the svlsimulator/TestCaseRunner/scenarioRunner directory as ./scenario_runner.sh Python Runner Commands top # The Python Runner supports the following commands: scenario_runner.sh help for help using the Python Runner scenario_runner.sh env to print useful environment variables scenario_runner.sh run to run a test case (with optional parameters) scenario_runner.sh run --help for help running a test case scenario_runner.sh version to print version information Python Runner Parameters top # The Python Runner supports the following command line parameters when running a VSE test case: -d, --duration DURATION Maximum scenario duration in seconds. (default: 20.0) -f, --force-duration Force simulation to end after given duration. If not set, simulation will end by given duration or at the time when all NPCs' waypoints have been reached. Python Runner Environment Variables top # The Python Runner supports the following environment variables (and default values) when running a test case: Environment Variable Description LGSVL__AUTOPILOT_0_HOST Autopilot bridge hostname or IP. If SVL Simulator is running on a different host than Autopilot, this must be set. LGSVL__AUTOPILOT_0_PORT Autopilot bridge port LGSVL__AUTOPILOT_0_VEHICLE_CONFIG Vehicle configuration to be loaded in Dreamview (Capitalization and spacing must match the dropdown in Dreamview) LGSVL__AUTOPILOT_0_VEHICLE_MODULES Comma-separated list of modules to be enabled in Dreamview. Items must be enclosed by double-quotes and there must not be spaces between the double-quotes and commas. (Capitalization and space must match the sliders in Dreamview) LGSVL__AUTOPILOT_HD_MAP HD map to be loaded in Dreamview (Capitalization and spacing must match the dropdown in Dreamview) LGSVL__DATE_TIME Date and time to start simulation at. Time is the local time in the time zone of the map origin. Format 'YYYY-mm-ddTHH:MM:SS' LGSVL__ENVIRONMENT_CLOUDINESS Value of clouds weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_DAMAGE Value of road damage effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_FOG Value of fog weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_RAIN Value of rain weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_WETNESS Value of wetness weather effect, clamped to [0.0, 1.0] LGSVL__MAP Name of map to be loaded in Simulator LGSVL__RANDOM_SEED Seed used to determine random factors (e.g. NPC type, color, behaviour) LGSVL__SIMULATION_DURATION_SECS The time length of the simulation [int] LGSVL__SIMULATOR_HOST SVL Simulator hostname or IP LGSVL__SIMULATOR_PORT SVL Simulator port LGSVL__SPAWN_BICYCLES Whether or not to spawn bicycles LGSVL__SPAWN_PEDESTRIANS Whether or not to spawn pedestrians LGSVL__SPAWN_TRAFFIC Whether or not to spawn NPC vehicles LGSVL__TIME_OF_DAY If LGSVL__DATE_TIME is not set, today's date is used and this sets the time of day to start simulation at using the time zone of the map origin, clamped to [0, 24] LGSVL__TIME_STATIC Whether or not time should remain static (True = time is static, False = time moves forward) LGSVL__VEHICLE_0 Name of EGO vehicle to be loaded in Simulator SCENARIOS_DIR Host folder to be mounted as /scenarios inside the container If simulator is being run on a separate machine than the Python Runner make sure to set the LGSVL__SIMULATOR_HOST environment variable with the IP address of the machine running the simulator; for example export LGSVL__SIMULATOR_HOST=192.168.0.2 . The LGSVL__AUTOPILOT_0_HOST machine is the machine which is running Apollo (and the CyberRT bridge) and is most likely also the same machine running Python Runner. If simulator and Apollo are running on the same machine, use the default value of localhost . Otherwise, set LGSVL__AUTOPILOT_0_HOST to the IP address of the machine running Apollo; for example export LGSVL__AUTOPILOT_0_HOST=192.168.0.1 . You may set SCENARIOS_DIR in order to mount a local directory which contains a VSE script that is not available inside the Python Runner container; for example export SCENARIOS_DIR=\"/home/<user>/my-vse-scripts/\" . Or you can specify the path to the script on the command line. Usage Example # Create a test scenario using the Visual Scenario Editor and make note of the directory to which it is saved. After placing an EgoAgent in the Visual Scenario Editor scene, click on the ego agent, then click the \"Edit\" button and select the desired ego vehicle from the scrolling list that appears. If the vehicle will be controlled by Apollo then select a vehicle that is configured for Apollo, e.g. \"Lincoln2017MKZ (Apollo 5.0)\", which is configured with Apollo sensors and also configured for test case analytics. You also need to select a destination point for the ego agent so that the VSE runner will set up apollo automatically, i.e., connect with apollo, start required modules and set destination. Also note that changing maps in Dreamview will disable the modules so you will need to re-enable them as described above. Running the VSE Scenario top # To run a locally-generated VSE test case called my-vse-test.json which is located in the ~/my-vse-scripts directory, with a test duration of 30 seconds, navigate to the svlsimulator/TestCaseRunner/scenarioRunner directory and type: $ export SCENARIOS_DIR=/home/<user>/my-vse-scripts/ $ ./scenario_runner.sh run my-vse-test.json -d 30 Setting SCENARIOS_DIR is not necessary if you specify the path to the script. In addition, when finished testing in API-Only mode, click on the red \"Stop\" (square/stop) button (at the bottom of the Web UI) to end the API-only simulation. Running VSE runtime template top # The same VSE scenario file can be uploaded to web UI and run automatically. See instructions here . Note : Visual Scenario runtime template based simulations can only be run on Linux simulator build. This is not supported on Windows. Known issues top # The VSE runner won't turn off apollo modules when the scenario finishes. You need to disable the modules once you finish the testing. If your runner is not working correctly, please make sure apollo has corresponding HD maps and vehicle calibration files, and bridge is running.","title":"Python vse runner"},{"location":"archive/running-simulations/python-vse-runner/#video","text":"","title":"Video"},{"location":"archive/running-simulations/python-vse-runner/#introduction","text":"TestCaseRunner is a tool for the SVL Simulator which executes test cases created with the Visual Scenario Editor (as well as other Python tests). These tests are executed by the TestCaseRunner using the LGSVL Python API . Note : This tool is only supported on Linux.","title":"Introduction"},{"location":"archive/running-simulations/python-vse-runner/#package-overview","text":"Python Runner is delivered as part of the svlsimulator-linux64-{release-version}.zip archive with the following directory layout: svlsimulator-linux64-{release-version} \u251c\u2500\u2500 TestCaseRunner \u2502 \u2514\u2500\u2500 scenarioRunner \u2502 \u2514\u2500\u2500 docker \u2502 \u2514\u2500\u2500 scenario_runner.sh The docker directory contains a docker image saved as a tarball. scenario_runner.sh along with the above docker image is used for Python API / Visual scenario editor a.k.a VSE \" runtime templates in web UI to run the scenarios automatically. Visual Scenario runtime template is a way to upload and run VSE test cases directly from web UI. scenario_runner.sh is the Python Runner script that executes VSE test cases in the simulator through the simulator's Python API. Also, if you want to run the VSE scenario manually using \"API only\" runtime template, then you can use this file.","title":"Package overview"},{"location":"archive/running-simulations/python-vse-runner/#python-runner-workflow","text":"Setup simulator in web UI Launch simulator in API mode Run the Python runner script Start the ego vehicle","title":"Python Runner workflow"},{"location":"archive/running-simulations/python-vse-runner/#requirements","text":"Python Runner has these dependencies: SVL Simulator -- the release tag of the simulator should match the release tag of Python Runner to ensure compatibility. Apollo 5.0 lgsvl fork Docker and Docker-compose","title":"Requirements"},{"location":"archive/running-simulations/python-vse-runner/#downloading-and-launching-the-svl-simulator","text":"The Simulator release can be downloaded as a prebuilt binary from the github releases page . Download and extract the zip archive at the desired location. In the unzipped directory, run the executable file named simulator to launch the SVL Simulator. The main window of the simulator will open. Click on the \"Link to Cloud\" or \"Open Browser\" button to launch the simulator Web UI in a web browser. The Web UI controls all aspects of the simulator. You will need to create an account to login to the Web UI. Once logged in you will be able to link your simulator into a cluster, select maps, configure vehicles, and create simulations to run in the simulator. For more information on initial setup of the simulator, refer to the Installing Simuator and Link to Cloud sections of the Running SVL Simulator documentation for an illustrated step-by-step walk through of this process.","title":"Downloading and launching the SVL Simulator"},{"location":"archive/running-simulations/python-vse-runner/#configure-api-only-mode-in-web-ui-(if-needed)","text":"From the simulator Web UI, you can find any needed maps and vehicles in the Store and add each of them to your library. Or, if you want to use a private map to create VSE tests, you will need to upload your map's asset bundle (refer to Adding a map for how to upload your own maps). Similarly, you can also use your own private vehicle (refer to Adding a vehicle for how to upload your own vehicles). Note for any Apollo compatible vehicle you use, you need to have ApolloControl sensor included in order to correctly start simulation. Refer to Apollo Control sensor configuration for more information. Refer to the Store for information on how to add the \"BorregasAve\" and \"SanFrancisco\" maps and \"Lincoln2017MKZ (Apollo 5.0)\" vehicle. Refer to API Only Simulation for an illustrated step-by-step walk through on creating an API Only simulation.","title":"Configure API Only mode in Web UI (if needed)"},{"location":"archive/running-simulations/python-vse-runner/#start-svl-simulator-in-api-only-mode","text":"In the Simulations tab in the Web UI locate the API Only simulation, you need to create a simulation with API Only if you don't have it (for steps refer to Runtime Template: API Only ). Then click the red \"Run Simulation\" button under the API Only simulation to start the simulation.","title":"Start SVL Simulator in API Only mode"},{"location":"archive/running-simulations/python-vse-runner/#launching-apollo-alongside-the-simulator","text":"","title":"Launching Apollo alongside the Simulator"},{"location":"archive/running-simulations/python-vse-runner/#installing-(and-building)-apollo-5.0","text":"Please follow the instructions on the SVL Simulator documentation website to for Running Apollo 5.0 with SVL Simulator if you have not already done so. This will involve installing Docker CE, Nvidia Docker and the LGSVL Docker image, then cloning the lgsvl fork of the Apollo 5.0 sources, and finally building Apollo and the bridge.","title":"Installing (and Building) Apollo 5.0"},{"location":"archive/running-simulations/python-vse-runner/#start-apollo-5.0","text":"Open a terminal, change directory to the apollo-5.0 project (lgsvl fork: https://github.com/lgsvl/apollo-5.0), and type: ./docker/dev_start.sh to start apollo container ./docker/dev_into.sh to enter the container Make sure you have built Apollo with GPU enabled options; for more details, refer to Running Apollo 5.0 with SVL Simulator . Make sure you have HD map files and vehicle calibration files for your VSE test case in apollo repository. bootstrap.sh && bridge.sh & to start the Apollo Dreamview Web UI and cyber_bridge. Open browser, enter address localhost:8888 to view Dreamview.","title":"Start Apollo 5.0"},{"location":"archive/running-simulations/python-vse-runner/#using-the-python-runner-for-vse-tests","text":"","title":"Using the Python Runner for VSE tests"},{"location":"archive/running-simulations/python-vse-runner/#python-runner","text":"Navigate to the unzipped directory of the SVL Simulator Linux package in a terminal window. The Python Runner script can be run from the svlsimulator/TestCaseRunner/scenarioRunner directory as ./scenario_runner.sh","title":"Python Runner"},{"location":"archive/running-simulations/python-vse-runner/#python-runner-commands","text":"The Python Runner supports the following commands: scenario_runner.sh help for help using the Python Runner scenario_runner.sh env to print useful environment variables scenario_runner.sh run to run a test case (with optional parameters) scenario_runner.sh run --help for help running a test case scenario_runner.sh version to print version information","title":"Python Runner Commands"},{"location":"archive/running-simulations/python-vse-runner/#python-runner-parameters","text":"The Python Runner supports the following command line parameters when running a VSE test case: -d, --duration DURATION Maximum scenario duration in seconds. (default: 20.0) -f, --force-duration Force simulation to end after given duration. If not set, simulation will end by given duration or at the time when all NPCs' waypoints have been reached.","title":"Python Runner Parameters"},{"location":"archive/running-simulations/python-vse-runner/#python-runner-environment-variables","text":"The Python Runner supports the following environment variables (and default values) when running a test case: Environment Variable Description LGSVL__AUTOPILOT_0_HOST Autopilot bridge hostname or IP. If SVL Simulator is running on a different host than Autopilot, this must be set. LGSVL__AUTOPILOT_0_PORT Autopilot bridge port LGSVL__AUTOPILOT_0_VEHICLE_CONFIG Vehicle configuration to be loaded in Dreamview (Capitalization and spacing must match the dropdown in Dreamview) LGSVL__AUTOPILOT_0_VEHICLE_MODULES Comma-separated list of modules to be enabled in Dreamview. Items must be enclosed by double-quotes and there must not be spaces between the double-quotes and commas. (Capitalization and space must match the sliders in Dreamview) LGSVL__AUTOPILOT_HD_MAP HD map to be loaded in Dreamview (Capitalization and spacing must match the dropdown in Dreamview) LGSVL__DATE_TIME Date and time to start simulation at. Time is the local time in the time zone of the map origin. Format 'YYYY-mm-ddTHH:MM:SS' LGSVL__ENVIRONMENT_CLOUDINESS Value of clouds weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_DAMAGE Value of road damage effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_FOG Value of fog weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_RAIN Value of rain weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_WETNESS Value of wetness weather effect, clamped to [0.0, 1.0] LGSVL__MAP Name of map to be loaded in Simulator LGSVL__RANDOM_SEED Seed used to determine random factors (e.g. NPC type, color, behaviour) LGSVL__SIMULATION_DURATION_SECS The time length of the simulation [int] LGSVL__SIMULATOR_HOST SVL Simulator hostname or IP LGSVL__SIMULATOR_PORT SVL Simulator port LGSVL__SPAWN_BICYCLES Whether or not to spawn bicycles LGSVL__SPAWN_PEDESTRIANS Whether or not to spawn pedestrians LGSVL__SPAWN_TRAFFIC Whether or not to spawn NPC vehicles LGSVL__TIME_OF_DAY If LGSVL__DATE_TIME is not set, today's date is used and this sets the time of day to start simulation at using the time zone of the map origin, clamped to [0, 24] LGSVL__TIME_STATIC Whether or not time should remain static (True = time is static, False = time moves forward) LGSVL__VEHICLE_0 Name of EGO vehicle to be loaded in Simulator SCENARIOS_DIR Host folder to be mounted as /scenarios inside the container If simulator is being run on a separate machine than the Python Runner make sure to set the LGSVL__SIMULATOR_HOST environment variable with the IP address of the machine running the simulator; for example export LGSVL__SIMULATOR_HOST=192.168.0.2 . The LGSVL__AUTOPILOT_0_HOST machine is the machine which is running Apollo (and the CyberRT bridge) and is most likely also the same machine running Python Runner. If simulator and Apollo are running on the same machine, use the default value of localhost . Otherwise, set LGSVL__AUTOPILOT_0_HOST to the IP address of the machine running Apollo; for example export LGSVL__AUTOPILOT_0_HOST=192.168.0.1 . You may set SCENARIOS_DIR in order to mount a local directory which contains a VSE script that is not available inside the Python Runner container; for example export SCENARIOS_DIR=\"/home/<user>/my-vse-scripts/\" . Or you can specify the path to the script on the command line.","title":"Python Runner Environment Variables"},{"location":"archive/running-simulations/python-vse-runner/#usage-example","text":"Create a test scenario using the Visual Scenario Editor and make note of the directory to which it is saved. After placing an EgoAgent in the Visual Scenario Editor scene, click on the ego agent, then click the \"Edit\" button and select the desired ego vehicle from the scrolling list that appears. If the vehicle will be controlled by Apollo then select a vehicle that is configured for Apollo, e.g. \"Lincoln2017MKZ (Apollo 5.0)\", which is configured with Apollo sensors and also configured for test case analytics. You also need to select a destination point for the ego agent so that the VSE runner will set up apollo automatically, i.e., connect with apollo, start required modules and set destination. Also note that changing maps in Dreamview will disable the modules so you will need to re-enable them as described above.","title":"Usage Example"},{"location":"archive/running-simulations/python-vse-runner/#running-the-vse-scenario","text":"To run a locally-generated VSE test case called my-vse-test.json which is located in the ~/my-vse-scripts directory, with a test duration of 30 seconds, navigate to the svlsimulator/TestCaseRunner/scenarioRunner directory and type: $ export SCENARIOS_DIR=/home/<user>/my-vse-scripts/ $ ./scenario_runner.sh run my-vse-test.json -d 30 Setting SCENARIOS_DIR is not necessary if you specify the path to the script. In addition, when finished testing in API-Only mode, click on the red \"Stop\" (square/stop) button (at the bottom of the Web UI) to end the API-only simulation.","title":"Running the VSE Scenario"},{"location":"archive/running-simulations/python-vse-runner/#running-vse-runtime-template","text":"The same VSE scenario file can be uploaded to web UI and run automatically. See instructions here . Note : Visual Scenario runtime template based simulations can only be run on Linux simulator build. This is not supported on Windows.","title":"Running VSE runtime template"},{"location":"archive/running-simulations/python-vse-runner/#known-issues","text":"The VSE runner won't turn off apollo modules when the scenario finishes. You need to disable the modules once you finish the testing. If your runner is not working correctly, please make sure apollo has corresponding HD maps and vehicle calibration files, and bridge is running.","title":"Known issues"},{"location":"creating-scenarios/api-example-descriptions/","text":"Python API Use Case Examples # The SVL Simulator team has created sample Python scripts that use the SVL Simulator Python API to test specific scenarios or perform certain tasks. These example scripts can be found on our Github here . Please contact us if you would like to contribute examples that you are using, or submit a pull request . Scenarios top # We have created basic sample scenarios using the Python API. See here for a step-by-step guide on how to run one of these scenarios. Several are based on the sample Test Cases from NHTSA . The below scenarios assume that the simulator can be connected to an instance of Apollo 5.0. See the guide for getting connected with Apollo 5.0 here . The Apollo modules that need to be started are shown below (localization, perception, planning, prediction, routing, traffic light, transform, control): It is recommended to start Apollo and the modules before running a scenario. Apollo's destination can be set after Localization and Routing have been started. Vehicle Following top # Scripts: Perform Vehicle Following This scenario simulates the EGO vehicle approaching a slower NPC from behind. The EGO is expected to accelerate up to the speed limit and catch up to the NPC. For this scenario, the destination is the end of the lane. Encroaching Oncoming Vehicle top # Scripts: Detect and Respond to Encroaching Oncoming Vehicle This scenario simulates the EGO vehicle approaching an oncoming NPC that is half in the EGO's lane making a collision imminent. The EGO is expected to avoid a collision. Here the NPC uses the waypoint system to define its path. With waypoints, the NPC ignores other traffic and does not attempt to avoid collisions. For this scenario, the destinaion is the end of the lane. The same destination can be used as the Vehicle Following scripts. Other Uses top # Collecting data in KITTI format top # Script: kitti_parser.py This script shows an example of collecting data in the KITTI format. This data can be used to train for detecting vehicles in images. This script spawns the ego vehicle in a random position in the San Francisco map. Then a number of NPC vehicles are randomly spawned in front of the ego vehicle. Camera and ground truth data is saved in the KITTI format. This data can be used to train for detecting vehicles in images. For more information on KITTI please see: http://www.cvlibs.net/datasets/kitti/index.php The data format is defined in a README file downloadable from: https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_object.zip Automated Driving System Test Cases top # The United States National Highway Traffic Safety Administration released a report describing a framework for establishing sample preliminary tests. The report is available online: A Framework for Automated Driving System Testable Cases and Scenarios We created several of the described tests available here: NHTSA-sample-tests These tests run the ADS at different speeds. To accomplish this with Apollo, the speed limit in the HD map of the appropriate lanes needs to be adjusted and the planning configuration should also be changed to limit Apollo's top speed. The ADS destination is described in the report. For our implementation of the Perform Lane Change tests, the same destination as the above Overtaker and Traffic Jam scenarios is used.","title":"Python API Use Case Examples [](#top)"},{"location":"creating-scenarios/api-example-descriptions/#scenarios","text":"We have created basic sample scenarios using the Python API. See here for a step-by-step guide on how to run one of these scenarios. Several are based on the sample Test Cases from NHTSA . The below scenarios assume that the simulator can be connected to an instance of Apollo 5.0. See the guide for getting connected with Apollo 5.0 here . The Apollo modules that need to be started are shown below (localization, perception, planning, prediction, routing, traffic light, transform, control): It is recommended to start Apollo and the modules before running a scenario. Apollo's destination can be set after Localization and Routing have been started.","title":"Scenarios"},{"location":"creating-scenarios/api-example-descriptions/#vehicle-following","text":"Scripts: Perform Vehicle Following This scenario simulates the EGO vehicle approaching a slower NPC from behind. The EGO is expected to accelerate up to the speed limit and catch up to the NPC. For this scenario, the destination is the end of the lane.","title":"Vehicle Following"},{"location":"creating-scenarios/api-example-descriptions/#encroaching-oncoming-vehicle","text":"Scripts: Detect and Respond to Encroaching Oncoming Vehicle This scenario simulates the EGO vehicle approaching an oncoming NPC that is half in the EGO's lane making a collision imminent. The EGO is expected to avoid a collision. Here the NPC uses the waypoint system to define its path. With waypoints, the NPC ignores other traffic and does not attempt to avoid collisions. For this scenario, the destinaion is the end of the lane. The same destination can be used as the Vehicle Following scripts.","title":"Encroaching Oncoming Vehicle"},{"location":"creating-scenarios/api-example-descriptions/#other-uses","text":"","title":"Other Uses"},{"location":"creating-scenarios/api-example-descriptions/#collecting-data-in-kitti-format","text":"Script: kitti_parser.py This script shows an example of collecting data in the KITTI format. This data can be used to train for detecting vehicles in images. This script spawns the ego vehicle in a random position in the San Francisco map. Then a number of NPC vehicles are randomly spawned in front of the ego vehicle. Camera and ground truth data is saved in the KITTI format. This data can be used to train for detecting vehicles in images. For more information on KITTI please see: http://www.cvlibs.net/datasets/kitti/index.php The data format is defined in a README file downloadable from: https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_object.zip","title":"Collecting data in KITTI format"},{"location":"creating-scenarios/api-example-descriptions/#automated-driving-system-test-cases","text":"The United States National Highway Traffic Safety Administration released a report describing a framework for establishing sample preliminary tests. The report is available online: A Framework for Automated Driving System Testable Cases and Scenarios We created several of the described tests available here: NHTSA-sample-tests These tests run the ADS at different speeds. To accomplish this with Apollo, the speed limit in the HD map of the appropriate lanes needs to be adjusted and the planning configuration should also be changed to limit Apollo's top speed. The ADS destination is described in the report. For our implementation of the Perform Lane Change tests, the same destination as the above Overtaker and Traffic Jam scenarios is used.","title":"Automated Driving System Test Cases"},{"location":"creating-scenarios/python-scenarios/","text":"Creating Python API Scenarios #","title":"Creating Python API Scenarios"},{"location":"digital-twin/gomentum-dtl/","text":"Example map: GoMentum Station Digital Twin Lite # We are providing the GoMentum Station Digital Twin Lite (GoMentum DTL) map as an example of a point cloud based environment. The GoMentum Station area is a testing ground dedicated for autonomous vehicles located in Concord, California . We drove our data collection car and collected sensor data in this area for generating the GoMentum DTL map. Using the point cloud importer and point cloud renderer , the Simulator can dynamically load and unload the point clouds based on the location of the ego vehicles. Thus, there is no limitation on the size of the map with improved performance of simulations. Figure: Preview of GoMentum Station Digital Twin Lite map Prerequisites top # SVL Simulator GoMentum DTL (Digital Twin Lite) map assetbundle Apollo 5.0 Instructions for how to run with Apollo 5.0 top # Follow these steps to run Apollo on GoMentum DTL: Launch SVL Simulator Click on the Open Browser button to launch the simulator Web UI in a web browser In the Simulations section, locate Local-Random: GoMentum DTL (Apollo) simulation under the Available from Others tab and add it by clicking the Add button Select your cluster from the cluster dropdown menu in the General tab Select the Apollo 5.0 as your sensor configuration for the Lincoln2017MKZ vehicle in the Test case tab Select the Apollo 5.0 as your autopilot and enter your bridge IP address and port number in the Autopilot tab Click the Publish button to finish the simulation setup in the Publish tab (Optional) GoMentum DTL map and Lincoln2017MKZ vehicle should be automatically added to the Library . If not, please follow the instructions for Adding a map and Adding a vehicle to add them manually. In the Vehicles section under the Library , locate Lincoln2017MKZ vehicle and click it to go into a sensor configuration page Select the Apollo 5.0 sensor configuration If you see a notification for any missing plugins, click the Add to Library button and make sure that you have all the sensor plugins listed in the configuration added to the Plugins section under the Library Locate Local-Random: GoMentum DTL (Apollo) simulation in the Simulations tab and click the Run Simulation button at the bottom to start simulation Local-Random: GoMentum DTL (Apollo) simulation should now be up and running in the main window of the simulator Finally, launch Apollo 5.0 alongside SVL Simulator Figure: Apollo dreamview and SVL Simulator detecting NPCs on the road with sensor visualizations enabled Figure: Apollo dreamview and SVL Simulator detecting an NPC in an intersection from top-down view Figure: Rviz showing point clouds published to ROS from SVL Simulator Known issues top # Overall, Apollo will be able to drive around most areas of GoMentum DTL with no issues, but there are some known issues in GoMentum DTL as this is an early access version. Here is a list of known issues for the current release that will be fixed soon in future releases. Dynamic objects top # Some other cars were captured while collecting sensor data from the field and remain as dummy noises on some roads. Later, we will have a Dynamic object removal feature in the Digital Twin Lite pipeline which removes dynamic objects such as cars or pedestrians from point clouds during the data processing. Figure: Streaks of blur on the road due to a moving car captured in ROSBAG Baked-in shadows top # Colorized point clouds have shadows baked in already (e.g., under trees) because shadows were captured in camera images during a data collection phase. In the future release, we\u2019ll remove the baked-in shadows from images and use simulated shadows instead based on the sun position in the simulator. Figure: Baked-in shadows from trees and buildings on the road","title":"Digital Twin Lite example map"},{"location":"digital-twin/gomentum-dtl/#prerequisites","text":"SVL Simulator GoMentum DTL (Digital Twin Lite) map assetbundle Apollo 5.0","title":"Prerequisites"},{"location":"digital-twin/gomentum-dtl/#instructions-for-how-to-run-with-apollo-5.0","text":"Follow these steps to run Apollo on GoMentum DTL: Launch SVL Simulator Click on the Open Browser button to launch the simulator Web UI in a web browser In the Simulations section, locate Local-Random: GoMentum DTL (Apollo) simulation under the Available from Others tab and add it by clicking the Add button Select your cluster from the cluster dropdown menu in the General tab Select the Apollo 5.0 as your sensor configuration for the Lincoln2017MKZ vehicle in the Test case tab Select the Apollo 5.0 as your autopilot and enter your bridge IP address and port number in the Autopilot tab Click the Publish button to finish the simulation setup in the Publish tab (Optional) GoMentum DTL map and Lincoln2017MKZ vehicle should be automatically added to the Library . If not, please follow the instructions for Adding a map and Adding a vehicle to add them manually. In the Vehicles section under the Library , locate Lincoln2017MKZ vehicle and click it to go into a sensor configuration page Select the Apollo 5.0 sensor configuration If you see a notification for any missing plugins, click the Add to Library button and make sure that you have all the sensor plugins listed in the configuration added to the Plugins section under the Library Locate Local-Random: GoMentum DTL (Apollo) simulation in the Simulations tab and click the Run Simulation button at the bottom to start simulation Local-Random: GoMentum DTL (Apollo) simulation should now be up and running in the main window of the simulator Finally, launch Apollo 5.0 alongside SVL Simulator Figure: Apollo dreamview and SVL Simulator detecting NPCs on the road with sensor visualizations enabled Figure: Apollo dreamview and SVL Simulator detecting an NPC in an intersection from top-down view Figure: Rviz showing point clouds published to ROS from SVL Simulator","title":"Instructions for how to run with Apollo 5.0"},{"location":"digital-twin/gomentum-dtl/#known-issues","text":"Overall, Apollo will be able to drive around most areas of GoMentum DTL with no issues, but there are some known issues in GoMentum DTL as this is an early access version. Here is a list of known issues for the current release that will be fixed soon in future releases.","title":"Known issues"},{"location":"digital-twin/gomentum-dtl/#dynamic-objects","text":"Some other cars were captured while collecting sensor data from the field and remain as dummy noises on some roads. Later, we will have a Dynamic object removal feature in the Digital Twin Lite pipeline which removes dynamic objects such as cars or pedestrians from point clouds during the data processing. Figure: Streaks of blur on the road due to a moving car captured in ROSBAG","title":"Dynamic objects"},{"location":"digital-twin/gomentum-dtl/#baked-in-shadows","text":"Colorized point clouds have shadows baked in already (e.g., under trees) because shadows were captured in camera images during a data collection phase. In the future release, we\u2019ll remove the baked-in shadows from images and use simulated shadows instead based on the sun position in the simulator. Figure: Baked-in shadows from trees and buildings on the road","title":"Baked-in shadows"},{"location":"distributed-simulation/custom-distributed-class/","text":"Custom Distributed Class # Distributed system supports sending distributed messages from any class implementing IMessageSender to the objects of classes IMessageReceiver . A custom class can implement both interfaces, address key property will be shared. Address Key top # Key property of the IMessageSender and IMessageReceiver is an address for the distributed messages. This key must be globally unique and deterministic, on every machine in every Simulation run the key of an object has to return the same value. Registration top # Every object that is going to send or receive distributed messages has to register itself in the MessagesManager . In the Simulator MessagesManager instance is available in the: Loader.Instance.Network.MessagesManager , this property has a null reference if the simulation does not use a cluster. Whenever an object is ready to send or receive distributed messages it has to register itself with the RegisterObject method of the MessagesManager and when an object will no longer send or receiver messages it has to unregister itself with UnregisterObject method of the MessagesManager . Messages received for the unregistered address key will be stored and passed to proper objects after registration. Message Sender top # IMessageSender implementation requires: UnicastMessage , a basic implementation invokes UnicastMessage method of an MessagesManager instance; BroadcastMessage , a basic implementation invokes BroadcastMessage method of an MessagesManager instance; UnicastInitialMessages , requires sending every data required for the object initialization using UnicastMessage method. Message Receiver top # IMessageReceiver implementation requires: ReceiveMessage , parses every incomming data after object is registered.","title":"Custom Distributed Class [](#top)"},{"location":"distributed-simulation/custom-distributed-class/#address-key","text":"Key property of the IMessageSender and IMessageReceiver is an address for the distributed messages. This key must be globally unique and deterministic, on every machine in every Simulation run the key of an object has to return the same value.","title":"Address Key"},{"location":"distributed-simulation/custom-distributed-class/#registration","text":"Every object that is going to send or receive distributed messages has to register itself in the MessagesManager . In the Simulator MessagesManager instance is available in the: Loader.Instance.Network.MessagesManager , this property has a null reference if the simulation does not use a cluster. Whenever an object is ready to send or receive distributed messages it has to register itself with the RegisterObject method of the MessagesManager and when an object will no longer send or receiver messages it has to unregister itself with UnregisterObject method of the MessagesManager . Messages received for the unregistered address key will be stored and passed to proper objects after registration.","title":"Registration"},{"location":"distributed-simulation/custom-distributed-class/#message-sender","text":"IMessageSender implementation requires: UnicastMessage , a basic implementation invokes UnicastMessage method of an MessagesManager instance; BroadcastMessage , a basic implementation invokes BroadcastMessage method of an MessagesManager instance; UnicastInitialMessages , requires sending every data required for the object initialization using UnicastMessage method.","title":"Message Sender"},{"location":"distributed-simulation/custom-distributed-class/#message-receiver","text":"IMessageReceiver implementation requires: ReceiveMessage , parses every incomming data after object is registered.","title":"Message Receiver"},{"location":"distributed-simulation/distributed-components/","text":"Distributed Components # DistributedComponent is an abstract class with the implementation of sending snapshots from the authoritative object to all connected peers. For example DistributedTransform synchronize transforms states in the clients' simulations to the corresponding transform state on the master basing on the position, rotation and scale sent in the snapshots. DistributedTransform component added to GameObject in the simulation will synchronize the transform, note that every DistributedComponent requires a DistributedObject added to the same GameObject or any parent GameObject. Custom Distributed Component top # Extending the DistributedComponent component requires the following implementations: ComponentKey property, if multiple components of this type are allowed in a single GameObject it has to be a unique key otherwise, it can be for example class name. GetSnapshot method which returns snapshot data inside a ByteStack object (refer to Distributed Messages for more information about ByteStack ). ApplySnapshot method which parses and applies the snapshot data from the message content to the object. Note that the ApplySnapshot method has to pop data in reverse order than GetSnapshot is pushing data. Distributed Components With Deltas top # A snapshot includes data required to recreate the same state on the client. Sending the whole snapshot with a whole object's state when only a single element changes will contain too much redundant data. DistributedComponentWithDeltas extends the basic implementation with two methods: SendDelta which sends the message with passed delta data inside a ByteStack object. ApplyDelta abstract method which parses and applies the delta data from the message content to the object. Note that the SendDelta method has to pop data in reverse order than ApplyDelta is pushing data. Distributed Transform top # DistributedTransform sends local position, local rotation and local scale of a transform component in the snapshots from the master to the clients. Only a single DistributedTransform component can be attached to a GameObject. Snapshots are send up to 60 times per second only if any element of the snapshot changes. The snapshots limit can be changed on the master by changing the SnapshotsPerSecondLimit property value Distributed Rigidbody top # DistributedRigidbody sends the position and rotation of the rigidbody in the same GameObject from the master to the clients. With the default setting snapshots are just applied to the rigidbodies. It is possible to change SimulationType to ExtrapolateVelocities , with this setting DistributedRigidbody extrapolates received velocity and angular velocity. Applied position and rotation includes the corrections calculated from the extrapolated velocities. Snapshots are send up to 60 times per second only if rigidbody is not in sleeping mode. The snapshots limit can be changed on the master by changing the SnapshotsPerSecondLimit property value","title":"Distributed Components [](#top)"},{"location":"distributed-simulation/distributed-components/#custom-distributed-component","text":"Extending the DistributedComponent component requires the following implementations: ComponentKey property, if multiple components of this type are allowed in a single GameObject it has to be a unique key otherwise, it can be for example class name. GetSnapshot method which returns snapshot data inside a ByteStack object (refer to Distributed Messages for more information about ByteStack ). ApplySnapshot method which parses and applies the snapshot data from the message content to the object. Note that the ApplySnapshot method has to pop data in reverse order than GetSnapshot is pushing data.","title":"Custom Distributed Component"},{"location":"distributed-simulation/distributed-components/#distributed-components-with-deltas","text":"A snapshot includes data required to recreate the same state on the client. Sending the whole snapshot with a whole object's state when only a single element changes will contain too much redundant data. DistributedComponentWithDeltas extends the basic implementation with two methods: SendDelta which sends the message with passed delta data inside a ByteStack object. ApplyDelta abstract method which parses and applies the delta data from the message content to the object. Note that the SendDelta method has to pop data in reverse order than ApplyDelta is pushing data.","title":"Distributed Components With Deltas"},{"location":"distributed-simulation/distributed-components/#distributed-transform","text":"DistributedTransform sends local position, local rotation and local scale of a transform component in the snapshots from the master to the clients. Only a single DistributedTransform component can be attached to a GameObject. Snapshots are send up to 60 times per second only if any element of the snapshot changes. The snapshots limit can be changed on the master by changing the SnapshotsPerSecondLimit property value","title":"Distributed Transform"},{"location":"distributed-simulation/distributed-components/#distributed-rigidbody","text":"DistributedRigidbody sends the position and rotation of the rigidbody in the same GameObject from the master to the clients. With the default setting snapshots are just applied to the rigidbodies. It is possible to change SimulationType to ExtrapolateVelocities , with this setting DistributedRigidbody extrapolates received velocity and angular velocity. Applied position and rotation includes the corrections calculated from the extrapolated velocities. Snapshots are send up to 60 times per second only if rigidbody is not in sleeping mode. The snapshots limit can be changed on the master by changing the SnapshotsPerSecondLimit property value","title":"Distributed Rigidbody"},{"location":"distributed-simulation/distributed-messages/","text":"Distributed Messages # DistributedMessage is a class that is used to exchange data between peers in the distributed system. Every message contains the Content where the actual data is stored and meta-data like address key, message type, and timestamp. Address Key top # AddressKey determines which object will receive the message on other simulations. DistributedComponent uses own key as an address key so the message will be received by corresponding components on other machines. AddressKey has to be set when sending a new message. Content top # Content contains the actual data of the message in the BytesStack object. Note that poping data required reverse order than pushing. Message Type top # DistributedMessageType determines how the message will be handled in the UDP protocol. This type has to be set when sending a new message. Available distributed message types: ReliableUnordered packets won't be dropped, won't be duplicated, can arrive without order. Sequenced packets can be dropped, won't be duplicated, will arrive in order. ReliableOrdered packets won't be dropped, won't be duplicated, will arrive in order. ReliableSequenced packets can be dropped (except the last one), won't be duplicated, will arrive in order. Unreliable packets can be dropped, can be duplicated, can arrive without order. ReliableOrdered messages are the most reliable, but may add significant delays, this type is best for the initialization messages like add new NPC command. Unreliable packets have the lowest delay between sending and handling a message but are not reliable. Snapshots are sent as unreliable , an incoming snapshot will override the data and delayed packets will not be used. Timestamp top # The distributed system adds UTC timestamp to every message right before sending it, there is no need to set timestamp value when sending a message. A timestamp of the received message is set before passing the message to the addressed object. This timestamp is already corrected on the client by the value of connection latency and shows the approximate UTC DateTime when the message has been sent by the master. Bytes Stack top # BytesStack is the message's content where the data is stored. Every data has to be represented as a set of bytes and has to be pushed to the stack. BytesStack class supports byte , int , uint , long , float , double , bool and string values. Operations that can be executed on those values are: Push - adds the value on the top of the stack; Fetch - reads the data from the top of the stack and does not remove it from the stack; Pop - reads the data from the top of the stack and removed it from the stack, note that Pop calls have to be called in reverse order than Push . Byte Compression top # ByteCompression class adds extension methods to the BytesStack which add different values to the stack with limited bytes count: CompressFloatToInt and DecompressFloatFromInt ; PushEnum and PopEnum ; PushCompressedColor and PopDecompressedColor ; PushCompressedVector3 and PushCompressedVector3 ; PushUncompressedVector3 and PopUncompressedVector3 ; PushCompressedPosition and PopDecompressedPosition - position bounds are limited to the map bounds; PushCompressedRotation and PopDecompressedRotation .","title":"Distributed Messages [](#top)"},{"location":"distributed-simulation/distributed-messages/#address-key","text":"AddressKey determines which object will receive the message on other simulations. DistributedComponent uses own key as an address key so the message will be received by corresponding components on other machines. AddressKey has to be set when sending a new message.","title":"Address Key"},{"location":"distributed-simulation/distributed-messages/#content","text":"Content contains the actual data of the message in the BytesStack object. Note that poping data required reverse order than pushing.","title":"Content"},{"location":"distributed-simulation/distributed-messages/#message-type","text":"DistributedMessageType determines how the message will be handled in the UDP protocol. This type has to be set when sending a new message. Available distributed message types: ReliableUnordered packets won't be dropped, won't be duplicated, can arrive without order. Sequenced packets can be dropped, won't be duplicated, will arrive in order. ReliableOrdered packets won't be dropped, won't be duplicated, will arrive in order. ReliableSequenced packets can be dropped (except the last one), won't be duplicated, will arrive in order. Unreliable packets can be dropped, can be duplicated, can arrive without order. ReliableOrdered messages are the most reliable, but may add significant delays, this type is best for the initialization messages like add new NPC command. Unreliable packets have the lowest delay between sending and handling a message but are not reliable. Snapshots are sent as unreliable , an incoming snapshot will override the data and delayed packets will not be used.","title":"Message Type"},{"location":"distributed-simulation/distributed-messages/#timestamp","text":"The distributed system adds UTC timestamp to every message right before sending it, there is no need to set timestamp value when sending a message. A timestamp of the received message is set before passing the message to the addressed object. This timestamp is already corrected on the client by the value of connection latency and shows the approximate UTC DateTime when the message has been sent by the master.","title":"Timestamp"},{"location":"distributed-simulation/distributed-messages/#bytes-stack","text":"BytesStack is the message's content where the data is stored. Every data has to be represented as a set of bytes and has to be pushed to the stack. BytesStack class supports byte , int , uint , long , float , double , bool and string values. Operations that can be executed on those values are: Push - adds the value on the top of the stack; Fetch - reads the data from the top of the stack and does not remove it from the stack; Pop - reads the data from the top of the stack and removed it from the stack, note that Pop calls have to be called in reverse order than Push .","title":"Bytes Stack"},{"location":"distributed-simulation/distributed-messages/#byte-compression","text":"ByteCompression class adds extension methods to the BytesStack which add different values to the stack with limited bytes count: CompressFloatToInt and DecompressFloatFromInt ; PushEnum and PopEnum ; PushCompressedColor and PopDecompressedColor ; PushCompressedVector3 and PushCompressedVector3 ; PushUncompressedVector3 and PopUncompressedVector3 ; PushCompressedPosition and PopDecompressedPosition - position bounds are limited to the map bounds; PushCompressedRotation and PopDecompressedRotation .","title":"Byte Compression"},{"location":"distributed-simulation/distributed-objects/","text":"Distributed Objects # DistributedObject component synchronizes the GameObject state on the cluster simulation clients. DistributedObject can limit broadcasts sent by the DistributedComponents added to the children GameObjects and by default changes the part of components' keys. Custom Key top # Every object synchronized in the cluster simulation required a unique key that is the same on every machine and cannot change after initialization. DistributedObject uses own path in the hierarchy as the unique key as a default implementation, but the synchronization will fail if there are two GameObjects with the same name in the same hierarchy path. The key of a DistributedObject can be changed by any component attached to the same GameObject which implements the IGloballyUniquelyIdentified interface. When registering a DistributedObject search if there is an IGloballyUniquelyIdentified implementation attached to the GameObject if an implementation is available GUID is used as the key prefix. If the GUID is null or empty registration waits until GUID changes to a not empty value. Advanced top # DistributedObject implementation provides features that require additional scripting. This section describe advanced use cases that can be achieved with the DistributedObject . Selective Distribution top # Some objects should not be distributed to all the connected clients. It is possible to limit the distribution by setting the Selective Distribution to True and adding endpoints to the list (using AddEndPointToSelectiveDistribution method). With Selective Distribution enabled only endpoints on the list will receive updates from the DistributedObject and all the DistributedComponents in children. Selective distribution is prepared for the use-case when more logic has to be handled by the client. For example, if a client should count the waypoints of selected NPC those waypoints have to be synchronized between that client and master. In this case, pathfinding will be calculated on the client, the master will perform the simulation updates and other clients will not be informed about those changes. Authoritative Object top # Available distributed components in the Simulator got different logic for the master and client. All the objects on the master are authoritative and send their state to the clients, where nonauthoritative objects apply the changes. This behavior can be changed by setting the custom value to the protected property IsAuthoritative , but it has to be done in the overridden Initialize method before calling the base method. In the case from the previous paragraph Selective Distribution client would perform the pathfinding and set the waypoints positions. The client sets the position of those GameObjects and has to send them to the master. To reverse the behavior where a master is sending the position of a DistributedRigidbody to the client, IsAuthoritative property has to be reversed ( false value on the master and true on the client). This way client will update the master with the Rigidbody changes.","title":"Distributed Objects [](#top)"},{"location":"distributed-simulation/distributed-objects/#custom-key","text":"Every object synchronized in the cluster simulation required a unique key that is the same on every machine and cannot change after initialization. DistributedObject uses own path in the hierarchy as the unique key as a default implementation, but the synchronization will fail if there are two GameObjects with the same name in the same hierarchy path. The key of a DistributedObject can be changed by any component attached to the same GameObject which implements the IGloballyUniquelyIdentified interface. When registering a DistributedObject search if there is an IGloballyUniquelyIdentified implementation attached to the GameObject if an implementation is available GUID is used as the key prefix. If the GUID is null or empty registration waits until GUID changes to a not empty value.","title":"Custom Key"},{"location":"distributed-simulation/distributed-objects/#advanced","text":"DistributedObject implementation provides features that require additional scripting. This section describe advanced use cases that can be achieved with the DistributedObject .","title":"Advanced"},{"location":"distributed-simulation/distributed-objects/#selective-distribution","text":"Some objects should not be distributed to all the connected clients. It is possible to limit the distribution by setting the Selective Distribution to True and adding endpoints to the list (using AddEndPointToSelectiveDistribution method). With Selective Distribution enabled only endpoints on the list will receive updates from the DistributedObject and all the DistributedComponents in children. Selective distribution is prepared for the use-case when more logic has to be handled by the client. For example, if a client should count the waypoints of selected NPC those waypoints have to be synchronized between that client and master. In this case, pathfinding will be calculated on the client, the master will perform the simulation updates and other clients will not be informed about those changes.","title":"Selective Distribution"},{"location":"distributed-simulation/distributed-objects/#authoritative-object","text":"Available distributed components in the Simulator got different logic for the master and client. All the objects on the master are authoritative and send their state to the clients, where nonauthoritative objects apply the changes. This behavior can be changed by setting the custom value to the protected property IsAuthoritative , but it has to be done in the overridden Initialize method before calling the base method. In the case from the previous paragraph Selective Distribution client would perform the pathfinding and set the waypoints positions. The client sets the position of those GameObjects and has to send them to the master. To reverse the behavior where a master is sending the position of a DistributedRigidbody to the client, IsAuthoritative property has to be reversed ( false value on the master and true on the client). This way client will update the master with the Rigidbody changes.","title":"Authoritative Object"},{"location":"distributed-simulation/distributed-python-api/","text":"Distributed Python API # Cluster Simulation performs changes only on the master simulation, clients' simulations apply the changes received from the master and don't require to react on every API command. Only selected commands are distributed to the clients, for example, AddAgent , LoadScene and Reset commands. Command Setup top # If command should be distributed to the clients it has to implement the IDistributedObject interface. Master simulation can modify the arguments that will be sent to the clients inside the Execute methods. Asynchronous Commands top # Some commands, like AddAgent and LoadScene , requires more time to execute as they may download required Asset Bundles from the server. In this case master simulation has to wait for all the cluster machines to execute the command. Simulator uses ILockingCommand interface to solve this use case. Commands that implement this interface locks the Python API process until command's Executed action is invoked on every machine.","title":"Distributed Python API [](#top)"},{"location":"distributed-simulation/distributed-python-api/#command-setup","text":"If command should be distributed to the clients it has to implement the IDistributedObject interface. Master simulation can modify the arguments that will be sent to the clients inside the Execute methods.","title":"Command Setup"},{"location":"distributed-simulation/distributed-python-api/#asynchronous-commands","text":"Some commands, like AddAgent and LoadScene , requires more time to execute as they may download required Asset Bundles from the server. In this case master simulation has to wait for all the cluster machines to execute the command. Simulator uses ILockingCommand interface to solve this use case. Commands that implement this interface locks the Python API process until command's Executed action is invoked on every machine.","title":"Asynchronous Commands"},{"location":"distributed-simulation/distributed-simulation-introduction/","text":"Introduction to Distributed Simulation # What is Distributed Simulation? top # Distributed simulation enables a cluster to run with multiple computing instances (main, clients). This enables you to run simulations that may require a large number of sensors, or with multiple ego vehicles, which may normally require a large amount of computing resources. Distributed simulation solves the inherent limitations of trying to run real-time simulations on one single-GPU instance by synchronizing across multiple machines (each with a GPU). What is a cluster? top # A Cluster is a single unit of simulation. It includes one or more machines that will perform a simulation. There is always one main machine in every cluster; the main machine performs the simulation updates like physics and synchronizes the simulation state with every client. See Clusters for details on editing clusters in the SVL Simulator web user interface. Simulation Synchronization top # For the proper functioning of the sensors distributed to the clients, it is required to synchronize the whole simulation environment. When the simulation changes on main, the main sends update messages to every client. Simulator by default synchronizes every Rigidbody in ego vehicles, NPCs, pedestrians and controllable objects instantiated by Simulator Manager. Main receives and distributes vehicle actions like setting lights state or wipers state, and distributes controllables control actions so every client can clone the simulation state. Components Synchronization top # Every component that has to be synchronized between cluster machines requires messages sender on the main and messages receiver on the clients. Taking a vehicle with Rigidbody as an example, Simulator adds DistributedObject component to the vehicle GameObject to synchronize enable and disable calls and DistributedRigidbody components which send required data to mock the state of Rigidbody on clients. By default, it is required that each DistributedObject has a unique path in the hierarchy - objects on the same hierarchy level require unique GameObject names. See Distributed Objects for details on distributing simulation objects. See Distributed Components for details on distributing components data. Sensors Distribution top # Main machine delegates sensors to clients, every sensor is simulated only on one machine. Some sensors like controls are simulated only on the main machine, and some sensors like camera sensors will always be delegated to the clients. See Sensors Distribution for details on distributing sensors. Custom Implementations top # Distributed simulation can be extended with custom implementations. See Distributed Python API for details on distributing simulation objects. See Custom Distributed Class for details on implementing a custom class that will distribute messages. See Distributed Messages for details on messages sent and received in the components.","title":"Introduction"},{"location":"distributed-simulation/distributed-simulation-introduction/#what-is-distributed-simulation","text":"Distributed simulation enables a cluster to run with multiple computing instances (main, clients). This enables you to run simulations that may require a large number of sensors, or with multiple ego vehicles, which may normally require a large amount of computing resources. Distributed simulation solves the inherent limitations of trying to run real-time simulations on one single-GPU instance by synchronizing across multiple machines (each with a GPU).","title":"What is Distributed Simulation"},{"location":"distributed-simulation/distributed-simulation-introduction/#what-is-a_cluster","text":"A Cluster is a single unit of simulation. It includes one or more machines that will perform a simulation. There is always one main machine in every cluster; the main machine performs the simulation updates like physics and synchronizes the simulation state with every client. See Clusters for details on editing clusters in the SVL Simulator web user interface.","title":"What is a cluster"},{"location":"distributed-simulation/distributed-simulation-introduction/#simulation-synchronization","text":"For the proper functioning of the sensors distributed to the clients, it is required to synchronize the whole simulation environment. When the simulation changes on main, the main sends update messages to every client. Simulator by default synchronizes every Rigidbody in ego vehicles, NPCs, pedestrians and controllable objects instantiated by Simulator Manager. Main receives and distributes vehicle actions like setting lights state or wipers state, and distributes controllables control actions so every client can clone the simulation state.","title":"Simulation Synchronization"},{"location":"distributed-simulation/distributed-simulation-introduction/#components-synchronization","text":"Every component that has to be synchronized between cluster machines requires messages sender on the main and messages receiver on the clients. Taking a vehicle with Rigidbody as an example, Simulator adds DistributedObject component to the vehicle GameObject to synchronize enable and disable calls and DistributedRigidbody components which send required data to mock the state of Rigidbody on clients. By default, it is required that each DistributedObject has a unique path in the hierarchy - objects on the same hierarchy level require unique GameObject names. See Distributed Objects for details on distributing simulation objects. See Distributed Components for details on distributing components data.","title":"Components Synchronization"},{"location":"distributed-simulation/distributed-simulation-introduction/#sensors-distribution","text":"Main machine delegates sensors to clients, every sensor is simulated only on one machine. Some sensors like controls are simulated only on the main machine, and some sensors like camera sensors will always be delegated to the clients. See Sensors Distribution for details on distributing sensors.","title":"Sensor Distribution"},{"location":"distributed-simulation/distributed-simulation-introduction/#custom-implementations","text":"Distributed simulation can be extended with custom implementations. See Distributed Python API for details on distributing simulation objects. See Custom Distributed Class for details on implementing a custom class that will distribute messages. See Distributed Messages for details on messages sent and received in the components.","title":"Custom Implementation"},{"location":"distributed-simulation/running-distributed-simulation/","text":"Running a Distributed Simulation # When distributed simulation starts working, a simulation on each machine will start, and you can check the list of sensors allocated to the machine in Sensor Visualizers . Requirements top # Distributed simulation needs multiple computers connected to the same router. The operating system of each computer can be either Linux or Windows 10. In this document, we'll use: Two Linux PCs (Master, Client PC) Apollo 5 on Master PC Instructions top # Install Apollo 5 top # If you already installed and built Apollo 5, jump to launching Apollo. docker pull lgsvl/apollo-5.0 git clone --recurse-submodules https://github.com/lgsvl/apollo-5.0.git ~/apollo-5.0 Building Apollo and bridge top # cd ~/apollo-5.0 ./docker/scripts/dev_start.sh ./docker/scripts/dev_into.sh ./apollo.sh build_gpu Launching Apollo 5 top # bootstrap.sh bridge.sh You can check the network address of your bridge. This network address should be used in bridge setup in the web user interface. Launching SVL Simulator and Setting Cluster top # In Master Simulator, Run Simulator. Click LINK TO CLOUD. In the Create new cluster tab, give cluster name into New Cluster and click Create cluster. In Client Simulator, Run Simulator. Click LINK TO CLOUD. In the Add to Existing cluster tab, choose cluster name that master simulator made and click Create cluster. After this, Clusters should show one cluster which has two computer name together. If this setup doesn't work, you should remove your pre-existed cluster. Make sure that you now have a cluster which has each machine listed together, like the following: Adding a Map top # If you already added a map you want to use, jump to Adding a vehicle . In the left tab, under Store, click on Maps and enter a map's name. Here we're gonna use BorregasAve map. Click on + icon. You can check the message in the left bottom like the following: \"BorregasAve\" has been successfully added to your library. Adding a Vehicle top # If you already added a vehicle you want to use, jump to Creating simulations . In the left tab, under Store, click on Vehicles and enter a vehicle's name. Here we're gonna use Lincoln2017MKZ. Click on + icon. You can check the message in the left bottom like the following: \"Lincoln2017MKZ\" has been successfully added to your library. Adding sensors top # In the left tab, click Vehicles under Library. Choose Lincoln2017MKZ. Click Apollo 5.0 under Sensor Configurations. If you see exclamation mark, click Add to Library button. Creating Simulations in Web User Interface top # Below is how to make a new simulation with a distributed simulation cluster setup. In the left side, click Simulations. Click Add New. Give name in the Simulation Name. Select cluster name in the Select Cluster. Choose other setups like test report, headless mode, interactive mode as you wish and click next. Select Random Traffic in Runtime Template. Choose map and vehicle essentially and others optionally and click next. Choose Apollo 5.0 in the Autopilot and enter exact your IP address plus port 9090 where Apollo 5 is running and click next. Click Publish. Running Simulation top # Click Run Simulation. Make sure the simulation is started on both the master and client machines. Checking Bridge Status top # Click a power cord shape icon in the bottom and make sure that master and client simulator have connected to bridge in the left tab. Visualizing Sensors top # Click the an eye shape icon and make sure master and client simulators have own sensor lists automatically allocated by distributed simulation setup. Make sure that master simulator has light-computational sensors like CAN bus, IMU, etc. and the client has heavy-computational sensors like LiDARs, Cameras.","title":"Running distributed simulation"},{"location":"distributed-simulation/running-distributed-simulation/#requirements","text":"Distributed simulation needs multiple computers connected to the same router. The operating system of each computer can be either Linux or Windows 10. In this document, we'll use: Two Linux PCs (Master, Client PC) Apollo 5 on Master PC","title":""},{"location":"distributed-simulation/running-distributed-simulation/#instructions","text":"","title":"Instructions"},{"location":"distributed-simulation/running-distributed-simulation/#install-apollo-5","text":"If you already installed and built Apollo 5, jump to launching Apollo. docker pull lgsvl/apollo-5.0 git clone --recurse-submodules https://github.com/lgsvl/apollo-5.0.git ~/apollo-5.0","title":"Install Apollo 5 top"},{"location":"distributed-simulation/running-distributed-simulation/#building-apollo-and-bridge","text":"cd ~/apollo-5.0 ./docker/scripts/dev_start.sh ./docker/scripts/dev_into.sh ./apollo.sh build_gpu","title":"Building Apollo and bridge top"},{"location":"distributed-simulation/running-distributed-simulation/#launching-apollo-5","text":"bootstrap.sh bridge.sh You can check the network address of your bridge. This network address should be used in bridge setup in the web user interface.","title":"Launching Apollo 5 top"},{"location":"distributed-simulation/running-distributed-simulation/#launching-svl-simulator-and-setting-cluster","text":"In Master Simulator, Run Simulator. Click LINK TO CLOUD. In the Create new cluster tab, give cluster name into New Cluster and click Create cluster. In Client Simulator, Run Simulator. Click LINK TO CLOUD. In the Add to Existing cluster tab, choose cluster name that master simulator made and click Create cluster. After this, Clusters should show one cluster which has two computer name together. If this setup doesn't work, you should remove your pre-existed cluster. Make sure that you now have a cluster which has each machine listed together, like the following:","title":"Launching SVL Simulator and Setting Cluster top"},{"location":"distributed-simulation/running-distributed-simulation/#adding-a-map","text":"If you already added a map you want to use, jump to Adding a vehicle . In the left tab, under Store, click on Maps and enter a map's name. Here we're gonna use BorregasAve map. Click on + icon. You can check the message in the left bottom like the following: \"BorregasAve\" has been successfully added to your library.","title":"Adding a Map top"},{"location":"distributed-simulation/running-distributed-simulation/#adding-a-vehicle","text":"If you already added a vehicle you want to use, jump to Creating simulations . In the left tab, under Store, click on Vehicles and enter a vehicle's name. Here we're gonna use Lincoln2017MKZ. Click on + icon. You can check the message in the left bottom like the following: \"Lincoln2017MKZ\" has been successfully added to your library.","title":"Adding  a Vehicle top"},{"location":"distributed-simulation/running-distributed-simulation/#adding-sensors","text":"In the left tab, click Vehicles under Library. Choose Lincoln2017MKZ. Click Apollo 5.0 under Sensor Configurations. If you see exclamation mark, click Add to Library button.","title":"Adding sensors top"},{"location":"distributed-simulation/running-distributed-simulation/#creating-simulations-in-web-user-interface","text":"Below is how to make a new simulation with a distributed simulation cluster setup. In the left side, click Simulations. Click Add New. Give name in the Simulation Name. Select cluster name in the Select Cluster. Choose other setups like test report, headless mode, interactive mode as you wish and click next. Select Random Traffic in Runtime Template. Choose map and vehicle essentially and others optionally and click next. Choose Apollo 5.0 in the Autopilot and enter exact your IP address plus port 9090 where Apollo 5 is running and click next. Click Publish.","title":"Creating Simulations in Web User Interface top"},{"location":"distributed-simulation/running-distributed-simulation/#running-simulation","text":"Click Run Simulation. Make sure the simulation is started on both the master and client machines.","title":"Running Simulation top"},{"location":"distributed-simulation/running-distributed-simulation/#checking-bridge-status","text":"Click a power cord shape icon in the bottom and make sure that master and client simulator have connected to bridge in the left tab.","title":"Checking Bridge Status top"},{"location":"distributed-simulation/running-distributed-simulation/#visualizing-sensors","text":"Click the an eye shape icon and make sure master and client simulators have own sensor lists automatically allocated by distributed simulation setup. Make sure that master simulator has light-computational sensors like CAN bus, IMU, etc. and the client has heavy-computational sensors like LiDARs, Cameras.","title":"Visualizing Sensors top"},{"location":"distributed-simulation/sensors-distribution/","text":"Sensors Distribution # Distribution sensors between different machines are the advantage of the cluster simulation. Distributed simulation synchronizes sensors' transforms between every cluster machine. One sensor can be simulated only on one machine, but one machine can still simulate multiple sensors. Sensor Setup top # Sensors distributed to the clients will not be simulated on the main, those sensors will not affect the simulation (for example manual control sensor has to be simulated on the main) and API requests callbacks will be delayed. Due to these restrictions distribution is disabled by default and enabling it requires additional setup. Every sensor which can be distributed to clients has to override the MainOnly value of the DistributionType property with MainOrClient or ClientOnly value. Sensors Load Balancing top # The current load balancing algorithm divides sensors into groups by their DistributionType type. Main distributes sensors by counting overhead sum and assigning each next sensor to the least overloaded machine available for the selected distribution type. Sensors can override the PerformanceLoad property to determine their load. The value 0.0 means that the sensor will not impact the performance at all, and the value 1.0 means that that the distributed simulation will try to simulate this sensor on a dedicated machine if possible, default value is 0.5 . Example sensors configurations are listed below: 1.0 PerformanceLoad for sensors like LiDAR , which parses the camera images and performs complex maths operations, should be classified under ClientOnly DistributionType type. 0.2 PerformanceLoad for sensors like RadarSensor , which performs complex maths operations, should be classified under MainOrClient DistributionType type. 0.05 PerformanceLoad for sensors like GpsSensor , which performs simple maths operations, should be classified under MainOrClient DistributionType type. Sensors like KeyboardControlSensor , which control objects in a simulation, have to be classified under MainOnly DistributionType type. As the main machine requires more resources it starts the algorithm with 0.15 overhead.","title":"Sensors Distribution [](#top)"},{"location":"distributed-simulation/sensors-distribution/#sensor-setup","text":"Sensors distributed to the clients will not be simulated on the main, those sensors will not affect the simulation (for example manual control sensor has to be simulated on the main) and API requests callbacks will be delayed. Due to these restrictions distribution is disabled by default and enabling it requires additional setup. Every sensor which can be distributed to clients has to override the MainOnly value of the DistributionType property with MainOrClient or ClientOnly value.","title":"Sensor Setup"},{"location":"distributed-simulation/sensors-distribution/#sensors-load-balancing","text":"The current load balancing algorithm divides sensors into groups by their DistributionType type. Main distributes sensors by counting overhead sum and assigning each next sensor to the least overloaded machine available for the selected distribution type. Sensors can override the PerformanceLoad property to determine their load. The value 0.0 means that the sensor will not impact the performance at all, and the value 1.0 means that that the distributed simulation will try to simulate this sensor on a dedicated machine if possible, default value is 0.5 . Example sensors configurations are listed below: 1.0 PerformanceLoad for sensors like LiDAR , which parses the camera images and performs complex maths operations, should be classified under ClientOnly DistributionType type. 0.2 PerformanceLoad for sensors like RadarSensor , which performs complex maths operations, should be classified under MainOrClient DistributionType type. 0.05 PerformanceLoad for sensors like GpsSensor , which performs simple maths operations, should be classified under MainOrClient DistributionType type. Sensors like KeyboardControlSensor , which control objects in a simulation, have to be classified under MainOnly DistributionType type. As the main machine requires more resources it starts the algorithm with 0.15 overhead.","title":"Sensors Load Balancing"},{"location":"getting-started/conventions/","text":"Conventions # Table of Contents SVL Simulator Coordinate Systems Converting Between Coordinate Systems Map-Origin Aligning Map Origin to real world coordinates Map-Orientation-in-Unity Vehicle-Root-Position Setting up BaseLink Transform-Sensor Customizing-Sensor-Configurations SVL Simulator Coordinate Systems top # Inside the simulator positions and rotations are represented by Transforms in Unity. Unity uses a left-handed ZXY coordinate system for transforms with the Y-axis being the vertical axis. Each simulation object such as vehicles, sensors, maps, traffic lights, etc. will have a transform associated with it. Understanding the transforms is especially important for defining custom sensor configurations for ego vehicles. Converting between coordinate systems top # The right-handed XYZ coordinate system is more commonly used in Robotics and Autonomous Vehicle applications and users may require to convert coordinate systems for some use-cases when using the SVL Simulator. The tables below summarize conversions for 3D position vectors: ZXY (Unity) XYZ (RH) Z X X -Y Y Z XYZ (RH) ZXY (Unity) X Z Y -X Z Y The tables below summarize conversions for 3D rotation vectors: ZXY (Unity) XYZ (RH) Z -X X Y Y -Z XYZ (RH) ZXY (Unity) X -Z Y X Z -Y The tables below summarize conversions for quaternions: ZXY (Unity) XYZ (RH) Z -X X Y Y -Z W W XYZ (RH) ZXY (Unity) X Y Y -Z Z -X W W Map Origin top # Each simulator map includes a game object called MapOrigin . The MapOrigin is the reference point of the map within Unity and has position coordinate values of (0, 0, 0) and rotation coordinate values of (0, -90 , 0). Position values can be changed to match Origin Easting and Origin Northing in the Scene but rotation values MUST stay at (0, -90, 0). The real-world position of the Map Origin is needed to output world coordinates using GNSS sensors. These values are stored within the gameObject as Easting and Northing values expressed in the UTM coordinate system as expressed below: Aligning Map Origin to real world coordinates # The position, rotation, and scale of the MapOrigin object are also involved in the transform between the virtual environment and real world map coordinates. Therefore, if the map is not perfect, instead of rotating and scaling the map, you can appropriately set values of the Transform component. In order make this process easier and achieve a good mapping of the virtual world to the real world, you can use Reference Points. Just add a Reference Point by clicking on the button in MapOrigin , then move the Point to a characteristic place and enter the coordinates of this point in the real world. Add at least 2 such points, then click Update Map Origin using Reference Points in MapOrigin . The values will be set to minimize mean squared error. You can see the error by selecting Reference Points. The green sphere shows GPS coordinates of the selected point (Latitude,Longitude). Map Orientation in Unity top # To ensure proper heading values in GNSS related topics, the map must be oriented in such a way the the Z-axis of the unity world coordinate is pointing East. Otherwise all orientation values will have an offset. This can be a bit unintuitive, since by default Unity will have the Z-axis pointing upward on the screen which can lead to users assuming that it aligns to the geographic North. The images below illustrate the map of GoMentum Station with the correct orientation in Unity compared to a satellie image of the area from Google Maps: As evident above, upon the first glance it appears that the GoMentum Station map in Unity is oriented incorrectly; however, if the user were to rotate the view-point so that the Z-axis would be pointing to the right-side of the screen (where the geographic East normally points), the two maps would align as seen in the image below: To switch Unity Editor scene view to this orientation, select the MapOrigin and click the top left button, Orient Scene . This will orient the scene view to the correct orientation and change the camera type to Isometric for easier alignment. The MapOrigin transform must stay at rotation (0,-90,0). Vehicle Root Position (BaseLink) top # The vehicle root is the reference point of the ego vehicle model defined in a game object called BaseLink under the ego vehicle. BaseLink is intended to be placed at the center of the rear axle, however, if the game object does not exist it will be created when the simulation starts and will be placed at the pivot of the ego vehicle model. All sensor positions entered into the sensor configuration in the web UI are defined in the coordinate frame attached to BaseLink . Setting up BaseLink top # Create a new GameObject named BaseLink and click 'Add Component' to add a script called BaseLink . Move position to the center of the rear axle. Add BaseLink.cs and link it to public variable in VehicleSMI.cs Transform Sensor top # Sometimes it can be useful to create a coordinate frame at some point on the ego vehicle to use as a reference for sensor positions. The Transform Sensor provides this functionality. The Transform Sensor is an \"empty\" sensor, meaning that it has no sensor functionality and only has a coordinate frame and can be used as a parent for other sensors in the Simulator sensor configuration. Customizing sensor configurations top # Customized sensor configurations are created in the SVL Simulator Web UI by providing JSON formatted descriptions (covered here ).","title":"Conventions"},{"location":"getting-started/conventions/#svl-simulator-coordinate-systems","text":"Inside the simulator positions and rotations are represented by Transforms in Unity. Unity uses a left-handed ZXY coordinate system for transforms with the Y-axis being the vertical axis. Each simulation object such as vehicles, sensors, maps, traffic lights, etc. will have a transform associated with it. Understanding the transforms is especially important for defining custom sensor configurations for ego vehicles.","title":"SVL Simulator Coordinate Systems"},{"location":"getting-started/conventions/#converting-between-coordinate-systems","text":"The right-handed XYZ coordinate system is more commonly used in Robotics and Autonomous Vehicle applications and users may require to convert coordinate systems for some use-cases when using the SVL Simulator. The tables below summarize conversions for 3D position vectors: ZXY (Unity) XYZ (RH) Z X X -Y Y Z XYZ (RH) ZXY (Unity) X Z Y -X Z Y The tables below summarize conversions for 3D rotation vectors: ZXY (Unity) XYZ (RH) Z -X X Y Y -Z XYZ (RH) ZXY (Unity) X -Z Y X Z -Y The tables below summarize conversions for quaternions: ZXY (Unity) XYZ (RH) Z -X X Y Y -Z W W XYZ (RH) ZXY (Unity) X Y Y -Z Z -X W W","title":"Converting Between Coordinate Systems"},{"location":"getting-started/conventions/#map-origin","text":"Each simulator map includes a game object called MapOrigin . The MapOrigin is the reference point of the map within Unity and has position coordinate values of (0, 0, 0) and rotation coordinate values of (0, -90 , 0). Position values can be changed to match Origin Easting and Origin Northing in the Scene but rotation values MUST stay at (0, -90, 0). The real-world position of the Map Origin is needed to output world coordinates using GNSS sensors. These values are stored within the gameObject as Easting and Northing values expressed in the UTM coordinate system as expressed below:","title":"Map-Origin"},{"location":"getting-started/conventions/#aligning-map-origin-to-real-world-coordinates","text":"The position, rotation, and scale of the MapOrigin object are also involved in the transform between the virtual environment and real world map coordinates. Therefore, if the map is not perfect, instead of rotating and scaling the map, you can appropriately set values of the Transform component. In order make this process easier and achieve a good mapping of the virtual world to the real world, you can use Reference Points. Just add a Reference Point by clicking on the button in MapOrigin , then move the Point to a characteristic place and enter the coordinates of this point in the real world. Add at least 2 such points, then click Update Map Origin using Reference Points in MapOrigin . The values will be set to minimize mean squared error. You can see the error by selecting Reference Points. The green sphere shows GPS coordinates of the selected point (Latitude,Longitude).","title":"Aligning Map Origin to real world coordinates"},{"location":"getting-started/conventions/#map-orientation-in-unity","text":"To ensure proper heading values in GNSS related topics, the map must be oriented in such a way the the Z-axis of the unity world coordinate is pointing East. Otherwise all orientation values will have an offset. This can be a bit unintuitive, since by default Unity will have the Z-axis pointing upward on the screen which can lead to users assuming that it aligns to the geographic North. The images below illustrate the map of GoMentum Station with the correct orientation in Unity compared to a satellie image of the area from Google Maps: As evident above, upon the first glance it appears that the GoMentum Station map in Unity is oriented incorrectly; however, if the user were to rotate the view-point so that the Z-axis would be pointing to the right-side of the screen (where the geographic East normally points), the two maps would align as seen in the image below: To switch Unity Editor scene view to this orientation, select the MapOrigin and click the top left button, Orient Scene . This will orient the scene view to the correct orientation and change the camera type to Isometric for easier alignment. The MapOrigin transform must stay at rotation (0,-90,0).","title":"Map-Orientation-in-Unity"},{"location":"getting-started/conventions/#vehicle-root-position","text":"The vehicle root is the reference point of the ego vehicle model defined in a game object called BaseLink under the ego vehicle. BaseLink is intended to be placed at the center of the rear axle, however, if the game object does not exist it will be created when the simulation starts and will be placed at the pivot of the ego vehicle model. All sensor positions entered into the sensor configuration in the web UI are defined in the coordinate frame attached to BaseLink .","title":"Vehicle-Root-Position"},{"location":"getting-started/conventions/#setting-up-baselink","text":"Create a new GameObject named BaseLink and click 'Add Component' to add a script called BaseLink . Move position to the center of the rear axle. Add BaseLink.cs and link it to public variable in VehicleSMI.cs","title":"Setting up BaseLink"},{"location":"getting-started/conventions/#transform-sensor","text":"Sometimes it can be useful to create a coordinate frame at some point on the ego vehicle to use as a reference for sensor positions. The Transform Sensor provides this functionality. The Transform Sensor is an \"empty\" sensor, meaning that it has no sensor functionality and only has a coordinate frame and can be used as a parent for other sensors in the Simulator sensor configuration.","title":"Transform-Sensor"},{"location":"getting-started/conventions/#customizing-sensor-configurations","text":"Customized sensor configurations are created in the SVL Simulator Web UI by providing JSON formatted descriptions (covered here ).","title":"Customizing-Sensor-Configurations"},{"location":"getting-started/getting-started/","text":"SVL Simulator: An Autonomous Vehicle Simulator # Website | Documentation | Download Table of Contents Introduction Getting Started Downloading and starting simulator Running the simulator Guide to simulator functionality Next Steps Autonomous software tutorials Python API Building and running from source Contact Citation Introduction top # LG Electronics America R&D Center has developed a Unity-based multi-robot simulator for autonomous vehicle developers. We provide an out-of-the-box solution which can meet the needs of developers wishing to focus on testing their autonomous vehicle algorithms. The SVL Simulator provides integration with the open source AD system platforms Apollo , developed by Baidu and Autoware.AI and Autoware.Auto developed by the Autoware Foundation . Getting Started top # Downloading and starting simulator top # To get started with the simulator we suggest using the release binaries, then: Review the System Requirements . Follow our Installing the SVL Simulator guide to download, install, and start the simulator. You should now have registered for an account, received the \"Complete registration\" email, clicked the \"Verify email\" link, and entered a name for your local machine (cluster). Running the simulator top # At this point you're probably eager to see SVL Simulator in action. You can check out the various maps, vehicles, and plugins available in the Store , and can of course create your own Simulations, but for now we will get started with a pre-configured simulation: Click on \"Simulations\" on the left side, and then click \"Available from Others\" in the middle of the page. This will display a list of pre-configured simulations that are available. Locate the Simulation titled \"Local-Random: CubeTown (Manual Drive)\". You can scroll down through the list of available simulations until you find it, or you can type \"Manual\" into the \"Search\" field to more quickly find it. Once you locate it, click the red \"+\" icon to add this simulation to your account and customize the simulation settings. Most of the settings for this simulation are pre-configured but you will need to specify where to run this simulation. In the General settings, click the \"Select Cluster\" field to select the local cluster you created earlier when you linked your local simulator to your cloud account. Then click \"Next\" to review the \"Test case\" settings. In the \"Test case\" settings, click the \"Select Sensor Configuration\" field and then select \"Keyboard Control\" from the list that appears. Then (scroll down if necessary and) click \"Next\" to review the \"Autopilot\" settings. In the \"Autopilot\" settings, there is nothing to set since we are going to use keyboard control to control the vehicle. Click \"Next\" to display the \"Publish\" view. In the \"Publish\" view, click \"Publish\" to publish (save) this simulation to your library. Note that your simulations are private and visible only to you unless you explicitly decide to share it with others. You can now run your simulation. Make sure the local simulator (cluster) you specified for the simulation is online, then click \"Run Simulation\" and switch back to the SVL Simulator window. If this is the first time running a new simulation, the simulator will download all required assets such as the map, the vehicle, and any specified plugins. These downloaded assets will be locally cached for future (or offline) use, after which you should see (in the SVL Simulator window) a red ego vehicle on the CubeTown map. Press the triangular play/pause button button (on the left side of the simulator menu which you'll find at the bottom edge of the simulator window) to start (un-pause) the simulation. Explore the simulator and then check out the following section for more helpful information on controlling the simulator: Drive the ego vehicle by pressing the up-arrow key to accelerate forward, left and right arrows to steer, and down-arrow to brake. Press \"F12\" to reset the ego vehicle to the default position. Press \"N\" to toggle NPC vehicles off and on, or \"P\" to toggle pedestrians. Check out the different camera modes . Use the right mouse button (click, hold, and drag) to look and rotate the camera view around the scene. When finished, return to your browser window and click \"Stop Simulation\" in the online user interface to end the simulation. Guide to simulator functionality top # Look through the simulation menu to learn about the various on-screen options for controlling the simulator. Discover other keyboard shortcuts for manually controlling the ego vehicle and the entire simulation. Change the weather by adding rain or clouds or adjust the time of day using the interactive menu . Explore the various sensor visualizer and bridge connection options available in the simulator (available when using a vehicle which is configured with sensors and a bridge). Learn about options available on the simulator main screen including the Visual Scenario Editor in online mode and how to run offline simulations in offline mode. Next Steps top # Autonomous software tutorials top # SVL Simulator supports several open source autonomous software platforms (as well as proprietary ones through the use of custom bridge plug-ins). Check out the following tutorials to learn how to use Apollo or Autoware with SVL Simulator: Use Apollo (Latest) with SVL Simulator. Use Apollo 5.0 with SVL Simulator. Use Autoware.AI with SVL Simulator. Use Autoware.Auto with SVL Simulator. Note: To run a simulation with Apollo or Autoware you will need to either create a new simulation or add a pre-configured one. To create and configure a new simulation, check out the detailed walk-through instructions in Running SVL Simulator . To customize and add a pre-configured simulation, return to \"Simulations: Available from Others\" in the online user interface (review Step 1, above ) and search for \"Apollo\" or \"Autoware\" or \"Local-Random\". To customize one of these pre-configured simulations, refer to the Simulations documentation. Python API top # Learn how to control and script simulations at runtime using the Python API . Note: Check out the pre-configured \"API Only\" simulation for use with Python API mode. Building and running from source top # Building the simulator from source is only recommended for developers who wish to customize the simulator, build plugins, or make new assets. Check out our build instructions to build the simulator from source. Contact top # Please feel free to provide feedback or ask questions by creating a Github issue . For inquiries about collaboration, get in touch at contact@svlsimulator.com . Citation top # For citation please use the following: @ARTICLE{2020arXiv200503778R, author = {{Rong}, Guodong and {Shin}, Byung Hyun and {Tabatabaee}, Hadi and {Lu}, Qiang and {Lemke}, Steve and {Mo{\\v{z}}eiko}, M{\\={a}}rti{\\c{n}}{\\v{s}} and {Boise}, Eric and {Uhm}, Geehoon and {Gerow}, Mark and {Mehta}, Shalin and {Agafonov}, Eugene and {Kim}, Tae Hyung and {Sterner}, Eric and {Ushiroda}, Keunhae and {Reyes}, Michael and {Zelenkovsky}, Dmitry and {Kim}, Seonman}, title = \"{SVL Simulator: A High Fidelity Simulator for Autonomous Driving}\", journal = {arXiv e-prints}, keywords = {Computer Science - Robotics, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control}, year = 2020, month = may, eid = {arXiv:2005.03778}, pages = {arXiv:2005.03778}, archivePrefix = {arXiv}, eprint = {2005.03778}, primaryClass = {cs.RO} }","title":"Run a simulation"},{"location":"getting-started/getting-started/#introduction","text":"LG Electronics America R&D Center has developed a Unity-based multi-robot simulator for autonomous vehicle developers. We provide an out-of-the-box solution which can meet the needs of developers wishing to focus on testing their autonomous vehicle algorithms. The SVL Simulator provides integration with the open source AD system platforms Apollo , developed by Baidu and Autoware.AI and Autoware.Auto developed by the Autoware Foundation .","title":"Introduction"},{"location":"getting-started/getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/getting-started/#downloading-and-starting-simulator","text":"To get started with the simulator we suggest using the release binaries, then: Review the System Requirements . Follow our Installing the SVL Simulator guide to download, install, and start the simulator. You should now have registered for an account, received the \"Complete registration\" email, clicked the \"Verify email\" link, and entered a name for your local machine (cluster).","title":"Downloading and starting simulator"},{"location":"getting-started/getting-started/#running-the-simulator","text":"At this point you're probably eager to see SVL Simulator in action. You can check out the various maps, vehicles, and plugins available in the Store , and can of course create your own Simulations, but for now we will get started with a pre-configured simulation: Click on \"Simulations\" on the left side, and then click \"Available from Others\" in the middle of the page. This will display a list of pre-configured simulations that are available. Locate the Simulation titled \"Local-Random: CubeTown (Manual Drive)\". You can scroll down through the list of available simulations until you find it, or you can type \"Manual\" into the \"Search\" field to more quickly find it. Once you locate it, click the red \"+\" icon to add this simulation to your account and customize the simulation settings. Most of the settings for this simulation are pre-configured but you will need to specify where to run this simulation. In the General settings, click the \"Select Cluster\" field to select the local cluster you created earlier when you linked your local simulator to your cloud account. Then click \"Next\" to review the \"Test case\" settings. In the \"Test case\" settings, click the \"Select Sensor Configuration\" field and then select \"Keyboard Control\" from the list that appears. Then (scroll down if necessary and) click \"Next\" to review the \"Autopilot\" settings. In the \"Autopilot\" settings, there is nothing to set since we are going to use keyboard control to control the vehicle. Click \"Next\" to display the \"Publish\" view. In the \"Publish\" view, click \"Publish\" to publish (save) this simulation to your library. Note that your simulations are private and visible only to you unless you explicitly decide to share it with others. You can now run your simulation. Make sure the local simulator (cluster) you specified for the simulation is online, then click \"Run Simulation\" and switch back to the SVL Simulator window. If this is the first time running a new simulation, the simulator will download all required assets such as the map, the vehicle, and any specified plugins. These downloaded assets will be locally cached for future (or offline) use, after which you should see (in the SVL Simulator window) a red ego vehicle on the CubeTown map. Press the triangular play/pause button button (on the left side of the simulator menu which you'll find at the bottom edge of the simulator window) to start (un-pause) the simulation. Explore the simulator and then check out the following section for more helpful information on controlling the simulator: Drive the ego vehicle by pressing the up-arrow key to accelerate forward, left and right arrows to steer, and down-arrow to brake. Press \"F12\" to reset the ego vehicle to the default position. Press \"N\" to toggle NPC vehicles off and on, or \"P\" to toggle pedestrians. Check out the different camera modes . Use the right mouse button (click, hold, and drag) to look and rotate the camera view around the scene. When finished, return to your browser window and click \"Stop Simulation\" in the online user interface to end the simulation.","title":"Running the simulator"},{"location":"getting-started/getting-started/#guide-to-simulator-functionality","text":"Look through the simulation menu to learn about the various on-screen options for controlling the simulator. Discover other keyboard shortcuts for manually controlling the ego vehicle and the entire simulation. Change the weather by adding rain or clouds or adjust the time of day using the interactive menu . Explore the various sensor visualizer and bridge connection options available in the simulator (available when using a vehicle which is configured with sensors and a bridge). Learn about options available on the simulator main screen including the Visual Scenario Editor in online mode and how to run offline simulations in offline mode.","title":"Guide to simulator functionality"},{"location":"getting-started/getting-started/#next-steps","text":"","title":"Next Steps"},{"location":"getting-started/getting-started/#autonomous-software-tutorials","text":"SVL Simulator supports several open source autonomous software platforms (as well as proprietary ones through the use of custom bridge plug-ins). Check out the following tutorials to learn how to use Apollo or Autoware with SVL Simulator: Use Apollo (Latest) with SVL Simulator. Use Apollo 5.0 with SVL Simulator. Use Autoware.AI with SVL Simulator. Use Autoware.Auto with SVL Simulator. Note: To run a simulation with Apollo or Autoware you will need to either create a new simulation or add a pre-configured one. To create and configure a new simulation, check out the detailed walk-through instructions in Running SVL Simulator . To customize and add a pre-configured simulation, return to \"Simulations: Available from Others\" in the online user interface (review Step 1, above ) and search for \"Apollo\" or \"Autoware\" or \"Local-Random\". To customize one of these pre-configured simulations, refer to the Simulations documentation.","title":"Autonomous software tutorials"},{"location":"getting-started/getting-started/#python-api","text":"Learn how to control and script simulations at runtime using the Python API . Note: Check out the pre-configured \"API Only\" simulation for use with Python API mode.","title":"Python API"},{"location":"getting-started/getting-started/#building-from-source","text":"Building the simulator from source is only recommended for developers who wish to customize the simulator, build plugins, or make new assets. Check out our build instructions to build the simulator from source.","title":"Building and running from source"},{"location":"getting-started/getting-started/#contact","text":"Please feel free to provide feedback or ask questions by creating a Github issue . For inquiries about collaboration, get in touch at contact@svlsimulator.com .","title":"Contact"},{"location":"getting-started/getting-started/#citation","text":"For citation please use the following: @ARTICLE{2020arXiv200503778R, author = {{Rong}, Guodong and {Shin}, Byung Hyun and {Tabatabaee}, Hadi and {Lu}, Qiang and {Lemke}, Steve and {Mo{\\v{z}}eiko}, M{\\={a}}rti{\\c{n}}{\\v{s}} and {Boise}, Eric and {Uhm}, Geehoon and {Gerow}, Mark and {Mehta}, Shalin and {Agafonov}, Eugene and {Kim}, Tae Hyung and {Sterner}, Eric and {Ushiroda}, Keunhae and {Reyes}, Michael and {Zelenkovsky}, Dmitry and {Kim}, Seonman}, title = \"{SVL Simulator: A High Fidelity Simulator for Autonomous Driving}\", journal = {arXiv e-prints}, keywords = {Computer Science - Robotics, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control}, year = 2020, month = may, eid = {arXiv:2005.03778}, pages = {arXiv:2005.03778}, archivePrefix = {arXiv}, eprint = {2005.03778}, primaryClass = {cs.RO} }","title":"Citation"},{"location":"getting-started/introduction/","text":"Introduction # What is SVL Simulator? # SVL Simulator is a simulation platform used for autonomous vehicle and robotic system development. As the development of advanced vehicles, transportation systems, and autonomous agents becomes increasingly complex, the testing needs and the infrastructure required to support them become critical to deploying safe systems on public roads and environments. SVL Simulator consists of the simulation software, software tools, the ecosystem of content and plugins that enable tailored use cases, and the cloud environment which enables simulation and scenario testing at scale. By simulating a virtual environment, one or more ego vehicles or autonomous systems and their sensors, and traffic and other dynamic objects, the simulation software provides a seamless and customizable interface with a user's System Under Test. This allows the developer to debug, perform modular testing, and perform integration testing. Who is SVL Simulator for? # SVL Simulator is built for engineers who build, test, and verify autonomous systems. We invite anyone involved in building future autonomous systems to try using our freely available software. What is SVL Simulator used for? # SVL Simulator enables you to perform integration testing, modular algorithm testing, and system verification. It encompasses the testing infrastructure and workflow required for safe autonomous vehicle or robot development and deployment. These are some of the specific applications developed using SVL Simulator: L4/L5 autonomous vehicle systems L2/L3 ADAS/AD systems Warehouse robotics Outdoor mobile robotics Future Mobility services Autonomous racing Sensor/sensor systems development and marketing Automotive and autonomous system security Synthetic data generation Real-time embedded systems for automotive What can you do with SVL Simulator? # Here are some of the specific use case examples for using SVL Simulator: Create and run scenarios involving complex traffic scenarios based on real-world data Debug and improve your localization module by testing in new Digital Twin environments Test planning module in isolation with virtual ground truth detections Test your autonomous vehicle stack end-to-end in real time (software-in-the-loop) Discover and prevent performance bottlenecks with hardware-in-the-loop simulation Keep track of development progress with test case reports for every simulation Parameterize and automate the execution of thousands of scenarios to discover high-value and interesting edge case scenarios Key Value-Add # Using SVL Simulator for simulation means you will have realistic, real-time, end-to-end simulation. We enable developers to connect their real System Under Test and rigorously test all parts of their system in simulation quickly, easily, and comprehensively. Key Features # This documentation website takes you through all of the features in SVL Simulator, but below are some of the key features: End to end simulation Real-time, high-performance simulation Multi-vehicle simulation Photorealistic environment simulation Extensibility and customizability Variety of scenario creation tools Procedural road network generation and environment creation tools HD map import, export, and annotation tools","title":"Introduction"},{"location":"getting-started/introduction/#what-is-svl-simulator","text":"SVL Simulator is a simulation platform used for autonomous vehicle and robotic system development. As the development of advanced vehicles, transportation systems, and autonomous agents becomes increasingly complex, the testing needs and the infrastructure required to support them become critical to deploying safe systems on public roads and environments. SVL Simulator consists of the simulation software, software tools, the ecosystem of content and plugins that enable tailored use cases, and the cloud environment which enables simulation and scenario testing at scale. By simulating a virtual environment, one or more ego vehicles or autonomous systems and their sensors, and traffic and other dynamic objects, the simulation software provides a seamless and customizable interface with a user's System Under Test. This allows the developer to debug, perform modular testing, and perform integration testing.","title":"What is SVL Simulator?"},{"location":"getting-started/introduction/#who-is-svl-simulator-for","text":"SVL Simulator is built for engineers who build, test, and verify autonomous systems. We invite anyone involved in building future autonomous systems to try using our freely available software.","title":"Who is SVL Simulator for?"},{"location":"getting-started/introduction/#what-is-svl-simulator-used-for","text":"SVL Simulator enables you to perform integration testing, modular algorithm testing, and system verification. It encompasses the testing infrastructure and workflow required for safe autonomous vehicle or robot development and deployment. These are some of the specific applications developed using SVL Simulator: L4/L5 autonomous vehicle systems L2/L3 ADAS/AD systems Warehouse robotics Outdoor mobile robotics Future Mobility services Autonomous racing Sensor/sensor systems development and marketing Automotive and autonomous system security Synthetic data generation Real-time embedded systems for automotive","title":"What is SVL Simulator used for?"},{"location":"getting-started/introduction/#what-can-you-do-with-svl-simulator","text":"Here are some of the specific use case examples for using SVL Simulator: Create and run scenarios involving complex traffic scenarios based on real-world data Debug and improve your localization module by testing in new Digital Twin environments Test planning module in isolation with virtual ground truth detections Test your autonomous vehicle stack end-to-end in real time (software-in-the-loop) Discover and prevent performance bottlenecks with hardware-in-the-loop simulation Keep track of development progress with test case reports for every simulation Parameterize and automate the execution of thousands of scenarios to discover high-value and interesting edge case scenarios","title":"What can you do with SVL Simulator?"},{"location":"getting-started/introduction/#key-value-add","text":"Using SVL Simulator for simulation means you will have realistic, real-time, end-to-end simulation. We enable developers to connect their real System Under Test and rigorously test all parts of their system in simulation quickly, easily, and comprehensively.","title":"Key Value-Add"},{"location":"getting-started/introduction/#key-features","text":"This documentation website takes you through all of the features in SVL Simulator, but below are some of the key features: End to end simulation Real-time, high-performance simulation Multi-vehicle simulation Photorealistic environment simulation Extensibility and customizability Variety of scenario creation tools Procedural road network generation and environment creation tools HD map import, export, and annotation tools","title":"Key Features"},{"location":"installation-guide/build-instructions/","text":"Simulator Build Instructions # This document provides instructions on building the simulator and assets from source using the Unity Editor in developer mode . Note: When a terminal is mentioned, it refers to cmd.exe on Windows and Terminal on Ubuntu. Table of Contents Installing the Unity Editor Installing Git LFS Building a standalone executable Testing the simulator build Building assets Using custom assets with the simulator binary Testing custom assets with the Unity editor Installing the Unity Editor top # Download and Install Unity Hub Ubuntu: You may need to allow the downloaded AppImage to be run as an executable Right-click the AppImage Select Properties Go to the Permissions tab Check Allow executing file as program Alternatively, in the terminal run sudo chmod +x UnityHub.AppImage Download and Install Unity 2020.3.3f1 from the Unity Download Archive : Click the Unity Hub button to have Unity Hub start the installation process If the installation process fails to start on Ubuntu: Right click the Unity Hub button and select Copy Link Address In a terminal, type <PATH_TO_UNITY_HUB> <COPIED_LINK> The copied link will be in the form unityhub://Unity-VERSION/XXXXXX (e.g. unityhub://2020.3.3f1/76626098c1c4 ) Thus, if the Unity Hub application is in the current directory, type ./UnityHub.AppImage unityhub://2020.3.3f1/76626098c1c4 Unity Hub will open and guide you through the installation of Unity Editor IMPORTANT include the Windows Build Support (Mono) for both Windows and Linux when installing Unity (Optional) include support for Visual Studio for easier debugging Verify installation Under the Installs tab of Unity Hub there should be the expected version shown. In the bottom-left corner of the version, there should be an icon of the other OS (e.g. on a Linux computer, the Windows logo will be shown) Installing Git LFS top # Make sure you have git-lfs installed before cloning the Simulator repository . Instructions for installation are here Verify installation In a terminal enter git lfs install > Git LFS initialized. should print out Building a standalone executable top # Clone simulator project from GitHub (open-source) release branch. Open a terminal and navigate to where you want the Simulator to be downloaded to If you want the Simulator in your Documents folder, use cd in the terminal so that the input for the terminal is similar to /Documents$ Open-source user: git clone https://github.com/lgsvl/simulator.git Verify download Above clone will create a Simulator folder Open a File Explorer and navigate to where the Simulator folder is Navigate to Simulator/Assets/Materials/EnvironmentMaterials/ There should be a EnvironmentDamageAlbedo.png in this folder Open the image, it should look like the image below If the image cannot be opened, Git LFS was not installed before cloning the repository Install Git LFS following step 4 In a terminal, navigate to the Simulator folder so that the terminal is similar to /Simulator$ git lfs pull Check the image again Note: Please checkout the \"release-*\" branches or release tags for stable (ready features) and \"master\" branch for unstable (preview of work in progress). Run Unity Hub In the Projects tab, click Add and select the Simulator folder that was created by git clone in Step 5 In the Projects tab, verify that the Simulator is using Unity Version 2020.3.3f1 from the dropdown Double-click the name of the project to launch Unity Editor Note: On Ubuntu 18.04, create an empty sample project before adding an existing project as mentioned in step 1. Open the Simulator menu from the top toolbar and select Build... to open the build window (shown below with a red outline) Select the target OS for the build using the Executable Platform dropdown menu Verify Build Simulator is checked for the Simulator to be built Select a folder that the simulator will be built in (Optional) Check Development Build to create a Development Build that includes debug symbols and enables the Profiler Click Build Once the build process is complete a simulator executable will be available at the specified location. Testing the simulator build top # Follow these steps for a quick test of the simulator build. For a more complete guide please see the tutorial on running the simulator . (Ubuntu) Install the Vulkan userspace library sudo apt-get install libvulkan1 Double-click the Simulator.exe that was built Click Open Browser , and login to the Web UI (you may need to sign up) You may be prompted to create a cluster for local simulations In the Store choose any maps and click Add to My Library ex. the BorregasAve map Back in the Store choose one of the default vehicles such as the Lincoln2017Mkz and click Add to My Library In the Simulations tab, Add New simulation with the added map and the newly cloned vehicle: Enter Simulation Name and Select Cluster then click Next . Select the Random Traffic runtime template, choose the map from your library and select the Lincoln2017MKZ vehicle with the Keyboard Control sensor configuration then click Next . Click Next on the Autopilot page and click Publish to create the simulation. Press the Run simulation button The Unity window should now show a vehicle in the built environment Building assets top # The simulator build tools allow developers to build assets as well as standalone simulator binaries. Assets refer to maps, vehicles, sensors, controllables, npcs, pedestrians and bridges. Asset bundles are built without needing to rebuild the entire simulator. NOTE: AssetBundles built with one version of the simulator are NOT guaranteed to work with other versions of the simulator. Asset bundles will need to be re-created. NOTE: NPCs and Pedestrians must be built locally for custom binaries. They must be added to AssetBundles folder to be loaded by the custom simulator binary. Follow these steps to build assets: Place the source code for the asset the correct directory under Simulator/Assets/External for it to be discovered by the build tool. Based on the category the asset falls under it should be placed inside on of these subdirectories: Environments Vehicles NPCs Pedestrians Sensors Controllables Bridges For example, to build the CubeTown map we would clone the repository into the Environments subdirectory. Press Ctrl + R to refresh assets (only needed if auto-refresh is disabled in the editor). The asset should now be detected and displayed in the build tool. In the case of this example, CubeTown will show up under Environments . Check the box next to any asset you wish to build and click the Build button at the bottom of the tool. Once the asset is built it will be available under the Simulator/AssetBundles in a subdirectory named after the category of the asset that was built. For this example, CubeTown will be at Simulator/AssetBundles/Environments/environment_CubeTown . Using custom assets with the simulator binary top # To use built assets with the simulator binary they must first be uploaded to the cloud. For more information on this see the Library guide. Testing custom assets with the Unity editor top # Custom assets can be tested with the Unity Editor by using the Developer Settings panel accessible from the Simulator menu located in the top toolbar. Only local assets are supported and JSON must be manually added so Developer Settings can look up sensor data. Users can toggle Developer Debug Mode in Simulator drop down menu to load local asset classes at runtime to be able to access break point functionality. Be sure to toggle off when finished debugging.","title":"Building from source"},{"location":"installation-guide/build-instructions/#installing-unity-editor","text":"Download and Install Unity Hub Ubuntu: You may need to allow the downloaded AppImage to be run as an executable Right-click the AppImage Select Properties Go to the Permissions tab Check Allow executing file as program Alternatively, in the terminal run sudo chmod +x UnityHub.AppImage Download and Install Unity 2020.3.3f1 from the Unity Download Archive : Click the Unity Hub button to have Unity Hub start the installation process If the installation process fails to start on Ubuntu: Right click the Unity Hub button and select Copy Link Address In a terminal, type <PATH_TO_UNITY_HUB> <COPIED_LINK> The copied link will be in the form unityhub://Unity-VERSION/XXXXXX (e.g. unityhub://2020.3.3f1/76626098c1c4 ) Thus, if the Unity Hub application is in the current directory, type ./UnityHub.AppImage unityhub://2020.3.3f1/76626098c1c4 Unity Hub will open and guide you through the installation of Unity Editor IMPORTANT include the Windows Build Support (Mono) for both Windows and Linux when installing Unity (Optional) include support for Visual Studio for easier debugging Verify installation Under the Installs tab of Unity Hub there should be the expected version shown. In the bottom-left corner of the version, there should be an icon of the other OS (e.g. on a Linux computer, the Windows logo will be shown)","title":"Installing the Unity Editor"},{"location":"installation-guide/build-instructions/#installing-git-lfs","text":"Make sure you have git-lfs installed before cloning the Simulator repository . Instructions for installation are here Verify installation In a terminal enter git lfs install > Git LFS initialized. should print out","title":"Installing Git LFS"},{"location":"installation-guide/build-instructions/#build-stand-alone","text":"Clone simulator project from GitHub (open-source) release branch. Open a terminal and navigate to where you want the Simulator to be downloaded to If you want the Simulator in your Documents folder, use cd in the terminal so that the input for the terminal is similar to /Documents$ Open-source user: git clone https://github.com/lgsvl/simulator.git Verify download Above clone will create a Simulator folder Open a File Explorer and navigate to where the Simulator folder is Navigate to Simulator/Assets/Materials/EnvironmentMaterials/ There should be a EnvironmentDamageAlbedo.png in this folder Open the image, it should look like the image below If the image cannot be opened, Git LFS was not installed before cloning the repository Install Git LFS following step 4 In a terminal, navigate to the Simulator folder so that the terminal is similar to /Simulator$ git lfs pull Check the image again Note: Please checkout the \"release-*\" branches or release tags for stable (ready features) and \"master\" branch for unstable (preview of work in progress). Run Unity Hub In the Projects tab, click Add and select the Simulator folder that was created by git clone in Step 5 In the Projects tab, verify that the Simulator is using Unity Version 2020.3.3f1 from the dropdown Double-click the name of the project to launch Unity Editor Note: On Ubuntu 18.04, create an empty sample project before adding an existing project as mentioned in step 1. Open the Simulator menu from the top toolbar and select Build... to open the build window (shown below with a red outline) Select the target OS for the build using the Executable Platform dropdown menu Verify Build Simulator is checked for the Simulator to be built Select a folder that the simulator will be built in (Optional) Check Development Build to create a Development Build that includes debug symbols and enables the Profiler Click Build Once the build process is complete a simulator executable will be available at the specified location.","title":"Building a standalone executable"},{"location":"installation-guide/build-instructions/#test-simulator","text":"Follow these steps for a quick test of the simulator build. For a more complete guide please see the tutorial on running the simulator . (Ubuntu) Install the Vulkan userspace library sudo apt-get install libvulkan1 Double-click the Simulator.exe that was built Click Open Browser , and login to the Web UI (you may need to sign up) You may be prompted to create a cluster for local simulations In the Store choose any maps and click Add to My Library ex. the BorregasAve map Back in the Store choose one of the default vehicles such as the Lincoln2017Mkz and click Add to My Library In the Simulations tab, Add New simulation with the added map and the newly cloned vehicle: Enter Simulation Name and Select Cluster then click Next . Select the Random Traffic runtime template, choose the map from your library and select the Lincoln2017MKZ vehicle with the Keyboard Control sensor configuration then click Next . Click Next on the Autopilot page and click Publish to create the simulation. Press the Run simulation button The Unity window should now show a vehicle in the built environment","title":"Testing the simulator build"},{"location":"installation-guide/build-instructions/#building-assets","text":"The simulator build tools allow developers to build assets as well as standalone simulator binaries. Assets refer to maps, vehicles, sensors, controllables, npcs, pedestrians and bridges. Asset bundles are built without needing to rebuild the entire simulator. NOTE: AssetBundles built with one version of the simulator are NOT guaranteed to work with other versions of the simulator. Asset bundles will need to be re-created. NOTE: NPCs and Pedestrians must be built locally for custom binaries. They must be added to AssetBundles folder to be loaded by the custom simulator binary. Follow these steps to build assets: Place the source code for the asset the correct directory under Simulator/Assets/External for it to be discovered by the build tool. Based on the category the asset falls under it should be placed inside on of these subdirectories: Environments Vehicles NPCs Pedestrians Sensors Controllables Bridges For example, to build the CubeTown map we would clone the repository into the Environments subdirectory. Press Ctrl + R to refresh assets (only needed if auto-refresh is disabled in the editor). The asset should now be detected and displayed in the build tool. In the case of this example, CubeTown will show up under Environments . Check the box next to any asset you wish to build and click the Build button at the bottom of the tool. Once the asset is built it will be available under the Simulator/AssetBundles in a subdirectory named after the category of the asset that was built. For this example, CubeTown will be at Simulator/AssetBundles/Environments/environment_CubeTown .","title":"Building assets"},{"location":"installation-guide/build-instructions/#using-custom-assets-with-the-simulator-binary","text":"To use built assets with the simulator binary they must first be uploaded to the cloud. For more information on this see the Library guide.","title":"Using custom assets with the simulator binary"},{"location":"installation-guide/build-instructions/#testing-custom-assets-with-the-unity-editor","text":"Custom assets can be tested with the Unity Editor by using the Developer Settings panel accessible from the Simulator menu located in the top toolbar. Only local assets are supported and JSON must be manually added so Developer Settings can look up sensor data. Users can toggle Developer Debug Mode in Simulator drop down menu to load local asset classes at runtime to be able to access break point functionality. Be sure to toggle off when finished debugging.","title":"Testing custom assets with the Unity editor"},{"location":"installation-guide/debugging-scripts/","text":"Debugging scripts # This document provides instructions on setting up Unity and IDEs for debugging C# Unity scripts. Table of Contents Debugging Unity scripts using VS Code (Ubuntu) Setup Debugging Debugging Unity using Visual Studio Setup Debugging Debugging Unity scripts using VS Code (Ubuntu) top # Setup top # Install Mono using instruction from official website . Run dotnet --info and msbuild -version commands in terminal, output should be similar to the picture below (your versions might be different). Install VS Code . Install C# and Debugger for Unity extensions in VS Code. Open up VS Code settings via File menu -> Preferences -> Settings and search for Omnisharp: Use Global Mono and set it to always . Click Restart Omnisharp from the notification that appears at the bottom-right. In Unity make sure that External Script Editor is set to code . Debugging top # In Unity double click on script of interest, VS Code should open it up. Click Debug view icon on left pane in VS Code. Select Unity editor in run configurations dropdown. If Unity editor is missing in dropdown, look for errors in log. Now click Run in VS Code, it should find active instance of Unity, attach to it, debugging starts now. Note: If clicking Run fails to attach to Unity editor, you can try going View -> Command Palette... -> Unity Attach Debugger and select Unity instance. If everything is setup correctly you should be able to debug scripts, use IntelliSense and code highlights. Debugging Unity using Visual Studio (Windows) top # Setup top # Install Visual Studio (free Community Edition works fine), don't forget to select Game development with Unity installation option. In Unity make sure that External Script Editor is set to Visual Studio . Debugging top # In Unity double click on script of interest, Visual Studio should open it up. Click Run (it should say Attach to Unity near green play icon). Now you should be able to use full Visual Studio C# development functionality.","title":"Debugging Unity scripts"},{"location":"installation-guide/debugging-scripts/#debugging-unity-scripts-ubuntu","text":"","title":"Debugging Unity scripts using VS Code (Ubuntu)"},{"location":"installation-guide/debugging-scripts/#debug-setup-ubuntu","text":"Install Mono using instruction from official website . Run dotnet --info and msbuild -version commands in terminal, output should be similar to the picture below (your versions might be different). Install VS Code . Install C# and Debugger for Unity extensions in VS Code. Open up VS Code settings via File menu -> Preferences -> Settings and search for Omnisharp: Use Global Mono and set it to always . Click Restart Omnisharp from the notification that appears at the bottom-right. In Unity make sure that External Script Editor is set to code .","title":"Setup"},{"location":"installation-guide/debugging-scripts/#debug-ubuntu","text":"In Unity double click on script of interest, VS Code should open it up. Click Debug view icon on left pane in VS Code. Select Unity editor in run configurations dropdown. If Unity editor is missing in dropdown, look for errors in log. Now click Run in VS Code, it should find active instance of Unity, attach to it, debugging starts now. Note: If clicking Run fails to attach to Unity editor, you can try going View -> Command Palette... -> Unity Attach Debugger and select Unity instance. If everything is setup correctly you should be able to debug scripts, use IntelliSense and code highlights.","title":"Debugging"},{"location":"installation-guide/debugging-scripts/#debugging-unity-scripts-windows","text":"","title":"Debugging Unity using Visual Studio"},{"location":"installation-guide/debugging-scripts/#debug-setup-windows","text":"Install Visual Studio (free Community Edition works fine), don't forget to select Game development with Unity installation option. In Unity make sure that External Script Editor is set to Visual Studio .","title":"Setup"},{"location":"installation-guide/debugging-scripts/#debug-windows","text":"In Unity double click on script of interest, Visual Studio should open it up. Click Run (it should say Attach to Unity near green play icon). Now you should be able to use full Visual Studio C# development functionality.","title":"Debugging"},{"location":"installation-guide/installing-simulator/","text":"Installing the SVL Simulator # Table of Contents Graphics Drivers Windows Linux Linux Vulkan Drivers Installing the simulator Link to Cloud Docker Installing Docker under Linux Installing Docker under Windows Testing Docker integration Graphics Drivers top # Windows top # Download driver manually from NVIDIA's website and install. Linux top # You can check if the NVIDIA drivers are already installed, and their version by running in terminal: $ nvidia-smi The output should be similar to: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.57 Driver Version: 450.57 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:01:00.0 On | N/A | | 0% 36C P8 11W / 280W | 202MiB / 11169MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1942 G /usr/lib/xorg/Xorg 114MiB | | 0 N/A N/A 2292 G /usr/bin/gnome-shell 84MiB | +-----------------------------------------------------------------------------+ If you do not have the latest NVIDIA drivers installed, then install the drivers for your system. For example: Run ubuntu-drivers devices to check the highest supported driver version for your system: == /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 == modalias : pci:v000010DEd00001B06sv00001462sd00003607bc03sc00i00 vendor : NVIDIA Corporation model : GP102 [GeForce GTX 1080 Ti] driver : nvidia-driver-435 - distro non-free driver : nvidia-driver-390 - distro non-free driver : nvidia-driver-450 - third-party free recommended driver : nvidia-driver-440 - distro non-free driver : xserver-xorg-video-nouveau - distro free builtin ... Install the appropriate driver: $ sudo apt install nvidia-driver-450 Vulkan Drivers top # Make sure the Vulkan userspace library is installed: $ sudo apt install libvulkan1 NOTE: If you also have mesa-vulkan-drivers installed (e.g. for Intel motherboard video) they can prevent the simulator from being able to access your NVIDIA GPU. If the simulator crashes when starting, try removing the mesa vulkan drivers: $ sudo apt remove mesa-vulkan-drivers Installing the simulator top # The Simulator release can be downloaded as a prebuilt binary from the simulator distribution . Download and extract the simulator binary zip file for your OS to a desired location. For Linux: svlsimulator-linux64-{release-version}.zip For Windows: svlsimulator-windows64-{release-version}.zip Enter the svlsimulator-{operating-system}64-{version-number} folder and run the executable application named simulator ( simulator.exe on Windows) by double-clicking it. Link to Cloud top # When the SVL Simulator program launches, it will connect to the SVL Simulator Cloud. Click Open Browser to open the simulator user interface in a web browser. If the Link to Cloud button is not available, check in the upper right corner to see if the simulator is Online or Offline . If offline, click the Offline button to re-connect the simulator to the cloud, then click Link to Cloud . You should arrive at the SVL Simulator Sign In page. If you already have an account, you can use your credentials to sign in. If you don\u2019t already have an account, click Sign Up to create an account. To create an account, fill out the Sign Up form. You will need to confirm your email address to be able to continue. Click the \"Verify email\" link in the \"Complete registration\" email. If you don't receive the confirmation email, please check your spam filter or add \"contact@svlsimulator.com\" to your approved senders list. After signing in (or signing up), you will be logged in and are almost ready to start creating and running simulations. Before you can create a simulation, you'll need to link your local simulator instance to your cloud account. You should now see the Clusters view, where you need to enter a cluster name (e.g. \"local-sim\") for your local machine which is running SVL Simulator and click Create cluster . If you wish to link additional machines into a cluster for distributed simulation, refer to Clusters Tab for more information. If you don't see the New Cluster form or get an error when Creating the Cluster , your running simulator may be offline. Return to the SVL Simulator program window and click the Offline button to re-connect the simulator to the cloud, then click Link to Cloud to return to the Clusters view. You should now be able to enter a name for your local machine (e.g. \"local-sim\") and click Create cluster . At this point your local machine should now be linked to the cloud in your new default (\"local-sim\") cluster. You are now ready to run simulations. Follow the instructions in Running the Simulator to run your first simulation. Docker top # Docker v20.10 or later must be installed on Linux or the latest version of Docker Desktop for Windows must be installed in order to run local simulations that use the Python API or VSE runtime templates . Installing Docker under Linux top # Follow the Docker installation instructions to install Docker. Alternatively, since Docker v20.10.7 is now supplied by the package docker.io available for 18.04 and 20.04 from the Ubuntu package repos, installation can be done with: sudo apt update && sudo apt install --no-install-recommends docker.io Installing Docker under Windows top # Install Docker Desktop using the installation binary from the docker site . Testing docker integration top # You can test Docker integration by pressing the ESC key to open the console and entering $ bash Simulator should start pulling, extracting and running the bash image from docker hub. After you see the bash prompt, you should be able to type exit to terminate the shell and then the Enter to close the console tab.","title":"Installation procedure"},{"location":"installation-guide/installing-simulator/#graphicsdrivers","text":"","title":"Graphics Drivers"},{"location":"installation-guide/installing-simulator/#windows","text":"Download driver manually from NVIDIA's website and install.","title":"Windows"},{"location":"installation-guide/installing-simulator/#linux","text":"You can check if the NVIDIA drivers are already installed, and their version by running in terminal: $ nvidia-smi The output should be similar to: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.57 Driver Version: 450.57 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:01:00.0 On | N/A | | 0% 36C P8 11W / 280W | 202MiB / 11169MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1942 G /usr/lib/xorg/Xorg 114MiB | | 0 N/A N/A 2292 G /usr/bin/gnome-shell 84MiB | +-----------------------------------------------------------------------------+ If you do not have the latest NVIDIA drivers installed, then install the drivers for your system. For example: Run ubuntu-drivers devices to check the highest supported driver version for your system: == /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 == modalias : pci:v000010DEd00001B06sv00001462sd00003607bc03sc00i00 vendor : NVIDIA Corporation model : GP102 [GeForce GTX 1080 Ti] driver : nvidia-driver-435 - distro non-free driver : nvidia-driver-390 - distro non-free driver : nvidia-driver-450 - third-party free recommended driver : nvidia-driver-440 - distro non-free driver : xserver-xorg-video-nouveau - distro free builtin ... Install the appropriate driver: $ sudo apt install nvidia-driver-450","title":"Linux"},{"location":"installation-guide/installing-simulator/#vulkan","text":"Make sure the Vulkan userspace library is installed: $ sudo apt install libvulkan1 NOTE: If you also have mesa-vulkan-drivers installed (e.g. for Intel motherboard video) they can prevent the simulator from being able to access your NVIDIA GPU. If the simulator crashes when starting, try removing the mesa vulkan drivers: $ sudo apt remove mesa-vulkan-drivers","title":"Linux Vulkan Drivers"},{"location":"installation-guide/installing-simulator/#installing-simulator","text":"The Simulator release can be downloaded as a prebuilt binary from the simulator distribution . Download and extract the simulator binary zip file for your OS to a desired location. For Linux: svlsimulator-linux64-{release-version}.zip For Windows: svlsimulator-windows64-{release-version}.zip Enter the svlsimulator-{operating-system}64-{version-number} folder and run the executable application named simulator ( simulator.exe on Windows) by double-clicking it.","title":"Installing the simulator"},{"location":"installation-guide/installing-simulator/#linktocloud","text":"When the SVL Simulator program launches, it will connect to the SVL Simulator Cloud. Click Open Browser to open the simulator user interface in a web browser. If the Link to Cloud button is not available, check in the upper right corner to see if the simulator is Online or Offline . If offline, click the Offline button to re-connect the simulator to the cloud, then click Link to Cloud . You should arrive at the SVL Simulator Sign In page. If you already have an account, you can use your credentials to sign in. If you don\u2019t already have an account, click Sign Up to create an account. To create an account, fill out the Sign Up form. You will need to confirm your email address to be able to continue. Click the \"Verify email\" link in the \"Complete registration\" email. If you don't receive the confirmation email, please check your spam filter or add \"contact@svlsimulator.com\" to your approved senders list. After signing in (or signing up), you will be logged in and are almost ready to start creating and running simulations. Before you can create a simulation, you'll need to link your local simulator instance to your cloud account. You should now see the Clusters view, where you need to enter a cluster name (e.g. \"local-sim\") for your local machine which is running SVL Simulator and click Create cluster . If you wish to link additional machines into a cluster for distributed simulation, refer to Clusters Tab for more information. If you don't see the New Cluster form or get an error when Creating the Cluster , your running simulator may be offline. Return to the SVL Simulator program window and click the Offline button to re-connect the simulator to the cloud, then click Link to Cloud to return to the Clusters view. You should now be able to enter a name for your local machine (e.g. \"local-sim\") and click Create cluster . At this point your local machine should now be linked to the cloud in your new default (\"local-sim\") cluster. You are now ready to run simulations. Follow the instructions in Running the Simulator to run your first simulation.","title":"Link to Cloud"},{"location":"installation-guide/installing-simulator/#docker","text":"Docker v20.10 or later must be installed on Linux or the latest version of Docker Desktop for Windows must be installed in order to run local simulations that use the Python API or VSE runtime templates .","title":"Docker"},{"location":"installation-guide/installing-simulator/#docker-linux","text":"Follow the Docker installation instructions to install Docker. Alternatively, since Docker v20.10.7 is now supplied by the package docker.io available for 18.04 and 20.04 from the Ubuntu package repos, installation can be done with: sudo apt update && sudo apt install --no-install-recommends docker.io","title":"Installing Docker under Linux"},{"location":"installation-guide/installing-simulator/#docker-linux","text":"Install Docker Desktop using the installation binary from the docker site .","title":"Installing Docker under Linux"},{"location":"installation-guide/installing-simulator/#testing-docker-integration","text":"You can test Docker integration by pressing the ESC key to open the console and entering $ bash Simulator should start pulling, extracting and running the bash image from docker hub. After you see the bash prompt, you should be able to type exit to terminate the shell and then the Enter to close the console tab.","title":"Testing Docker integration"},{"location":"installation-guide/running-linux-gpu-applications-on-windows/","text":"Running Linux GPU Applications on Windows # Introduction top # Some of the GPU-requiring tools commonly used with SVL Simulator, like Apollo or Autoware.Auto , might require the Linux operating system to run. If you don't have Linux available or prefer to use Windows, it's now possible to run Linux programs using Windows Subsystem for Linux (WSL). Features available in WSL are dependent on the version of Windows 10 . Windows build 21362 contains the version of WSL required to run Linux GPU applications on Windows. As an alternative, you can set up a desktop environment ). Installation top # NOTE: At the time of writing, most of the drivers and software required to use GPU-PV in WSL are still in preview. This might require using their pre-release versions. Details can be found in the specific sub-sections. Verify Windows 10 version top # In the Windows command line, enter: winver The reported OS Build should be 21362 or higher. If the build number is lower, and your system is up to date, the required Windows version is not yet a part of public release. To use it, you will have to join Windows Insider Program on the Dev Channel. Doing so will let you use preview builds of Windows 10. If you're interested, please follow official instructions and update your Windows version. Install WSL 2 top # To install WSL 2, please refer to the official documentation . We recommend using Ubuntu 18.04 or Ubuntu 20.04 Linux distribution. This tutorial will assume Ubuntu 20.04 is installed. If you have installed WSL previously, make sure you're using WSL version 2. To check for version, enter: wsl -l -v The distribution you're planning to use should report 2 under VERSION . If you're using WSL 1, update it to WSL 2. Make sure your Linux kernel version is up to date. You can update it through an elevated Windows command line by entering: wsl --update Install NVIDIA drivers top # As of the end of September 2021, the NVIDIA drivers for CUDA on WSL are still in public preview. Version 470.14 (with CUDA 11.3) or higher is required. You can check your current driver version through Windows command line, by entering: nvidia-smi If your driver version is older, you can download required version from official NVIDIA web page . To verify that your GPU is working inside WSL, you can build and run one of default CUDA examples using WSL terminal: cd /usr/local/cuda/samples/4_Finance/BlackScholes make ./BlackScholes Your console should provide you with output similar to the one shown below. This means that your GPU is accessible inside of WSL and can be used to run CUDA programs. [./BlackScholes] - Starting... GPU Device 0: \"Pascal\" with compute capability 6.1 Initializing data... ...allocating CPU memory for options. ...allocating GPU memory for options. ...generating input data in CPU mem. ...copying input data to GPU mem. Data init done. Executing Black-Scholes GPU kernel (512 iterations)... Options count : 8000000 BlackScholesGPU() time : 0.363584 msec Effective memory bandwidth: 220.031695 GB/s Gigaoptions per second : 22.003170 BlackScholes, Throughput = 22.0032 GOptions/s, Time = 0.00036 s, Size = 8000000 options, NumDevsUsed = 1, Workgroup = 128 Reading back GPU results... Checking the results... ...running CPU calculations. Comparing the results... L1 norm: 1.741792E-07 Max absolute error: 1.192093E-05 Shutting down... ...releasing GPU memory. ...releasing CPU memory. Shutdown done. [BlackScholes] - Test Summary NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. Test passed Install Docker Desktop for Windows top # If you plan to use Docker inside your WSL 2 distro, we suggest to install Docker Desktop by following official documentation . Alternatively, you can decide to skip Docker Desktop and use Docker and NVIDIA Container Toolkit installed directly from your WSL 2. We recommend the first option - this tutorial will assume Docker Desktop for Windows is used. If you insist on using the second option, you can find instructions in official NVIDIA documentation . If you're already using Docker Desktop for Windows, make sure version 3.1 or higher is installed. After installing and launching Docker Desktop, navigate to Settings -> General and make sure that the option Use the WSL 2 based engine is enabled. After that, navigate to Settings -> Resources -> WSL integration and make sure that integration with your WSL 2 distro is enabled. Whenever you're using Docker from WSL, the Docker Desktop application must be running on your Windows machine - otherwise WSL won't be able to recognize service docker . To verify that Docker environment is running properly, enter your WSL terminal and launch a sample CUDA docker image from NVIDIA: docker run --rm -it --gpus=all --env NVIDIA_DISABLE_REQUIRE=1 nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark NOTE: Parameter --env NVIDIA_DISABLE_REQUIRE=1 disables the CUDA version check. This is required as of the end of September 2021 due to bug in NVIDIA drivers (CUDA version reported in WSL is lower than installed). Your console should provide you with output similar to the one shown below. This means that your GPU is accessible inside of Docker containers and can be used to run CUDA programs. Run \"nbody -benchmark [-numbodies=<numBodies>]\" to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=<N> (number of bodies (>= 1) to run in simulation) -device=<d> (where d=0,1,2.... for the CUDA device to use) -numdevices=<i> (where i=(number of CUDA devices > 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=<file.bin> (load a tipsy model file for simulation) NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. > Windowed mode > Simulation data stored in video memory > Single precision floating point simulation > 1 Devices used for simulation GPU Device 0: \"Pascal\" with compute capability 6.1 > Compute 6.1 CUDA device: [NVIDIA GeForce GTX 1080] 20480 bodies, total time for 10 iterations: 16.788 ms = 249.832 billion interactions per second = 4996.645 single-precision GFLOP/s at 20 flops per interaction Set up desktop environment (optional) top # If you want to run any GUI-based applications inside your WSL or Docker environment, you have configure the X Window System. By default, display options inside of WSL are not configured and no valid output device is registered. This means not only that GUI will not be displayed, but also that any program attempting to output something to screen might not work correctly or fail to launch. If you have Windows 10 build 21362 or higher, you don't have to do anything - along your WSL instance, WSLg should have been automatically installed. WSLg pipes X11 and Wayland (used to run Linux GUI applications) directly into Windows graphical user interface. This means that any GUI application launched inside WSL will simply open on your Windows desktop. To verify that's the case, run any GUI-based application from your WSL instance. As an example, you can use one of OpenGL test applications. In your WSL terminal, enter: glxgears This should open new window with three spinning gears. If your Linux distro does not have this installed by default, you can get glxgears application from mesa-utils package: sudo apt install mesa-utils Networking considerations top # Compared to using Docker on Linux, using it on Windows through Docker Desktop with the WSL 2 backend has some significant differences in networking. Whenever you want to use any kind of networking functionality in a container running on Docker Desktop, make sure to take the points below into account: The --net=host option for docker run will not behave as expected. The Docker daemon runs inside an isolated network namespace, which, from the perspective of the Docker container, is a host network. You won't be able to access containers started with this option through the usual means. Docker Desktop provides special host name ( host.docker.internal ) that resolves to your host machine. It always resolves to an IP reachable from the container, and resolves to 127.0.0.1 on the host. If you can't connect to your container through localhost , try using host.docker.internal instead. Since you can't use the --net=host option, all of the ports that will be used to communicate with the container have to be explicitly exposed using -p flags (see official documentation for details). They will be tunneled both to the WSL 2 network namespace and the Windows host.","title":"Running Linux GPU applications on Windows"},{"location":"installation-guide/running-linux-gpu-applications-on-windows/#introduction","text":"Some of the GPU-requiring tools commonly used with SVL Simulator, like Apollo or Autoware.Auto , might require the Linux operating system to run. If you don't have Linux available or prefer to use Windows, it's now possible to run Linux programs using Windows Subsystem for Linux (WSL). Features available in WSL are dependent on the version of Windows 10 . Windows build 21362 contains the version of WSL required to run Linux GPU applications on Windows. As an alternative, you can set up a desktop environment ).","title":"Introduction"},{"location":"installation-guide/running-linux-gpu-applications-on-windows/#installation","text":"NOTE: At the time of writing, most of the drivers and software required to use GPU-PV in WSL are still in preview. This might require using their pre-release versions. Details can be found in the specific sub-sections.","title":"Installation"},{"location":"installation-guide/running-linux-gpu-applications-on-windows/#verify-windows-10-version","text":"In the Windows command line, enter: winver The reported OS Build should be 21362 or higher. If the build number is lower, and your system is up to date, the required Windows version is not yet a part of public release. To use it, you will have to join Windows Insider Program on the Dev Channel. Doing so will let you use preview builds of Windows 10. If you're interested, please follow official instructions and update your Windows version.","title":"Verify Windows 10 version"},{"location":"installation-guide/running-linux-gpu-applications-on-windows/#install-wsl-2","text":"To install WSL 2, please refer to the official documentation . We recommend using Ubuntu 18.04 or Ubuntu 20.04 Linux distribution. This tutorial will assume Ubuntu 20.04 is installed. If you have installed WSL previously, make sure you're using WSL version 2. To check for version, enter: wsl -l -v The distribution you're planning to use should report 2 under VERSION . If you're using WSL 1, update it to WSL 2. Make sure your Linux kernel version is up to date. You can update it through an elevated Windows command line by entering: wsl --update","title":"Install WSL 2"},{"location":"installation-guide/running-linux-gpu-applications-on-windows/#install-nvidia-drivers","text":"As of the end of September 2021, the NVIDIA drivers for CUDA on WSL are still in public preview. Version 470.14 (with CUDA 11.3) or higher is required. You can check your current driver version through Windows command line, by entering: nvidia-smi If your driver version is older, you can download required version from official NVIDIA web page . To verify that your GPU is working inside WSL, you can build and run one of default CUDA examples using WSL terminal: cd /usr/local/cuda/samples/4_Finance/BlackScholes make ./BlackScholes Your console should provide you with output similar to the one shown below. This means that your GPU is accessible inside of WSL and can be used to run CUDA programs. [./BlackScholes] - Starting... GPU Device 0: \"Pascal\" with compute capability 6.1 Initializing data... ...allocating CPU memory for options. ...allocating GPU memory for options. ...generating input data in CPU mem. ...copying input data to GPU mem. Data init done. Executing Black-Scholes GPU kernel (512 iterations)... Options count : 8000000 BlackScholesGPU() time : 0.363584 msec Effective memory bandwidth: 220.031695 GB/s Gigaoptions per second : 22.003170 BlackScholes, Throughput = 22.0032 GOptions/s, Time = 0.00036 s, Size = 8000000 options, NumDevsUsed = 1, Workgroup = 128 Reading back GPU results... Checking the results... ...running CPU calculations. Comparing the results... L1 norm: 1.741792E-07 Max absolute error: 1.192093E-05 Shutting down... ...releasing GPU memory. ...releasing CPU memory. Shutdown done. [BlackScholes] - Test Summary NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. Test passed","title":"Install NVIDIA drivers"},{"location":"installation-guide/running-linux-gpu-applications-on-windows/#install-docker-desktop-for-windows","text":"If you plan to use Docker inside your WSL 2 distro, we suggest to install Docker Desktop by following official documentation . Alternatively, you can decide to skip Docker Desktop and use Docker and NVIDIA Container Toolkit installed directly from your WSL 2. We recommend the first option - this tutorial will assume Docker Desktop for Windows is used. If you insist on using the second option, you can find instructions in official NVIDIA documentation . If you're already using Docker Desktop for Windows, make sure version 3.1 or higher is installed. After installing and launching Docker Desktop, navigate to Settings -> General and make sure that the option Use the WSL 2 based engine is enabled. After that, navigate to Settings -> Resources -> WSL integration and make sure that integration with your WSL 2 distro is enabled. Whenever you're using Docker from WSL, the Docker Desktop application must be running on your Windows machine - otherwise WSL won't be able to recognize service docker . To verify that Docker environment is running properly, enter your WSL terminal and launch a sample CUDA docker image from NVIDIA: docker run --rm -it --gpus=all --env NVIDIA_DISABLE_REQUIRE=1 nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark NOTE: Parameter --env NVIDIA_DISABLE_REQUIRE=1 disables the CUDA version check. This is required as of the end of September 2021 due to bug in NVIDIA drivers (CUDA version reported in WSL is lower than installed). Your console should provide you with output similar to the one shown below. This means that your GPU is accessible inside of Docker containers and can be used to run CUDA programs. Run \"nbody -benchmark [-numbodies=<numBodies>]\" to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=<N> (number of bodies (>= 1) to run in simulation) -device=<d> (where d=0,1,2.... for the CUDA device to use) -numdevices=<i> (where i=(number of CUDA devices > 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=<file.bin> (load a tipsy model file for simulation) NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. > Windowed mode > Simulation data stored in video memory > Single precision floating point simulation > 1 Devices used for simulation GPU Device 0: \"Pascal\" with compute capability 6.1 > Compute 6.1 CUDA device: [NVIDIA GeForce GTX 1080] 20480 bodies, total time for 10 iterations: 16.788 ms = 249.832 billion interactions per second = 4996.645 single-precision GFLOP/s at 20 flops per interaction","title":"Install Docker Desktop for Windows"},{"location":"installation-guide/running-linux-gpu-applications-on-windows/#set-up-desktop-environment-(optional)","text":"If you want to run any GUI-based applications inside your WSL or Docker environment, you have configure the X Window System. By default, display options inside of WSL are not configured and no valid output device is registered. This means not only that GUI will not be displayed, but also that any program attempting to output something to screen might not work correctly or fail to launch. If you have Windows 10 build 21362 or higher, you don't have to do anything - along your WSL instance, WSLg should have been automatically installed. WSLg pipes X11 and Wayland (used to run Linux GUI applications) directly into Windows graphical user interface. This means that any GUI application launched inside WSL will simply open on your Windows desktop. To verify that's the case, run any GUI-based application from your WSL instance. As an example, you can use one of OpenGL test applications. In your WSL terminal, enter: glxgears This should open new window with three spinning gears. If your Linux distro does not have this installed by default, you can get glxgears application from mesa-utils package: sudo apt install mesa-utils","title":"Set up desktop environment (optional)"},{"location":"installation-guide/running-linux-gpu-applications-on-windows/#networking-considerations","text":"Compared to using Docker on Linux, using it on Windows through Docker Desktop with the WSL 2 backend has some significant differences in networking. Whenever you want to use any kind of networking functionality in a container running on Docker Desktop, make sure to take the points below into account: The --net=host option for docker run will not behave as expected. The Docker daemon runs inside an isolated network namespace, which, from the perspective of the Docker container, is a host network. You won't be able to access containers started with this option through the usual means. Docker Desktop provides special host name ( host.docker.internal ) that resolves to your host machine. It always resolves to an IP reachable from the container, and resolves to 127.0.0.1 on the host. If you can't connect to your container through localhost , try using host.docker.internal instead. Since you can't use the --net=host option, all of the ports that will be used to communicate with the container have to be explicitly exposed using -p flags (see official documentation for details). They will be tunneled both to the WSL 2 network namespace and the Windows host.","title":"Networking considerations"},{"location":"installation-guide/system-requirements/","text":"System requirements # The recommended system specification for running SVL Simulator (locally) are as follows: at least 4 GHz Quad core CPU NVIDIA GTX 1080 (8GB memory) or higher Windows 10 (64-bit), Ubuntu 18.04 (64-bit), or Ubuntu 20.04 (64-bit) While lower-specification hardware may be able to run SVL Simulator, there may be issues with the performance required to interface properly with a user\u2019s System Under Test. SVL Simulator is currently supported for Windows (64-bit) and Linux (64-bit). For optimal performance, Windows is recommended. Currently, the full functionality of SVL Simulator in Developer Mode (in Unity Editor) is supported on Windows only. End-to-end automatic simulations using PythonAPI Runtime template or Visual Scenario Runtime template are supported on Linux only. On Windows, SVL Simulator requires a graphics card that supports DirectX 11. On Linux, SVL Simulator requires a graphics card that supports Vulkan 1.1. If running Apollo or Autoware on the same system as the Simulator, it is recommended that the GPU have at least 10GB of memory. If running Apollo or Autoware on a different system as the Simulator, a gigabit connection between the systems is required (a gigabit switch is sufficient, gigabit internet is not required). For best results top # System and graphics performance varies tremendously from lightweight gaming laptops to high end graphics workstations. There are many different hardware factors that can influence overall performance including CPU model, clock frequency, number of cores, system RAM, GPU model and available GPU memory. In addition, the Simulator configuration (environment, vehicle, number and type of sensors) can affect performance, as well as the autonomous software configuration (e.g. enabled modules). Keep an eye on CPU load, system memory, and GPU memory. Running out of any of these can cause a variety of problems such as low frame rates or autonomous software modules being unable to function. The minimum goal for real time simulation should be 15fps (or the desired sensor frame rate) since that will avoid dropping frames for critical sensors like camera and LiDAR sensors. For complex multi-sensor simulations, refer to the Distributed Simulation docs for information on multi-machine (multi-GPU) distributed simulation. While it is possible to run sophisticated autonomous software stacks like Apollo or Autoware on the same machine that runs SVL Simulator, it will challenge even the highest performing systems. For best results, the Simulator should be run on a separate machine from the autonomous software. If it is not practical or possible to run autonomous software on a separate machine then you might want to consider using ground truth sensors in place of perception and traffic signal modules, as documented in Modular Testing . This will greatly reduce the CPU and GPU requirements of an autonomous stack such as Apollo and increase the likelihood that you can run it on the same machine with SVL Simulator.","title":"System requirements"},{"location":"installation-guide/system-requirements/#for-best-results","text":"System and graphics performance varies tremendously from lightweight gaming laptops to high end graphics workstations. There are many different hardware factors that can influence overall performance including CPU model, clock frequency, number of cores, system RAM, GPU model and available GPU memory. In addition, the Simulator configuration (environment, vehicle, number and type of sensors) can affect performance, as well as the autonomous software configuration (e.g. enabled modules). Keep an eye on CPU load, system memory, and GPU memory. Running out of any of these can cause a variety of problems such as low frame rates or autonomous software modules being unable to function. The minimum goal for real time simulation should be 15fps (or the desired sensor frame rate) since that will avoid dropping frames for critical sensors like camera and LiDAR sensors. For complex multi-sensor simulations, refer to the Distributed Simulation docs for information on multi-machine (multi-GPU) distributed simulation. While it is possible to run sophisticated autonomous software stacks like Apollo or Autoware on the same machine that runs SVL Simulator, it will challenge even the highest performing systems. For best results, the Simulator should be run on a separate machine from the autonomous software. If it is not practical or possible to run autonomous software on a separate machine then you might want to consider using ground truth sensors in place of perception and traffic signal modules, as documented in Modular Testing . This will greatly reduce the CPU and GPU requirements of an autonomous stack such as Apollo and increase the likelihood that you can run it on the same machine with SVL Simulator.","title":"For best results"},{"location":"plugins/bridge-plugins/","text":"Bridge Plugins # Bridge plugins are custom bridge implementations that can be used by an ego vehicle to send and receive sensor data. Bridge plugins must be built by the simulator and the resulting bundle named bridge_XXX must be placed in the AssetBundles/Bridges folder. If running the binary, this folder is included in the downloaded .zip file. If running in Unity Editor (Developer Mode), the sensor will be built into the folder directly. This must be done before running the simulator (running the executable or pressing Play in the Editor). Building bridge plugins to a bundle is done as below: Open Simulator -> Build... menu item Select bridge plugins in Bridges section of build window Build plugins with Build button To make a bridge plugin, create a folder in Assets/External/Bridges , for example Assets/External/Bridge/CustomBridge . Inside this folder you must place all the scripts that will be used by the simulator to build C# managed assembly. Implementation top # A bridge plugin must provide two separate classes inheriting the following interfaces: 1) IBridgeFactory 2) IBridgeInstance Factory class is used to provide meta-information about bridge and a few factory methods. It will be created only once. Instance class represents the actual bridge implementation that is able to send and/or receive data. It will be created once for every vehicle where it is used. Bridge Factory top # public interface IBridgeFactory { IBridgeInstance CreateInstance(); void Register(IBridgePlugin plugin); void RegPublisher<DataType, BridgeType>(IBridgePlugin plugin, Func<DataType, BridgeType> converter); void RegSubscriber<DataType, BridgeType>(IBridgePlugin plugin, Func<BridgeType, DataType> converter); } This interface requires the class to expose four public methods: 1) CreateInstance - called to create new bridge instance. Typically this method simply returns new object of class that implements IBridgeInstance interface. 2) Register - called by simulator allow factory register supported functionality with plugin interface. See the section below on plugin registration. 3) RegPublisher and RegSubscriber - called by custom sensors to register custom data types for publisher or subscriber functionality. Factory can use this method itself to register its supported data types for simulator built-in sensors. Classes inheriting from this interface must have BridgeName and BridgeType attribute applied to inform name and type that will be used to identify this bridge. For example: [BridgeName(\"MyCustomBridge\", \"MyCustomBridgeType\")] public class MyCustomBridgeFactory : IBridgeFactory { ... } Bridge Instance top # public interface IBridgeInstance { Status Status { get; } void Connect(string connection); void Disconnect(); } This interface requires the class to expose three public methods: 1) Status - getter that will be called by simulator to query status of bridge. For example, sensors will be sending data only if Status is set to Connected state. 1) Connect - called when simulator wants to initate new connection. Connection string is passed unmodified from web interface where it can be set to anything. It is responsibility of bridge implementation to validate then accept or reject this string. 2) Disconnect - called when simulator wants to close connection. Typically called upon unloading vehicle from scene. Bridge Plugin Registration top # Upon creating a bridge factory, the simulator will call Register method to allow factory and pass plugin instance that factory should use to register available data types and publishers/subscribers. This information is not static as custom sensor plugins may want to register their custom types with one or multiple bridge factories. Simulator provides many different data types, for example ImageData or PointCloudData that represents specific instances of data published or subscribed by sensors. Factory must inform plugin about which types it supports. This is performed by calling AddType<DataType>(string bridgeTypeName) on plugin instance. DataType is one of the data types simulator supports. bridgeTypeName is an arbitrary string used to show name of this data type in UI. For each subscriber or publisher bridge factory should call AddSubscriberCreator<DataType> or AddPublisherCreator<DataType> on plugin instance. These methods should provide delegate that can be used to create subscriber or publisher instances for each sensor that wants to use specific data type. void AddSubscriberCreator<DataType>(SubscriberCreator<DataType> subscriber); void AddPublisherCreator<DataType>(PublisherCreator<DataType> publisher); See IBridgePlugin.cs source code for more information on delegate arguments and return values. Typically, the bridge will call AddType + AddSubscriberCreator from RegSubscriber method, and AddType + AddPublisherCreator from RegPublisher method. This should be done for all default simulator data types simulator supports. Additionally, custom sensor plugins may choose to call your bridge RegPublisher and RegSubscriber methods to add custom data types they which to publish or subscribe. Please refer to ComfortSensor and LaneFollowingSensor sensor plugins for examples of this functionality. Registering Plugin with Web UI top # All bridge plugins must be registered with the SVL Simulator Web UI to be usable. This currently needs to be done manually by contacting us . Custom bridge should have same name in the followings: a folder, a file, a class. Rename folder name. cd Simulator/Assets/External/Bridges mv ROS2 ROS2CustomBridge Rename file name. cd ROS2CustomBridge mv ROS2.cs ROS2CustomBridge.cs Rename class and bridge name. The following diff block shows how to register ROS2CustomBridge renamed from ROS2. ```diff --- a/ROS2.cs +++ b/ROS2CustomBridge.cs Publish = 3, } public partial class Ros2BridgeInstance : IBridgeInstance public partial class ROS2CustomBridge : IBridgeInstance ... public Status Status { get; private set; } = Status.Disconnected; public Ros2BridgeInstance() public ROS2CustomBridge() --- a/Ros2BridgeFactory.cs +++ b/Ros2BridgeFactory.cs - [BridgeName(\"ROS2\", \"ROS2\")] + [BridgeName(\"ROS2CustomBridge\", \"ROS2\")] public class Ros2BridgeFactory : IBridgeFactory { - public IBridgeInstance CreateInstance() => new Ros2BridgeInstance(); + public IBridgeInstance CreateInstance() => new ROS2CustomBridge(); public void Register(IBridgePlugin plugin) ... plugin.AddPublisherCreator( (instance, topic) => { var ros2Instance = instance as Ros2BridgeInstance; var ros2Instance = instance as ROS2CustomBridge; ros2Instance.AddPublisher (topic); ... public void RegPublisher<DataType, BridgeType>(IBridgePlugin plugin, Func<DataType, BridgeType> converter) { plugin.AddType (Ros2Utils.GetMessageType ()); plugin.AddType (\"ROS2CustomBridge\"); plugin.AddPublisherCreator( (instance, topic) => { var ros2Instance = instance as Ros2BridgeInstance; var ros2Instance = instance as ROS2CustomBridge; ros2Instance.AddPublisher (topic); ... public void RegSubscriber<DataType, BridgeType>(IBridgePlugin plugin, Func<BridgeType, DataType> converter) { plugin.AddType (Ros2Utils.GetMessageType ()); plugin.AddType (\"ROS2CustomBridge\"); plugin.AddSubscriberCreator ( (instance, topic, callback) => (instance as Ros2BridgeInstance).AddSubscriber (topic, (instance, topic, callback) => (instance as ROS2CustomBridge).AddSubscriber (topic, rawData => callback(converter(Ros2Serialization.Unserialize (rawData))) ) ); --- a/Ros2Writer.cs +++ b/Ros2Writer.cs public class Ros2Writer<BridgeType> { Ros2BridgeInstance Instance; ROS2CustomBridge Instance; byte[] Topic; public Ros2Writer(Ros2BridgeInstance instance, string topic) public Ros2Writer(ROS2CustomBridge instance, string topic) { Instance = instance; Topic = Encoding.ASCII.GetBytes(topic); ... public Ros2PointCloudWriter(Ros2BridgeInstance instance, string topic) public Ros2PointCloudWriter(ROS2CustomBridge instance, string topic) { Writer = new Ros2Writer (instance, topic); } ``` Build the custom bridge in Unity Editor and upload it into Plugins under Library in WISE. Examples top # Open-source examples are available: ROS Bridge - ROS bridge implementation for Autoware ROSApollo Bridge - ROS bridge implementation for Apollo ROS2 Bridge - ROS2 bridge implementation CyberRT Bridge - CyberRT bridge implementation Logging Bridge - simple bridge that logs all published data to a .txt file.","title":"Bridge plugins"},{"location":"plugins/bridge-plugins/#implementation","text":"A bridge plugin must provide two separate classes inheriting the following interfaces: 1) IBridgeFactory 2) IBridgeInstance Factory class is used to provide meta-information about bridge and a few factory methods. It will be created only once. Instance class represents the actual bridge implementation that is able to send and/or receive data. It will be created once for every vehicle where it is used.","title":"Implementation"},{"location":"plugins/bridge-plugins/#bridge-factory","text":"public interface IBridgeFactory { IBridgeInstance CreateInstance(); void Register(IBridgePlugin plugin); void RegPublisher<DataType, BridgeType>(IBridgePlugin plugin, Func<DataType, BridgeType> converter); void RegSubscriber<DataType, BridgeType>(IBridgePlugin plugin, Func<BridgeType, DataType> converter); } This interface requires the class to expose four public methods: 1) CreateInstance - called to create new bridge instance. Typically this method simply returns new object of class that implements IBridgeInstance interface. 2) Register - called by simulator allow factory register supported functionality with plugin interface. See the section below on plugin registration. 3) RegPublisher and RegSubscriber - called by custom sensors to register custom data types for publisher or subscriber functionality. Factory can use this method itself to register its supported data types for simulator built-in sensors. Classes inheriting from this interface must have BridgeName and BridgeType attribute applied to inform name and type that will be used to identify this bridge. For example: [BridgeName(\"MyCustomBridge\", \"MyCustomBridgeType\")] public class MyCustomBridgeFactory : IBridgeFactory { ... }","title":"Bridge Factory"},{"location":"plugins/bridge-plugins/#bridge-instance","text":"public interface IBridgeInstance { Status Status { get; } void Connect(string connection); void Disconnect(); } This interface requires the class to expose three public methods: 1) Status - getter that will be called by simulator to query status of bridge. For example, sensors will be sending data only if Status is set to Connected state. 1) Connect - called when simulator wants to initate new connection. Connection string is passed unmodified from web interface where it can be set to anything. It is responsibility of bridge implementation to validate then accept or reject this string. 2) Disconnect - called when simulator wants to close connection. Typically called upon unloading vehicle from scene.","title":"Bridge Instance"},{"location":"plugins/bridge-plugins/#bridge-plugin-registration","text":"Upon creating a bridge factory, the simulator will call Register method to allow factory and pass plugin instance that factory should use to register available data types and publishers/subscribers. This information is not static as custom sensor plugins may want to register their custom types with one or multiple bridge factories. Simulator provides many different data types, for example ImageData or PointCloudData that represents specific instances of data published or subscribed by sensors. Factory must inform plugin about which types it supports. This is performed by calling AddType<DataType>(string bridgeTypeName) on plugin instance. DataType is one of the data types simulator supports. bridgeTypeName is an arbitrary string used to show name of this data type in UI. For each subscriber or publisher bridge factory should call AddSubscriberCreator<DataType> or AddPublisherCreator<DataType> on plugin instance. These methods should provide delegate that can be used to create subscriber or publisher instances for each sensor that wants to use specific data type. void AddSubscriberCreator<DataType>(SubscriberCreator<DataType> subscriber); void AddPublisherCreator<DataType>(PublisherCreator<DataType> publisher); See IBridgePlugin.cs source code for more information on delegate arguments and return values. Typically, the bridge will call AddType + AddSubscriberCreator from RegSubscriber method, and AddType + AddPublisherCreator from RegPublisher method. This should be done for all default simulator data types simulator supports. Additionally, custom sensor plugins may choose to call your bridge RegPublisher and RegSubscriber methods to add custom data types they which to publish or subscribe. Please refer to ComfortSensor and LaneFollowingSensor sensor plugins for examples of this functionality.","title":"Bridge Plugin Registration"},{"location":"plugins/bridge-plugins/#registering-plugin-with-web-ui","text":"All bridge plugins must be registered with the SVL Simulator Web UI to be usable. This currently needs to be done manually by contacting us . Custom bridge should have same name in the followings: a folder, a file, a class. Rename folder name. cd Simulator/Assets/External/Bridges mv ROS2 ROS2CustomBridge Rename file name. cd ROS2CustomBridge mv ROS2.cs ROS2CustomBridge.cs Rename class and bridge name. The following diff block shows how to register ROS2CustomBridge renamed from ROS2. ```diff --- a/ROS2.cs +++ b/ROS2CustomBridge.cs Publish = 3, } public partial class Ros2BridgeInstance : IBridgeInstance public partial class ROS2CustomBridge : IBridgeInstance ... public Status Status { get; private set; } = Status.Disconnected; public Ros2BridgeInstance() public ROS2CustomBridge() --- a/Ros2BridgeFactory.cs +++ b/Ros2BridgeFactory.cs - [BridgeName(\"ROS2\", \"ROS2\")] + [BridgeName(\"ROS2CustomBridge\", \"ROS2\")] public class Ros2BridgeFactory : IBridgeFactory { - public IBridgeInstance CreateInstance() => new Ros2BridgeInstance(); + public IBridgeInstance CreateInstance() => new ROS2CustomBridge(); public void Register(IBridgePlugin plugin) ... plugin.AddPublisherCreator( (instance, topic) => { var ros2Instance = instance as Ros2BridgeInstance; var ros2Instance = instance as ROS2CustomBridge; ros2Instance.AddPublisher (topic); ... public void RegPublisher<DataType, BridgeType>(IBridgePlugin plugin, Func<DataType, BridgeType> converter) { plugin.AddType (Ros2Utils.GetMessageType ()); plugin.AddType (\"ROS2CustomBridge\"); plugin.AddPublisherCreator( (instance, topic) => { var ros2Instance = instance as Ros2BridgeInstance; var ros2Instance = instance as ROS2CustomBridge; ros2Instance.AddPublisher (topic); ... public void RegSubscriber<DataType, BridgeType>(IBridgePlugin plugin, Func<BridgeType, DataType> converter) { plugin.AddType (Ros2Utils.GetMessageType ()); plugin.AddType (\"ROS2CustomBridge\"); plugin.AddSubscriberCreator ( (instance, topic, callback) => (instance as Ros2BridgeInstance).AddSubscriber (topic, (instance, topic, callback) => (instance as ROS2CustomBridge).AddSubscriber (topic, rawData => callback(converter(Ros2Serialization.Unserialize (rawData))) ) ); --- a/Ros2Writer.cs +++ b/Ros2Writer.cs public class Ros2Writer<BridgeType> { Ros2BridgeInstance Instance; ROS2CustomBridge Instance; byte[] Topic; public Ros2Writer(Ros2BridgeInstance instance, string topic) public Ros2Writer(ROS2CustomBridge instance, string topic) { Instance = instance; Topic = Encoding.ASCII.GetBytes(topic); ... public Ros2PointCloudWriter(Ros2BridgeInstance instance, string topic) public Ros2PointCloudWriter(ROS2CustomBridge instance, string topic) { Writer = new Ros2Writer (instance, topic); } ``` Build the custom bridge in Unity Editor and upload it into Plugins under Library in WISE.","title":"Registering Plugin with Web UI"},{"location":"plugins/bridge-plugins/#examples","text":"Open-source examples are available: ROS Bridge - ROS bridge implementation for Autoware ROSApollo Bridge - ROS bridge implementation for Apollo ROS2 Bridge - ROS2 bridge implementation CyberRT Bridge - CyberRT bridge implementation Logging Bridge - simple bridge that logs all published data to a .txt file.","title":"Examples"},{"location":"plugins/chargingstation-controllable/","text":"Controllable - ChargingStation # This contains detailed documentation on the ChargingStation controllable plugin. To use this Controllable Plugin: 1) Clone the repo into Assets/External/Controllables/ChargingStation inside of your Simulator Unity Project 2) Build the Controllable Plugin for use with the Simulator, navigate to the Simulator -> Build Controllables Unity Editor menu item. Select ChargingStation controllable and build. Output bundle will be in AssetBundles/Controllables folder in root of Simulator Unity Project 3) Simulator will load, at runtime, all custom Controllable Plugin bundles in AssetBundles/Controllables directory Custom Logic top # To implement custom logic, contained in a given Controllable Plugin project there must be an IControllable implementation. An example of this is in TrafficCone.cs The interface requires the following to be implemented: public bool Spawned { get; set; } = false; public string UID { get; set; } public string GUID => UID; public string ControlType { get; set; } = \"chargingstation\"; public string CurrentState { get; set; } public string[] ValidStates { get; set; } = new[] { \"on\", \"off\" }; public string[] ValidActions { get; set; } = new string[] { }; public List<ControlAction> DefaultControlPolicy { get; set; } = new List<ControlAction> { new ControlAction { Action = \"state\", Value = \"off\" } }; public List<ControlAction> CurrentControlPolicy { get; set; } public Control(List<ControlAction> controlActions) { // } On Awake() CurrentControlPolicy and CurrentState must be set, e.g. private void Awake() { Lights.AddRange(GetComponentsInChildren<Light>()); SetLights(false); ChargingStationRenderer = GetComponent<Renderer>(); foreach (var mat in ChargingStationRenderer.materials) { if (mat.name.Contains(\"Emission\")) { EmissionMaterials.Add(mat); } } CurrentControlPolicy = DefaultControlPolicy; Control(CurrentControlPolicy); } Control checks the parsed ControlActions and sets the CurrentState public void Control(List<ControlAction> controlActions) { foreach (var action in controlActions) { switch (action.Action) { case \"state\": SetChargingStationState(action.Value); break; default: Debug.LogError($\"'{action.Value}' is an invalid action for '{ControlType}'\"); break; } } } public void SetChargingStationState(string value) { if (!ValidStates.Contains(value)) { Debug.LogError($\"'{value}' is an invalid state for '{ControlType}'\"); return; } CurrentState = value; switch (CurrentState) { case \"on\": EmissionMaterials?.ForEach(x => x.SetColor(\"_EmissiveColor\", Color.white)); EmissionMaterials?.ForEach(x => x.SetFloat(\"_EmissiveIntensity\", 3f)); SetLights(true); break; case \"off\": EmissionMaterials?.ForEach(x => x.SetColor(\"_EmissiveColor\", Color.black)); EmissionMaterials?.ForEach(x => x.SetFloat(\"_EmissiveIntensity\", 0f)); SetLights(false); break; default: break; } } private void SetLights(bool state) { Lights.ForEach(l => l.enabled = state); } Python API example top # Each controllable object has its own valid actions (e.g., green, yellow, red, trigger, wait, loop, on, off, \"\") that it can take and is controlled based on control policy , which defines rules for control actions. To get a list of controllable objects in a scene: controllables = sim.get_controllables() For a controllable object of interest, you can get following information: station = controllables[0] print(\"Type:\", station.type) print(\"Transform:\", station.transform) print(\"Current state:\", station.current_state) print(\"Valid actions:\", station.valid_actions) For control policy, each controllable object always has default control policy (read-only). When you load a scene for the first time or reset a scene to the initial state, a controllable object resets current control policy to default one follows it. You can get default control policy and current control policy as follows: print(\"Default control policy:\", station.default_control_policy) print(\"Current control policy:\", station.control_policy) To change a current control policy, you can create a new control policy and call control function as below: control_policy = \"on\" station.control(control_policy) To add a plugin controllable and set object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0,0,0) state.transform.rotation = lgsvl.Vector(0,0,0) state.velocity = lgsvl.Vector(0,10,0) state.angular_velocity = lgsvl.Vector(6.5,0,0) cone = sim.controllable_add(\"ChargingStation\", state) To get plugin controllable object state station.object_state To set plugin controllable object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0, 0, -10) station.object_state = state Controllables can also have a Unity RigidBody component at the root and apply velocity from the API. If no rigidbody exists then the velocity is ignored. #!/usr/bin/env python3 # # Copyright (c) 2020-2021 LG Electronics, Inc. # # This software contains code licensed as described in LICENSE. # from environs import Env import lgsvl sim = lgsvl.Simulator(env.str(\"LGSVL__SIMULATOR_HOST\", lgsvl.wise.SimulatorSettings.simulator_host), env.int(\"LGSVL__SIMULATOR_PORT\", lgsvl.wise.SimulatorSettings.simulator_port)) if sim.current_scene == lgsvl.wise.DefaultAssets.map_borregasave: sim.reset() else: sim.load(lgsvl.wise.DefaultAssets.map_borregasave) spawns = sim.get_spawn() state = lgsvl.AgentState() forward = lgsvl.utils.transform_to_forward(spawns[0]) right = lgsvl.utils.transform_to_right(spawns[0]) up = lgsvl.utils.transform_to_up(spawns[0]) state.transform = spawns[0] ego = sim.add_agent(env.str(\"LGSVL__VEHICLE_0\", lgsvl.wise.DefaultAssets.ego_lincoln2017mkz_apollo5), lgsvl.AgentType.EGO, state) print(\"Python API Quickstart #28: How to Add/Control Charging Station\") obj_state = lgsvl.ObjectState() obj_state.transform.position = lgsvl.Vector(38, 0, 7) obj_state.transform.rotation = lgsvl.Vector(0, 180, 0) station = sim.controllable_add(\"ChargingStation\", obj_state) station = sim.get_controllable(lgsvl.Vector(38, 0, 7), \"chargingstation\") print(\"\\n# Charging Station of interest:\") print(station) seconds = 1 input(\"\\nPress Enter to run simulation for {} seconds\".format(seconds)) print(\"\\nRunning simulation for {} seconds...\".format(seconds)) sim.run(seconds) # Get current controllable state print(\"\\n# Current charging station control policy:\") print(station.control_policy) print(\"\\n# Current charging station object state\") print(station.object_state) print(\"\\n# Update charging station object state\") new_state = lgsvl.ObjectState() new_state.transform.position = lgsvl.Vector(38, 0, 7) new_state.transform.rotation = lgsvl.Vector(0, 180, 0) station.object_state = new_state print(\"\\n# New object state\") print(station.object_state) # Set time of day for light effects sim.set_time_of_day(19.0) print(sim.time_of_day) # Create a new control policy control_policy = \"on\" # Control this traffic light with a new control policy station.control(control_policy) print(\"\\n# Updated control policy:\") print(station.control_policy) # Get current state of charging station print(\"\\n# Current station state:\") print(station.current_state) print(\"\\nDone!\")","title":"Controllable - ChargingStation"},{"location":"plugins/chargingstation-controllable/#custom-logic","text":"To implement custom logic, contained in a given Controllable Plugin project there must be an IControllable implementation. An example of this is in TrafficCone.cs The interface requires the following to be implemented: public bool Spawned { get; set; } = false; public string UID { get; set; } public string GUID => UID; public string ControlType { get; set; } = \"chargingstation\"; public string CurrentState { get; set; } public string[] ValidStates { get; set; } = new[] { \"on\", \"off\" }; public string[] ValidActions { get; set; } = new string[] { }; public List<ControlAction> DefaultControlPolicy { get; set; } = new List<ControlAction> { new ControlAction { Action = \"state\", Value = \"off\" } }; public List<ControlAction> CurrentControlPolicy { get; set; } public Control(List<ControlAction> controlActions) { // } On Awake() CurrentControlPolicy and CurrentState must be set, e.g. private void Awake() { Lights.AddRange(GetComponentsInChildren<Light>()); SetLights(false); ChargingStationRenderer = GetComponent<Renderer>(); foreach (var mat in ChargingStationRenderer.materials) { if (mat.name.Contains(\"Emission\")) { EmissionMaterials.Add(mat); } } CurrentControlPolicy = DefaultControlPolicy; Control(CurrentControlPolicy); } Control checks the parsed ControlActions and sets the CurrentState public void Control(List<ControlAction> controlActions) { foreach (var action in controlActions) { switch (action.Action) { case \"state\": SetChargingStationState(action.Value); break; default: Debug.LogError($\"'{action.Value}' is an invalid action for '{ControlType}'\"); break; } } } public void SetChargingStationState(string value) { if (!ValidStates.Contains(value)) { Debug.LogError($\"'{value}' is an invalid state for '{ControlType}'\"); return; } CurrentState = value; switch (CurrentState) { case \"on\": EmissionMaterials?.ForEach(x => x.SetColor(\"_EmissiveColor\", Color.white)); EmissionMaterials?.ForEach(x => x.SetFloat(\"_EmissiveIntensity\", 3f)); SetLights(true); break; case \"off\": EmissionMaterials?.ForEach(x => x.SetColor(\"_EmissiveColor\", Color.black)); EmissionMaterials?.ForEach(x => x.SetFloat(\"_EmissiveIntensity\", 0f)); SetLights(false); break; default: break; } } private void SetLights(bool state) { Lights.ForEach(l => l.enabled = state); }","title":"Custom Logic"},{"location":"plugins/chargingstation-controllable/#python-api-example","text":"Each controllable object has its own valid actions (e.g., green, yellow, red, trigger, wait, loop, on, off, \"\") that it can take and is controlled based on control policy , which defines rules for control actions. To get a list of controllable objects in a scene: controllables = sim.get_controllables() For a controllable object of interest, you can get following information: station = controllables[0] print(\"Type:\", station.type) print(\"Transform:\", station.transform) print(\"Current state:\", station.current_state) print(\"Valid actions:\", station.valid_actions) For control policy, each controllable object always has default control policy (read-only). When you load a scene for the first time or reset a scene to the initial state, a controllable object resets current control policy to default one follows it. You can get default control policy and current control policy as follows: print(\"Default control policy:\", station.default_control_policy) print(\"Current control policy:\", station.control_policy) To change a current control policy, you can create a new control policy and call control function as below: control_policy = \"on\" station.control(control_policy) To add a plugin controllable and set object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0,0,0) state.transform.rotation = lgsvl.Vector(0,0,0) state.velocity = lgsvl.Vector(0,10,0) state.angular_velocity = lgsvl.Vector(6.5,0,0) cone = sim.controllable_add(\"ChargingStation\", state) To get plugin controllable object state station.object_state To set plugin controllable object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0, 0, -10) station.object_state = state Controllables can also have a Unity RigidBody component at the root and apply velocity from the API. If no rigidbody exists then the velocity is ignored. #!/usr/bin/env python3 # # Copyright (c) 2020-2021 LG Electronics, Inc. # # This software contains code licensed as described in LICENSE. # from environs import Env import lgsvl sim = lgsvl.Simulator(env.str(\"LGSVL__SIMULATOR_HOST\", lgsvl.wise.SimulatorSettings.simulator_host), env.int(\"LGSVL__SIMULATOR_PORT\", lgsvl.wise.SimulatorSettings.simulator_port)) if sim.current_scene == lgsvl.wise.DefaultAssets.map_borregasave: sim.reset() else: sim.load(lgsvl.wise.DefaultAssets.map_borregasave) spawns = sim.get_spawn() state = lgsvl.AgentState() forward = lgsvl.utils.transform_to_forward(spawns[0]) right = lgsvl.utils.transform_to_right(spawns[0]) up = lgsvl.utils.transform_to_up(spawns[0]) state.transform = spawns[0] ego = sim.add_agent(env.str(\"LGSVL__VEHICLE_0\", lgsvl.wise.DefaultAssets.ego_lincoln2017mkz_apollo5), lgsvl.AgentType.EGO, state) print(\"Python API Quickstart #28: How to Add/Control Charging Station\") obj_state = lgsvl.ObjectState() obj_state.transform.position = lgsvl.Vector(38, 0, 7) obj_state.transform.rotation = lgsvl.Vector(0, 180, 0) station = sim.controllable_add(\"ChargingStation\", obj_state) station = sim.get_controllable(lgsvl.Vector(38, 0, 7), \"chargingstation\") print(\"\\n# Charging Station of interest:\") print(station) seconds = 1 input(\"\\nPress Enter to run simulation for {} seconds\".format(seconds)) print(\"\\nRunning simulation for {} seconds...\".format(seconds)) sim.run(seconds) # Get current controllable state print(\"\\n# Current charging station control policy:\") print(station.control_policy) print(\"\\n# Current charging station object state\") print(station.object_state) print(\"\\n# Update charging station object state\") new_state = lgsvl.ObjectState() new_state.transform.position = lgsvl.Vector(38, 0, 7) new_state.transform.rotation = lgsvl.Vector(0, 180, 0) station.object_state = new_state print(\"\\n# New object state\") print(station.object_state) # Set time of day for light effects sim.set_time_of_day(19.0) print(sim.time_of_day) # Create a new control policy control_policy = \"on\" # Control this traffic light with a new control policy station.control(control_policy) print(\"\\n# Updated control policy:\") print(station.control_policy) # Get current state of charging station print(\"\\n# Current station state:\") print(station.current_state) print(\"\\nDone!\")","title":"Python API example"},{"location":"plugins/controllable-plugins/","text":"Controllable Plugins # Controllable plugins are custom controllables that can be added to a scene at runtime with the API. Before running the simulator (running the executable or pressing Play in the Editor) controllable plugins must be built by the simulator using Simulator -> Build... menu and the built asset placed in the AssetBundles/Controllables folder. Open-source example: TrafficCone Table of Contents Building Controllable Plugins Creating Controllable Plugins Controllable Logic Building Controllable Plugins top # Open Simulator -> Build... menu item Select controllable plugins in \"Controllables\" section of build window Build plugins with \"Build\" button The bundle named controllable_XXX will be placed in the AssetBundles/Controllables folder. If building the binary, this folder is included in the target destination. See build instructions for more details. Creating Controllable Plugins top # Create a folder in Assets/External/Controllables , e.g. Assets/External/Controllables/TrafficCone. Inside this folder you must place the controllable prefab with same name ( TrafficCone.prefab ) that will be used by simulator to instantiate at runtime, the controllable logic script and Models folder with materials and textures. This prefab must have a logic script that inherits interface IControllable and added to the root of the prefab. The controllable tag and layer for traffic cone is set to Obstacle. If collisions are desired, then add a collider components. A Rigidbody component can be added if velocity changes are desired, if not, velocity change commands will be ignored. To create a prefab: Right-click in the scene hierarchy and select Create Empty Change the name to the name of the controllable (e.g. TrafficCone ) In the Inspector for this object, select Add Component Search for the controllable script Add Rigidbody if needed Add Collider if needed Drag this object from the scene hierarchy into the project folder to create a new prefab, delete prefab in Hierarchy panel Controllable Logic top # Additionally place a C# script which will be compiled & bundled with prefab, as well as any additional Unity resources (shaders, materials, textures, etc...). This is required even for simple objects such as a Traffic Cone. Controllable scripts must inherit interface IControllable which allows controllables to receive API commands. In addition, it must implement all interface variables and methods. See the below code block from TrafficCone.cs: namespace Simulator.Controllable { public class TrafficCone : MonoBehaviour, IControllable { public bool Spawned { get; set; } public string UID { get; set; } public string GUID => UID; public string ControlType { get; set; } = \"cone\"; public string CurrentState { get; set; } public string[] ValidStates { get; set; } = new string[] { }; public string[] ValidActions { get; set; } = new string[] { }; public List<ControlAction> DefaultControlPolicy { get; set; } = new List<ControlAction> { new ControlAction { Action = \"state\", Value = \"\" } }; public List<ControlAction> CurrentControlPolicy { get; set; } private void Awake() { CurrentControlPolicy = DefaultControlPolicy; Control(CurrentControlPolicy); } protected void OnDestroy() { Resources.UnloadUnusedAssets(); } public void Control(List<ControlAction> controlActions) { foreach (var action in controlActions) { switch (action.Action) { case \"state\": CurrentState = action.Value; break; default: Debug.LogError($\"'{action.Action}' is an invalid action for '{ControlType}'\"); break; } } } } }","title":"Controllable plugins"},{"location":"plugins/controllable-plugins/#building-controllable-plugins","text":"Open Simulator -> Build... menu item Select controllable plugins in \"Controllables\" section of build window Build plugins with \"Build\" button The bundle named controllable_XXX will be placed in the AssetBundles/Controllables folder. If building the binary, this folder is included in the target destination. See build instructions for more details.","title":"Building Controllable Plugins"},{"location":"plugins/controllable-plugins/#creating-controllable-plugins","text":"Create a folder in Assets/External/Controllables , e.g. Assets/External/Controllables/TrafficCone. Inside this folder you must place the controllable prefab with same name ( TrafficCone.prefab ) that will be used by simulator to instantiate at runtime, the controllable logic script and Models folder with materials and textures. This prefab must have a logic script that inherits interface IControllable and added to the root of the prefab. The controllable tag and layer for traffic cone is set to Obstacle. If collisions are desired, then add a collider components. A Rigidbody component can be added if velocity changes are desired, if not, velocity change commands will be ignored. To create a prefab: Right-click in the scene hierarchy and select Create Empty Change the name to the name of the controllable (e.g. TrafficCone ) In the Inspector for this object, select Add Component Search for the controllable script Add Rigidbody if needed Add Collider if needed Drag this object from the scene hierarchy into the project folder to create a new prefab, delete prefab in Hierarchy panel","title":"Creating Controllable Plugins"},{"location":"plugins/controllable-plugins/#controllable-logic","text":"Additionally place a C# script which will be compiled & bundled with prefab, as well as any additional Unity resources (shaders, materials, textures, etc...). This is required even for simple objects such as a Traffic Cone. Controllable scripts must inherit interface IControllable which allows controllables to receive API commands. In addition, it must implement all interface variables and methods. See the below code block from TrafficCone.cs: namespace Simulator.Controllable { public class TrafficCone : MonoBehaviour, IControllable { public bool Spawned { get; set; } public string UID { get; set; } public string GUID => UID; public string ControlType { get; set; } = \"cone\"; public string CurrentState { get; set; } public string[] ValidStates { get; set; } = new string[] { }; public string[] ValidActions { get; set; } = new string[] { }; public List<ControlAction> DefaultControlPolicy { get; set; } = new List<ControlAction> { new ControlAction { Action = \"state\", Value = \"\" } }; public List<ControlAction> CurrentControlPolicy { get; set; } private void Awake() { CurrentControlPolicy = DefaultControlPolicy; Control(CurrentControlPolicy); } protected void OnDestroy() { Resources.UnloadUnusedAssets(); } public void Control(List<ControlAction> controlActions) { foreach (var action in controlActions) { switch (action.Action) { case \"state\": CurrentState = action.Value; break; default: Debug.LogError($\"'{action.Action}' is an invalid action for '{ControlType}'\"); break; } } } } }","title":"Controllable Logic"},{"location":"plugins/developer-debug-mode/","text":"Developer Debug Mode # Developer Debug Mode is an Unity Editor-only mode to allow for debugging of scripts inside of plugins whose code you have locally. Table of Contents Enabling Developer Debug Mode # To Enable or Disable Developer Debug Mode: Select Simulator -> Dev Debug Mode menu item inside the Unity Editor. NOTE: Developer Debug mode only works through the project in Unity, not the binary build. Using Developer Debug Mode # Once Developer Debug Mode is on, any local plugins used in a simulation you run from web user interface will load from your local code rather than the bundle downloaded from web. For more on how to create plugins in Unity: - Bridges - Controllables - NPCs - Pedestrians - Sensors In addition to not needing to upload to web user interface while making plugin changes this also allows breakpoint debugging through Visual Studio. For instructions on setting up Unity and IDEs for debugging C# Unity scripts, follow Debugging scripts guide . For more on how to set up Unity Breakpoint debugging check out here . NOTE: Two plugins with the same name will conflict over the same namespace, and once loaded into memory plugins with scripts cannot be unloaded. To switch between local plugins and those downloaded from web user interface or vice versa, exit play mode in the Unity Editor and toggle your Developer Debug Mode settings between simulation runs.","title":"Developer Debug mode"},{"location":"plugins/lidar-plugin/","text":"LiDAR Sensor Plugin # This page introduces the Velodyne LiDAR Sensor plugin, as well as how to build your own LiDAR sensor plugin. Table of Contents Velodyne LiDAR Sensor Plugin Velodyne LiDAR Sensor JSON options Velodyne LiDAR Sensor Usage Running with Autoware Running with Apollo 5.0 Build Your Own LiDAR Sensor Plugin Velodyne LiDAR Sensor Plugin top # This sensor plugin is for Velodyne LiDAR . VLP-16, VLP-32C and VLS-128 are currently supported. The built asset bundle of this plugin (named sensor_VelodyneLidarSensor ) can be found in AssetBundles/Sensors folder when you unzip the downloaded SVL Simulator (i.e. in the same level of the Simulator executable). The Velodyne LiDAR Sensor is implemented following exact intrinsics of real Velodyne LiDAR, such as elevation angles and azimuth offsets. Particularly, each laser beam in Velodyne LiDAR sensor has azimuth offset same as the real LiDAR, while the normal LiDAR Sensor assumes all laser beams are on same vertical line (i.e. no azimuth offset). In contrast to the standard LiDAR Sensor , which generates point cloud and publishes it via bridge, Velodyne LiDAR sensor generates data packets and position packets and sends them out via UDP socket. Velodyne driver running on the host machine (the machine which receives the packets) is responsible for converting these packets into point cloud and publish it out. This will greatly alleviate the burden on bridge bandwidth, so that the simulation can support more sensors (e.g. camera sensors) simultaneously. See this issue for an example of exhausted bridge bandwidth. Velodyne LiDAR Sensor JSON options top # Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF VelodyneLidarType defines type of Velodyne LiDAR String VLP_16 HostName IP address of host String UDPPortData UDP port for data packets Int 2368 UDPPortPosition UDP port for position packets Int 8308 * Most of parameters except the last four are same as LiDAR Sensor . Details of last four parameters are as follows: Value of VelodyneLidarType can only be \"VLP_16\", \"VLP_32C\" or \"VLS_128\". Note that it uses underscore ('_') not dash ('-'). HostName is the IP address of the machine which receives the UDP packets (a.k.a. host machine). UDPPortData and UDPPortPosition are UPD ports for data packets and position packets. If more than one Velodyne LiDAR plugin is used, each one should have a unique port. VerticalRayAngles , LaserCount , FieldOfView , and CenterAngle will be ignored for Velodyne LiDAR since they will be set internally following the corresponding model spec. VLP-32C configuration sample: { \"type\": \"VelodyneLidarSensor\", \"name\": \"Velodyne VLP-32C\", \"params\": { \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\", \"VelodyneLidarType\": \"VLP_32C\", \"HostName\": \"127.0.0.1\", \"UdpPortData\": 2368, \"UdpPortPosition\": 8308 }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Velodyne LiDAR Sensor Usage top # Running with Autoware top # Autoware is based on ROS. For ROS-based systems, ROS Velodyne driver can be used. Detailed steps of running ROS Velodyne driver are as follows: 1. Create a workspace folder and enter it mkdir velodyne_ws && cd velodyne_ws 2. Clone ROS Velodyne driver into src folder git clone https://github.com/ros-drivers/velodyne.git src 3. Build the Velodyne drive as a ROS node catkin_make 4. Setup running environment source /opt/$ROS_DISTRO/setup.bash source devel/setup.bash 5. Configuration of device IP Before running the Velodyne driver, you need to modify the launch files to setup device IP (i.e. the IP of the machine where the SVL Simulator is running). For VLP-16, edit velodyne_ws/src/velodyne/velodyne_pointcloud/launch/VLP16_points.launch and put the device IP after device_ip . For VLP-32, edit velodyne_ws/src/velodyne/velodyne_pointcloud/launch/VLP-32C_points.launch and put the device IP after device_ip . ROS Velodyne driver does not support VLS-128 for now. For more details please refer to the official page . 6. Launch Velodyne driver For VLP-16: roslaunch velodyne_pointcloud VLP16_points.launch For VLP-32C: roslaunch velodyne_pointcloud VLP-32C_points.launch If you have SVL Simulator running on client machine, you should be able to see UDP packets received on both data port and position port, and ROS topic /velodyne_points is published by the driver. You can also use RViz to visualize the point cloud in that topic. Fig. 1 shows point cloud of VLP-32C visualized in the simulator, Fig. 1: Visualized point clouds of VLP-32C LiDAR in SVL Simulator. and Fig. 2 shows the same point cloud visualized in RViz (click to see in full resolution): Fig. 2: Visualized point clouds of VLP-32C LiDAR in RViz. Note that the output topic name ( /velodyne_points ) of ROS Velodyne driver is hard-coded and not configurable, while Autoware assumes point cloud published into ROS topic /points_raw . To have ROS Velodyne driver running with Autoware, you have to either use <remap> tag in Autoware launch files to may /velodyne_points to /points_raw , or modify the topic name in the source code of ROS Velodyne driver and rebuild it. Running with Apollo 5.0 top # Apollo 5.0 is based on CyberRT and comes with its own Velodyne driver . Detailed steps of running ROS Velodyne driver are as follows: 1. Follow these instructions to start Apollo 5.0 and launch bridge. 2. (optional) Configure the LiDAR model if your LiDAR setting is different to the default setting of Apollo 5.0. To configure the LiDAR model, you can edit velodyne.dag file. Note that if more than one LiDAR is used, each has different data port and position port (configured in their corresponding .conf files . You need to set UDPPortData and UDPPortPosition for each Velodyne LiDAR sensor accordingly. The default launch file and dag file of Apollo 5.0 use VLS-128 and VLP-16 LiDARs. If you need to use VLP-32C, in addition to add VLP-32C to dag file, you may need to modify static_transform_conf.pb.txt to include your own VLP-32C extrinsics if you want to get compensated point cloud. 3. Launch GPS , Localization , Transform , and Velodyne modules in Module Controller page of Dreamview. Fig. 3 shows the Dreamview web interface: Fig. 3: Dreamview web interface. On the Simulator side, you can add Velodyne LiDAR sensor into our sample JSON . If you have SVL Simulator running on client machine, you should be able to see UDP packets received on both data port and position port, and cyber_monitor should shows point clouds published into corresponding Cyber channels. You can also use cyber_visualizer to visualize the point cloud in those channels. Fig. 4 shows the point cloud of VLP-32C LiDAR visualized in Cyber Visualizer (click to see in full resolution): Fig. 4: Visualized point clouds of VLP-32C LiDAR in Cyber Visualizer. Build Your Own LiDAR Sensor Plugin top # If you want to build your own LiDAR sensor plugin to support other LiDAR models, you can follow the general instructions on building sensor plugins. Instead of deriving your plugin class from SensorBase , you can derive your class from LidarSensorBase , so that you can reuse most of the code there, focusing only on raw data generation and sending.","title":"<a name=\"top\"></a> LiDAR Sensor Plugin"},{"location":"plugins/lidar-plugin/#velodyne-lidar-sensor-plugins","text":"This sensor plugin is for Velodyne LiDAR . VLP-16, VLP-32C and VLS-128 are currently supported. The built asset bundle of this plugin (named sensor_VelodyneLidarSensor ) can be found in AssetBundles/Sensors folder when you unzip the downloaded SVL Simulator (i.e. in the same level of the Simulator executable). The Velodyne LiDAR Sensor is implemented following exact intrinsics of real Velodyne LiDAR, such as elevation angles and azimuth offsets. Particularly, each laser beam in Velodyne LiDAR sensor has azimuth offset same as the real LiDAR, while the normal LiDAR Sensor assumes all laser beams are on same vertical line (i.e. no azimuth offset). In contrast to the standard LiDAR Sensor , which generates point cloud and publishes it via bridge, Velodyne LiDAR sensor generates data packets and position packets and sends them out via UDP socket. Velodyne driver running on the host machine (the machine which receives the packets) is responsible for converting these packets into point cloud and publish it out. This will greatly alleviate the burden on bridge bandwidth, so that the simulation can support more sensors (e.g. camera sensors) simultaneously. See this issue for an example of exhausted bridge bandwidth.","title":"Velodyne LiDAR Sensor Plugin"},{"location":"plugins/lidar-plugin/#velodyne-lidar-sensor-json-options","text":"Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF VelodyneLidarType defines type of Velodyne LiDAR String VLP_16 HostName IP address of host String UDPPortData UDP port for data packets Int 2368 UDPPortPosition UDP port for position packets Int 8308 * Most of parameters except the last four are same as LiDAR Sensor . Details of last four parameters are as follows: Value of VelodyneLidarType can only be \"VLP_16\", \"VLP_32C\" or \"VLS_128\". Note that it uses underscore ('_') not dash ('-'). HostName is the IP address of the machine which receives the UDP packets (a.k.a. host machine). UDPPortData and UDPPortPosition are UPD ports for data packets and position packets. If more than one Velodyne LiDAR plugin is used, each one should have a unique port. VerticalRayAngles , LaserCount , FieldOfView , and CenterAngle will be ignored for Velodyne LiDAR since they will be set internally following the corresponding model spec. VLP-32C configuration sample: { \"type\": \"VelodyneLidarSensor\", \"name\": \"Velodyne VLP-32C\", \"params\": { \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\", \"VelodyneLidarType\": \"VLP_32C\", \"HostName\": \"127.0.0.1\", \"UdpPortData\": 2368, \"UdpPortPosition\": 8308 }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Velodyne LiDAR Sensor JSON options"},{"location":"plugins/lidar-plugin/#velodyne-lidar-sensor-usage","text":"","title":"Velodyne LiDAR Sensor Usage"},{"location":"plugins/lidar-plugin/#running-with-autoware","text":"Autoware is based on ROS. For ROS-based systems, ROS Velodyne driver can be used. Detailed steps of running ROS Velodyne driver are as follows: 1. Create a workspace folder and enter it mkdir velodyne_ws && cd velodyne_ws 2. Clone ROS Velodyne driver into src folder git clone https://github.com/ros-drivers/velodyne.git src 3. Build the Velodyne drive as a ROS node catkin_make 4. Setup running environment source /opt/$ROS_DISTRO/setup.bash source devel/setup.bash 5. Configuration of device IP Before running the Velodyne driver, you need to modify the launch files to setup device IP (i.e. the IP of the machine where the SVL Simulator is running). For VLP-16, edit velodyne_ws/src/velodyne/velodyne_pointcloud/launch/VLP16_points.launch and put the device IP after device_ip . For VLP-32, edit velodyne_ws/src/velodyne/velodyne_pointcloud/launch/VLP-32C_points.launch and put the device IP after device_ip . ROS Velodyne driver does not support VLS-128 for now. For more details please refer to the official page . 6. Launch Velodyne driver For VLP-16: roslaunch velodyne_pointcloud VLP16_points.launch For VLP-32C: roslaunch velodyne_pointcloud VLP-32C_points.launch If you have SVL Simulator running on client machine, you should be able to see UDP packets received on both data port and position port, and ROS topic /velodyne_points is published by the driver. You can also use RViz to visualize the point cloud in that topic. Fig. 1 shows point cloud of VLP-32C visualized in the simulator, Fig. 1: Visualized point clouds of VLP-32C LiDAR in SVL Simulator. and Fig. 2 shows the same point cloud visualized in RViz (click to see in full resolution): Fig. 2: Visualized point clouds of VLP-32C LiDAR in RViz. Note that the output topic name ( /velodyne_points ) of ROS Velodyne driver is hard-coded and not configurable, while Autoware assumes point cloud published into ROS topic /points_raw . To have ROS Velodyne driver running with Autoware, you have to either use <remap> tag in Autoware launch files to may /velodyne_points to /points_raw , or modify the topic name in the source code of ROS Velodyne driver and rebuild it.","title":"Running with Autoware"},{"location":"plugins/lidar-plugin/#running-with-apollo-5.0","text":"Apollo 5.0 is based on CyberRT and comes with its own Velodyne driver . Detailed steps of running ROS Velodyne driver are as follows: 1. Follow these instructions to start Apollo 5.0 and launch bridge. 2. (optional) Configure the LiDAR model if your LiDAR setting is different to the default setting of Apollo 5.0. To configure the LiDAR model, you can edit velodyne.dag file. Note that if more than one LiDAR is used, each has different data port and position port (configured in their corresponding .conf files . You need to set UDPPortData and UDPPortPosition for each Velodyne LiDAR sensor accordingly. The default launch file and dag file of Apollo 5.0 use VLS-128 and VLP-16 LiDARs. If you need to use VLP-32C, in addition to add VLP-32C to dag file, you may need to modify static_transform_conf.pb.txt to include your own VLP-32C extrinsics if you want to get compensated point cloud. 3. Launch GPS , Localization , Transform , and Velodyne modules in Module Controller page of Dreamview. Fig. 3 shows the Dreamview web interface: Fig. 3: Dreamview web interface. On the Simulator side, you can add Velodyne LiDAR sensor into our sample JSON . If you have SVL Simulator running on client machine, you should be able to see UDP packets received on both data port and position port, and cyber_monitor should shows point clouds published into corresponding Cyber channels. You can also use cyber_visualizer to visualize the point cloud in those channels. Fig. 4 shows the point cloud of VLP-32C LiDAR visualized in Cyber Visualizer (click to see in full resolution): Fig. 4: Visualized point clouds of VLP-32C LiDAR in Cyber Visualizer.","title":"Running with Apollo 5.0"},{"location":"plugins/lidar-plugin/#build-your-own-lidar-sensor-plugin","text":"If you want to build your own LiDAR sensor plugin to support other LiDAR models, you can follow the general instructions on building sensor plugins. Instead of deriving your plugin class from SensorBase , you can derive your class from LidarSensorBase , so that you can reuse most of the code there, focusing only on raw data generation and sending.","title":"Build Your Own LiDAR Sensor Plugin"},{"location":"plugins/npc-plugins/","text":"NPC Plugins # Non ego traffic vehicles (NPCs) are plugins that have the ability to be customized in appearance and behavior in your simulations. This allows you to add local variations of vehicle styles and brands or implement encounters with vehicles displaying distinguished behaviors, such as erratic driving or vehicles stopping frequently (e.g., delivery vehicles, trash pick up, bicycles). NOTE: Default NPCs are no longer included in simulator source and must be built locally from source for custom binaries. See build instructions . Table of Contents NPC Models NPC Animations Building NPC Plugins NPC Behaviors Creating NPC Behavior Plugins Creating NPC Model Plugins NPC Meta Data NPC Models top # NPC Models allow customization of NPC visuals, most notably the 3D model of vehicles, but also the material used for the car paint or windows. With supporting custom NPC models we also added the support for motorbikes and vehicles with more two axles as well as more than one steering axle as is found on heavy vehicles. NPC models must include a named Collider node that is a simplified mesh to attach a Unity mesh collider. The mesh renderer is NOT active and the collider must be marked Convex. NPCs now support different RigidBody and WheelCollider settings. Users can add these components exactly like EGO vehicle creation and they will override the NPC defaults. This allows support for smaller or larger vehicles with very different physics settings. RigidBody component can be added to the root of the NPC model. RigidBody reference must be set in the inspector if it is added to the model. Wheel Collider Holder object can be added to the root of the model with child wheel collider objects, aligned at the origin. Wheel Collider Holder object reference must be set in the inspector if it is added to the model. Wheel Data must be set for each wheel if a Wheel Collider Holder object was set in the inspector. We classify NPCs into size categories which decide the spawning frequency as well as which paint colors are more common for which NPC size type. The following table lists all currently implemented size types and the weight that correlates to their relative probability of encountering this type. NPC Size Type Spawn Weight Compact 5 MidSize 6 Luxury 2 Sport 1 LightTruck 2 SUV 4 MiniVan 3 Large 2 Emergency 1 Bus 1 Trailer 0 Motorcycle 1 Bicycle 1 F1Tenth 1 In this example, trailer type has a weight of zero, indicating that it should not spawn by itself, while MidSize type has the highest weight, indicating that it is the most frequent vehicle on the road. The NPC Type is configured via a Component added to the root of the prefab . Similarly, colors for each spawned vehicle is picked from a table for each vehicle type. NPC asset bundles are detected from <simulator installation path>/AssetBundles/NPCs and added to the simulation automatically. NPC Animations top # NPC animations are supported in SVL Simulator. They must be included in the model data and require an AnimationController.controller asset. This controller is the animation state machine that is referenced in the Animator component on the model root object. In the animation controller, the animation parameter named speed must be setup and set as a transition condition. With this parameter, npc animation speed and transitions can be effected by the npc rigidbody speed. This is the only default parameter supported in the npc controller. Building NPC Plugins top # NPCs can be grouped into collections to allow you to import regional collections of vehicles. Place NPCs in Assets/External/NPCs/CollectionFolder/ in separate named folders to have all CollectionFolder vehicles show up in the build menu in a group. To build an external NPC, Open Simulator -> Build... menu item Select an NPC in the build window Click Build The bundle named npc_XXX will be placed in the AssetBundles/NPCs/CollectionFolder folder in source code. This is where simulator loads NPC bundles at runtime. To have NPCs load in a binary, they must be placed in the AssetBundles/NPCs/ folder in that binary. See build instructions for more details. NPC Behaviors top # NPC Behaviors allow NPC Vehicles to behave in custom ways different from the provided built-in NPC behavior and are enabled and configured through the Python API. Before running the simulator (running the executable or pressing Play in the Editor) NPC behavior plugins must be built by the simulator using Simulator -> Build... menu and the built asset placed in the AssetBundles/NPCs folder in the source code or binary. Open-source example: DrunkDriver Creating NPC Behavior Plugins top # Create a folder in under your collection folder for each behavior, eg. Assets/External/NPCs/BehavioursNPC/DrunkDriver/ . If your plugin is a pure behavior, that is it has no visual components, we require one C# file to have the same name as the folder under the collection folder, for example note how the name DrunkDriver repeats in both the folder name and file name: Assets/External/NPCs/BehavioursNPC/DrunkDriver/DrunkDriver.cs . The main class has to inherit from NPCBehaviourBase and implement its abstract methods to hook into the NPC simulation framework. It is also possible to inherit from NPCLaneFollowingBehaviour if you want to implement a slight modification of the default lane following behavior, such as this simple DrunkDriver example: using System.Collections; using System.Collections.Generic; using UnityEngine; using Simulator.Api; using Simulator.Map; using Simulator.Utilities; using SimpleJSON; // In this example, we try to simulate a driver under ther influence of alcohol. // We modify the default NPCLaneFollowingBehaviour because in principle this behavior // shuld just add variation the the default behavior. public class NPCDrunkDriverBehaviour : NPCLaneFollowBehaviour { public float steerCorrectionMinTime = 0.0f; public float steerCorrectionMaxTime = 0.4f; public float steerDriftMin = 0.00f; public float steerDriftMax = 0.09f; protected float currentSteerDrift = 0.0f; protected float nextSteerCorrection = 0; // This function in the base class controls the NPC steering protected override void SetTargetTurn() { // we reduce the frequency at which steering is updated to the target heading to // simulate loss of attention or reduced reaction time if(nextSteerCorrection < Time.fixedTime) { float steerCorrectionIn = RandomGenerator.NextFloat(steerCorrectionMinTime, steerCorrectionMaxTime); nextSteerCorrection = Time.fixedTime + steerCorrectionIn; // we add drift to the steering to simulate loss of fine motor skills currentSteerDrift = RandomGenerator.NextFloat(steerDriftMin, steerDriftMax); currentSteerDrift = currentSteerDrift * Mathf.Abs(RandomGenerator.NextFloat(-1.0f, 1.0f)); // we can reuse the base steering at reduced frequency base.SetTargetTurn(); } else { // steering drift correlating to driving speed currentTurn += currentSteerDrift * currentSpeed; } } } If you want to configure aspects of your NPC behavior from the Python API, you can add a class implementing the ICommand interface as shown in this example: class DrunkDriverControl : ICommand { public string Name => \"agent/drunk/config\"; public void Execute(JSONNode args) { var uid = args[\"uid\"].Value; var api = ApiManager.Instance; if (!api.Agents.TryGetValue(uid, out GameObject npc)) { api.SendError(this, $\"Agent '{uid}' not found\"); return; } var behaviour = npc.GetComponent<NPCDrunkDriverBehaviour>(); if (behaviour == null) { api.SendError(this, $\"Agent '{uid}' is not a drunk driving NPC agent\"); return; } if(args.HasKey(\"correctionMinTime\")) behaviour.steerCorrectionMinTime = args[\"correctionMinTime\"].AsFloat; if(args.HasKey(\"correctionMaxTime\")) behaviour.steerCorrectionMaxTime = args[\"correctionMaxTime\"].AsFloat; if(args.HasKey(\"steerDriftMin\")) behaviour.steerDriftMin = args[\"steerDriftMin\"].AsFloat; if(args.HasKey(\"steerDriftMax\")) behaviour.steerDriftMax = args[\"steerDriftMax\"].AsFloat; api.SendResult(this); } } example usage from Python: # common setup code omitted, check PythonAPI/quickstart for examples # you can check if it has been loaded: behaviours = sim.available_npc_behaviours for i in range(len(behaviours)): if behaviours[i][\"name\"]==\"NPCDrunkDriverBehaviour\": drunkDriverAvailable = True #later... npc = sim.add_agent(agent, lgsvl.AgentType.NPC, state) if drunkDriverAvailable: inp = input(\"make drunk driver? yN\") if (inp == \"y\" or inp == \"Y\"): npc.set_behaviour(\"NPCDrunkDriverBehaviour\") # usage of example command from C# plugin. npc.remote.command(\"agent/drunk/config\", { \"uid\": npc.uid, \"correctionMinTime\":0.0, \"correctionMaxTime\":0.6, \"steerDriftMin\": 0.00, \"steerDriftMax\":0.09}) # can still use lane following methods as those are inherited npc.follow_closest_lane(True, 5.6) Creating NPC Model Plugins top # Create a folder under your collection folder for each Model, eg. Assets/External/NPCs/YourCollection/YourNPC/ . All assets of a NPC such as textures, additional materials or scripts are expected to live inside its folder. When an NPC is spawned, a NPCController script is attached to the root of the prefab instance, and it will try to find several components to animate the vehicle. To identify the components, we look at the Name property of the Components as follows: GameObject Name Component Type Description LightHeadLeft Light Headlights which are controlled by the NPCController to off, regular, high beam etc states LightHeadRight Light Headlights which are controlled by the NPCController to off, regular, high beam etc states LightBrakeLeft Light Braking lights, controlled by the NPCController to reflect the vehicle braking LightBrakeRight Light Braking lights, controlled by the NPCController to reflect the vehicle braking IndicatorLeftFront Light Left indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorLeftRear Light Left indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorRightFront Light Right indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorRightRear Light Right indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorReverse Light Light turns on when NPC is reversing Body Renderer Main vehicle body mesh Body Material Material will have its _BaseColor changed to give each NPC a random color from a list Collider Renderer Simplified collision mesh set to convex and less than 256 polygons LightHead Material Material which is controlled to emit light in conjunction with headlight Lights LightBrake Material Material which is controlled to emit light in conjunction with brake Lights IndicatorLeft Material Material which is controlled to emit light in conjunction with left indicator Lights IndicatorRight Material Material which is controlled to emit light in conjunction with right indicator Lights IndicatorReverse Material Material which is controlled to emit light in conjunction with reversing Light Wheel Renderer Renderer is used to create WheelCollider. NPCs with less than four wheels are assumed to be bikes \"Wheel\" and \"Front\" or \"Rear\" Renderer Wheels that are supposed to turn when steering NPC Meta Data top # Additional information about the NPC prefab is added by attaching a Unity MonoBehaviour Component called NPCMetaData added to each prefab.","title":"NPC plugins"},{"location":"plugins/npc-plugins/#npc-models","text":"NPC Models allow customization of NPC visuals, most notably the 3D model of vehicles, but also the material used for the car paint or windows. With supporting custom NPC models we also added the support for motorbikes and vehicles with more two axles as well as more than one steering axle as is found on heavy vehicles. NPC models must include a named Collider node that is a simplified mesh to attach a Unity mesh collider. The mesh renderer is NOT active and the collider must be marked Convex. NPCs now support different RigidBody and WheelCollider settings. Users can add these components exactly like EGO vehicle creation and they will override the NPC defaults. This allows support for smaller or larger vehicles with very different physics settings. RigidBody component can be added to the root of the NPC model. RigidBody reference must be set in the inspector if it is added to the model. Wheel Collider Holder object can be added to the root of the model with child wheel collider objects, aligned at the origin. Wheel Collider Holder object reference must be set in the inspector if it is added to the model. Wheel Data must be set for each wheel if a Wheel Collider Holder object was set in the inspector. We classify NPCs into size categories which decide the spawning frequency as well as which paint colors are more common for which NPC size type. The following table lists all currently implemented size types and the weight that correlates to their relative probability of encountering this type. NPC Size Type Spawn Weight Compact 5 MidSize 6 Luxury 2 Sport 1 LightTruck 2 SUV 4 MiniVan 3 Large 2 Emergency 1 Bus 1 Trailer 0 Motorcycle 1 Bicycle 1 F1Tenth 1 In this example, trailer type has a weight of zero, indicating that it should not spawn by itself, while MidSize type has the highest weight, indicating that it is the most frequent vehicle on the road. The NPC Type is configured via a Component added to the root of the prefab . Similarly, colors for each spawned vehicle is picked from a table for each vehicle type. NPC asset bundles are detected from <simulator installation path>/AssetBundles/NPCs and added to the simulation automatically.","title":"NPC Models"},{"location":"plugins/npc-plugins/#npc-animations","text":"NPC animations are supported in SVL Simulator. They must be included in the model data and require an AnimationController.controller asset. This controller is the animation state machine that is referenced in the Animator component on the model root object. In the animation controller, the animation parameter named speed must be setup and set as a transition condition. With this parameter, npc animation speed and transitions can be effected by the npc rigidbody speed. This is the only default parameter supported in the npc controller.","title":"NPC Animations"},{"location":"plugins/npc-plugins/#building-npc-plugins","text":"NPCs can be grouped into collections to allow you to import regional collections of vehicles. Place NPCs in Assets/External/NPCs/CollectionFolder/ in separate named folders to have all CollectionFolder vehicles show up in the build menu in a group. To build an external NPC, Open Simulator -> Build... menu item Select an NPC in the build window Click Build The bundle named npc_XXX will be placed in the AssetBundles/NPCs/CollectionFolder folder in source code. This is where simulator loads NPC bundles at runtime. To have NPCs load in a binary, they must be placed in the AssetBundles/NPCs/ folder in that binary. See build instructions for more details.","title":"Building NPC Plugins"},{"location":"plugins/npc-plugins/#npc-behaviors","text":"NPC Behaviors allow NPC Vehicles to behave in custom ways different from the provided built-in NPC behavior and are enabled and configured through the Python API. Before running the simulator (running the executable or pressing Play in the Editor) NPC behavior plugins must be built by the simulator using Simulator -> Build... menu and the built asset placed in the AssetBundles/NPCs folder in the source code or binary. Open-source example: DrunkDriver","title":"NPC Behaviors"},{"location":"plugins/npc-plugins/#creating-npc-behavior-plugins","text":"Create a folder in under your collection folder for each behavior, eg. Assets/External/NPCs/BehavioursNPC/DrunkDriver/ . If your plugin is a pure behavior, that is it has no visual components, we require one C# file to have the same name as the folder under the collection folder, for example note how the name DrunkDriver repeats in both the folder name and file name: Assets/External/NPCs/BehavioursNPC/DrunkDriver/DrunkDriver.cs . The main class has to inherit from NPCBehaviourBase and implement its abstract methods to hook into the NPC simulation framework. It is also possible to inherit from NPCLaneFollowingBehaviour if you want to implement a slight modification of the default lane following behavior, such as this simple DrunkDriver example: using System.Collections; using System.Collections.Generic; using UnityEngine; using Simulator.Api; using Simulator.Map; using Simulator.Utilities; using SimpleJSON; // In this example, we try to simulate a driver under ther influence of alcohol. // We modify the default NPCLaneFollowingBehaviour because in principle this behavior // shuld just add variation the the default behavior. public class NPCDrunkDriverBehaviour : NPCLaneFollowBehaviour { public float steerCorrectionMinTime = 0.0f; public float steerCorrectionMaxTime = 0.4f; public float steerDriftMin = 0.00f; public float steerDriftMax = 0.09f; protected float currentSteerDrift = 0.0f; protected float nextSteerCorrection = 0; // This function in the base class controls the NPC steering protected override void SetTargetTurn() { // we reduce the frequency at which steering is updated to the target heading to // simulate loss of attention or reduced reaction time if(nextSteerCorrection < Time.fixedTime) { float steerCorrectionIn = RandomGenerator.NextFloat(steerCorrectionMinTime, steerCorrectionMaxTime); nextSteerCorrection = Time.fixedTime + steerCorrectionIn; // we add drift to the steering to simulate loss of fine motor skills currentSteerDrift = RandomGenerator.NextFloat(steerDriftMin, steerDriftMax); currentSteerDrift = currentSteerDrift * Mathf.Abs(RandomGenerator.NextFloat(-1.0f, 1.0f)); // we can reuse the base steering at reduced frequency base.SetTargetTurn(); } else { // steering drift correlating to driving speed currentTurn += currentSteerDrift * currentSpeed; } } } If you want to configure aspects of your NPC behavior from the Python API, you can add a class implementing the ICommand interface as shown in this example: class DrunkDriverControl : ICommand { public string Name => \"agent/drunk/config\"; public void Execute(JSONNode args) { var uid = args[\"uid\"].Value; var api = ApiManager.Instance; if (!api.Agents.TryGetValue(uid, out GameObject npc)) { api.SendError(this, $\"Agent '{uid}' not found\"); return; } var behaviour = npc.GetComponent<NPCDrunkDriverBehaviour>(); if (behaviour == null) { api.SendError(this, $\"Agent '{uid}' is not a drunk driving NPC agent\"); return; } if(args.HasKey(\"correctionMinTime\")) behaviour.steerCorrectionMinTime = args[\"correctionMinTime\"].AsFloat; if(args.HasKey(\"correctionMaxTime\")) behaviour.steerCorrectionMaxTime = args[\"correctionMaxTime\"].AsFloat; if(args.HasKey(\"steerDriftMin\")) behaviour.steerDriftMin = args[\"steerDriftMin\"].AsFloat; if(args.HasKey(\"steerDriftMax\")) behaviour.steerDriftMax = args[\"steerDriftMax\"].AsFloat; api.SendResult(this); } } example usage from Python: # common setup code omitted, check PythonAPI/quickstart for examples # you can check if it has been loaded: behaviours = sim.available_npc_behaviours for i in range(len(behaviours)): if behaviours[i][\"name\"]==\"NPCDrunkDriverBehaviour\": drunkDriverAvailable = True #later... npc = sim.add_agent(agent, lgsvl.AgentType.NPC, state) if drunkDriverAvailable: inp = input(\"make drunk driver? yN\") if (inp == \"y\" or inp == \"Y\"): npc.set_behaviour(\"NPCDrunkDriverBehaviour\") # usage of example command from C# plugin. npc.remote.command(\"agent/drunk/config\", { \"uid\": npc.uid, \"correctionMinTime\":0.0, \"correctionMaxTime\":0.6, \"steerDriftMin\": 0.00, \"steerDriftMax\":0.09}) # can still use lane following methods as those are inherited npc.follow_closest_lane(True, 5.6)","title":"Creating NPC Behavior Plugins"},{"location":"plugins/npc-plugins/#creating-npc-model-plugins","text":"Create a folder under your collection folder for each Model, eg. Assets/External/NPCs/YourCollection/YourNPC/ . All assets of a NPC such as textures, additional materials or scripts are expected to live inside its folder. When an NPC is spawned, a NPCController script is attached to the root of the prefab instance, and it will try to find several components to animate the vehicle. To identify the components, we look at the Name property of the Components as follows: GameObject Name Component Type Description LightHeadLeft Light Headlights which are controlled by the NPCController to off, regular, high beam etc states LightHeadRight Light Headlights which are controlled by the NPCController to off, regular, high beam etc states LightBrakeLeft Light Braking lights, controlled by the NPCController to reflect the vehicle braking LightBrakeRight Light Braking lights, controlled by the NPCController to reflect the vehicle braking IndicatorLeftFront Light Left indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorLeftRear Light Left indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorRightFront Light Right indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorRightRear Light Right indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorReverse Light Light turns on when NPC is reversing Body Renderer Main vehicle body mesh Body Material Material will have its _BaseColor changed to give each NPC a random color from a list Collider Renderer Simplified collision mesh set to convex and less than 256 polygons LightHead Material Material which is controlled to emit light in conjunction with headlight Lights LightBrake Material Material which is controlled to emit light in conjunction with brake Lights IndicatorLeft Material Material which is controlled to emit light in conjunction with left indicator Lights IndicatorRight Material Material which is controlled to emit light in conjunction with right indicator Lights IndicatorReverse Material Material which is controlled to emit light in conjunction with reversing Light Wheel Renderer Renderer is used to create WheelCollider. NPCs with less than four wheels are assumed to be bikes \"Wheel\" and \"Front\" or \"Rear\" Renderer Wheels that are supposed to turn when steering","title":"Creating NPC Model Plugins"},{"location":"plugins/npc-plugins/#npc-meta-data","text":"Additional information about the NPC prefab is added by attaching a Unity MonoBehaviour Component called NPCMetaData added to each prefab.","title":"NPC Meta Data"},{"location":"plugins/pedestrian-plugins/","text":"Pedestrian Plugins # Pedestrians are plugins that have the ability to be customized in appearance, animation and behavior in your simulations. This allows you to add variations of pedestrian types. Asset source must be cloned and built locally. See build instructions . Pedestrian asset bundles are detected from <simulator installation path>/AssetBundles/Pedestrians and must be placed there manually for custom binaries or in source code for developer mode. They are loaded automatically on editor or simulation start. Table of Contents Pedestrian Models Pedestrian Animations Pedestrian Ground Truth Building Pedestrian Plugins Pedestrian Models top # Pedestrian models allow customization of pedestrian meshes and animations. We support bipedal, and quadrupedal rigs. Create a new collection folder in Assets/External/Pedestrians/YourPedestrianCollection . Create a folder for each type of pedestrian. Place all textures, meshes, animation controller and one prefab in this folder. Be sure the prefab is named the same as the containing folder. Pedestrian Animations top # SVL Simulator supports human and bird bipedal, and quadrupedal animations. They must be included in the model data and require an AnimationController.controller asset. This controller is the animation state machine that is referenced in the Animator component on the prefab root object. In the animation controller, the animation parameter named speed must be setup and set as a transition condition. With this parameter, pedestrian animation speed and transitions, can be effected by the pedestrian rigidbody speed. This is the only default parameter supported in the pedestrian controller. Pedestrian Ground Truth top # SVL Simulator supports run-time creation of ground truth bounding boxes for pedestrians with skinned mesh renderers. Any other render will use default values for ground truth. Users must adjust the skinned mesh renderer bounds to best fit the mesh. Building Pedestrian Plugins top # Pedestrians are grouped into collections to organize each type. Place pedestrians groups in Assets/External/Pedestrians/CollectionFolder/ in separate named folders to have all CollectionFolder vehicles show up in the build menu in a group. To build a pedestrian, Open Simulator -> Build... menu item Select a pedestrian in the build window Click Build The bundle named npc_XXX will be placed in the AssetBundles/Pedestrians/CollectionFolder folder in source code. This is where simulator loads pedestrian bundles at runtime. To have pedestrians load in a binary, they must be placed in the AssetBundles/Pedestrians/ folder in that binary. See build instructions for more details.","title":"Pedestrian plugins"},{"location":"plugins/pedestrian-plugins/#pedestrian-models","text":"Pedestrian models allow customization of pedestrian meshes and animations. We support bipedal, and quadrupedal rigs. Create a new collection folder in Assets/External/Pedestrians/YourPedestrianCollection . Create a folder for each type of pedestrian. Place all textures, meshes, animation controller and one prefab in this folder. Be sure the prefab is named the same as the containing folder.","title":"Pedestrian Models"},{"location":"plugins/pedestrian-plugins/#pedestrian-animations","text":"SVL Simulator supports human and bird bipedal, and quadrupedal animations. They must be included in the model data and require an AnimationController.controller asset. This controller is the animation state machine that is referenced in the Animator component on the prefab root object. In the animation controller, the animation parameter named speed must be setup and set as a transition condition. With this parameter, pedestrian animation speed and transitions, can be effected by the pedestrian rigidbody speed. This is the only default parameter supported in the pedestrian controller.","title":"Pedestrian Animations"},{"location":"plugins/pedestrian-plugins/#pedestrian-ground-truth","text":"SVL Simulator supports run-time creation of ground truth bounding boxes for pedestrians with skinned mesh renderers. Any other render will use default values for ground truth. Users must adjust the skinned mesh renderer bounds to best fit the mesh.","title":"Pedestrian Ground Truth"},{"location":"plugins/pedestrian-plugins/#building-pedestrian-plugins","text":"Pedestrians are grouped into collections to organize each type. Place pedestrians groups in Assets/External/Pedestrians/CollectionFolder/ in separate named folders to have all CollectionFolder vehicles show up in the build menu in a group. To build a pedestrian, Open Simulator -> Build... menu item Select a pedestrian in the build window Click Build The bundle named npc_XXX will be placed in the AssetBundles/Pedestrians/CollectionFolder folder in source code. This is where simulator loads pedestrian bundles at runtime. To have pedestrians load in a binary, they must be placed in the AssetBundles/Pedestrians/ folder in that binary. See build instructions for more details.","title":"Building Pedestrian Plugins"},{"location":"plugins/sensor-plugins/","text":"Sensor Plugins # Sensor plugins are custom sensors that can be added to a vehicle configuration. Sensor plugins must be built by the simulator and the resultant bundle named sensor_XXX must be placed in the AssetBundles/Sensors folder. If running the binary, this folder is included in the downloaded .zip. If running in Editor, the sensor will be built into the folder directly. This must be done before running the simulator (running the executable or pressing Play in the Editor). The sensor can be added to a vehicle configuration just like other sensors, see here Building sensor plugins to bundle is done as below 1. Open Simulator -> Build... menu item 2. Select sensor plugins in \"Sensor\" section of build window 3. Build plugins with \"Build\" button To make sensor plugin, create folder in Assets/External/Sensors , for example Assets/External/Sensors/CustomCameraSensor . Inside this folder you must place sensor prefab with same name ( CustomCameraSensor.prefab ) that will be used by simulator to instantiate at runtime. This prefab must have the sensor script added to the root of the prefab. To create a prefab: Right-click in the scene hierarchy and select Create Empty Change the name to the name of the sensor (e.g. CustomCameraSensor ) In the Inspector for this object, select Add Component Search for the sensor script Drag this object from the scene hierarchy into the project folder Additionally you can place C# scripts which will be compiled & bundled with prefab, as well as any additional Unity resources (shaders, materials, textures, etc...). Sensor plugins must have SensorType attribute which specifies the kind of sensor being implemented as well as the type of data that the sensor sends over the bridge. In addition, it must have SensorBase as the base class and must implement the Initialize , Deinitialize , OnBridgeSetup , OnVisualize , and OnVisualizeToggle methods. Sensors can optionally include CheckVisible method to prevent NPC or Pedestrians from spawning in bounds of the sensor. If the sensor can be simulated on the client machine in a distributed simulation, see sensors distribution to learn how to configure it. See the below codeblock from the ColorCamera sensor: namespace Simulator.Sensors { // The SensorType's name will match the `type` when defining a sensor in the JSON configuration of a vehicle // The requiredType list is required if data will be sent over the bridge. It can otherwise be empty. // Publishable data types are: // CanBusData, CLockData, Detected2DObjectData, Detected3DObjectData, DetectedRadarObjectData, // GpsData, ImageData, ImuData, PointCloudData, SignalData, VehicleControlData [SensorType(\"Custom Color Camera\", new[] { typeof(ImageData)})] // Inherits Monobehavior // SensorBase also defines the parameters Name, Topic, and Frame public partial class CustomCameraSensor : SensorBase { private Camera Camera; IBridge Bridge; IWriter<ImageData> Writer; // These public variables can be set in the JSON configuration [SensorParameter] [Range(1, 128)] public int JpegQuality = 75; // By adding the AnalysisMeasurement tag, the last recorded value of this field // will be reported in the analysis tab in WISE. MeasurementType allows you to // control what units the field will be represented with [AnalysisMeasurement(MeasurementType.Fps)] public float TargetFPS = 0f; // Called to initialize the sensor plugin protected override void Initialize() { } // Called to deinitialize the sensor plugin protected override void Deinitialize() { } //Sets up the bridge to send this sensor's data public override void OnBridgeSetup(IBridge bridge) { Bridge = bridge; Writer = bridge.AddWriter<ImageData>(Topic); } // Defines how the sensor data will be visualized in the simulator public override void OnVisualize(Visualizer visualizer) { Debug.Assert(visualizer != null); visualizer.UpdateRenderTexture(Camera.activeTexture, Camera.aspect); } // Called when user toggles visibility of sensor visualization // This function needs to be implemented, but otherwise can be empty public override void OnVisualizeToggle(bool state) { } // Called when NPC and Pedestrian managers need to check if visible by sensor // camera or bounds before placing object in scene public override void CheckVisible(Bounds bounds) { var activeCameraPlanes = GeometryUtility.CalculateFrustumPlanes(Camera); return GeometryUtility.TestPlanesAABB(activeCameraPlanes, bounds); } // Sensors can implement custom analysis event callbacks for failing criteria, // collect relevant data and log it in the post simulation analysis. id should // be ego vehicle id which all sensors are childed to private void LowFPSEvent(uint id) { Hashtable data = new Hashtable { { \"Id\", id }, { \"Type\", \"LowFPS\" }, { \"Time\", SimulatorManager.Instance.GetSessionElapsedTimeSpan().ToString() }, }; SimulatorManager.Instance.AnalysisManager.AddEvent(data); } } } SensorBase in inherited from Unity's Monobehavior so any of the Messages can be used to control how and when the sensor collects data. Open-source examples are available: Comfort Sensor","title":"Sensor plugins"},{"location":"python-api/api-quickstart-descriptions/","text":"Python API Quickstart Script Descriptions # This document describes the example Python scripts that use the SVL Simulator Python API. These scripts are located here . You can find the documentation on the API here . 01-connecting-to-simulator.py : How to connect to an already running instance of the simulator and some information you can get about the instance 02-loading-scene-show-spawns.py : How to load a scene and get the scene's predefined spawn transforms 03-raycast.py : How to create an EGO vehicle and do raycasting from a point 04-ego-drive-straight.py : How to create an agent with a velocity and then run the simulator for a set amount of time 05-ego-drive-in-circle.py : How to apply control to an EGO vehicle and then run the simulator indefinitely 06-save-camera-image.py : How to save a camera image in different formats and with various settings 07-save-lidar-point-cloud.py : How to save a LiDAR point cloud 08-create-npc.py : How to create several types of NPC vehicles and spawn them in different positions 09-reset-scene.py : How to empty the scene of all EGOs, NPCs, and Pedestrians, but keep the scene loaded 10-npc-follow-the-lane.py : How to create NPCs and then let them drive in the nearest annotated lane 11-collision-callbacks.py : How to setup the simulator so that whenever the 3 created agents collide with anything, the name of the agent and the collision point is printed 12-create-npc-on-lane.py : How to create NPC vehicles in random position in a radius around the EGO vehicle, but the NPCs are placed on the nearest lane to the initial random position 13-npc-follow-waypoints.py : How to create a list of waypoints with fixed wait times and direct an NPC to follow them 14-create-pedestrians.py : How to create pedestrians in rows in front of the spawn position 15-pedestrian-walk-randomly.py : How to start and stop a pedestrian walking randomly on the sidewalk 16-pedestrian-follow-waypoints.py : How to create a list of waypoints and direct a pedestrian to follow them 17-many-pedestrians-walking.py : How to generate an army of pedestrians and have them walk back and forth 18-weather-effects.py : How to get the current weather state of the simulator and how to adjust the various settings 19-time-of-day.py : How to get the time of date in the simulator and how to set it to a fixed time and a moving time 20-enable-sensors.py : How to enable a specific sensor so that it can send data over a bridge 21-map-coordinates.py : How to convert from simulator coordinates to GPS coordinates and back. Latitude/Longitude and Northing/Easting are supported along with altitude and orientation 22-connecting-bridge.py : How to command an EGO vehicle to connect to a bridge at a specific IP address and port and then wait for the connection to be established 23-npc-callbacks.py : How to setup the simulator so that whenever an NPC reaches a stopline or changes lane, the name of the NPC is printed 24-ego-drive-straight-non-realtime.py : How to run the simulator at non-realtime. 25-waypoint-flying-npc.py : How to use waypoints to define customized motion for NPC. 26-npc-trigger-waypoints.py : How to use trigger waypoints that pause npc motion until an ego vehicle approaches. 27-control-traffic-lights.py : How to get and set the control policy of a controllable object (e.g., changing a traffic light signal) 28-control-traffic-cone.py : How to add and move a controllable object (e.g. a traffic cone) 29-add-random-agents.py : How to use random NPCs and pedestrians in a simulation 30-time-to-collision-trigger.py How to use time-to-collision triggers 31-wait-for-distance-trigger.py How to use wait-for-distance triggers 32-pedestrian-time-to-collision.py How to use time-to-collision on pedestrians 33-ego-drive-stepped.py How to run a stepped simulation using the Python Api 34-simulator-cam-set.py How to set up fixed camera positions in a simulation 35-spawn-multi-robots.py How to spawn multiple robots and connect to a bridge 36-send-destination-to-nav2.py How to send destinations of a robot to Navigation2 stack over a bridge 98-npc-behaviour.py How to get and set available NPC behaviors 99-utils-examples.py : How to use several of the utility scripts to transform an arbitrary point to the coordinate system of a local transform (relative to sensor)","title":"Python API quickstart guide"},{"location":"python-api/dreamview-api/","text":"Dreamview API # Overview top # The Dreamview API is a subpackage of the the SVL Simulator Python API which communicates with Apollo's Dreamview module. Apollo's Dreamview provides a web ui that allows users to enable/disable different modules within Apollo and to set destinations to navigate to. The Dreamview API enables users to automate this procedure with Python scripts. This document provides an overview of the API. Table of Contents Overview Requirements Usage Guide Connection Methods check_module_status disable_apollo disable_module enable_apollo enable_module get_current_map get_current_vehicle get_module_status reconnect set_destination set_hd_map set_setup_mode set_vehicle setup_apollo Example Requirements top # Installation of the Python API - requires Python 3.5 or later. Apollo 5.0 or later - instructions here . ApolloControl sensor plugin (optional) - This plugin is already included inside the SVL Simulator release build available on GitHub. For custom builds, the sensor plugin should be placed inside the AssetBundles/Sensors folder of the simulator. The sensor checks to see if Apollo is ready (sending control commands) before starting the simulation. CheckControl sensor plugin (optional) - This plugin is already included inside the SVL Simulator release build available on GitHub. For custom builds, the sensor plugin should be placed inside the AssetBundles/Sensors folder of the simulator. The sensor checks to see if Apollo is ready (sending control commands) before starting the simulation. Usage Guide top # Connection top # The main class in the Dreamview API is the Connection class that allows establishing a connection with Dreamview. To instantiate, the constructor may be called as follows: dreamview.Connection(simulator, ego_agent, ip='localhost', port='8888') where: simulator is an lgsvl.Simulator object ego_agent is an lgsvl.EgoVehicle object - this is intended to be used with a vehicle equipped with Apollo 5.0 ip : address of the machine where the Apollo stack is running (defaults to 'localhost') port : the port number for Dreamview (defaults to '8888') Methods top # check_module_status top # check_module_status(self, modules) Checks if all modules in a provided list are enabled. disable_apollo top # disable_apollo(self) Disables all Apollo modules. disable_module top # disable_module(self, module) Disables a specific Apollo module. module is the name of the Apollo 5.0 module as seen in the \"Module Controller\" tab of Dreamview. enable_apollo top # enable_apollo(self, dest_x, dest_z, modules) Enables a list of modules and then sets the destination. enable_module top # enable_module(self, module) Enables a specific Apollo module. module is the name of the Apollo 5.0 module as seen in the \"Module Controller\" tab of Dreamview. get_current_map top # get_current_map(self) Returns the current HD map loaded in Dreamview. get_current_vehicle top # get_current_vehicle(self) Returns the current Vehicle configuration loaded in Dreamview. get_module_status top # get_module_status(self) Returns a dict where the key is the name of the module and value is a bool based on the module's current status. reconnect top # reconnect(self) Closes the websocket connection and re-creates it so that data can be received again. set_destination top # set_destination(self, x_long_east, z_lat_north, y=0, coord_type=CoordType.Unity) Sends a RoutingRequest to Apollo for the provided coordinates. This function can accept a variety of Coordinate systems. If using Unity World Coordinate System: x_long_east = x z_lat_north = z y = y If using Latitude/Longitude: x_long_east = Longitude z_lat_north = Latitude If using Easting/Northing: x_long_east = Easting z_lat_north = Northing set_hd_map top # set_hd_map(self, map) Selects the provided HD map. Folders in /apollo/modules/map/data/ are the available HD maps. Map options in Dreamview are the folder names with the following changes: underscores (_) are replaced with spaces the first letter of each word is capitalized map parameter is the modified folder name. map should match one of the options in the right-most drop down in the top-right corner of Dreamview. set_setup_mode top # set_setup_mode(self, mode) mode is the name of the Apollo 5.0 mode as seen in the left-most drop down in the top-right corner of Dreamview. set_vehicle top # set_vehicle(self, vehicle, gps_offset_x=0.0, gps_offset_y=0.0, gps_offset_z=-1.348) Selects the provided vehicle configuration. Folders in /apollo/modules/calibration/data/ are the available vehicle calibrations. Vehicle options in Dreamview are the folder names with the following changes: underscores (_) are replaced with spaces the first letter of each word is capitalized vehicle parameter is the modified folder name. vehicle should match one of the options in the middle drop down in the top-right corner of Dreamview. setup_apollo top # setup_apollo(self, dest_x, dest_z, modules, default_timeout=60.0) Starts a list of Apollo modules and sets the destination. Will wait for Control module to send a message before returning. Control sending a message indicates that all modules are working and Apollo is ready to continue. Example top # This example will start a simulation that will drive an ego vehicle to a destination using Apollo. The simulation will start once Apollo is ready. To run this example you will need to have an Apollo vehicle with the ApolloControl sensor added. To add the sensor to the vehicle, add the following to the vehicle configuration JSON file: { \"type\" : \"ApolloControlSensor\", \"name\" : \"Apollo Control Sensor\", \"params\": { \"Topic\": \"/apollo/control\" } } Before running the Python script, start the Apollo docker container (see instructions ) and start dreamview and the cyber bridge using the following commands: bootstrap.sh bridge.sh You should not start any modules or interact with dreamview manually. You can view dreamview by navigating to localhost:8888 in a web browser. Now you can start an API only simulation and run the Python script below: import os import lgsvl from environs import env env = Env() sim = lgsvl.Simulator( env.str(\"LGSVL__SIMULATOR_HOST\", lgsvl.wise.SimulatorSettings.simulator_host), env.int(\"LGSVL__SIMULATOR_PORT\", lgsvl.wise.SimulatorSettings.simulator_port) ) if sim.current_scene == lgsvl.wise.DefaultAssets.map_borregasave: sim.reset() else: sim.load(lgsvl.wise.DefaultAssets.map_borregasave) spawns = sim.get_spawn() state = lgsvl.AgentState() state.transform = spawns[0] ego = sim.add_agent(lgsvl.wise.DefaultAssets.ego_lincoln2017mkz_apollo5, lgsvl.AgentType.EGO, state) ego.connect_bridge( env.str(\"LGSVL__AUTOPILOT_0_HOST\", lgsvl.wise.SimulatorSettings.bridge_host), env.int(\"LGSVL__AUTOPILOT_0_PORT\", lgsvl.wise.SimulatorSettings.bridge_port) ) # Dreamview setup dv = lgsvl.dreamview.Connection(sim, ego, env.str(\"LGSVL__AUTOPILOT_0_HOST\", \"127.0.0.1\")) dv.set_hd_map('Borregas Ave') dv.set_vehicle('Lincoln2017MKZ') modules = [ 'Localization', 'Perception', 'Transform', 'Routing', 'Prediction', 'Planning', 'Camera', 'Traffic Light', 'Control' ] destination = spawns[0].destinations[0] dv.setup_apollo(destination.position.x, destination.position.z, modules) sim.run() Upon successful execution, the Ego vehicle should navigate to a destination on the far side of the map. Note that Apollo setup can take quite a while (often up to 30 seconds) before it is ready for the simulation to be run.","title":"Dreamview API"},{"location":"python-api/dreamview-api/#overview","text":"The Dreamview API is a subpackage of the the SVL Simulator Python API which communicates with Apollo's Dreamview module. Apollo's Dreamview provides a web ui that allows users to enable/disable different modules within Apollo and to set destinations to navigate to. The Dreamview API enables users to automate this procedure with Python scripts. This document provides an overview of the API.","title":"Overview"},{"location":"python-api/dreamview-api/#requirements","text":"Installation of the Python API - requires Python 3.5 or later. Apollo 5.0 or later - instructions here . ApolloControl sensor plugin (optional) - This plugin is already included inside the SVL Simulator release build available on GitHub. For custom builds, the sensor plugin should be placed inside the AssetBundles/Sensors folder of the simulator. The sensor checks to see if Apollo is ready (sending control commands) before starting the simulation. CheckControl sensor plugin (optional) - This plugin is already included inside the SVL Simulator release build available on GitHub. For custom builds, the sensor plugin should be placed inside the AssetBundles/Sensors folder of the simulator. The sensor checks to see if Apollo is ready (sending control commands) before starting the simulation.","title":"Requirements"},{"location":"python-api/dreamview-api/#usage-guide","text":"","title":"Usage Guide"},{"location":"python-api/dreamview-api/#connection","text":"The main class in the Dreamview API is the Connection class that allows establishing a connection with Dreamview. To instantiate, the constructor may be called as follows: dreamview.Connection(simulator, ego_agent, ip='localhost', port='8888') where: simulator is an lgsvl.Simulator object ego_agent is an lgsvl.EgoVehicle object - this is intended to be used with a vehicle equipped with Apollo 5.0 ip : address of the machine where the Apollo stack is running (defaults to 'localhost') port : the port number for Dreamview (defaults to '8888')","title":"Connection"},{"location":"python-api/dreamview-api/#methods","text":"","title":"Methods"},{"location":"python-api/dreamview-api/#check_module_status","text":"check_module_status(self, modules) Checks if all modules in a provided list are enabled.","title":"check_module_status"},{"location":"python-api/dreamview-api/#disable_apollo","text":"disable_apollo(self) Disables all Apollo modules.","title":"disable_apollo"},{"location":"python-api/dreamview-api/#disable_module","text":"disable_module(self, module) Disables a specific Apollo module. module is the name of the Apollo 5.0 module as seen in the \"Module Controller\" tab of Dreamview.","title":"disable_module"},{"location":"python-api/dreamview-api/#enable_apollo","text":"enable_apollo(self, dest_x, dest_z, modules) Enables a list of modules and then sets the destination.","title":"enable_apollo"},{"location":"python-api/dreamview-api/#enable_module","text":"enable_module(self, module) Enables a specific Apollo module. module is the name of the Apollo 5.0 module as seen in the \"Module Controller\" tab of Dreamview.","title":"enable_module"},{"location":"python-api/dreamview-api/#get_current_map","text":"get_current_map(self) Returns the current HD map loaded in Dreamview.","title":"get_current_map"},{"location":"python-api/dreamview-api/#get_current_vehicle","text":"get_current_vehicle(self) Returns the current Vehicle configuration loaded in Dreamview.","title":"get_current_vehicle"},{"location":"python-api/dreamview-api/#get_module_status","text":"get_module_status(self) Returns a dict where the key is the name of the module and value is a bool based on the module's current status.","title":"get_module_status"},{"location":"python-api/dreamview-api/#reconnect","text":"reconnect(self) Closes the websocket connection and re-creates it so that data can be received again.","title":"reconnect"},{"location":"python-api/dreamview-api/#set_destination","text":"set_destination(self, x_long_east, z_lat_north, y=0, coord_type=CoordType.Unity) Sends a RoutingRequest to Apollo for the provided coordinates. This function can accept a variety of Coordinate systems. If using Unity World Coordinate System: x_long_east = x z_lat_north = z y = y If using Latitude/Longitude: x_long_east = Longitude z_lat_north = Latitude If using Easting/Northing: x_long_east = Easting z_lat_north = Northing","title":"set_destination"},{"location":"python-api/dreamview-api/#set_hd_map","text":"set_hd_map(self, map) Selects the provided HD map. Folders in /apollo/modules/map/data/ are the available HD maps. Map options in Dreamview are the folder names with the following changes: underscores (_) are replaced with spaces the first letter of each word is capitalized map parameter is the modified folder name. map should match one of the options in the right-most drop down in the top-right corner of Dreamview.","title":"set_hd_map"},{"location":"python-api/dreamview-api/#set_setup_mode","text":"set_setup_mode(self, mode) mode is the name of the Apollo 5.0 mode as seen in the left-most drop down in the top-right corner of Dreamview.","title":"set_setup_mode"},{"location":"python-api/dreamview-api/#set_vehicle","text":"set_vehicle(self, vehicle, gps_offset_x=0.0, gps_offset_y=0.0, gps_offset_z=-1.348) Selects the provided vehicle configuration. Folders in /apollo/modules/calibration/data/ are the available vehicle calibrations. Vehicle options in Dreamview are the folder names with the following changes: underscores (_) are replaced with spaces the first letter of each word is capitalized vehicle parameter is the modified folder name. vehicle should match one of the options in the middle drop down in the top-right corner of Dreamview.","title":"set_vehicle"},{"location":"python-api/dreamview-api/#setup_apollo","text":"setup_apollo(self, dest_x, dest_z, modules, default_timeout=60.0) Starts a list of Apollo modules and sets the destination. Will wait for Control module to send a message before returning. Control sending a message indicates that all modules are working and Apollo is ready to continue.","title":"setup_apollo"},{"location":"python-api/dreamview-api/#example","text":"This example will start a simulation that will drive an ego vehicle to a destination using Apollo. The simulation will start once Apollo is ready. To run this example you will need to have an Apollo vehicle with the ApolloControl sensor added. To add the sensor to the vehicle, add the following to the vehicle configuration JSON file: { \"type\" : \"ApolloControlSensor\", \"name\" : \"Apollo Control Sensor\", \"params\": { \"Topic\": \"/apollo/control\" } } Before running the Python script, start the Apollo docker container (see instructions ) and start dreamview and the cyber bridge using the following commands: bootstrap.sh bridge.sh You should not start any modules or interact with dreamview manually. You can view dreamview by navigating to localhost:8888 in a web browser. Now you can start an API only simulation and run the Python script below: import os import lgsvl from environs import env env = Env() sim = lgsvl.Simulator( env.str(\"LGSVL__SIMULATOR_HOST\", lgsvl.wise.SimulatorSettings.simulator_host), env.int(\"LGSVL__SIMULATOR_PORT\", lgsvl.wise.SimulatorSettings.simulator_port) ) if sim.current_scene == lgsvl.wise.DefaultAssets.map_borregasave: sim.reset() else: sim.load(lgsvl.wise.DefaultAssets.map_borregasave) spawns = sim.get_spawn() state = lgsvl.AgentState() state.transform = spawns[0] ego = sim.add_agent(lgsvl.wise.DefaultAssets.ego_lincoln2017mkz_apollo5, lgsvl.AgentType.EGO, state) ego.connect_bridge( env.str(\"LGSVL__AUTOPILOT_0_HOST\", lgsvl.wise.SimulatorSettings.bridge_host), env.int(\"LGSVL__AUTOPILOT_0_PORT\", lgsvl.wise.SimulatorSettings.bridge_port) ) # Dreamview setup dv = lgsvl.dreamview.Connection(sim, ego, env.str(\"LGSVL__AUTOPILOT_0_HOST\", \"127.0.0.1\")) dv.set_hd_map('Borregas Ave') dv.set_vehicle('Lincoln2017MKZ') modules = [ 'Localization', 'Perception', 'Transform', 'Routing', 'Prediction', 'Planning', 'Camera', 'Traffic Light', 'Control' ] destination = spawns[0].destinations[0] dv.setup_apollo(destination.position.x, destination.position.z, modules) sim.run() Upon successful execution, the Ego vehicle should navigate to a destination on the far side of the map. Note that Apollo setup can take quite a while (often up to 30 seconds) before it is ready for the simulation to be run.","title":"Example"},{"location":"python-api/python-api/","text":"Python API Guide # Overview top # SVL Simulator exposes runtime functionality to a Python API which you can use to manipulate object placement and vehicle movement in a loaded scene, retrieve sensor configuration and data, control weather, time state, and more. The interface to listen for incoming API calls is defined in config.yml . See Configuration File and Command-Line Parameters for more information. Table of Contents Overview Requirements Quickstart Core Concepts Simulation Non-real-time Simulation Agents EGO Vehicle NPC Vehicles Pedestrians Callbacks Agent Callbacks 'EgoVehicle NpcVehicle Callbacks Pedestrian Callbacks Sensors Camera Sensor LiDAR Sensor IMU Sensor GPS Sensor Radar Sensor CAN bus Video Recording Sensor Weather and Time of Day Control Controllable Objects Helper Functions Changelog Requirements top # Using Python API requires Python version 3.6 or later. Quickstart top # Python API is available in separate repository: https://github.com/lgsvl/PythonAPI After cloning or downloading it from the git repository follow these steps: Run the following command to install Python files and necessary dependencies: pip3 install --user -e . Now launch the simulator, either using the binary file or from Unity Editor. The simulator by default listens for connections on port 8181 on localhost . See the guide for Running SVL Simulator for more information on how to set up and run the simulator. Click the Open Browser button to open the Simulator UI, then click Sign in . Enter login credentials or click Sign Up to create an account. You will need to add assets such as maps and vehicles to your library to use in simulations. These assets can either be added from the Store or uploaded to the cloud. Quickstart scripts require the BorregasAve map and the Lincoln2017MKZ vehicle, which are included in your library by default. You will need to run a simulation using the API Only runtime template to run Python API scripts on your host machine to control the simulation. See the document on Simulations for information on how to create a simulation. Select the newly created Simulation and click the Run Simulation button. Run the following example to see the API in action: ./quickstart/05-ego-drive-in-circle.py This will load the BorregasAve.unity scene, instantiate one EGO vehicle, then ask you to press Enter to start driving EGO vehicle in a circle. When the script is running, it will apply throttle and steering commands to make the car move. Core concepts top # The Simulator and API communicate by sending json over a websocket server running on port 8181 . The API client can either be on the same machine or on any other machine on the same network. API exposes the following main types: Simulator - main object for connecting to simulator and creating other objects Agent - superclass of vehicles and pedestrian EgoVehicle - EGO vehicle with accurate physics simulation and sensors NpcVehicle - NPC vehicle with simplified physics, useful for creating many background vehicles Pedestrian - pedestrian walking on sidewalks Vehicles and Pedestrian are a subclasses of Agent which has common properties like transform , position , and velocity . All coordinates in the API return values in the Unity coordinate system. This coordinate system uses meters as a unit of distance and is a left-handed coordinate system where x points left, z points forward, and y points up. The Simulator class provides helper methods to convert coordinates to and from latitude/longitude and northing/easting values. Simulation top # To connect to the simulator you need to an instance of the Simulator class: import lgsvl sim = lgsvl.Simulator(address = \"localhost\", port = 8181) You can specify a different address as hostname or IP address. By default only port 8181 is used for API connection. Only one client can be connected to simulator at a time. Next, load the scene (sometimes called map). This is done by the load method: sim.load(scene = \"aae03d2a-b7ca-4a88-9e41-9035287a12cc\", seed = 650387) scene is a string representing the UUID of the Map in the Web UI. Scenes can be added either from the map tab under Store or by manually uploading. Some of the well-known scenes available in the Map tab under Store are: Map name UUID Description BorregasAve aae03d2a-b7ca-4a88-9e41-9035287a12cc a Digital Twin of a real-world suburban street block in Sunnyvale, CA AutonomouStuff 2aae5d39-a11c-4516-87c4-cdc9ca784551 small office parking lot in SanJose, CA Shalun 97128028-33c7-4411-b1ec-d693ed35071f an autonomous vehicle testing facility in Taiwan (More information here ) SanFrancisco 5d272540-f689-4355-83c7-03bf11b6865f a real world urban environment from San Francisco, CA GoMentum Station 979dd7f3-b25b-47f0-ab10-a6effb370138 a Digital Twin of a real-world autonomous vehicle testing facility in Concord, CA CubeTown 06773677-1ce3-492f-9fe2-b3147e126e27 a virtual environment with block obstacles used to perform basic testing of vehicles SingleLaneRoad a6e2d149-6a18-4b83-9029-4411d7b2e69a a simple two-way single-lane road Seed (optional) is an Integer (-2,147,483,648 - 2,147,483,647) that determines the \"random\" behavior of the NPC vehicles and rain effects. Check out the Store in the Web UI for list of available scenes. Once a scene is loaded you can instantiate agents and run simulations. See the Agents section on how to create vehicles and pedestrians. Loading scenes takes a while, to reset a scene to the initial state without reloading it call the reset method: sim.reset() This will remove any vehicles or callbacks currently registered. After setting up the scene in a desired state you can start advancing time. During Python code execution time is stopped in the simulator. To run the simulator in realtime, call the run method: sim.run(time_limit = 5.0) run accepts an optional argument for a time limit specifying how long to run. The default value of 0 will run infinitely. Diagram illustrating API execution: Non-real-time Simulation top # The simulator can be run at faster-than-real-time speeds depending on the performance of the computer running the simulator. This is done by calling the run method with the time_scale argument: sim.run(time_limit = 6, time_scale = 2) run takes a 2nd optional argument specifying how much faster to run. In the above example, if the computer is fast enough, the run call will finish in 3 seconds (6 divided by 2), but 6 virtual seconds of data would be generated. If only time_scale is specified or time_limit = 0, then simulation will run continuously at non-realtime speed. The value of time_scale can be lower than 1 which gives ability to run simulation in slower than real time. Agents top # You can create vehicles and pedestrians by calling the add_agent method of the Simulator object. Example: ego = sim.add_agent(name = \"511086bd-97ad-4109-b0ad-654ba662fbcf\", \\ agent_type = lgsvl.AgentType.EGO, state = None) This will create the Lincoln2017MKZ vehicle with the Keyboard Control sensor configuration. This vehicle and sensor configuration are available by default in My Library . In this instance the UUID for the desired sensor configuration is entered for the name argument. The currently available AgentTypes are: AgentType.EGO - EGO vehicle AgentType.NPC - NPC vehicle AgentType.PEDESTRIAN - pedestrian Ego agents are called by the UUIDs of their sensor configurations in the WebUI. To access the UUID of a sensor configuration click on a particular vehicle in My Library to expand the detailed view and click on the ID icon for the desired sensor configuration to copy its UUID to the clipboard. NPC agents are called by their name directly. Available NPC vehicles: Sedan SUV Jeep Hatchback SchoolBus BoxTruck Similarly, pedestrian agents are also called by their names directly. Available pedestrian types: Bob EntrepreneurFemale Howard Johny Pamela Presley Red Robin Stephen Zoe If an incorrect name is entered, a Python exception will be thrown. Optionally you can create agents in specific positions and orientations in the scene. For this you need to use the AgentState class. For example: state = lgsvl.AgentState() state.transform.position = lgsvl.Vector(10, 0, 30) state.transform.rotation.y = 90 ego = sim.add_agent(\"511086bd-97ad-4109-b0ad-654ba662fbcf\", lgsvl.AgentType.EGO, state) This will create a vehicle at position x = 10 , z = 30 which is rotated 90 degrees around the vertical axis. The position and rotation are set in the world coordinates space. You can always adjust the position, rotation, velocity and angular velocity of the agent at any later time: s = ego.state s.velocity.x = -50 ego.state = s This will set x component of velocity (in world coordinate space) to -50 meters per second and leave y and z components of velocity unmodified. All agents have the following common functionality: state - property to get or set agent state (position, velocity, ...) transform - property to get transform member of the state (shortcut for state.transform ) bounding_box - property to get bounding box in local coordinate space. Note that bounding box is not centered around (0, 0, 0) - it depends on the actual geometry of the agent. on_collision - method to set a callback function to be called when the agent collides with something (other agent or static obstacle), see callbacks section for more information. EGO vehicle top # EGO vehicle has following additional functionality: apply_control - method to apply specified throttle, break, steering or other actions to vehicle. Pass sticky=True to apply these values on every simulation update iteration. get_sensors - method to return list of Python-Api compatible sensors connect_bridge - method to connect to ROS or Cyber RT bridge bridge_connected - bool property, True if bridge is connected set_initial_pose - method to publish an initial pose of EGO vehicle to ROS set_destination - method to publish a destination pose of EGO vehicle to ROS You can control the movement of the EGO vehicle either by manually specifying state, applying manual control, or connecting through the bridge. Example to apply constant 20% throttle to EGO vehicle: ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) c = lgsvl.VehicleControl() c.throttle = 0.2 ego.apply_control(c, True) NPC vehicles top # You can create multiple NPC vehicles on the map to drive along the lanes or follow specific waypoints on the map. NPC vehicle has the following additional functionality: change_lane - method to make the vehicle change lanes follow - method to make vehicle follow specific waypoints follow_closest_lane - method to make vehicle follow lanes on_waypoint_reached - method to set callback function which is called for every waypoint the vehicle reaches on_stop_line - method to set callback function which is called when vehicle reaches a stop line at intersection on_lane_change - method to set callback function which is called when vehicle decides to change lanes You can control the movement of an NPC vehicle either by manually specifying state, or instructing it to follow waypoints or lanes. To make an NPC follow waypoints prepare a list of DriveWaypoint objects and call the follow method for the npc vehicle: npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) waypoints = [ lgsvl.DriveWaypoint(lgsvl.Vector(1,0,3), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(5,0,3), 10, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(1,0,5), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, a desired velocity in m/s, a desired angular orientation as a vector of Euler angles, an optional wait-time for the vehicle to stay idle, an bool on whether the NPC should be active while idling, and an optional trigger distance. The NPC will ignore all traffic rules and will not avoid collisions to try to get to the next waypoint. The angular orientation of the NPC will be interpolated in such a manner that it will pass through the waypoint at the angle specified in the DriveWaypoint . The trigger distance, if used, provides a method to pause the NPC until an ego vehicle approaches. The NPC will begin to drive as soon as its distance to an ego vehicle drops below the value specified as trigger distance in the DriveWaypoint . You can receive information on progress by setting the on_waypoint_reached callback. Example (see callbacks for more details): npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) def on_waypoint(agent, index): print(\"waypoint {} reached\".format(index)) npc.follow(waypoints, loop=True) npc.on_waypoint_reached(on_waypoint) sim.run() follow_closest_lane will make the NPC vehicle follow whatever lane is the closest. Upon reaching intersections it will randomly decide to either drive straight or turn. You can also spawn a pool of NPC vehicles with the same behavior as NPCs added to a non-API simulation. They will follow the map annotations, obey speed limits, obey traffic signals, and attempt to avoid accidents. These NPCs cannot be directly controlled. sim.add_random_agents(lgsvl.AgentType.NPC) Pedestrians top # You can create Pedestrian agents that will allow you to create pedestrians on sidewalks and make them walk. Pedestrians have the following additional functionality: walk_randomly - method to make pedestrian walk randomly on the sidewalk follow - method to make pedestrian follow specific waypoints on_waypoint_reached - method to set callback function which is called for every waypoint reached You can control the movement of pedestrians either by manually specifying state, or instructing them to follow waypoints or walk randomly. To make pedestrians follow waypoints prepare a list of WalkWaypoint objects and call the follow method for pedestrians: npc = sim.add_agent(\"Bob\", lgsvl.AgentType.PEDESTRIAN) waypoints = [ lgsvl.WalkWaypoint(lgsvl.Vector(1,0,3), 5, 0), lgsvl.WalkWaypoint(lgsvl.Vector(5,0,3), 10, 0), lgsvl.WalkWaypoint(lgsvl.Vector(1,0,5), 5, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, an optional idle time that the pedestrian will spend standing in-place when it reaches the waypoint, and an optional trigger distance. You can receive information on progress by setting the on_waypoint_reached callback. You can also spawn a pool of pedestrians with the same behavior as pedestrians added to a non-API simulation. They will follow the map annotations and path find. These pedestrians cannot be directly controlled. sim.add_random_agents(lgsvl.AgentType.PEDESTRIAN) Callbacks top # The Python API can invoke callbacks to inform you of specific events that occur during simulator runtime. Callbacks are invoked from inside the Simulator.run() method and while a callback is running the simulation time is paused. Once the callback finishes time is resumed and the simulation resumes execution. You can call Simulator.stop() to stop further execution and return immediately from the callback. The internals of this process are illustrated in the following sequence diagram: Here the code resumes simulation after the first callback, but stops execution when the second callback is handled. You set callback functions by calling on_NAME method of object, see information below. Agent Callbacks top # on_collision - called when agent collides with something (other agent or stationary obstacle). Example usage: def on_collision(agent1, agent2, contact): name1 = \"STATIC OBSTACLE\" if agent1 is None else agent1.name name2 = \"STATIC OBSTACLE\" if agent2 is None else agent2.name print(\"{} collided with {} at {}\".format(name1, name2, contact)) ego.on_collision(on_collision) Callback receives three arguments: (agent1, agent2, contact) - the first two are the agents that collide, one of them can be None if it is a stationary obstacle like a building or a traffic light pole, and the third is the world position of the contact point. EgoVehicle Callbacks top # In addition to Agent callbacks, EgoVehicle has one extra callback. on_custom - called when a Sensor Plugin sends a callback; accepts three arguments: (agent, kind, context) - agent instance, kind of sensor plugin as string, JSON context See Sensor Plugins for more information. NpcVehicle Callbacks top # In addition to Agent callbacks, NpcVehicle has three extra callbacks: on_waypoint_reached - called when vehicle reaches a waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer on_stop_line - called when vehicle stops at a stop line for a traffic light or stop sign; accepts one argument: (agent) - agent instance on_lane_change - called when vehicle starts changing lane; accepts one argument: (agent) - agent instance Pedestrian Callbacks top # In addition to Agent callbacks, Pedestrian has one extra callback. on_waypoint_reached - called when pedestrian reaches waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer. Sensors top # EGO vehicles have sensors attached. You can view the configuration of the sensors in the Web Ui. The following sensor classes have been defined to facilitate their use with the Python Api. These classes can only be used if the sensor configuration of the ego vehicle includes the sensor. CameraSensor - see Camera sensor LidarSensor - see LiDAR sensor ImuSensor - see IMU sensor GpsSensor - see GPS sensor RadarSensor - see Radar sensor CanBusSensor - see CAN bus sensor VideoRecordingSensor - see Video Recording sensor Each sensor has the following common members: name - name of sensor, to differentiate sensors of the same type, for example, to choose one out of multiple cameras attached to EgoVehicle transform - property that contains position and rotation of a sensor relative to the agent transform enabled - bool property, set to True if sensor is enabled for capturing and sending data to ROS or Cyber bridge Camera Sensor top # The Camera sensor has the following read only properties: frequency - rate at which images are captured & sent to ROS or Cyber bridge width - image width height - image height fov - vertical field of view in degrees near_plane - distance of near plane far_plane - distance of far plane format - format of image (\"RGB\" for 24-bit color image, \"DEPTH\" - 8-bit grayscale depth buffer, \"SEGMENTATION\" - 24-bit color image with semantic/instance segmentation) Camera image can be saved to disk by calling save : ego = sim.add_agent(\"47b529db-0593-4908-b3e7-4b24a32a0f70\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Main Camera\": sensor.save(\"main-camera.png\", compression=0) save method accepts a path relative to the running simulator, and an optional compression for png files (0...9) or quality (0..100) for jpeg files. LiDAR Sensor top # LiDAR sensor has following read only properties: min_distance - minimal distance for capturing points max_distance - maximum distance for capturing points rays - how many laser rays (vertically) to use rotations - frequency of rotation, typically 10Hz measurements - how many measurements per rotation each ray is taking fov - vertical field of view (bottom to top ray) in degrees angle - angle LiDAR is tilted (middle of fov view) compensated - bool, whether LiDAR point cloud is compensated LiDAR point cloud can be saved to disk as a .pcd file by calling save : ego = sim.add_agent(\"47b529db-0593-4908-b3e7-4b24a32a0f70\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Lidar\": sensor.save(\"lidar.pcd\") A .pcd file is in the binary Point Cloud Data format where each point has x/y/z coordinates as 4-byte floats and a 1-byte unsigned int as intensity (0...255). IMU Sensor top # You can use the IMU sensor to get its position in the vehicle. All measurements an IMU would provide can be obtained by using the transform property of the agent. GPS Sensor top # You can retrieve the current GPS location from the GPS sensor by calling data : data = gps_sensor.data() print(\"Latitude:\", data.latitude) Returned data will contain following fields: latitude longitude northing easting altitude orientation - rotation around up-axis in degrees Radar Sensor top # Currently the Radar sensor can be used only to get its position and rotation in the vehicle. Radar measurements can be received in ROS or Cyber by setting the enabled property of the sensor. CAN bus top # Currently CAN bus can be used only to get its position and rotation in the vehicle. CAN bus messages can be received in ROS or Cyber by setting the enabled property of the sensor. Video Recording Sensor top # The Video Recording sensor is used to record a video of test cases for playback afterward. The following parameters can be set to configure the video recording: width - width of the video in pixels height - height of the video in pixels framerate - the number of frames per second in the video min_distance - the minimum distance from the camera for which objects are rendered max_distance - the maximum distance from the camera for which objects are rendered fov - the vertical field of view of the camera in degrees quality - the target constant quality level for VBR rate control (0 to 51, 0 means automatic) bitrate - the average number of bits per second max_bitrate - the maximum number of bits per second Weather and Time of Day Control top # You can control the weather properties of the simulation by reading or writing to the weather property. You can set rain , fog , wetness , cloudiness , and damage (referring to road damage) as a float in 0..1 range. Example: w = sim.weather w.rain = 0.5 # set rain to 50% sim.weather = w Changing time of day allows to control whether the loaded scene appears as day or night. To get the current time read the time_of_day property: print(\"Current time of day:\", sim.time_of_day) It will return a float between 0 and 24. To set time of day call set_time_of_day : sim.set_time_of_day(10, fixed=True) This will set current time of day to 10 AM of the current date. The date and time of day are important because they determine the position of the sun and directly effect lighting in the scene. The optional bool argument fixed indicates whether the simulation should advance this time automatically or freeze it and not change it ( fixed=True ). For fine-tuned control of time of day, or to set a custom date along with the time of day call set_datetime which takes a Python datetime object as input: from datetime import datetime dt = datetime( year=2020, month=12, day=25, hour=13, minute = 0, second = 0 ) sim.set_datetime(dt) Controllable Objects top # A controllable object is an object that you can control by performing an action using Python APIs. Each controllable object has its own valid actions (e.g., green, yellow, red, trigger, wait, loop) that it can take and is controlled based on control policy , which defines rules for control actions. For example, a traffic light is a controllable object, and you can change its behavior by updating control policy: \"trigger=50;green=1;yellow=1.5;red=2;loop\" trigger=50 - Wait until an ego vehicle approaches this controllable object within 50 meters green=1 - Change current state to green and wait for 1 second yellow=1.5 - Change current state to yellow and wait for 1.5 second red=2 - Change current state to red and wait for 2 second loop - Loop over this control policy from the beginning Available controllable object types: signal cone All Controllable objects can be added or removed dynamically. When reset() is called, all Controllables are removed and the ones in the map (if any) are added back. Controllable objects can be loaded plugins at runtime. Plugin must include IControllable and be built using the Simulator build process from the Assets/External/Controllables folder. To get a list of controllable objects in a scene: controllables = sim.get_controllables() For a controllable object of interest, you can get following information: signal = controllables[0] print(\"Type:\", signal.type) print(\"Transform:\", signal.transform) print(\"Current state:\", signal.current_state) print(\"Valid actions:\", signal.valid_actions) For control policy, each controllable object always has default control policy (read-only). When you load a scene for the first time or reset a scene to the initial state, a controllable object resets current control policy to default one follows it. You can get default control policy and current control policy as follows: print(\"Default control policy:\", signal.default_control_policy) print(\"Current control policy:\", signal.control_policy) To change a current control policy, you can create a new control policy and call control function as below: control_policy = \"trigger=50;green=1;yellow=1.5;red=2;loop\" signal.control(control_policy) To add a plugin controllable and set object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0,0,0) state.transform.rotation = lgsvl.Vector(0,0,0) state.velocity = lgsvl.Vector(0,10,0) state.angular_velocity = lgsvl.Vector(6.5,0,0) cone = sim.controllable_add(\"TrafficCone\", state) To get plugin controllable object state cone.object_state To set plugin controllable object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0, 0, -10) cone.object_state = state Helper Functions top # Simulator class offers following helper functions: version - property that returns current version of simulator as string layer - property that returns all named Unity physics layers current_scene - property that returns currently loaded scene as string, None if none is loaded current_frame - property that returns currently simulated frame number as integer current_time - property that returns currently simulation time in seconds as float get_spawn - method that returns list of Spawn objects representing good positions to place vehicles in the map. This list can be empty, it depends on how the map is prepared in Unity. Returned Spawn objects contain a transform which holds position and rotation members as a Vector , as well as destinations which holds valid destination points for an ego vehicle starting at the spawn point as an array of transforms . get_agents - method that returns a list of currently available agent objects added with add_agent map_from_nav - method that maps transform (position & rotation) in Nav2 coordinates to Unity coordinates set_nav_origin - method that sets NavOrigin values in a scene get_nav_origin - method that gets NavOrigin values in a scene To map points in Unity coordinates to GPS coordinates the Simulator class offers the following two functions: map_to_gps - maps transform (position & rotation) to GPS location, returns same type as GPS Sensor data method map_from_gps - maps GPS location (latitude/longitude or northing/easting) to transform raycast - shoots a ray from specific location and returns closest object it hits map_from_gps accepts two different inputs - latitude/longitude or northing/easting. Examples: tr1 = sim.map_from_gps(latitude=10, longitude=-30) tr2 = sim.map_from_gps(northing=123455, easting=552341) Optionally you can pass altitude and orientation. raycast method can be used in following way: origin = lgsvl.Vector(10, 0, 20) direction = lgsvl.Vector(1, 0, 0) hit = sim.raycast(origin, direction, layer_mask=1) if hit: print(\"Distance right:\", hit.distance) This will shoot a ray in the positive x -axis direction from the (10, 0, 20) coordinates. A RaycastHit object with distance , point and normal fields is returned if something is hit, otherwise None is returned. When raycasting you should specify a layer_mask argument that specifies which objects to check collision with. It corresponds to layers in the Unity project - check the project for actual values or use the layer property. layer_mask Name Description 0 Default roads must use this layer* 1 TransparentFX used to ignore FX with transparency* 2 Ignore Raycast used to ignore any raycasts against* 4 Water not used* 5 UI used to cull UI in scene* 8 PostProcessing used to cull postprocessing effects 9 Agent ego vehicles 10 NPC npc vehicles 11 Pedestrian pedestrians 12 Obstacle sign poles, buildings 13 Sensor used to cull sensor effects 14 GroundTruthRange used to cull ground truth range 15 GroundTruth used to cull ground truth triggers 16 Lane used to cull lane triggers 31 SkyEffects used to cull clouds * Default Unity Physics Layers Changelog top # 2020-08-28 Added support for time-to-collision and distance-to-collision triggers for NPCs and pedestrians 2020-05-22 Added suport for setting pedestrian travel speed 2020-01-30 Extended controllable objects to support plugins - see controllable plugins 2019-09-05 Extended DriveWaypoint to support angle, idle time and trigger distance Added controllable objects - use Simulator.get_controllables 2019-08-12 Added time_scale argument to run function for running simulation in non-realtime Added seed argument to Simulator.load for deterministic NPCs 2019-04-19 initial release","title":"Python API guide"},{"location":"python-api/python-api/#overview","text":"SVL Simulator exposes runtime functionality to a Python API which you can use to manipulate object placement and vehicle movement in a loaded scene, retrieve sensor configuration and data, control weather, time state, and more. The interface to listen for incoming API calls is defined in config.yml . See Configuration File and Command-Line Parameters for more information.","title":"Overview"},{"location":"python-api/python-api/#requirements","text":"Using Python API requires Python version 3.6 or later.","title":"Requirements"},{"location":"python-api/python-api/#quickstart","text":"Python API is available in separate repository: https://github.com/lgsvl/PythonAPI After cloning or downloading it from the git repository follow these steps: Run the following command to install Python files and necessary dependencies: pip3 install --user -e . Now launch the simulator, either using the binary file or from Unity Editor. The simulator by default listens for connections on port 8181 on localhost . See the guide for Running SVL Simulator for more information on how to set up and run the simulator. Click the Open Browser button to open the Simulator UI, then click Sign in . Enter login credentials or click Sign Up to create an account. You will need to add assets such as maps and vehicles to your library to use in simulations. These assets can either be added from the Store or uploaded to the cloud. Quickstart scripts require the BorregasAve map and the Lincoln2017MKZ vehicle, which are included in your library by default. You will need to run a simulation using the API Only runtime template to run Python API scripts on your host machine to control the simulation. See the document on Simulations for information on how to create a simulation. Select the newly created Simulation and click the Run Simulation button. Run the following example to see the API in action: ./quickstart/05-ego-drive-in-circle.py This will load the BorregasAve.unity scene, instantiate one EGO vehicle, then ask you to press Enter to start driving EGO vehicle in a circle. When the script is running, it will apply throttle and steering commands to make the car move.","title":"Quickstart"},{"location":"python-api/python-api/#core-concepts","text":"The Simulator and API communicate by sending json over a websocket server running on port 8181 . The API client can either be on the same machine or on any other machine on the same network. API exposes the following main types: Simulator - main object for connecting to simulator and creating other objects Agent - superclass of vehicles and pedestrian EgoVehicle - EGO vehicle with accurate physics simulation and sensors NpcVehicle - NPC vehicle with simplified physics, useful for creating many background vehicles Pedestrian - pedestrian walking on sidewalks Vehicles and Pedestrian are a subclasses of Agent which has common properties like transform , position , and velocity . All coordinates in the API return values in the Unity coordinate system. This coordinate system uses meters as a unit of distance and is a left-handed coordinate system where x points left, z points forward, and y points up. The Simulator class provides helper methods to convert coordinates to and from latitude/longitude and northing/easting values.","title":"Core Concepts"},{"location":"python-api/python-api/#simulation","text":"To connect to the simulator you need to an instance of the Simulator class: import lgsvl sim = lgsvl.Simulator(address = \"localhost\", port = 8181) You can specify a different address as hostname or IP address. By default only port 8181 is used for API connection. Only one client can be connected to simulator at a time. Next, load the scene (sometimes called map). This is done by the load method: sim.load(scene = \"aae03d2a-b7ca-4a88-9e41-9035287a12cc\", seed = 650387) scene is a string representing the UUID of the Map in the Web UI. Scenes can be added either from the map tab under Store or by manually uploading. Some of the well-known scenes available in the Map tab under Store are: Map name UUID Description BorregasAve aae03d2a-b7ca-4a88-9e41-9035287a12cc a Digital Twin of a real-world suburban street block in Sunnyvale, CA AutonomouStuff 2aae5d39-a11c-4516-87c4-cdc9ca784551 small office parking lot in SanJose, CA Shalun 97128028-33c7-4411-b1ec-d693ed35071f an autonomous vehicle testing facility in Taiwan (More information here ) SanFrancisco 5d272540-f689-4355-83c7-03bf11b6865f a real world urban environment from San Francisco, CA GoMentum Station 979dd7f3-b25b-47f0-ab10-a6effb370138 a Digital Twin of a real-world autonomous vehicle testing facility in Concord, CA CubeTown 06773677-1ce3-492f-9fe2-b3147e126e27 a virtual environment with block obstacles used to perform basic testing of vehicles SingleLaneRoad a6e2d149-6a18-4b83-9029-4411d7b2e69a a simple two-way single-lane road Seed (optional) is an Integer (-2,147,483,648 - 2,147,483,647) that determines the \"random\" behavior of the NPC vehicles and rain effects. Check out the Store in the Web UI for list of available scenes. Once a scene is loaded you can instantiate agents and run simulations. See the Agents section on how to create vehicles and pedestrians. Loading scenes takes a while, to reset a scene to the initial state without reloading it call the reset method: sim.reset() This will remove any vehicles or callbacks currently registered. After setting up the scene in a desired state you can start advancing time. During Python code execution time is stopped in the simulator. To run the simulator in realtime, call the run method: sim.run(time_limit = 5.0) run accepts an optional argument for a time limit specifying how long to run. The default value of 0 will run infinitely. Diagram illustrating API execution:","title":"Simulation"},{"location":"python-api/python-api/#non-realtime-simulation","text":"The simulator can be run at faster-than-real-time speeds depending on the performance of the computer running the simulator. This is done by calling the run method with the time_scale argument: sim.run(time_limit = 6, time_scale = 2) run takes a 2nd optional argument specifying how much faster to run. In the above example, if the computer is fast enough, the run call will finish in 3 seconds (6 divided by 2), but 6 virtual seconds of data would be generated. If only time_scale is specified or time_limit = 0, then simulation will run continuously at non-realtime speed. The value of time_scale can be lower than 1 which gives ability to run simulation in slower than real time.","title":"Non-real-time Simulation"},{"location":"python-api/python-api/#agents","text":"You can create vehicles and pedestrians by calling the add_agent method of the Simulator object. Example: ego = sim.add_agent(name = \"511086bd-97ad-4109-b0ad-654ba662fbcf\", \\ agent_type = lgsvl.AgentType.EGO, state = None) This will create the Lincoln2017MKZ vehicle with the Keyboard Control sensor configuration. This vehicle and sensor configuration are available by default in My Library . In this instance the UUID for the desired sensor configuration is entered for the name argument. The currently available AgentTypes are: AgentType.EGO - EGO vehicle AgentType.NPC - NPC vehicle AgentType.PEDESTRIAN - pedestrian Ego agents are called by the UUIDs of their sensor configurations in the WebUI. To access the UUID of a sensor configuration click on a particular vehicle in My Library to expand the detailed view and click on the ID icon for the desired sensor configuration to copy its UUID to the clipboard. NPC agents are called by their name directly. Available NPC vehicles: Sedan SUV Jeep Hatchback SchoolBus BoxTruck Similarly, pedestrian agents are also called by their names directly. Available pedestrian types: Bob EntrepreneurFemale Howard Johny Pamela Presley Red Robin Stephen Zoe If an incorrect name is entered, a Python exception will be thrown. Optionally you can create agents in specific positions and orientations in the scene. For this you need to use the AgentState class. For example: state = lgsvl.AgentState() state.transform.position = lgsvl.Vector(10, 0, 30) state.transform.rotation.y = 90 ego = sim.add_agent(\"511086bd-97ad-4109-b0ad-654ba662fbcf\", lgsvl.AgentType.EGO, state) This will create a vehicle at position x = 10 , z = 30 which is rotated 90 degrees around the vertical axis. The position and rotation are set in the world coordinates space. You can always adjust the position, rotation, velocity and angular velocity of the agent at any later time: s = ego.state s.velocity.x = -50 ego.state = s This will set x component of velocity (in world coordinate space) to -50 meters per second and leave y and z components of velocity unmodified. All agents have the following common functionality: state - property to get or set agent state (position, velocity, ...) transform - property to get transform member of the state (shortcut for state.transform ) bounding_box - property to get bounding box in local coordinate space. Note that bounding box is not centered around (0, 0, 0) - it depends on the actual geometry of the agent. on_collision - method to set a callback function to be called when the agent collides with something (other agent or static obstacle), see callbacks section for more information.","title":"Agents"},{"location":"python-api/python-api/#ego-vehicle","text":"EGO vehicle has following additional functionality: apply_control - method to apply specified throttle, break, steering or other actions to vehicle. Pass sticky=True to apply these values on every simulation update iteration. get_sensors - method to return list of Python-Api compatible sensors connect_bridge - method to connect to ROS or Cyber RT bridge bridge_connected - bool property, True if bridge is connected set_initial_pose - method to publish an initial pose of EGO vehicle to ROS set_destination - method to publish a destination pose of EGO vehicle to ROS You can control the movement of the EGO vehicle either by manually specifying state, applying manual control, or connecting through the bridge. Example to apply constant 20% throttle to EGO vehicle: ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) c = lgsvl.VehicleControl() c.throttle = 0.2 ego.apply_control(c, True)","title":"EGO Vehicle"},{"location":"python-api/python-api/#npc-vehicles","text":"You can create multiple NPC vehicles on the map to drive along the lanes or follow specific waypoints on the map. NPC vehicle has the following additional functionality: change_lane - method to make the vehicle change lanes follow - method to make vehicle follow specific waypoints follow_closest_lane - method to make vehicle follow lanes on_waypoint_reached - method to set callback function which is called for every waypoint the vehicle reaches on_stop_line - method to set callback function which is called when vehicle reaches a stop line at intersection on_lane_change - method to set callback function which is called when vehicle decides to change lanes You can control the movement of an NPC vehicle either by manually specifying state, or instructing it to follow waypoints or lanes. To make an NPC follow waypoints prepare a list of DriveWaypoint objects and call the follow method for the npc vehicle: npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) waypoints = [ lgsvl.DriveWaypoint(lgsvl.Vector(1,0,3), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(5,0,3), 10, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(1,0,5), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, a desired velocity in m/s, a desired angular orientation as a vector of Euler angles, an optional wait-time for the vehicle to stay idle, an bool on whether the NPC should be active while idling, and an optional trigger distance. The NPC will ignore all traffic rules and will not avoid collisions to try to get to the next waypoint. The angular orientation of the NPC will be interpolated in such a manner that it will pass through the waypoint at the angle specified in the DriveWaypoint . The trigger distance, if used, provides a method to pause the NPC until an ego vehicle approaches. The NPC will begin to drive as soon as its distance to an ego vehicle drops below the value specified as trigger distance in the DriveWaypoint . You can receive information on progress by setting the on_waypoint_reached callback. Example (see callbacks for more details): npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) def on_waypoint(agent, index): print(\"waypoint {} reached\".format(index)) npc.follow(waypoints, loop=True) npc.on_waypoint_reached(on_waypoint) sim.run() follow_closest_lane will make the NPC vehicle follow whatever lane is the closest. Upon reaching intersections it will randomly decide to either drive straight or turn. You can also spawn a pool of NPC vehicles with the same behavior as NPCs added to a non-API simulation. They will follow the map annotations, obey speed limits, obey traffic signals, and attempt to avoid accidents. These NPCs cannot be directly controlled. sim.add_random_agents(lgsvl.AgentType.NPC)","title":"NPC Vehicles"},{"location":"python-api/python-api/#pedestrians","text":"You can create Pedestrian agents that will allow you to create pedestrians on sidewalks and make them walk. Pedestrians have the following additional functionality: walk_randomly - method to make pedestrian walk randomly on the sidewalk follow - method to make pedestrian follow specific waypoints on_waypoint_reached - method to set callback function which is called for every waypoint reached You can control the movement of pedestrians either by manually specifying state, or instructing them to follow waypoints or walk randomly. To make pedestrians follow waypoints prepare a list of WalkWaypoint objects and call the follow method for pedestrians: npc = sim.add_agent(\"Bob\", lgsvl.AgentType.PEDESTRIAN) waypoints = [ lgsvl.WalkWaypoint(lgsvl.Vector(1,0,3), 5, 0), lgsvl.WalkWaypoint(lgsvl.Vector(5,0,3), 10, 0), lgsvl.WalkWaypoint(lgsvl.Vector(1,0,5), 5, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, an optional idle time that the pedestrian will spend standing in-place when it reaches the waypoint, and an optional trigger distance. You can receive information on progress by setting the on_waypoint_reached callback. You can also spawn a pool of pedestrians with the same behavior as pedestrians added to a non-API simulation. They will follow the map annotations and path find. These pedestrians cannot be directly controlled. sim.add_random_agents(lgsvl.AgentType.PEDESTRIAN)","title":"Pedestrians"},{"location":"python-api/python-api/#callbacks","text":"The Python API can invoke callbacks to inform you of specific events that occur during simulator runtime. Callbacks are invoked from inside the Simulator.run() method and while a callback is running the simulation time is paused. Once the callback finishes time is resumed and the simulation resumes execution. You can call Simulator.stop() to stop further execution and return immediately from the callback. The internals of this process are illustrated in the following sequence diagram: Here the code resumes simulation after the first callback, but stops execution when the second callback is handled. You set callback functions by calling on_NAME method of object, see information below.","title":"Callbacks"},{"location":"python-api/python-api/#agent-callbacks","text":"on_collision - called when agent collides with something (other agent or stationary obstacle). Example usage: def on_collision(agent1, agent2, contact): name1 = \"STATIC OBSTACLE\" if agent1 is None else agent1.name name2 = \"STATIC OBSTACLE\" if agent2 is None else agent2.name print(\"{} collided with {} at {}\".format(name1, name2, contact)) ego.on_collision(on_collision) Callback receives three arguments: (agent1, agent2, contact) - the first two are the agents that collide, one of them can be None if it is a stationary obstacle like a building or a traffic light pole, and the third is the world position of the contact point.","title":"Agent Callbacks"},{"location":"python-api/python-api/#egovehicle-callbacks","text":"In addition to Agent callbacks, EgoVehicle has one extra callback. on_custom - called when a Sensor Plugin sends a callback; accepts three arguments: (agent, kind, context) - agent instance, kind of sensor plugin as string, JSON context See Sensor Plugins for more information.","title":"'EgoVehicle"},{"location":"python-api/python-api/#npcvehicle-callbacks","text":"In addition to Agent callbacks, NpcVehicle has three extra callbacks: on_waypoint_reached - called when vehicle reaches a waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer on_stop_line - called when vehicle stops at a stop line for a traffic light or stop sign; accepts one argument: (agent) - agent instance on_lane_change - called when vehicle starts changing lane; accepts one argument: (agent) - agent instance","title":"NpcVehicle Callbacks"},{"location":"python-api/python-api/#pedestrian-callbacks","text":"In addition to Agent callbacks, Pedestrian has one extra callback. on_waypoint_reached - called when pedestrian reaches waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer.","title":"Pedestrian Callbacks"},{"location":"python-api/python-api/#sensors","text":"EGO vehicles have sensors attached. You can view the configuration of the sensors in the Web Ui. The following sensor classes have been defined to facilitate their use with the Python Api. These classes can only be used if the sensor configuration of the ego vehicle includes the sensor. CameraSensor - see Camera sensor LidarSensor - see LiDAR sensor ImuSensor - see IMU sensor GpsSensor - see GPS sensor RadarSensor - see Radar sensor CanBusSensor - see CAN bus sensor VideoRecordingSensor - see Video Recording sensor Each sensor has the following common members: name - name of sensor, to differentiate sensors of the same type, for example, to choose one out of multiple cameras attached to EgoVehicle transform - property that contains position and rotation of a sensor relative to the agent transform enabled - bool property, set to True if sensor is enabled for capturing and sending data to ROS or Cyber bridge","title":"Sensors"},{"location":"python-api/python-api/#camera-sensor","text":"The Camera sensor has the following read only properties: frequency - rate at which images are captured & sent to ROS or Cyber bridge width - image width height - image height fov - vertical field of view in degrees near_plane - distance of near plane far_plane - distance of far plane format - format of image (\"RGB\" for 24-bit color image, \"DEPTH\" - 8-bit grayscale depth buffer, \"SEGMENTATION\" - 24-bit color image with semantic/instance segmentation) Camera image can be saved to disk by calling save : ego = sim.add_agent(\"47b529db-0593-4908-b3e7-4b24a32a0f70\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Main Camera\": sensor.save(\"main-camera.png\", compression=0) save method accepts a path relative to the running simulator, and an optional compression for png files (0...9) or quality (0..100) for jpeg files.","title":"Camera Sensor"},{"location":"python-api/python-api/#lidar-sensor","text":"LiDAR sensor has following read only properties: min_distance - minimal distance for capturing points max_distance - maximum distance for capturing points rays - how many laser rays (vertically) to use rotations - frequency of rotation, typically 10Hz measurements - how many measurements per rotation each ray is taking fov - vertical field of view (bottom to top ray) in degrees angle - angle LiDAR is tilted (middle of fov view) compensated - bool, whether LiDAR point cloud is compensated LiDAR point cloud can be saved to disk as a .pcd file by calling save : ego = sim.add_agent(\"47b529db-0593-4908-b3e7-4b24a32a0f70\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Lidar\": sensor.save(\"lidar.pcd\") A .pcd file is in the binary Point Cloud Data format where each point has x/y/z coordinates as 4-byte floats and a 1-byte unsigned int as intensity (0...255).","title":"LiDAR Sensor"},{"location":"python-api/python-api/#imu-sensor","text":"You can use the IMU sensor to get its position in the vehicle. All measurements an IMU would provide can be obtained by using the transform property of the agent.","title":"IMU Sensor"},{"location":"python-api/python-api/#gps-sensor","text":"You can retrieve the current GPS location from the GPS sensor by calling data : data = gps_sensor.data() print(\"Latitude:\", data.latitude) Returned data will contain following fields: latitude longitude northing easting altitude orientation - rotation around up-axis in degrees","title":"GPS Sensor"},{"location":"python-api/python-api/#radar-sensor","text":"Currently the Radar sensor can be used only to get its position and rotation in the vehicle. Radar measurements can be received in ROS or Cyber by setting the enabled property of the sensor.","title":"Radar Sensor"},{"location":"python-api/python-api/#can-bus","text":"Currently CAN bus can be used only to get its position and rotation in the vehicle. CAN bus messages can be received in ROS or Cyber by setting the enabled property of the sensor.","title":"CAN bus"},{"location":"python-api/python-api/#video-recording","text":"The Video Recording sensor is used to record a video of test cases for playback afterward. The following parameters can be set to configure the video recording: width - width of the video in pixels height - height of the video in pixels framerate - the number of frames per second in the video min_distance - the minimum distance from the camera for which objects are rendered max_distance - the maximum distance from the camera for which objects are rendered fov - the vertical field of view of the camera in degrees quality - the target constant quality level for VBR rate control (0 to 51, 0 means automatic) bitrate - the average number of bits per second max_bitrate - the maximum number of bits per second","title":"Video Recording Sensor"},{"location":"python-api/python-api/#weather-and-time-of-day-control","text":"You can control the weather properties of the simulation by reading or writing to the weather property. You can set rain , fog , wetness , cloudiness , and damage (referring to road damage) as a float in 0..1 range. Example: w = sim.weather w.rain = 0.5 # set rain to 50% sim.weather = w Changing time of day allows to control whether the loaded scene appears as day or night. To get the current time read the time_of_day property: print(\"Current time of day:\", sim.time_of_day) It will return a float between 0 and 24. To set time of day call set_time_of_day : sim.set_time_of_day(10, fixed=True) This will set current time of day to 10 AM of the current date. The date and time of day are important because they determine the position of the sun and directly effect lighting in the scene. The optional bool argument fixed indicates whether the simulation should advance this time automatically or freeze it and not change it ( fixed=True ). For fine-tuned control of time of day, or to set a custom date along with the time of day call set_datetime which takes a Python datetime object as input: from datetime import datetime dt = datetime( year=2020, month=12, day=25, hour=13, minute = 0, second = 0 ) sim.set_datetime(dt)","title":"Weather and Time of Day Control"},{"location":"python-api/python-api/#controllable-objects","text":"A controllable object is an object that you can control by performing an action using Python APIs. Each controllable object has its own valid actions (e.g., green, yellow, red, trigger, wait, loop) that it can take and is controlled based on control policy , which defines rules for control actions. For example, a traffic light is a controllable object, and you can change its behavior by updating control policy: \"trigger=50;green=1;yellow=1.5;red=2;loop\" trigger=50 - Wait until an ego vehicle approaches this controllable object within 50 meters green=1 - Change current state to green and wait for 1 second yellow=1.5 - Change current state to yellow and wait for 1.5 second red=2 - Change current state to red and wait for 2 second loop - Loop over this control policy from the beginning Available controllable object types: signal cone All Controllable objects can be added or removed dynamically. When reset() is called, all Controllables are removed and the ones in the map (if any) are added back. Controllable objects can be loaded plugins at runtime. Plugin must include IControllable and be built using the Simulator build process from the Assets/External/Controllables folder. To get a list of controllable objects in a scene: controllables = sim.get_controllables() For a controllable object of interest, you can get following information: signal = controllables[0] print(\"Type:\", signal.type) print(\"Transform:\", signal.transform) print(\"Current state:\", signal.current_state) print(\"Valid actions:\", signal.valid_actions) For control policy, each controllable object always has default control policy (read-only). When you load a scene for the first time or reset a scene to the initial state, a controllable object resets current control policy to default one follows it. You can get default control policy and current control policy as follows: print(\"Default control policy:\", signal.default_control_policy) print(\"Current control policy:\", signal.control_policy) To change a current control policy, you can create a new control policy and call control function as below: control_policy = \"trigger=50;green=1;yellow=1.5;red=2;loop\" signal.control(control_policy) To add a plugin controllable and set object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0,0,0) state.transform.rotation = lgsvl.Vector(0,0,0) state.velocity = lgsvl.Vector(0,10,0) state.angular_velocity = lgsvl.Vector(6.5,0,0) cone = sim.controllable_add(\"TrafficCone\", state) To get plugin controllable object state cone.object_state To set plugin controllable object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0, 0, -10) cone.object_state = state","title":"Controllable Objects"},{"location":"python-api/python-api/#helper-functions","text":"Simulator class offers following helper functions: version - property that returns current version of simulator as string layer - property that returns all named Unity physics layers current_scene - property that returns currently loaded scene as string, None if none is loaded current_frame - property that returns currently simulated frame number as integer current_time - property that returns currently simulation time in seconds as float get_spawn - method that returns list of Spawn objects representing good positions to place vehicles in the map. This list can be empty, it depends on how the map is prepared in Unity. Returned Spawn objects contain a transform which holds position and rotation members as a Vector , as well as destinations which holds valid destination points for an ego vehicle starting at the spawn point as an array of transforms . get_agents - method that returns a list of currently available agent objects added with add_agent map_from_nav - method that maps transform (position & rotation) in Nav2 coordinates to Unity coordinates set_nav_origin - method that sets NavOrigin values in a scene get_nav_origin - method that gets NavOrigin values in a scene To map points in Unity coordinates to GPS coordinates the Simulator class offers the following two functions: map_to_gps - maps transform (position & rotation) to GPS location, returns same type as GPS Sensor data method map_from_gps - maps GPS location (latitude/longitude or northing/easting) to transform raycast - shoots a ray from specific location and returns closest object it hits map_from_gps accepts two different inputs - latitude/longitude or northing/easting. Examples: tr1 = sim.map_from_gps(latitude=10, longitude=-30) tr2 = sim.map_from_gps(northing=123455, easting=552341) Optionally you can pass altitude and orientation. raycast method can be used in following way: origin = lgsvl.Vector(10, 0, 20) direction = lgsvl.Vector(1, 0, 0) hit = sim.raycast(origin, direction, layer_mask=1) if hit: print(\"Distance right:\", hit.distance) This will shoot a ray in the positive x -axis direction from the (10, 0, 20) coordinates. A RaycastHit object with distance , point and normal fields is returned if something is hit, otherwise None is returned. When raycasting you should specify a layer_mask argument that specifies which objects to check collision with. It corresponds to layers in the Unity project - check the project for actual values or use the layer property. layer_mask Name Description 0 Default roads must use this layer* 1 TransparentFX used to ignore FX with transparency* 2 Ignore Raycast used to ignore any raycasts against* 4 Water not used* 5 UI used to cull UI in scene* 8 PostProcessing used to cull postprocessing effects 9 Agent ego vehicles 10 NPC npc vehicles 11 Pedestrian pedestrians 12 Obstacle sign poles, buildings 13 Sensor used to cull sensor effects 14 GroundTruthRange used to cull ground truth range 15 GroundTruth used to cull ground truth triggers 16 Lane used to cull lane triggers 31 SkyEffects used to cull clouds * Default Unity Physics Layers","title":"Helper Functions"},{"location":"python-api/python-api/#changelog","text":"2020-08-28 Added support for time-to-collision and distance-to-collision triggers for NPCs and pedestrians 2020-05-22 Added suport for setting pedestrian travel speed 2020-01-30 Extended controllable objects to support plugins - see controllable plugins 2019-09-05 Extended DriveWaypoint to support angle, idle time and trigger distance Added controllable objects - use Simulator.get_controllables 2019-08-12 Added time_scale argument to run function for running simulation in non-realtime Added seed argument to Simulator.load for deterministic NPCs 2019-04-19 initial release","title":"Changelog"},{"location":"release-notes/changelog/","text":"Changelog # All notable changes and release notes for SVL Simulator will be documented in this file. [2021.3] - 2021-09-31 # Added # Content creation: Annotation size support Map preview option can be set by user SDF importer and STL fixes and improvements Lighting for indoor environments Rain colliders for indoor environments New indoor environments with costmaps : Warehouse , WarehouseDT Add map support for NavOrigin Sensors: Sensors support multiple data feeds, e.g. RealSenseSensor Async GPU readback support for Lidar, DirectionalLidar, VelodyneLidar for performance boost LaserScan support for Lidar2D 2D/3D Ground Truth sensor accuracy improvements New sensors: Lidar2DSensor , DestinationSensor , IntelRealSense Agents: NPCManager extension for creating complex NPC behaviors like Parking NPC support customized rigidbody and wheel collider settings: F1Tenth New pedestrian models: See VRU/Walkers: Jun, Yun General: Loaded Assets main menu option for assets in memory Docker support for enabling scenario runners on windows Console for Docker logs Graphics setting menu Main Menu simulator/unity/webUrl display for debugging Added TargetGear for Autoware Auto reverse driving Visual scenario editor: Ego Lane Following feature for VSE playback Support for the BezierSpline paths in the waypoints Support for the NavOrigin Added acceleration support for agents' waypoints Python API: Add Python API commands for Destination sensor API function to set/get NavOrigin in/from scene API versioning support Web user interface: Local Automation support Local automation API token access token creation, listing, and deletion in User Profile page Added preservation of the order of sensors in a sensor configuration Added 'Add to Library' modal preview Added preview images for private asset links in Skype, Facebook, LinkedIn, etc. Added preview images for Simulation links in Skype, Facebook, LinkedIn, etc. Implemented accurate trending algorithm 'Draft' feature for Simulation creation page Improved Simulations gallery loading performance Changed # Moved simulator input actions to sensor bundles Visual scenario editor: Smoother agent rotation while in playback mode Fix for editing controllables policy and resetting map signals policies Miscellaneous Github issue fixes: API connection fixes Bridge connection fixes Custom Bridge fixes Point Cloud renderer fixes Add checks for pedestrian and traffic annotation for api random agents Development Settings fixes and improvements. Supports cloud assets Import HDMap tool fixes BaseLink fixes Radar velocity fix Build asset fixes NPC segmentation id issues in API Fixed OpenDrive MapOrigin export logic Fix export point cloud feature in simulator editor. Requires latest Lidar Sensor OpenDrive, Apollo Lane, Sign and Signal IDs import & export Refresh cached sims on deleted asset NPC/Pedestrian pool no longer removes existing agents Fixed NPC controller stopping all coroutines on the initialization Fix for Input package error on lost focus Correct name of Cyber bridge asmdef file Fixed initializing spawns manager if there are no active agents Bridge client watchdog coroutine Fix lighting on API agent spawn Removed NavSystem from pedestrian waypoint behavior Fixed invariant float and double parameters in NMEA Fixed SF, Shalun, Highway101 map origin rotation issues Package updates: Update WebSocketSharp Added more debugging logs for FBX2glTF Remove old input unity work arounds for Linux Extracted Pedestrians' behaviours from the PedestrianController Remove Apollo 3.0 support Adding detail map and albedo tint to environment shader Web user interface changes: Updated User Content License to User General Content License User Premium Content license introduction and sort order change Improved asset update progress indicator Improved asset upload error message displays and language Fixed Sharing window issue and Test Report snapshot issue with APIOnly simulations Fixed version compatibility validation when simulation does not have cluster assigned Plugin preview image improvement Fixed simulation assets dropdown when there are more than 30 assets in Library Fixed asset previews in Skype and Facebook Removed markdown markup language from descriptions in link previews Increased size restriction for asset bundle Fixed critical bug related to vehicle asset not being found when sensor configuration's plugin was deleted Fixed critical bug related to asset deletion [2021.2.2] - 2021-08-18 # Added # Support for indoor robotics and Nav2 navigation stack Sample indoor environment for robots: LGSeocho Sample two-wheel drive robot model: LGCLOi DifferentialDriveControl sensor to control two-wheel drive robots in ROS/ROS 2 Support for Unity's ArticulationBody for ego vehicles (robot models) Changed # Fixed error reporting for bridge connection Fixed baselink offset in sensor visualizer for sensor configurations Added Development as version for custom builds or when running in Developer Mode Fixed NPC vehicle and pedestrian spawn position issue [2021.2.1] - 2021-07-23 # Changed # Reduce duplicate bridge messages by only loading one bridge in Developer Mode Add log about which assembly a sensor was loaded from to help debug bridges Create folder when user defined data directory path points to absent folder (not recursively) Update default data folder name to \u201cSVLSimulator-\\<version>\u201d Fix empty version issue by providing fallback simulator version Fix to allow different versions of simulator to connect to cloud Support bridge type attribute in manifest Remove error when disconnecting from cloud (WebException: task was cancelled) [2021.2] - 2021-06-30 # Added # External Pedestrians and NPC (other traffic vehicles) with animation support New NPCs and Pedestrians like Scooters, Bicyclists and Animals External Bridge Support Multiple simulator asset bundle versions support Rain Collision effect Support for simulations without EGO ( Spectator Ego ) New \u201cDriver view\u201d camera state New API to set simulator camera state Annotation tool to set accurate values (position, rotation and scale) for MapOrigin New \u201cclear cache\u201d option Ability to use proxy server for cloud_url in config.yml New enhanced web user interface: Improved visual sensor editor Ability to clone Sensors in sensor configuration User warning messages for privilege based restriction Markdown support for Asset description Changed # Unity Upgrade to 2020.3.3f1 Revert FBX package to 3.2.1-preview.2 Remove overdraw with HDRP flare Improved NPC handling: Fixed rotating NPC wheels when using NPCWaypointBehaviour Pedestrian ground truth from bounds Improvements and bug fixes around Distributed Simulation with multi-ego support: Fixed API mode for distributed simulation on a single machine Fixed pedestrian animations on client machine Improved distributing VehicleActions Support for the ArticulationBody in distributed simulations Improved load balancing of the sensors Updated Sensor support: Fix TF visual and sensor hierarchy update input for g920 wheel for windows Publish camera intrinsic parameters to ROS and ROS2 Rework lidar to use cubemap Improved lighting and rain: Fix brown lighting Rotate sun and moon to match map rotation Rain collision improvements and fixes Fix overlapping rain chunks Enhanced Visual Scenario Editor support: Minor UX fixes Fixed changing agents' variants Fix issue with agent dropdown not refreshing on click Loading pedestrians from config Improvements around HD Map Mesh Generator : Improve data handling in HD map mesh builder Erosion and re-leveling for point cloud mesh Work in progress improvements on Developer mode: Rework developer settings Bundle version uptick and unity editor check on developer debug mode for bridges Sensor debug developer loading More enhanced and user friendly web user interface: Password Sensitivity improvements GLTF editor bug fixes and improvements HD map preview improvements Simulation invalid items management WCAG accessibility UI compatibility Live Test Results view in Simulation Profile Updated algorithm for Trending assets view Simulation form auto-save Misc Improvements: Support default light layers in point cloud renderer Loads assembly with the ego name from asset bundle Added public driver view transform to be set in the Inspector Asmdef added, loader fixed to deal with bundle/WISE name mismatch Loader: improve the error message about BundleFormat Make sure Simulator.Editor.Build.Run exits on any exception Trimming last slash in the cloud url if needed. get full path from data path provided [2021.1.1] - 2021-05-20 # Added # 3D Visualizer to help create sensor configuration on a vehicle HD map visualizer on map profile pages Ability to share test reports with other users \"Supported Simulator versions\" value on asset profile page Improved sharing feature for private assets and simulations Save and Exit button on sensor configuration page Tutorials to help new users on how to interact with the web user interface pages Email confirmation upon sensor plugin sharing Changed # Improved quick filters on Store and Library pages Redirect to existing asset profile page when uploading an already uploaded asset Request access button on private plugin pages Require adding a vehicle sensor configuration when creating a simulation Fixed issues with multi-vehicle simulation using same vehicle and sensor configuration Improvements to cluster page user experience Show proper names for missing sensors on invalid simulations Proper error messages for wrong information on registration page Proper error messages when maximum allowed cluster limit is reached [2021.1] - 2021-03-23 # Added # New cloud-based web user interface including: Asset Store for downloading assets, uploading and sharing assets publicly user account password management user assets management library support for private assets private assets/simulations sharing with registered users cluster/Simulation status handling SVL Simulator content from this release will be available in the Asset Store New and updated runtime templates for simulations with automatic bringup and execution: Random Traffic Visual Scenario Editor Python API API Only Visual Scenario Editor for creating scenarios including support for: traffic vehicle and pedestrian waypoints previews copy/paste/undo feature ability to specify ego vehicle target destination support for controllables with ability to edit default policy color selection for actors waypoint positions snapped to HD map lanes support for distance, time, and time-to-collision triggers traffic lights control Automatically generated test case reports of simulations including: success/failure scenario evaluation simulation statistics analytics data per sensor video recording of simulation multiple violation event detection: collision, stopline, stopsign, ego-stuck, sudden jerk/brake/turn, speed-limit, etc. PythonAPI: many improvements including: added cloudiness and road damage to weather parameters ability to specify date in environment configuration added Dreamview API as sub-module added callback agents_traversed_waypoints added ability to set speed at WalkWaypoints added waypoint trigger system ability to set simulator camera free-state position and rotation sample Python API scripts based on NHTSA-defined scenarios for end-to-end testing additional Python API test samples: random-traffic mode, cut-in, sudden-braking, etc. Environment configuration improvements: moon position in nighttime environment configurations Improved environment road wetness layer UI for FPS and SIM time Improved rain particles Increased types of traffic lights, model improvements Improvements in Developer Mode : developer settings to use ego vehicle within Developer Mode ability to select and add actors to simulation in Developer Settings added sensor debug mode added tool to rotate Editor scene view to -X north Features for traffic agents in simulation: support for bicyclist traffic agents support for custom traffic vehicle behavior plugins pedestrian crosswalk support with yield for traffic vehicles (SanFrancisco map) added direction for pedestrian crosswalk annotation tool spawning NPCs now raycasts target bounds from egos' positions New map/environment creation improvements: default ego destination points for LG-provided maps HD map road network environment generation tool test maps for NHTSA-like test cases: Straight1LanePedestrianCrosswalk, Straight1LaneSame, etc. General simulation improvements: ability to create and run a Distributed Simulation from web user interface offline mode for running saved simulation configurations improved cache control for downloaded assets Sensors improvements: new sensors : StopLine, HUDKeyboardControl, AutowareAIControl, LGSVLControl, etc. improved LiDAR simulation performance sun-flare, raindrops, greyscale, and video artifacts effects for sensors support for sensor post-distortion effects Default values for simple and complex sensor parameter types Enabled sensors to load texture assets Improvements around support for AD stacks: Optimized ROS2 interface and bridge Support for Apollo 6.0 and master Changed # Product branding changed from \"LGSVL Simulator\" to \"SVL Simulator\" Old local web user interface is now deprecated. Users need to migrate to new web user interface Old content website will be deprecated All content now managed on cloud - downloaded as needed by simulation configuration Unity update to 2019.4.18f1 All sensors are now sensor plugins - default sensors are part of user plugin library Sensor plugins no longer bundled with simulator executable Distributed simulation: fixed pedestrian movement animation bug Distributed simulation: video artifact fix Fixed wrong timestamp error in clock and IMU sensors Fix acceleration/angular velocity in Apollo imu/corrected_imu Fixed way of handling failed downloads Fixed bugs and issues in OpenDrive HD map importer and exporter Point cloud rendering: added alternative method for removing obscured points (reduces flickering, requires depth prepass) Point cloud rendering: Fill scarce depth map holes through neighborhood sampling Point Cloud Rendering: Fix skybox blending in unlit point cloud Improved traffic lights in environments Rain/clouds/lights improvements NPC spawn logic improvements Fix old NPC models and prefab Adjust camera distance from ego bounds HD map import: Fix keyError in lanelet2 and improve grouping lanes at intersections HD map import: Fix map origin update during OpenDrive map import [2020.06] - 2020-08-31 # Added # Support modular testing of planning with Apollo 5.0 Bridge plugin system with example ( LoggingBridge ) Added default ultrasonic distance sensor Sensor TF visualizer in sensor visualizations Added button in map annotation tool to add synthetic lane boundary lines automatically Python API: added callback agents_traversed_waypoints Python API: added ability to set speed at WalkWaypoints Python API: Added waypoint trigger system Changed # Fixed 90 degree orientation offset in GPS odometry sensor Replaced default ROS2 bridge to improved custom ROS2 bridge instead of ros2-web-bridge Unity update to 2019.3.15f1 Fixed timestamps in GPS, IMU Added timestamp to default LiDAR sensor PointCloud message Fixed bugs in OpenDrive map importer to correctly compute reference lines and lane widths for edge cases Improved OpenDrive map exporter to export parametric cubic curves instead of simple lines for each reference line segment [2020.05] - 2020-06-02 # Added # Support for custom traffic vehicle behavior plugins Support for Apollo 5.5 ( latest master ) Support for distributed simulation Velodyne VLS-128 support Python API function to spawn random non-ego vehicles and pedestrians Unlit shadows support for point cloud environment rendering AD system-agnostic message definitions to \"lgsvl_msgs\" for vehicle control, CAN bus and status data, etc. Changed # Refactored traffic agent Waypoint Mode with fixes for idle and trigger waypoints Changed gnss_odom message coordinate system to right-handed coordinate system Fixed \"ignore map origin\" option for gnss_odom message Fixed acceleration values in IMU sensor Fix sky flickering issue during point cloud rendering Fixed LiDAR readback error Removed and replaced documentation source files from main repository to separate Documentation repository [2020.03] - 2020-04-03 # Added # Velodyne VLP-16 and VLP-32C support through custom LiDAR sensor plugin Support for LiDAR sensor plugin with direct UDP socket interface Support for point cloud environment rendering Extensible vehicle dynamics Functional Mockup Interface 2.0 (FMI 2.0) interface for vehicle dynamics Support for fisheye camera lens distortion Support for sensor_msgs/NavSatFix message type for GPS messages in ROS2 Vehicle Odometry sensor for reporting steering angles and velocity in ROS2 Changed # Upgraded to Unity 2019.3.3f1 Fixed black artifacts in Linux bug Fixed display issues in Unity Editor bug Fixed camera sensor invalid data bug 3D Ground Truth sensor now generates different ID for vehicle after each respawn [2020.01] - 2020-01-31 # Added # Controllable Object plugin support, dynamically add and control Controllables with the Python API Ability to create LiDAR sensor with non-uniform beam distribution Ability to add distortion effects to Camera sensors Support for Autoware.Auto messages (ROS2) Ability to create custom message types for sensor plugins TransformSensor which can act as a parent for other sensors VehicleStateSensor which is needed by Autoware.Auto Changed # Added SanFrancisco map as a default map option Allow null texture bundles without an error Pedestrian mass reduced to 70kg Fixed lagging of 2D Ground Truth boxes (in visualization and data) ComfortSensor included with Simulator binary TrafficCone included with Simulator binary Fixed exported HD map lane relations Fixed ROS2 GPS [2019.12] - 2020-01-21 # Added # Custom callback for Python API, useful in sensor plugins. Ability to specify count and types of NPCs to spawn on the map. Changed # Optimized texture usage in maps - significantly reduces size on disk and GPU memory used. Allow boundary type to be optional for Apollo HD map import. Fix Python API to be able load maps & vehicles that are shared between multiple users. Separated bundle file format version for map and vehicle bundles. Improved headlights for EGO vehicles. Improved HD map annotation icons. Fixed Sedan NPC mesh. [2019.11] - 2019-11-19 # Added # OpenDrive 1.4 HD map import. Rain drops on ground when it is raining. Separated Apollo HD map export between version 3.0 and 5.0 to support 5.0 specific features. Cache vehicle prefabs when loading same vehicle multiple times into simulation. Ability to login to account via command-line. Ability for vehicle to have multiple interior lights. Fixes #474. Allow Color, Depth and Semantic cameras to have higher capture & publish rate. Building & loading custom sensors as plugins. Changed # Fixed traffic lights signal colors on Shalun map. Fixed exceptions when NPCs are despawned while still in intersection. Fixed errors when adding pedestrian to map without NavMesh. Fixed Lanelet2 boundary import and export. Multiple fixes for OpenDrive import and export. Fixed wrong raycast layers in 2D Ground Truth sensor to detect if NPC is visible. Fixed missing timestamp when publishing ROS/Cyber messages from 2D Ground Truth sensor. Limit number of spawned NPCs and vehicles for large maps. [2019.10] - 2019-10-28 # Added # Apollo HD map import Accurate sun position in sky based of map location. Including time of sunrise and sunset. Control calibration sensor to help calibrating AD stack control. Ported Shalun map from previous non-HDRP simulator version. Ground Truth sensor for traffic light. Python API method to get controllable object by position. Python API method to convert multiple map coordinates in single call. Python API method to perform multiple ray-casts in single call. Sensor for controlling vehicle with steering wheel (Logitech G920). Platform independent asset bundles (Windows and Linux). Allow to set custom data path for database and downloaded bundles. Visualize data values for non-image senors (GPS, IMU, etc). Populate scene with required objects when new scene is created in Unity Editor. Changed # Fixed exceptions in ROS Bridge where if it receives message on topic that it has not subscribed. Fixed 3D Ground Truth sensor to report correct NPC orientation angles. Fixed Radar sensor to visualize pedestrians. Fixed Color camera to render mountains in BorregaAve. Fixed EGO vehicle collision callback to Python API. Fixed WebUI redirect loop that happens if you are logged out. Fixed reported NPC vehicle speed. Fixes #347 and #317. Fixed gear shifting for EGO vehicle control. Fixes #389. Fixed NPC waypoint following where NPCs stopped if assigned speed is too low. Fixed semantic segmentation for vehicles and pedestrians added with Python API. Fixed ROS2 message publishing (seq field was missing in Header). Fixes #413. Fixed issue with database on some non-English locales. Fixes #381. Fixed point cloud generation in Unity Editor. Fixed browser loosing cookie when session ends in WebUI. Fixed slowness in Python API when running without access to Internet. Fixed issue when multiple users could not use same map url. Improved error messages when simulation fails to start. [2019.09] - 2019-09-06 # Added # Sensor visualization UI HD map export to OpenDrive 1.4 format ROS service support for ROS bridge Python API to support more robust waypoints for NPC vehicles Python API with ability to control traffic lights on map Hyundai Nexo vehicle Changed # Improved NPC movement and right turns on red traffic light Fixed NPC vehicle despawning logic so they don't get stuck in intersections Change NPC vehicles colliders from box to mesh to improves collision precision Updated generated protobuf message classes for latest Apollo 5.0 Fixed 3D Ground Truth message type for ROS Fixed 3D and 2D Ground Truth bounding box locations [2019.07] - 2019-08-09 # Added # Separate Asset Bundles for environments and vehicles Fully deterministic physics simulation Faster-than-real-time capability with Python API Lanelet2 HD map format import/export Ability to edit sensor configuration dynamically Multi-user support - account login allows different users to login to one machine running simulator BorregasAve 3D environment as a default provided map AutonomouStuff 3D environment as a default provided map of parking lot SingleLaneRoad 3D environment as a default provided map CubeTown 3D environment as a default provided map Lexus RX and Lincoln MKZ vehicle support LiDAR outputs intensity value based on reflectivity of material (instead of color) Support for Apollo 5.0 Support for Autoware 1.12 Light reflections on road from wetness Better sky rendering, including clouds Ability to import point cloud files for visualization Changed # User interface - use web UI for main user interaction Render pipeline - Unity High Definition Render Pipeline Significantly improved LiDAR simulation performance using multithreading Improved map annotation for easier use in Editor Improved point cloud generation from 3D environment in Editor Removed # Support for Duckiebot, EP_rigged, SF_rigged, Tugbot vehicles Support for SimpleLoop, SimpleMap, SimpleRoom, Duckietown, DuckieDownTown, SanFrancisco, Shalun maps [2019.05 and older] # Please see release notes for previous versions on our Github releases page.","title":"Release notes"},{"location":"release-notes/changelog/#20213-2021-09-31","text":"","title":"[2021.3] - 2021-09-31"},{"location":"release-notes/changelog/#added","text":"Content creation: Annotation size support Map preview option can be set by user SDF importer and STL fixes and improvements Lighting for indoor environments Rain colliders for indoor environments New indoor environments with costmaps : Warehouse , WarehouseDT Add map support for NavOrigin Sensors: Sensors support multiple data feeds, e.g. RealSenseSensor Async GPU readback support for Lidar, DirectionalLidar, VelodyneLidar for performance boost LaserScan support for Lidar2D 2D/3D Ground Truth sensor accuracy improvements New sensors: Lidar2DSensor , DestinationSensor , IntelRealSense Agents: NPCManager extension for creating complex NPC behaviors like Parking NPC support customized rigidbody and wheel collider settings: F1Tenth New pedestrian models: See VRU/Walkers: Jun, Yun General: Loaded Assets main menu option for assets in memory Docker support for enabling scenario runners on windows Console for Docker logs Graphics setting menu Main Menu simulator/unity/webUrl display for debugging Added TargetGear for Autoware Auto reverse driving Visual scenario editor: Ego Lane Following feature for VSE playback Support for the BezierSpline paths in the waypoints Support for the NavOrigin Added acceleration support for agents' waypoints Python API: Add Python API commands for Destination sensor API function to set/get NavOrigin in/from scene API versioning support Web user interface: Local Automation support Local automation API token access token creation, listing, and deletion in User Profile page Added preservation of the order of sensors in a sensor configuration Added 'Add to Library' modal preview Added preview images for private asset links in Skype, Facebook, LinkedIn, etc. Added preview images for Simulation links in Skype, Facebook, LinkedIn, etc. Implemented accurate trending algorithm 'Draft' feature for Simulation creation page Improved Simulations gallery loading performance","title":"Added"},{"location":"release-notes/changelog/#changed","text":"Moved simulator input actions to sensor bundles Visual scenario editor: Smoother agent rotation while in playback mode Fix for editing controllables policy and resetting map signals policies Miscellaneous Github issue fixes: API connection fixes Bridge connection fixes Custom Bridge fixes Point Cloud renderer fixes Add checks for pedestrian and traffic annotation for api random agents Development Settings fixes and improvements. Supports cloud assets Import HDMap tool fixes BaseLink fixes Radar velocity fix Build asset fixes NPC segmentation id issues in API Fixed OpenDrive MapOrigin export logic Fix export point cloud feature in simulator editor. Requires latest Lidar Sensor OpenDrive, Apollo Lane, Sign and Signal IDs import & export Refresh cached sims on deleted asset NPC/Pedestrian pool no longer removes existing agents Fixed NPC controller stopping all coroutines on the initialization Fix for Input package error on lost focus Correct name of Cyber bridge asmdef file Fixed initializing spawns manager if there are no active agents Bridge client watchdog coroutine Fix lighting on API agent spawn Removed NavSystem from pedestrian waypoint behavior Fixed invariant float and double parameters in NMEA Fixed SF, Shalun, Highway101 map origin rotation issues Package updates: Update WebSocketSharp Added more debugging logs for FBX2glTF Remove old input unity work arounds for Linux Extracted Pedestrians' behaviours from the PedestrianController Remove Apollo 3.0 support Adding detail map and albedo tint to environment shader Web user interface changes: Updated User Content License to User General Content License User Premium Content license introduction and sort order change Improved asset update progress indicator Improved asset upload error message displays and language Fixed Sharing window issue and Test Report snapshot issue with APIOnly simulations Fixed version compatibility validation when simulation does not have cluster assigned Plugin preview image improvement Fixed simulation assets dropdown when there are more than 30 assets in Library Fixed asset previews in Skype and Facebook Removed markdown markup language from descriptions in link previews Increased size restriction for asset bundle Fixed critical bug related to vehicle asset not being found when sensor configuration's plugin was deleted Fixed critical bug related to asset deletion","title":"Changed"},{"location":"release-notes/changelog/#202122-2021-08-18","text":"","title":"[2021.2.2] - 2021-08-18"},{"location":"release-notes/changelog/#added_1","text":"Support for indoor robotics and Nav2 navigation stack Sample indoor environment for robots: LGSeocho Sample two-wheel drive robot model: LGCLOi DifferentialDriveControl sensor to control two-wheel drive robots in ROS/ROS 2 Support for Unity's ArticulationBody for ego vehicles (robot models)","title":"Added"},{"location":"release-notes/changelog/#changed_1","text":"Fixed error reporting for bridge connection Fixed baselink offset in sensor visualizer for sensor configurations Added Development as version for custom builds or when running in Developer Mode Fixed NPC vehicle and pedestrian spawn position issue","title":"Changed"},{"location":"release-notes/changelog/#202121-2021-07-23","text":"","title":"[2021.2.1] - 2021-07-23"},{"location":"release-notes/changelog/#changed_2","text":"Reduce duplicate bridge messages by only loading one bridge in Developer Mode Add log about which assembly a sensor was loaded from to help debug bridges Create folder when user defined data directory path points to absent folder (not recursively) Update default data folder name to \u201cSVLSimulator-\\<version>\u201d Fix empty version issue by providing fallback simulator version Fix to allow different versions of simulator to connect to cloud Support bridge type attribute in manifest Remove error when disconnecting from cloud (WebException: task was cancelled)","title":"Changed"},{"location":"release-notes/changelog/#20212-2021-06-30","text":"","title":"[2021.2] - 2021-06-30"},{"location":"release-notes/changelog/#added_2","text":"External Pedestrians and NPC (other traffic vehicles) with animation support New NPCs and Pedestrians like Scooters, Bicyclists and Animals External Bridge Support Multiple simulator asset bundle versions support Rain Collision effect Support for simulations without EGO ( Spectator Ego ) New \u201cDriver view\u201d camera state New API to set simulator camera state Annotation tool to set accurate values (position, rotation and scale) for MapOrigin New \u201cclear cache\u201d option Ability to use proxy server for cloud_url in config.yml New enhanced web user interface: Improved visual sensor editor Ability to clone Sensors in sensor configuration User warning messages for privilege based restriction Markdown support for Asset description","title":"Added"},{"location":"release-notes/changelog/#changed_3","text":"Unity Upgrade to 2020.3.3f1 Revert FBX package to 3.2.1-preview.2 Remove overdraw with HDRP flare Improved NPC handling: Fixed rotating NPC wheels when using NPCWaypointBehaviour Pedestrian ground truth from bounds Improvements and bug fixes around Distributed Simulation with multi-ego support: Fixed API mode for distributed simulation on a single machine Fixed pedestrian animations on client machine Improved distributing VehicleActions Support for the ArticulationBody in distributed simulations Improved load balancing of the sensors Updated Sensor support: Fix TF visual and sensor hierarchy update input for g920 wheel for windows Publish camera intrinsic parameters to ROS and ROS2 Rework lidar to use cubemap Improved lighting and rain: Fix brown lighting Rotate sun and moon to match map rotation Rain collision improvements and fixes Fix overlapping rain chunks Enhanced Visual Scenario Editor support: Minor UX fixes Fixed changing agents' variants Fix issue with agent dropdown not refreshing on click Loading pedestrians from config Improvements around HD Map Mesh Generator : Improve data handling in HD map mesh builder Erosion and re-leveling for point cloud mesh Work in progress improvements on Developer mode: Rework developer settings Bundle version uptick and unity editor check on developer debug mode for bridges Sensor debug developer loading More enhanced and user friendly web user interface: Password Sensitivity improvements GLTF editor bug fixes and improvements HD map preview improvements Simulation invalid items management WCAG accessibility UI compatibility Live Test Results view in Simulation Profile Updated algorithm for Trending assets view Simulation form auto-save Misc Improvements: Support default light layers in point cloud renderer Loads assembly with the ego name from asset bundle Added public driver view transform to be set in the Inspector Asmdef added, loader fixed to deal with bundle/WISE name mismatch Loader: improve the error message about BundleFormat Make sure Simulator.Editor.Build.Run exits on any exception Trimming last slash in the cloud url if needed. get full path from data path provided","title":"Changed"},{"location":"release-notes/changelog/#202111-2021-05-20","text":"","title":"[2021.1.1] - 2021-05-20"},{"location":"release-notes/changelog/#added_3","text":"3D Visualizer to help create sensor configuration on a vehicle HD map visualizer on map profile pages Ability to share test reports with other users \"Supported Simulator versions\" value on asset profile page Improved sharing feature for private assets and simulations Save and Exit button on sensor configuration page Tutorials to help new users on how to interact with the web user interface pages Email confirmation upon sensor plugin sharing","title":"Added"},{"location":"release-notes/changelog/#changed_4","text":"Improved quick filters on Store and Library pages Redirect to existing asset profile page when uploading an already uploaded asset Request access button on private plugin pages Require adding a vehicle sensor configuration when creating a simulation Fixed issues with multi-vehicle simulation using same vehicle and sensor configuration Improvements to cluster page user experience Show proper names for missing sensors on invalid simulations Proper error messages for wrong information on registration page Proper error messages when maximum allowed cluster limit is reached","title":"Changed"},{"location":"release-notes/changelog/#20211-2021-03-23","text":"","title":"[2021.1] - 2021-03-23"},{"location":"release-notes/changelog/#added_4","text":"New cloud-based web user interface including: Asset Store for downloading assets, uploading and sharing assets publicly user account password management user assets management library support for private assets private assets/simulations sharing with registered users cluster/Simulation status handling SVL Simulator content from this release will be available in the Asset Store New and updated runtime templates for simulations with automatic bringup and execution: Random Traffic Visual Scenario Editor Python API API Only Visual Scenario Editor for creating scenarios including support for: traffic vehicle and pedestrian waypoints previews copy/paste/undo feature ability to specify ego vehicle target destination support for controllables with ability to edit default policy color selection for actors waypoint positions snapped to HD map lanes support for distance, time, and time-to-collision triggers traffic lights control Automatically generated test case reports of simulations including: success/failure scenario evaluation simulation statistics analytics data per sensor video recording of simulation multiple violation event detection: collision, stopline, stopsign, ego-stuck, sudden jerk/brake/turn, speed-limit, etc. PythonAPI: many improvements including: added cloudiness and road damage to weather parameters ability to specify date in environment configuration added Dreamview API as sub-module added callback agents_traversed_waypoints added ability to set speed at WalkWaypoints added waypoint trigger system ability to set simulator camera free-state position and rotation sample Python API scripts based on NHTSA-defined scenarios for end-to-end testing additional Python API test samples: random-traffic mode, cut-in, sudden-braking, etc. Environment configuration improvements: moon position in nighttime environment configurations Improved environment road wetness layer UI for FPS and SIM time Improved rain particles Increased types of traffic lights, model improvements Improvements in Developer Mode : developer settings to use ego vehicle within Developer Mode ability to select and add actors to simulation in Developer Settings added sensor debug mode added tool to rotate Editor scene view to -X north Features for traffic agents in simulation: support for bicyclist traffic agents support for custom traffic vehicle behavior plugins pedestrian crosswalk support with yield for traffic vehicles (SanFrancisco map) added direction for pedestrian crosswalk annotation tool spawning NPCs now raycasts target bounds from egos' positions New map/environment creation improvements: default ego destination points for LG-provided maps HD map road network environment generation tool test maps for NHTSA-like test cases: Straight1LanePedestrianCrosswalk, Straight1LaneSame, etc. General simulation improvements: ability to create and run a Distributed Simulation from web user interface offline mode for running saved simulation configurations improved cache control for downloaded assets Sensors improvements: new sensors : StopLine, HUDKeyboardControl, AutowareAIControl, LGSVLControl, etc. improved LiDAR simulation performance sun-flare, raindrops, greyscale, and video artifacts effects for sensors support for sensor post-distortion effects Default values for simple and complex sensor parameter types Enabled sensors to load texture assets Improvements around support for AD stacks: Optimized ROS2 interface and bridge Support for Apollo 6.0 and master","title":"Added"},{"location":"release-notes/changelog/#changed_5","text":"Product branding changed from \"LGSVL Simulator\" to \"SVL Simulator\" Old local web user interface is now deprecated. Users need to migrate to new web user interface Old content website will be deprecated All content now managed on cloud - downloaded as needed by simulation configuration Unity update to 2019.4.18f1 All sensors are now sensor plugins - default sensors are part of user plugin library Sensor plugins no longer bundled with simulator executable Distributed simulation: fixed pedestrian movement animation bug Distributed simulation: video artifact fix Fixed wrong timestamp error in clock and IMU sensors Fix acceleration/angular velocity in Apollo imu/corrected_imu Fixed way of handling failed downloads Fixed bugs and issues in OpenDrive HD map importer and exporter Point cloud rendering: added alternative method for removing obscured points (reduces flickering, requires depth prepass) Point cloud rendering: Fill scarce depth map holes through neighborhood sampling Point Cloud Rendering: Fix skybox blending in unlit point cloud Improved traffic lights in environments Rain/clouds/lights improvements NPC spawn logic improvements Fix old NPC models and prefab Adjust camera distance from ego bounds HD map import: Fix keyError in lanelet2 and improve grouping lanes at intersections HD map import: Fix map origin update during OpenDrive map import","title":"Changed"},{"location":"release-notes/changelog/#202006-2020-08-31","text":"","title":"[2020.06] - 2020-08-31"},{"location":"release-notes/changelog/#added_5","text":"Support modular testing of planning with Apollo 5.0 Bridge plugin system with example ( LoggingBridge ) Added default ultrasonic distance sensor Sensor TF visualizer in sensor visualizations Added button in map annotation tool to add synthetic lane boundary lines automatically Python API: added callback agents_traversed_waypoints Python API: added ability to set speed at WalkWaypoints Python API: Added waypoint trigger system","title":"Added"},{"location":"release-notes/changelog/#changed_6","text":"Fixed 90 degree orientation offset in GPS odometry sensor Replaced default ROS2 bridge to improved custom ROS2 bridge instead of ros2-web-bridge Unity update to 2019.3.15f1 Fixed timestamps in GPS, IMU Added timestamp to default LiDAR sensor PointCloud message Fixed bugs in OpenDrive map importer to correctly compute reference lines and lane widths for edge cases Improved OpenDrive map exporter to export parametric cubic curves instead of simple lines for each reference line segment","title":"Changed"},{"location":"release-notes/changelog/#202005-2020-06-02","text":"","title":"[2020.05] - 2020-06-02"},{"location":"release-notes/changelog/#added_6","text":"Support for custom traffic vehicle behavior plugins Support for Apollo 5.5 ( latest master ) Support for distributed simulation Velodyne VLS-128 support Python API function to spawn random non-ego vehicles and pedestrians Unlit shadows support for point cloud environment rendering AD system-agnostic message definitions to \"lgsvl_msgs\" for vehicle control, CAN bus and status data, etc.","title":"Added"},{"location":"release-notes/changelog/#changed_7","text":"Refactored traffic agent Waypoint Mode with fixes for idle and trigger waypoints Changed gnss_odom message coordinate system to right-handed coordinate system Fixed \"ignore map origin\" option for gnss_odom message Fixed acceleration values in IMU sensor Fix sky flickering issue during point cloud rendering Fixed LiDAR readback error Removed and replaced documentation source files from main repository to separate Documentation repository","title":"Changed"},{"location":"release-notes/changelog/#202003-2020-04-03","text":"","title":"[2020.03] - 2020-04-03"},{"location":"release-notes/changelog/#added_7","text":"Velodyne VLP-16 and VLP-32C support through custom LiDAR sensor plugin Support for LiDAR sensor plugin with direct UDP socket interface Support for point cloud environment rendering Extensible vehicle dynamics Functional Mockup Interface 2.0 (FMI 2.0) interface for vehicle dynamics Support for fisheye camera lens distortion Support for sensor_msgs/NavSatFix message type for GPS messages in ROS2 Vehicle Odometry sensor for reporting steering angles and velocity in ROS2","title":"Added"},{"location":"release-notes/changelog/#changed_8","text":"Upgraded to Unity 2019.3.3f1 Fixed black artifacts in Linux bug Fixed display issues in Unity Editor bug Fixed camera sensor invalid data bug 3D Ground Truth sensor now generates different ID for vehicle after each respawn","title":"Changed"},{"location":"release-notes/changelog/#202001-2020-01-31","text":"","title":"[2020.01] - 2020-01-31"},{"location":"release-notes/changelog/#added_8","text":"Controllable Object plugin support, dynamically add and control Controllables with the Python API Ability to create LiDAR sensor with non-uniform beam distribution Ability to add distortion effects to Camera sensors Support for Autoware.Auto messages (ROS2) Ability to create custom message types for sensor plugins TransformSensor which can act as a parent for other sensors VehicleStateSensor which is needed by Autoware.Auto","title":"Added"},{"location":"release-notes/changelog/#changed_9","text":"Added SanFrancisco map as a default map option Allow null texture bundles without an error Pedestrian mass reduced to 70kg Fixed lagging of 2D Ground Truth boxes (in visualization and data) ComfortSensor included with Simulator binary TrafficCone included with Simulator binary Fixed exported HD map lane relations Fixed ROS2 GPS","title":"Changed"},{"location":"release-notes/changelog/#201912-2020-01-21","text":"","title":"[2019.12] - 2020-01-21"},{"location":"release-notes/changelog/#added_9","text":"Custom callback for Python API, useful in sensor plugins. Ability to specify count and types of NPCs to spawn on the map.","title":"Added"},{"location":"release-notes/changelog/#changed_10","text":"Optimized texture usage in maps - significantly reduces size on disk and GPU memory used. Allow boundary type to be optional for Apollo HD map import. Fix Python API to be able load maps & vehicles that are shared between multiple users. Separated bundle file format version for map and vehicle bundles. Improved headlights for EGO vehicles. Improved HD map annotation icons. Fixed Sedan NPC mesh.","title":"Changed"},{"location":"release-notes/changelog/#201911-2019-11-19","text":"","title":"[2019.11] - 2019-11-19"},{"location":"release-notes/changelog/#added_10","text":"OpenDrive 1.4 HD map import. Rain drops on ground when it is raining. Separated Apollo HD map export between version 3.0 and 5.0 to support 5.0 specific features. Cache vehicle prefabs when loading same vehicle multiple times into simulation. Ability to login to account via command-line. Ability for vehicle to have multiple interior lights. Fixes #474. Allow Color, Depth and Semantic cameras to have higher capture & publish rate. Building & loading custom sensors as plugins.","title":"Added"},{"location":"release-notes/changelog/#changed_11","text":"Fixed traffic lights signal colors on Shalun map. Fixed exceptions when NPCs are despawned while still in intersection. Fixed errors when adding pedestrian to map without NavMesh. Fixed Lanelet2 boundary import and export. Multiple fixes for OpenDrive import and export. Fixed wrong raycast layers in 2D Ground Truth sensor to detect if NPC is visible. Fixed missing timestamp when publishing ROS/Cyber messages from 2D Ground Truth sensor. Limit number of spawned NPCs and vehicles for large maps.","title":"Changed"},{"location":"release-notes/changelog/#201910-2019-10-28","text":"","title":"[2019.10] - 2019-10-28"},{"location":"release-notes/changelog/#added_11","text":"Apollo HD map import Accurate sun position in sky based of map location. Including time of sunrise and sunset. Control calibration sensor to help calibrating AD stack control. Ported Shalun map from previous non-HDRP simulator version. Ground Truth sensor for traffic light. Python API method to get controllable object by position. Python API method to convert multiple map coordinates in single call. Python API method to perform multiple ray-casts in single call. Sensor for controlling vehicle with steering wheel (Logitech G920). Platform independent asset bundles (Windows and Linux). Allow to set custom data path for database and downloaded bundles. Visualize data values for non-image senors (GPS, IMU, etc). Populate scene with required objects when new scene is created in Unity Editor.","title":"Added"},{"location":"release-notes/changelog/#changed_12","text":"Fixed exceptions in ROS Bridge where if it receives message on topic that it has not subscribed. Fixed 3D Ground Truth sensor to report correct NPC orientation angles. Fixed Radar sensor to visualize pedestrians. Fixed Color camera to render mountains in BorregaAve. Fixed EGO vehicle collision callback to Python API. Fixed WebUI redirect loop that happens if you are logged out. Fixed reported NPC vehicle speed. Fixes #347 and #317. Fixed gear shifting for EGO vehicle control. Fixes #389. Fixed NPC waypoint following where NPCs stopped if assigned speed is too low. Fixed semantic segmentation for vehicles and pedestrians added with Python API. Fixed ROS2 message publishing (seq field was missing in Header). Fixes #413. Fixed issue with database on some non-English locales. Fixes #381. Fixed point cloud generation in Unity Editor. Fixed browser loosing cookie when session ends in WebUI. Fixed slowness in Python API when running without access to Internet. Fixed issue when multiple users could not use same map url. Improved error messages when simulation fails to start.","title":"Changed"},{"location":"release-notes/changelog/#201909-2019-09-06","text":"","title":"[2019.09] - 2019-09-06"},{"location":"release-notes/changelog/#added_12","text":"Sensor visualization UI HD map export to OpenDrive 1.4 format ROS service support for ROS bridge Python API to support more robust waypoints for NPC vehicles Python API with ability to control traffic lights on map Hyundai Nexo vehicle","title":"Added"},{"location":"release-notes/changelog/#changed_13","text":"Improved NPC movement and right turns on red traffic light Fixed NPC vehicle despawning logic so they don't get stuck in intersections Change NPC vehicles colliders from box to mesh to improves collision precision Updated generated protobuf message classes for latest Apollo 5.0 Fixed 3D Ground Truth message type for ROS Fixed 3D and 2D Ground Truth bounding box locations","title":"Changed"},{"location":"release-notes/changelog/#201907-2019-08-09","text":"","title":"[2019.07] - 2019-08-09"},{"location":"release-notes/changelog/#added_13","text":"Separate Asset Bundles for environments and vehicles Fully deterministic physics simulation Faster-than-real-time capability with Python API Lanelet2 HD map format import/export Ability to edit sensor configuration dynamically Multi-user support - account login allows different users to login to one machine running simulator BorregasAve 3D environment as a default provided map AutonomouStuff 3D environment as a default provided map of parking lot SingleLaneRoad 3D environment as a default provided map CubeTown 3D environment as a default provided map Lexus RX and Lincoln MKZ vehicle support LiDAR outputs intensity value based on reflectivity of material (instead of color) Support for Apollo 5.0 Support for Autoware 1.12 Light reflections on road from wetness Better sky rendering, including clouds Ability to import point cloud files for visualization","title":"Added"},{"location":"release-notes/changelog/#changed_14","text":"User interface - use web UI for main user interaction Render pipeline - Unity High Definition Render Pipeline Significantly improved LiDAR simulation performance using multithreading Improved map annotation for easier use in Editor Improved point cloud generation from 3D environment in Editor","title":"Changed"},{"location":"release-notes/changelog/#removed","text":"Support for Duckiebot, EP_rigged, SF_rigged, Tugbot vehicles Support for SimpleLoop, SimpleMap, SimpleRoom, Duckietown, DuckieDownTown, SanFrancisco, Shalun maps","title":"Removed"},{"location":"release-notes/changelog/#201905-and-older","text":"Please see release notes for previous versions on our Github releases page.","title":"[2019.05 and older]"},{"location":"release-notes/limitations-license-notice/","text":"Limitations and License Notice # Restrictions and limitations # This documentation website contains an overview of many features and aspects of SVL Simulator, including planned releases or architectures that may not be provided at the time of delivery of this documentation. Such architectures, descriptions, and other features and aspects may be subject to change in the future, in LG Electronics USA\u2019s sole discretion. License agreement conditions # All content in this documentation, as well as products and services mentioned or provided along with this document, unless explicitly stated otherwise by Zenith Electronics, LLC, a subsidiary of LG Electronics USA, Inc \"LG\"), in writing, are subject to the terms and conditions in the Terms of Use . Not part of this release # SVL Simulator includes the simulation platform, tools, and relevant content and services pertaining to simulation. While it may include possible reference implementations and modifications to open source autonomous driving systems, SVL Simulator does not officially provide autonomous vehicle systems that have been tested in the real world. LG Electronics USA disclaims any liability for issues that may arise related to open source autonomous driving systems in connection with user's evaluation and usage of SVL Simulator software and/or its further development.","title":"Limitations and license notice"},{"location":"release-notes/limitations-license-notice/#restrictions-and-limitations","text":"This documentation website contains an overview of many features and aspects of SVL Simulator, including planned releases or architectures that may not be provided at the time of delivery of this documentation. Such architectures, descriptions, and other features and aspects may be subject to change in the future, in LG Electronics USA\u2019s sole discretion.","title":"Restrictions and limitations"},{"location":"release-notes/limitations-license-notice/#license-agreement-conditions","text":"All content in this documentation, as well as products and services mentioned or provided along with this document, unless explicitly stated otherwise by Zenith Electronics, LLC, a subsidiary of LG Electronics USA, Inc \"LG\"), in writing, are subject to the terms and conditions in the Terms of Use .","title":"License agreement conditions"},{"location":"release-notes/limitations-license-notice/#not-part-of-this-release","text":"SVL Simulator includes the simulation platform, tools, and relevant content and services pertaining to simulation. While it may include possible reference implementations and modifications to open source autonomous driving systems, SVL Simulator does not officially provide autonomous vehicle systems that have been tested in the real world. LG Electronics USA disclaims any liability for issues that may arise related to open source autonomous driving systems in connection with user's evaluation and usage of SVL Simulator software and/or its further development.","title":"Not part of this release"},{"location":"release-notes/release-contents/","text":"Release contents # SVL Simulator is delivered as several zip archives in the following directory layout: \u2514\u2500\u2500 2021.3 \u251c\u2500\u2500 svlsimulator-linux64-2021.3.zip \u251c\u2500\u2500 svlsimulator-windows64-2021.3.zip \u251c\u2500\u2500 svlsimulator-source-2021.3.zip \u251c\u2500\u2500 svlsimulator-source-2021.3.tar.gz svlsimulator-linux64-2021.3.zip : A zip archive of the Linux binary executable of SVL Simulator. svlsimulator-windows64-2021.3.zip : A zip archive of the Windows binary executable of SVL Simulator. svlsimulator-source-2021.3.zip : A zip archive of the source code of SVL Simulator. This is needed to run the simulator in Developer Mode. svlsimulator-source-2021.3.tar.gz : A tarball archive of the source code of SVL Simulator. This is needed to run the simulator in Developer Mode.","title":"Contents of this release"},{"location":"release-notes/release-features/","text":"2021.3 Release Features # Scenario runners support for Windows top # With Docker integration, we have extended scenario runners support on SVL Simulator running on Windows. Now it is possible to run end-to-end simulations using Python API and Visual Scenario Editor runtime templates on Windows as well. Please follow our updated installation instructions to get started. Local Automation (CI/CD) support top # With Local Automation , you can now trigger simulation jobs from scripts without needing to manually interact with the web user interface. This empowers you to begin integrating SVL Simulator with existing automated testing or CI workflows (like Gitlab or Jenkins). Extended Multi-Robot support top # We have several features to extend support for robotics simulation use cases. We support multi-robot simulations by using namespaces on sensor topics. Please refer to our multi-robot sensor configuration for an example. We also added new sensors ( Lidar2DSensor , DestinationSensor , IntelRealSense ) and new indoor environments ( Warehouse , WarehouseDT ) with costmaps for robotics development. We have created an SDF importer to help import robot and environments from SDF world files. The Python API has been extended to enable setting a NavOrigin and destination with ROS2 Navigation stack. Finally, we have added support for indoor lighting and rain effects for creating improved indoor environments. Updated License Terms top # There are some updates on our user uploaded content license terms. Please review the updated license terms at our website . Deprecating Ubuntu 18.04 in next release top # We will be deprecating official Simulator support for Ubuntu 18.04 starting with our next 2021.4 release. Improved 2D/3D Ground Truth top # Perception sensor 2D and 3D will now use image data to create tight bounding boxes to increase coverage and accuracy. If any pixel is visible, entity will be detected by the ground truth sensor. Vehicles no longer require box colliders, bounds are calculated based on mesh. Developer Debug Mode top # To help during plugin development, we have added Developer Debug Mode to allow for debugging of scripts inside local plugin code. Improved Visual Scenario Editor top # You can now create more complex and improved scenarios using our visual scenario editor . It supports BezierSpline paths, acceleration for agents\u2019 waypoints, improved controllables policies, support for NavOrigin for robotics, and improved scenario playback to validate before running in simulation. Extended NPC Behaviors top # You can now use custom NPC Managers in NPC behavior plugins to create complex behaviors (like Parking). Easily extend input controls top # We have moved simulator input actions to sensor bundles to make it easy to add new input controllers like gamepads and other wheel controllers. You no longer need to make changes to the simulator source code for adding new input devices. Bug fixes and improvements top # For a full list of bug fixes, improvements, and all other changes since the last release, please see the release notes .","title":"Release features"},{"location":"release-notes/release-features/#scenario-runner-windows","text":"With Docker integration, we have extended scenario runners support on SVL Simulator running on Windows. Now it is possible to run end-to-end simulations using Python API and Visual Scenario Editor runtime templates on Windows as well. Please follow our updated installation instructions to get started.","title":"Scenario runners support for Windows"},{"location":"release-notes/release-features/#local-automation-support","text":"With Local Automation , you can now trigger simulation jobs from scripts without needing to manually interact with the web user interface. This empowers you to begin integrating SVL Simulator with existing automated testing or CI workflows (like Gitlab or Jenkins).","title":"\u2019Local"},{"location":"release-notes/release-features/#extended-multi-robot-support","text":"We have several features to extend support for robotics simulation use cases. We support multi-robot simulations by using namespaces on sensor topics. Please refer to our multi-robot sensor configuration for an example. We also added new sensors ( Lidar2DSensor , DestinationSensor , IntelRealSense ) and new indoor environments ( Warehouse , WarehouseDT ) with costmaps for robotics development. We have created an SDF importer to help import robot and environments from SDF world files. The Python API has been extended to enable setting a NavOrigin and destination with ROS2 Navigation stack. Finally, we have added support for indoor lighting and rain effects for creating improved indoor environments.","title":"\u2019Extended"},{"location":"release-notes/release-features/#updated-license-terms","text":"There are some updates on our user uploaded content license terms. Please review the updated license terms at our website .","title":"\u2019Updated"},{"location":"release-notes/release-features/#deprecating-ubuntu-18-04","text":"We will be deprecating official Simulator support for Ubuntu 18.04 starting with our next 2021.4 release.","title":"\u2019Deprecating"},{"location":"release-notes/release-features/#improved-ground-truth","text":"Perception sensor 2D and 3D will now use image data to create tight bounding boxes to increase coverage and accuracy. If any pixel is visible, entity will be detected by the ground truth sensor. Vehicles no longer require box colliders, bounds are calculated based on mesh.","title":"\u2019Improved"},{"location":"release-notes/release-features/#added-developer-debug-mode","text":"To help during plugin development, we have added Developer Debug Mode to allow for debugging of scripts inside local plugin code.","title":"\u2019Developer"},{"location":"release-notes/release-features/#improved-vse","text":"You can now create more complex and improved scenarios using our visual scenario editor . It supports BezierSpline paths, acceleration for agents\u2019 waypoints, improved controllables policies, support for NavOrigin for robotics, and improved scenario playback to validate before running in simulation.","title":"\u2019Improved"},{"location":"release-notes/release-features/#extended-npc-behaviors","text":"You can now use custom NPC Managers in NPC behavior plugins to create complex behaviors (like Parking).","title":"\u2019Extended"},{"location":"release-notes/release-features/#extend-input-controls","text":"We have moved simulator input actions to sensor bundles to make it easy to add new input controllers like gamepads and other wheel controllers. You no longer need to make changes to the simulator source code for adding new input devices.","title":"\u2019Easily"},{"location":"release-notes/release-features/#bug-fixes-and-improvements","text":"For a full list of bug fixes, improvements, and all other changes since the last release, please see the release notes .","title":"Bug Fixes and Improvements"},{"location":"running-simulations/api-how-to-run-scenario/","text":"How To Run a Scenario or Test Case # The following steps detail how to run the Vehicle Following scenario. This scenario and other example scenarios can be found on our examples page . These scenarios use the SVL Simulator Python API. The SVL Simulator Python API is available on GitHub: https://github.com/lgsvl/PythonAPI After cloning or downloading it from the git repository follow these steps: Run the following command to install Python files and necessary dependencies: pip3 install --user -e . Create and start an API Only simulation. Set environment variables LGSVL__SIMULATOR_HOST and LGSVL__AUTOPILOT_0_HOST LGSVL__SIMULATOR_HOST is where the simulator will be run. The default value for this is \"localhost\" and does not need to be set if the simulator is running on the same machine that the Python script will be run from. LGSVL__AUTOPILOT_0_HOST is where the AD stack will be run. This is relative to where the simulator is run. The default value is \"localhost\" which is the same machine as the simulator. For example, if computer A will run the simulator and computer B will run the AD stack LGSVL__SIMULATOR_HOST should be set to the IP of computer A LGSVL__AUTOPILOT_0_HOST should be set to the IP of computer B To set the variables for the current terminal window use export LGSVL__SIMULATOR_HOST=192.168.1.100 export LGSVL__AUTOPILOT_0_HOST=192.168.1.101 Start your AD stack. The example scripts are written for Apollo 5.0. See below for how to edit the scripts to work with other AD stacks. Select the MKZ as the vehicle SingleLaneRoad for the map Start all modules and the bridge (if relevant) Run the script ./VF_S_25_Slow.py Set the destination for the AD stack. For this scenario, the destination is the end of the current lane. The AV should start driving forward towards the NPC. It should avoid crashing into the NPC. How to Edit the EGO vehicle top # In each of the example scenarios and test cases, there is a section that setups up the EGO vehicle: If using a different AD stack, the vehicle type needs to be changed. Change the agent name (orange string) to what is desired. e.g. For Autoware it might be \"Lexus2016RXHybrid (Autoware)\"","title":"How To Run a Scenario or Test Case"},{"location":"running-simulations/api-how-to-run-scenario/#how-to-edit-the-ego-vehicle","text":"In each of the example scenarios and test cases, there is a section that setups up the EGO vehicle: If using a different AD stack, the vehicle type needs to be changed. Change the agent name (orange string) to what is desired. e.g. For Autoware it might be \"Lexus2016RXHybrid (Autoware)\"","title":"How to Edit the EGO vehicle"},{"location":"running-simulations/developer-mode/","text":"Developer Mode # Some functionality of SVL Simulator is only available in Developer Mode, by opening the SVL Simulator source code as a project in Unity Editor. The available functionality includes: HD map annotation tool ( tutorial ) Import/export of supported HD map formats HD map mesh generation ( tutorial ) Developing and building environments ( tutorial ) Developing and building vehicles ( tutorial ) Developing and building custom plugins ( sensor plugin , controllable plugin ) Building custom simulator binary executables ( tutorial ) Ability to customize vehicle dynamics with FMI All other functionalities of SVL Simulator are available by running the binary executable and do not require Developer Mode. Setup top # There are a few steps needed to set up the development environment for Developer Mode, which is officially supported only on Windows. While much of the SVL Simulator Developer Mode functionality may work on Linux, we do not guarantee Developer Mode to be fully supported on Linux. Download and install Unity Hub ( https://unity3d.com/get-unity/download ) Ubuntu: You may need to allow the downloaded AppImage to be run as an executable Right-click the AppImage Select Properties Go to the Permissions tab Check Allow executing file as program Alternatively, in the terminal run sudo chmod +x UnityHub.AppImage Download and install Unity 2020.3.3f1 with Windows and Linux build support modules ( https://unity3d.com/get-unity/download/archive ) IMPORTANT include support for both Windows and Linux when installing Unity (Optional) include support for Visual Studio for easier debugging Unity Download Archive Click the Unity Hub button to have Unity Hub handle the installation process Make sure you have git-lfs installed before cloning this repository . Instructions for installation are here Verify installation with: $ git lfs install Git LFS initialized. Clone simulator from GitHub: $ git clone --single-branch https://github.com/lgsvl/simulator.git Be sure to clone latest release branch, not master. Run Unity Hub In the Projects tab, click Add and select the folder that the Simulator was cloned to In the Installs tab, click Locate and choose the Unity launcher in the Unity2020.3.3f1 folder In the Projects tab, verify that the Simulator is using Unity Version 2020.3.3f1 from the dropdown Double-click the name of the project to launch Unity Editor If this is an updated Unity version, let Unity recompile shaders and then close Unity and open the project again.","title":"Developer mode"},{"location":"running-simulations/developer-mode/#setup","text":"There are a few steps needed to set up the development environment for Developer Mode, which is officially supported only on Windows. While much of the SVL Simulator Developer Mode functionality may work on Linux, we do not guarantee Developer Mode to be fully supported on Linux. Download and install Unity Hub ( https://unity3d.com/get-unity/download ) Ubuntu: You may need to allow the downloaded AppImage to be run as an executable Right-click the AppImage Select Properties Go to the Permissions tab Check Allow executing file as program Alternatively, in the terminal run sudo chmod +x UnityHub.AppImage Download and install Unity 2020.3.3f1 with Windows and Linux build support modules ( https://unity3d.com/get-unity/download/archive ) IMPORTANT include support for both Windows and Linux when installing Unity (Optional) include support for Visual Studio for easier debugging Unity Download Archive Click the Unity Hub button to have Unity Hub handle the installation process Make sure you have git-lfs installed before cloning this repository . Instructions for installation are here Verify installation with: $ git lfs install Git LFS initialized. Clone simulator from GitHub: $ git clone --single-branch https://github.com/lgsvl/simulator.git Be sure to clone latest release branch, not master. Run Unity Hub In the Projects tab, click Add and select the folder that the Simulator was cloned to In the Installs tab, click Locate and choose the Unity launcher in the Unity2020.3.3f1 folder In the Projects tab, verify that the Simulator is using Unity Version 2020.3.3f1 from the dropdown Double-click the name of the project to launch Unity Editor If this is an updated Unity version, let Unity recompile shaders and then close Unity and open the project again.","title":"Setup"},{"location":"running-simulations/local-automation/","text":"Local Automation (CI/CD) # Table of Contents Overview Tutorial Local Automation Commands Prerequisites Generating an Access Token Wrapper Script Usage Pattern Running a Simulation Running Multiple Simulations Sequentially Running Multiple Simulations Simultaneously Stopping a Simulation Overview top # Local Automation allows you to start, check on, and, if necessary, stop simulation jobs from scripts without needing to manually interact with the web user interface . This empowers you to integrate SVL Simulator with existing automated testing or CI workflows (for example, Github/Gitlab integration, or Jenkins). Note that you will still need to use the web user interface to create and setup simulations beforehand. A POSIX shell script downloaded from the web user interface wraps the usage of a container pulled from there in which the actual client resides. Access is authenticated and authorized via an access token that you must generate using the web user interface. Tutorial top # For step-by-step instructions on running simulations using the local automation tool, see our local automation tutorial . Local Automation Commands top # The following commands are available: Entity Command Arguments Description cli initialize Initialize a CLI session cleanup Cleanup anything cached by the CLI session help Output details about the commands simulation start SIMULATION-ID Start a simulation check SIMULATION-ID Check on a simulation stop SIMULATION-ID Stop a simulation help Output details about the commands simulation check returns one of these values upon success: Value Description progressing The simulation has been started and is progressing. success The simulation has completed; this is the status of its last test result. failure The simulation has completed; this is the status of its last test result. error The simulation has completed; this is the status of its last test result. virgin There is no last test result because the simulation has never been run. unknown Unable to determine. The other commands return nothing upon success. Upon error, the commands return a message suitable for printing, but not parsing because it's not guaranteed to be stable across releases. The errors detected include: Access to SIMULATION-ID not allowed Cluster not online Invalid access token Malformed SIMULATION-ID Non-existent SIMULATION-ID Simulation already running The HTTP response status code is not exposed. Prerequisites top # The wrapper script requires the docker.io and python3-minimal packages. If there's no output from command -v docker , install it by running: sudo apt update && sudo apt install --no-install-recommends docker.io If there's no output from command -v python3 , install it by running: sudo apt update && sudo apt install --no-install-recommends python3-minimal Generating an Access Token top # As mentioned in Overview , an access token is needed in order to use Local Automation. Follow the instructions below to generate one. Click on your name in the upper righthand corner and select My Profile Expand the Advanced section and scroll down to Access Token for Local Automation : Name or describe what the token will be used for in Note and press Create : Make sure to retain a copy of your access token. Treat it as a password -- anyone who has it can access your simulations using Local Automation. Wrapper Script top # Download the wrapper script svlsim.sh from the web user interface: wget -nv https://wise.svlsimulator.com/scripts/svlsim.sh It is intended that svlsim.sh be checked into your source control as a peer of your CI/CD scripts. cli initialize will fail if svlsim.sh needs to be updated (by re-downloading). Usage Pattern top # This is the pattern for executing a command: export SVLSIM__ACCESS_TOKEN=\"<ACCESS-TOKEN>\" # SVLSIM__LOG_FORMAT defaults to \"[%(levelname)6s] [%(name)s] %(message)s\", the same as # that used by the \"lgsvl\" module (aka \"Python API\"). # export SVLSIM__LOG_FORMAT=\"<PYTHON_LOG_FORMAT>\" # SVLSIM__LOG_LEVEL defaults to \"INFO\". # export SVLSIM__LOG_LEVEL=\"<PYTHON_LOG_LEVEL>\" ... if result=$(sh svlsim.sh ENTITY COMMAND [ARG ...] 2>> LOGFILE); then : SUCCESS # ${result} is a single return value. else : ERROR # ${result} is an error message suitable for printing (but not parsing). fi Information suitable for logging is written to stderr. The format of what's logged can be altered by setting the SVLSIM__LOG_FORMAT environment variable to a Python 3 logging format string formed from these attributes . The logging level can be altered by setting SVLSIM__LOG_LEVEL to one of these levels . The logging message that's written is not guaranteed to be stable across releases and should not be parsed. There is no connection timeout: svlsim.sh will wait forever for the response from the web user interface. Running a Simulation top # A script to run a simulation should look something like what follows below. You must have created the simulation using the web user interface and it must be ready to run, ie, it must be valid and the cluster on which it will run must be online. export SVLSIM__ACCESS_TOKEN=\"<ACCESS-TOKEN>\" # export SVLSIM__LOG_FORMAT=\"<PYTHON_LOG_FORMAT>\" # export SVLSIM__LOG_LEVEL=\"<PYTHON_LOG_LEVEL>\" logfile=\"<PATH-TO-LOGFILE>\" simulation_id=\"<UUID taken from the web user interface>\" # If desired, cleanup anything cached by the CLI session, eg, the container image. # A successful \"cli cleanup\" outputs nothing. We recommend that you use Bash so that # \"trap 0\" will catch exits caused by signals. # trap 'result=$(sh svlsim.sh cli cleanup 2>> ${logfile}) || echo \"ERROR: ${result}\"' 0 # Initialize a CLI session. A successful \"cli initialize\" outputs nothing. if result=$(sh svlsim.sh cli initialize 2> ${logfile}); then : SUCCESS else # One error will be that svlsim.sh is out-of-date and needs to be re-downloaded. echo \"ABORT: ${result}\" exit 1 fi # Start a simulation and return immediately. A successful \"simulation start\" outputs # nothing. This command does not wait in order to allow a CI script to launch multiple # (different) simulations simultaneously. if result=$(sh svlsim.sh simulation start ${simulation_id} 2>> ${logfile}); then : SUCCESS else echo \"ABORT: ${result}\" exit 1 fi # Check on the simulation until it's finished. \"simulation check\" outputs one of: # - progressing # - success # - failure # - error # - virgin # - unknown while true; do # Wait time is arbitrary. sleep 2 if status=$(sh svlsim.sh simulation check ${simulation_id} 2>> ${logfile}); then if [ ${status} != \"progressing\" ]; then break fi else echo \"ABORT: ${status}\" exit 1 fi done You must use the web user interface to view the test results or see the message associated with the error status. Running Multiple Simulations Sequentially top # To run multiple simulations sequentially, replace the simulation start and simulation check blocks from above with: simulations=\"<SIMULATION-ID-1> <SIMULATION-ID-2> ...\" for simulation_id in ${simulations}; do ( if result=$(sh svlsim.sh simulation start ${simulation_id} 2>> ${logfile}); then : SUCCESS else echo \"ERROR: ${simulation_id}: ${result}\" exit 1 fi while true; do # Wait time is arbitrary. sleep 2 if status=$(sh svlsim.sh simulation check ${simulation_id} 2>> ${logfile}); then if [ ${status} != \"progressing\" ]; then break fi else echo \"ERROR: ${simulation_id}: ${status}\" exit 1 fi done ) done Running Multiple Simulations Simultaneously top # To run multiple simulations simultaneously, replace the simulation start and simulation check blocks from above with: # Note that this has been written for Bash. declare -r simulations=\"<SIMULATION-ID-1> <SIMULATION-ID-2> ...\" # Start all of the simulations. declare -A running_simulations=() for simulation_id in ${simulations}; do if result=$(sh svlsim.sh simulation start ${simulation_id} 2>> ${logfile}); then running_simulations[${simulation_id}]=\"started\" else echo \"ERROR: ${simulation_id}: ${result}\" # Or stop the others that were previously started and abort? fi done # Check those that are (still) running. while ${#running_simulations[*]} -gt 0; do # Wait time is arbitrary. sleep 2 for simulation_id in ${running_simulations}; do if status=$(sh svlsim.sh simulation check ${simulation_id} 2>> ${logfile}); then if [ ${status} = \"progressing\" ]; then running_simulations[${simulation_id}]=${status} else echo \"${simulation_id} done: ${status}\" unset running_simulations[${simulation_id}] fi else echo \"ERROR: ${simulation_id}: ${status}\" unset running_simulations[${simulation_id}] fi done done Stopping a Simulation top # To stop a running simulation, run: # A successful \"simulation stop\" outputs nothing. The command returns immediately. # You should use \"simulation check\" to discover when the simulation has completed. if status=$(sh svlsim.sh simulation stop ${simulation_id} 2>> ${logfile}); then : SUCCESS else echo \"ABORT: ${status}\" exit 1 fi","title":"Local Automation (CI/CD)"},{"location":"running-simulations/local-automation/#overview","text":"Local Automation allows you to start, check on, and, if necessary, stop simulation jobs from scripts without needing to manually interact with the web user interface . This empowers you to integrate SVL Simulator with existing automated testing or CI workflows (for example, Github/Gitlab integration, or Jenkins). Note that you will still need to use the web user interface to create and setup simulations beforehand. A POSIX shell script downloaded from the web user interface wraps the usage of a container pulled from there in which the actual client resides. Access is authenticated and authorized via an access token that you must generate using the web user interface.","title":"Overview"},{"location":"running-simulations/local-automation/#tutorial","text":"For step-by-step instructions on running simulations using the local automation tool, see our local automation tutorial .","title":"Tutorial"},{"location":"running-simulations/local-automation/#commands","text":"The following commands are available: Entity Command Arguments Description cli initialize Initialize a CLI session cleanup Cleanup anything cached by the CLI session help Output details about the commands simulation start SIMULATION-ID Start a simulation check SIMULATION-ID Check on a simulation stop SIMULATION-ID Stop a simulation help Output details about the commands simulation check returns one of these values upon success: Value Description progressing The simulation has been started and is progressing. success The simulation has completed; this is the status of its last test result. failure The simulation has completed; this is the status of its last test result. error The simulation has completed; this is the status of its last test result. virgin There is no last test result because the simulation has never been run. unknown Unable to determine. The other commands return nothing upon success. Upon error, the commands return a message suitable for printing, but not parsing because it's not guaranteed to be stable across releases. The errors detected include: Access to SIMULATION-ID not allowed Cluster not online Invalid access token Malformed SIMULATION-ID Non-existent SIMULATION-ID Simulation already running The HTTP response status code is not exposed.","title":"Local Automation Commands"},{"location":"running-simulations/local-automation/#prerequisites","text":"The wrapper script requires the docker.io and python3-minimal packages. If there's no output from command -v docker , install it by running: sudo apt update && sudo apt install --no-install-recommends docker.io If there's no output from command -v python3 , install it by running: sudo apt update && sudo apt install --no-install-recommends python3-minimal","title":"Prerequisites"},{"location":"running-simulations/local-automation/#generatingtoken","text":"As mentioned in Overview , an access token is needed in order to use Local Automation. Follow the instructions below to generate one. Click on your name in the upper righthand corner and select My Profile Expand the Advanced section and scroll down to Access Token for Local Automation : Name or describe what the token will be used for in Note and press Create : Make sure to retain a copy of your access token. Treat it as a password -- anyone who has it can access your simulations using Local Automation.","title":"Generating an Access Token"},{"location":"running-simulations/local-automation/#wrapperscript","text":"Download the wrapper script svlsim.sh from the web user interface: wget -nv https://wise.svlsimulator.com/scripts/svlsim.sh It is intended that svlsim.sh be checked into your source control as a peer of your CI/CD scripts. cli initialize will fail if svlsim.sh needs to be updated (by re-downloading).","title":"Wrapper Script"},{"location":"running-simulations/local-automation/#usagepattern","text":"This is the pattern for executing a command: export SVLSIM__ACCESS_TOKEN=\"<ACCESS-TOKEN>\" # SVLSIM__LOG_FORMAT defaults to \"[%(levelname)6s] [%(name)s] %(message)s\", the same as # that used by the \"lgsvl\" module (aka \"Python API\"). # export SVLSIM__LOG_FORMAT=\"<PYTHON_LOG_FORMAT>\" # SVLSIM__LOG_LEVEL defaults to \"INFO\". # export SVLSIM__LOG_LEVEL=\"<PYTHON_LOG_LEVEL>\" ... if result=$(sh svlsim.sh ENTITY COMMAND [ARG ...] 2>> LOGFILE); then : SUCCESS # ${result} is a single return value. else : ERROR # ${result} is an error message suitable for printing (but not parsing). fi Information suitable for logging is written to stderr. The format of what's logged can be altered by setting the SVLSIM__LOG_FORMAT environment variable to a Python 3 logging format string formed from these attributes . The logging level can be altered by setting SVLSIM__LOG_LEVEL to one of these levels . The logging message that's written is not guaranteed to be stable across releases and should not be parsed. There is no connection timeout: svlsim.sh will wait forever for the response from the web user interface.","title":"Usage Pattern"},{"location":"running-simulations/local-automation/#runningasimulation","text":"A script to run a simulation should look something like what follows below. You must have created the simulation using the web user interface and it must be ready to run, ie, it must be valid and the cluster on which it will run must be online. export SVLSIM__ACCESS_TOKEN=\"<ACCESS-TOKEN>\" # export SVLSIM__LOG_FORMAT=\"<PYTHON_LOG_FORMAT>\" # export SVLSIM__LOG_LEVEL=\"<PYTHON_LOG_LEVEL>\" logfile=\"<PATH-TO-LOGFILE>\" simulation_id=\"<UUID taken from the web user interface>\" # If desired, cleanup anything cached by the CLI session, eg, the container image. # A successful \"cli cleanup\" outputs nothing. We recommend that you use Bash so that # \"trap 0\" will catch exits caused by signals. # trap 'result=$(sh svlsim.sh cli cleanup 2>> ${logfile}) || echo \"ERROR: ${result}\"' 0 # Initialize a CLI session. A successful \"cli initialize\" outputs nothing. if result=$(sh svlsim.sh cli initialize 2> ${logfile}); then : SUCCESS else # One error will be that svlsim.sh is out-of-date and needs to be re-downloaded. echo \"ABORT: ${result}\" exit 1 fi # Start a simulation and return immediately. A successful \"simulation start\" outputs # nothing. This command does not wait in order to allow a CI script to launch multiple # (different) simulations simultaneously. if result=$(sh svlsim.sh simulation start ${simulation_id} 2>> ${logfile}); then : SUCCESS else echo \"ABORT: ${result}\" exit 1 fi # Check on the simulation until it's finished. \"simulation check\" outputs one of: # - progressing # - success # - failure # - error # - virgin # - unknown while true; do # Wait time is arbitrary. sleep 2 if status=$(sh svlsim.sh simulation check ${simulation_id} 2>> ${logfile}); then if [ ${status} != \"progressing\" ]; then break fi else echo \"ABORT: ${status}\" exit 1 fi done You must use the web user interface to view the test results or see the message associated with the error status.","title":"Running a Simulation"},{"location":"running-simulations/local-automation/#runningmultiplesequentially","text":"To run multiple simulations sequentially, replace the simulation start and simulation check blocks from above with: simulations=\"<SIMULATION-ID-1> <SIMULATION-ID-2> ...\" for simulation_id in ${simulations}; do ( if result=$(sh svlsim.sh simulation start ${simulation_id} 2>> ${logfile}); then : SUCCESS else echo \"ERROR: ${simulation_id}: ${result}\" exit 1 fi while true; do # Wait time is arbitrary. sleep 2 if status=$(sh svlsim.sh simulation check ${simulation_id} 2>> ${logfile}); then if [ ${status} != \"progressing\" ]; then break fi else echo \"ERROR: ${simulation_id}: ${status}\" exit 1 fi done ) done","title":"Running Multiple Simulations Sequentially"},{"location":"running-simulations/local-automation/#runningmultiplesimultaneously","text":"To run multiple simulations simultaneously, replace the simulation start and simulation check blocks from above with: # Note that this has been written for Bash. declare -r simulations=\"<SIMULATION-ID-1> <SIMULATION-ID-2> ...\" # Start all of the simulations. declare -A running_simulations=() for simulation_id in ${simulations}; do if result=$(sh svlsim.sh simulation start ${simulation_id} 2>> ${logfile}); then running_simulations[${simulation_id}]=\"started\" else echo \"ERROR: ${simulation_id}: ${result}\" # Or stop the others that were previously started and abort? fi done # Check those that are (still) running. while ${#running_simulations[*]} -gt 0; do # Wait time is arbitrary. sleep 2 for simulation_id in ${running_simulations}; do if status=$(sh svlsim.sh simulation check ${simulation_id} 2>> ${logfile}); then if [ ${status} = \"progressing\" ]; then running_simulations[${simulation_id}]=${status} else echo \"${simulation_id} done: ${status}\" unset running_simulations[${simulation_id}] fi else echo \"ERROR: ${simulation_id}: ${status}\" unset running_simulations[${simulation_id}] fi done done","title":"Running Multiple Simulations Simultaneously"},{"location":"running-simulations/local-automation/#stoppingasimulation","text":"To stop a running simulation, run: # A successful \"simulation stop\" outputs nothing. The command returns immediately. # You should use \"simulation check\" to discover when the simulation has completed. if status=$(sh svlsim.sh simulation stop ${simulation_id} 2>> ${logfile}); then : SUCCESS else echo \"ABORT: ${status}\" exit 1 fi","title":"Stopping a Simulation"},{"location":"running-simulations/offline-mode/","text":"Offline Mode # Offline Mode allows you to run simulations without needing an active Internet connection. Simulations that have been run at least once on the cluster are cached locally, allowing you to run them again directly from the main SVL Simulator executable without needing to open your browser and be online. On a local cluster running SVL Simulator that is linked to your account, you can see the \"Online\" status at the top right of the main menu. Clicking the \"Online\" status button reveals an option to \"Go Offline\". Once offline, your local cluster is still linked to your account, but can now run cached simulations without being online. You will be able to view a list of simulations that have previously been run on the cluster. Click the Play button after selecting the simulation to run. In order to create a new simulation and run it, or change any settings, you will need to switch back online and modify from your account in the web user interface.","title":"Offline mode"},{"location":"running-simulations/python-runtime/","text":"Simulator Python API Runtime # Table of Contents Using the Python API runtime Reading Logs Example Python Script Environment Variables Using the Python API runtime top # The simulator provides a Python API runtime to allow running Python scripts to control the simulation directly in the WebUI. To use the Python API runtime: Go to the Simulations tab Click Add New Fill out the Simulation Name , Select Cluster , and optionally a description and click Next In the Runtime Template dropdown menu, select Python API Select the Map and Vehicle and Sensor Configuration you will be using in from the drop-down menus. This will set the LGSVL__MAP and LGSVL__VEHICLE_0 environment variables to the UUID of the selected map and vehicle. These environment variables can be used in the Python script to load the correct map and vehicle sensor configuration. In the text box titled Python script , enter your Python code directly and click Next Any maps or vehicles used in the Python script must be added to the users library An example Python script is available below , which requires the BorregasAve map and Lincoln2017MKZ vehicle to be added to the user library There will be no need to specify an Autopilot; click Next again Publish the new simulation to use it Once published, the new simulation will appear in your Simulations page and can be run by clicking on the Run Simulation button. Note: The Python script should not prompt for user input, otherwise it will result in a timeout as there is no way to get user input after the simulation has started. For example, all of the Python API examples in the quickstart folder in the Python API repository ask for user input before starting the simulation. To use one of the scripts from quickstart all of the input(...) calls should be removed. Reading Logs top # In cases where the Python script throws an error the simulator will not be able to run the script properly (or at all) and errors will be written to Playor.log located in the simulator's persistent data path. By default this would be ~/.config/unity3d/LGElectronics/SVLSimulator/Player.log . Note: Player.log contains all logs from the simulator (not just Python API runtime logs). Example Python Script top # Below is an example script to use with the Python API runtime. This script requires the BorregasAve map and the Lincoln2017MKZ vehicle to be added to the user's library. After running the script, the ego vehicle will be spawned at the center of an intersection and an NPC vehicle will pass it on its right, pausing for half a second at every waypoint. import os import lgsvl import copy sim = lgsvl.Simulator(os.environ.get(\"LGSVL__SIMULATOR_HOST\"), 8181) # BorregasAve default map must be added to your personal library sim.load(os.environ.get(\"LGSVL__MAP\")) spawns = sim.get_spawn() forward = lgsvl.utils.transform_to_forward(spawns[0]) right = lgsvl.utils.transform_to_right(spawns[0]) # EGO state = lgsvl.AgentState() state.transform = spawns[0] ego_state = copy.deepcopy(state) ego_state.transform.position += 50 * forward ego_state.transform.position -= 3 * right # The Lincoln2017MKZ default vehicle must be added to your vehicle library a = sim.add_agent(os.environ.get(\"LGSVL__VEHICLE_0\"), lgsvl.AgentType.EGO, ego_state) # NPC npc_state = copy.deepcopy(state) npc_state.transform.position += 10 * forward npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC, npc_state) vehicles = { a: \"EGO\", npc: \"Sedan\", } # This block creates the list of waypoints that the NPC will follow # Each waypoint is an position vector paired with the speed that the NPC will drive to it waypoints = [] x_max = 2 z_delta = 12 layer_mask = 0 layer_mask |= 1 << 0 # 0 is the layer for the road (default) for i in range(20): speed = 24# if i % 2 == 0 else 12 px = 0 pz = (i + 1) * z_delta # Waypoint angles are input as Euler angles (roll, pitch, yaw) angle = spawns[0].rotation # Raycast the points onto the ground because BorregasAve is not flat hit = sim.raycast(spawns[0].position + pz * forward, lgsvl.Vector(0,-1,0), layer_mask) # NPC will wait for 1 second at each waypoint wp = lgsvl.DriveWaypoint(hit.point, speed, angle, 1) waypoints.append(wp) # The NPC needs to be given the list of waypoints. # A bool can be passed as the 2nd argument that controls whether or not the NPC loops over the waypoints (default false) npc.follow(waypoints) sim.run(20) Environment Variables top # The simulator uses a set of predefined environment variables to configure the simulation. These variables are listed below. Environment Variable Description LGSVL__SIMULATOR_HOST SVL Simulator hostname or IP LGSVL__SIMULATOR_PORT SVL Simulator port LGSVL__MAP UUID of map to be loaded in Simulator LGSVL__VEHICLE_0 UUID of the first EGO vehicle/sensor configuration to be loaded in Simulator. Subsequent EGO vehicles can be accessed by incrementing the number, e.g. LGSVL_VEHICLE_1 . LGSVL__AUTOPILOT_0_HOST Autopilot bridge hostname or IP. If SVL Simulator is running on a different host than Autopilot, this must be set. LGSVL__AUTOPILOT_0_PORT Autopilot bridge port LGSVL__AUTOPILOT_0_VEHICLE_CONFIG Vehicle configuration to be loaded in Dreamview (Capitalization and spacing must match the dropdown in Dreamview) (e.g. Lincoln2017MKZ) LGSVL__AUTOPILOT_0_VEHICLE_MODULES Comma-separated list of modules to be enabled in Dreamview. Items must be enclosed by double-quotes and there must not be spaces between the double-quotes and commas. (Capitalization and space must match the sliders in Dreamview) LGSVL__AUTOPILOT_HD_MAP HD map to be loaded in Dreamview (Capitalization and spacing must match the dropdown in Dreamview) LGSVL__DATE_TIME Date and time to start simulation at. Time is the local time in the time zone of the map origin. Format 'YYYY-mm-ddTHH:MM:SS' LGSVL__ENVIRONMENT_CLOUDINESS Value of clouds weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_DAMAGE Value of road damage effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_FOG Value of fog weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_RAIN Value of rain weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_WETNESS Value of wetness weather effect, clamped to [0.0, 1.0] LGSVL__RANDOM_SEED Seed used to determine random factors (e.g. NPC type, color, behaviour) LGSVL__SIMULATION_DURATION_SECS The time length of the simulation [int] LGSVL__SPAWN_BICYCLES Whether or not to spawn bicycles [boolean] (not yet supported) LGSVL__SPAWN_PEDESTRIANS Whether or not to spawn pedestrians [boolean] LGSVL__SPAWN_TRAFFIC Whether or not to spawn NPC vehicles [boolean] LGSVL__TIME_STATIC Whether or not time should remain static [boolean] ( True = time is static, False = time moves forward)","title":"Simulator Python API Runtime[](#top)"},{"location":"running-simulations/python-runtime/#using-the-python-api-runtime","text":"The simulator provides a Python API runtime to allow running Python scripts to control the simulation directly in the WebUI. To use the Python API runtime: Go to the Simulations tab Click Add New Fill out the Simulation Name , Select Cluster , and optionally a description and click Next In the Runtime Template dropdown menu, select Python API Select the Map and Vehicle and Sensor Configuration you will be using in from the drop-down menus. This will set the LGSVL__MAP and LGSVL__VEHICLE_0 environment variables to the UUID of the selected map and vehicle. These environment variables can be used in the Python script to load the correct map and vehicle sensor configuration. In the text box titled Python script , enter your Python code directly and click Next Any maps or vehicles used in the Python script must be added to the users library An example Python script is available below , which requires the BorregasAve map and Lincoln2017MKZ vehicle to be added to the user library There will be no need to specify an Autopilot; click Next again Publish the new simulation to use it Once published, the new simulation will appear in your Simulations page and can be run by clicking on the Run Simulation button. Note: The Python script should not prompt for user input, otherwise it will result in a timeout as there is no way to get user input after the simulation has started. For example, all of the Python API examples in the quickstart folder in the Python API repository ask for user input before starting the simulation. To use one of the scripts from quickstart all of the input(...) calls should be removed.","title":"Using the Python API runtime"},{"location":"running-simulations/python-runtime/#reading-logs","text":"In cases where the Python script throws an error the simulator will not be able to run the script properly (or at all) and errors will be written to Playor.log located in the simulator's persistent data path. By default this would be ~/.config/unity3d/LGElectronics/SVLSimulator/Player.log . Note: Player.log contains all logs from the simulator (not just Python API runtime logs).","title":"Reading Logs"},{"location":"running-simulations/python-runtime/#example-python-script","text":"Below is an example script to use with the Python API runtime. This script requires the BorregasAve map and the Lincoln2017MKZ vehicle to be added to the user's library. After running the script, the ego vehicle will be spawned at the center of an intersection and an NPC vehicle will pass it on its right, pausing for half a second at every waypoint. import os import lgsvl import copy sim = lgsvl.Simulator(os.environ.get(\"LGSVL__SIMULATOR_HOST\"), 8181) # BorregasAve default map must be added to your personal library sim.load(os.environ.get(\"LGSVL__MAP\")) spawns = sim.get_spawn() forward = lgsvl.utils.transform_to_forward(spawns[0]) right = lgsvl.utils.transform_to_right(spawns[0]) # EGO state = lgsvl.AgentState() state.transform = spawns[0] ego_state = copy.deepcopy(state) ego_state.transform.position += 50 * forward ego_state.transform.position -= 3 * right # The Lincoln2017MKZ default vehicle must be added to your vehicle library a = sim.add_agent(os.environ.get(\"LGSVL__VEHICLE_0\"), lgsvl.AgentType.EGO, ego_state) # NPC npc_state = copy.deepcopy(state) npc_state.transform.position += 10 * forward npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC, npc_state) vehicles = { a: \"EGO\", npc: \"Sedan\", } # This block creates the list of waypoints that the NPC will follow # Each waypoint is an position vector paired with the speed that the NPC will drive to it waypoints = [] x_max = 2 z_delta = 12 layer_mask = 0 layer_mask |= 1 << 0 # 0 is the layer for the road (default) for i in range(20): speed = 24# if i % 2 == 0 else 12 px = 0 pz = (i + 1) * z_delta # Waypoint angles are input as Euler angles (roll, pitch, yaw) angle = spawns[0].rotation # Raycast the points onto the ground because BorregasAve is not flat hit = sim.raycast(spawns[0].position + pz * forward, lgsvl.Vector(0,-1,0), layer_mask) # NPC will wait for 1 second at each waypoint wp = lgsvl.DriveWaypoint(hit.point, speed, angle, 1) waypoints.append(wp) # The NPC needs to be given the list of waypoints. # A bool can be passed as the 2nd argument that controls whether or not the NPC loops over the waypoints (default false) npc.follow(waypoints) sim.run(20)","title":"Example Python Script"},{"location":"running-simulations/python-runtime/#environment-variables","text":"The simulator uses a set of predefined environment variables to configure the simulation. These variables are listed below. Environment Variable Description LGSVL__SIMULATOR_HOST SVL Simulator hostname or IP LGSVL__SIMULATOR_PORT SVL Simulator port LGSVL__MAP UUID of map to be loaded in Simulator LGSVL__VEHICLE_0 UUID of the first EGO vehicle/sensor configuration to be loaded in Simulator. Subsequent EGO vehicles can be accessed by incrementing the number, e.g. LGSVL_VEHICLE_1 . LGSVL__AUTOPILOT_0_HOST Autopilot bridge hostname or IP. If SVL Simulator is running on a different host than Autopilot, this must be set. LGSVL__AUTOPILOT_0_PORT Autopilot bridge port LGSVL__AUTOPILOT_0_VEHICLE_CONFIG Vehicle configuration to be loaded in Dreamview (Capitalization and spacing must match the dropdown in Dreamview) (e.g. Lincoln2017MKZ) LGSVL__AUTOPILOT_0_VEHICLE_MODULES Comma-separated list of modules to be enabled in Dreamview. Items must be enclosed by double-quotes and there must not be spaces between the double-quotes and commas. (Capitalization and space must match the sliders in Dreamview) LGSVL__AUTOPILOT_HD_MAP HD map to be loaded in Dreamview (Capitalization and spacing must match the dropdown in Dreamview) LGSVL__DATE_TIME Date and time to start simulation at. Time is the local time in the time zone of the map origin. Format 'YYYY-mm-ddTHH:MM:SS' LGSVL__ENVIRONMENT_CLOUDINESS Value of clouds weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_DAMAGE Value of road damage effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_FOG Value of fog weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_RAIN Value of rain weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_WETNESS Value of wetness weather effect, clamped to [0.0, 1.0] LGSVL__RANDOM_SEED Seed used to determine random factors (e.g. NPC type, color, behaviour) LGSVL__SIMULATION_DURATION_SECS The time length of the simulation [int] LGSVL__SPAWN_BICYCLES Whether or not to spawn bicycles [boolean] (not yet supported) LGSVL__SPAWN_PEDESTRIANS Whether or not to spawn pedestrians [boolean] LGSVL__SPAWN_TRAFFIC Whether or not to spawn NPC vehicles [boolean] LGSVL__TIME_STATIC Whether or not time should remain static [boolean] ( True = time is static, False = time moves forward)","title":"Environment Variables"},{"location":"running-simulations/python-testcase/","text":"Python Test Cases # Video # Introduction # Test Case mode is a new feature in the SVL Simulator which executes test cases written in Python using the LGSVL Python API . Python scripts can be used to implement many different kinds of tests to run in the SVL Simulator. Among the general use-cases of Python Test Case mode are: Generating and testing random variations of various driving test cases Performing automated (e.g. acceptance) tests based on a set of repeatable test cases Performing automated regressions tests Python Test Case mode provides the ability to configure and execute several sample test cases available under \"Available from Others\" view in Simulations, using Apollo for Autonomous Driving. Refer to Python Test Case Simulation for an illustrated step-by-step walk through on selecting and configuring a python test case simulation. This document describes how to set up and run these test cases with Apollo. For example, one provided example is a \"cut-in\" test case which is performed on the San Francisco map (shown below). Another provided example is a \"sudden braking\" test case which is performed on the SingleLaneRoad map (shown below). Additional examples demonstrate pedestrian-crossing test case which can be performed on the Straight1LanePedestrianCrosswalk map and the red-light runner test case which is performed on the Borregas Ave map. (Screenshot of the \"cut-in\" test case on the San Francisco map, above) (Screenshot of the \"sudden-braking\" test case on the SingleLandRoad map, above) Requirements # Python Test Case Mode has these dependencies: SVL Simulator -- the release tag of the simulator should match the release tag of Python Runner to ensure compatibility Apollo 5.0 lgsvl fork Docker and Docker-compose Downloading and launching SVL Simulator top # The Simulator release can be downloaded as a prebuilt binary from the github release page . Download and extract the zip archive at the desired location. In the unzipped directory, run the executable file named simulator to launch the SVL Simulator. The main window of the simulator will open. Click on the \"Link to Cloud\" or \"Open Browser\" button to launch the simulator Web UI in a web browser. The Web UI controls all aspects of the simulator. You will need to create an account to login to the Web UI. Once logged in you will be able to link your simulator into a cluster, select maps, configure vehicles, and create simulations to run in the simulator. For more information on initial setup of the simulator, refer to the Installing Simuator and Link to Cloud sections of the Running SVL Simulator documentation for an illustrated step-by-step walk through of this process. Installing and Building Apollo 5.0 top # Please follow the instructions on the SVL Simulator documentation website to for Running Apollo 5.0 with SVL Simulator if you have not already done so. This will involve installing Docker CE, Nvidia Docker and the LGSVL Docker image, then cloning the lgsvl fork of the Apollo 5.0 sources, and finally building Apollo and the bridge. Important : Confirm that Apollo is able to drive the car to a selected destination when running in a \"Random Interactive\" simulation before attempting to use Test Case Mode. Refer to Random Interactive Simulation for instructions on setting up a random interactive simulation you can use to test Apollo functionality with the simulator. Python Test Case Mode workflow # Configure Test Case simulation in Web UI Launch Apollo, configure Dreamview, and enable modules Start SVL Simulator in Test Case mode Configure ego destination in Dreamview Configure Test Case simulation in Web UI (if needed) top # From the simulator Web UI, you will need to locate in the Store, a map and vehicle, and add each of them to your library. Refer to the Store for information on how to add the \"SanFrancisco\" map, \"Lincoln2017MKZ\" vehicle and sensors needed for \"Apollo 5.0 (full analysis)\" sensor configuration on this vehicle. Refer to Python Test Case Simulation for an illustrated step-by-step walk through on selecting and configuring a Python test case simulation. Note : The public \"Cut-in Scenario\" test case simulation should be ready to use if Apollo is running on the same machine as the simulator but if Apollo is running elsewhere, be sure to update the LGSVL__AUTOPILOT_0_HOST as described in the Python Test Case Simulation walk through. Launch Apollo, Configure Dreamview, and Enable Modules top # Open a terminal, change directory to the apollo-5.0 project (lgsvl fork: https://github.com/lgsvl/apollo-5.0), and type: ./docker/dev_start.sh to start apollo container ./docker/dev_into.sh to enter the container Make sure you have built Apollo with GPU enabled options; for more details, refer to Running Apollo 5.0 with SVL Simulator . bootstrap.sh && bridge.sh & to start the Apollo Dreamview Web UI and cyber_bridge. Open browser, enter address localhost:8888 to load Dreamview, and select Lincoln2017MKZ from the vehicle menu. Be sure to also select the correct map in Dreamview for the desired test case: For the Cut In scenario, select San Francisco from the map menu. For the Pedestrian Crossing scenario, select Straight 1 Lane Pedestrian Crosswalk from the map menu. For the Red Light Runner scenario, select Borregas Ave from the map menu. For the Sudden Braking scenario, select Single Lane Road from the map menu. Enable required modules in the Module Controller view before using the Python Runner including Camera , Localization , Perception , Planning , Prediction , Routing , Traffic Light , and Transform . Enable the Control module in the Module Controller view as well. Note : The ego vehicle will not drive until a destination is selected and a routing request is made in the Route Editing view. However, a destination cannot be selected until the simulation is started and Apollo is able to localize the ego on the map. Note : When changing the map selection in Dreamview, all modules will be disabled and will need to be re-enabled. Start SVL Simulator in Test Case mode top # In the Simulations tab in the Web UI locate the Cut-in Scenario scenario simulation. Then click the red \"Run Simulation\" button under the Cut-in Scenario to start the simulation. Note : The ego vehicle will not drive until a destination is selected and a routing request is made in the Route Editing view. However, a destination cannot be selected until the simulation is started and Apollo is able to localize the ego on the map. Configure Ego Destination in Dreamview top # After starting the simulation, Apollo should receive sensor information and localize the car onto the selected map. In Dreamview, switch to Route Editing view, zoom out, scroll if needed, and click to set the destination in the same ego lane before the next intersection. Then click \"Send Routing Request\" to plan the route. In Module View make sure all required modules are still enabled. The ego vehicle should begin driving. The NPC should cut in front of the ego vehicle, and the ego vehicle should slow or stop to avoid hitting the NPC. This test case can be repeated by clicking the red \"Run\" (triangle/play) button in the Web UI. Since the destination and route plan are already configured, re-starting the simulation will reset the ego start location and Apollo should immediately being driving. Example Test Cases # There are several example test cases provided in this preview release. Each test case has a required map, and a suggested destination, as summarized in the following table: Test Case Vehicle Sensor configuration Map Destination cut-in.py Lincoln2017MKZ Apollo 5.0 (full analysis) San Francisco same lane before next intersection ped-crossing.py Lincoln2017MKZ Apollo 5.0 (full analysis) Straight1LanePedestrianCrosswalk end of the map red-light-runner.py Lincoln2017MKZ Apollo 5.0 (full analysis) Borregas Ave other side of intersection sudden-braking.py Lincoln2017MKZ Apollo 5.0 (full analysis) SingleLaneRoad end of the map Refer to the Store for information on how to add the \"SanFrancisco\" map, \"Lincoln2017MKZ\" vehicle and sensors needed for \"Apollo 5.0 (full analysis)\" sensor configuration. Refer to Python Test Case Simulation for an illustrated step-by-step walk through on selecting and configuring a python test case simulation. Note that each map (environment) must be available in the Simulator Maps view, and the corresponding Apollo HD map must be selected in Dreamview. Also note that changing maps in Dreamview will disable the modules so you will need to re-enable them as described above. Details for each of the test cases are provided below. Cut-in Test Case top # On a two lane road, an NPC cuts in front of ego vehicle from an adjacent lane while maintaining previous speed, with a small enough distance in front of ego vehicle such that ego vehicle will need to react by either changing speed (braking) or turning or swerving away. This test case runs on the San Francisco map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the San Francisco map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Cut In\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination in the same ego lane before the next intersection. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled. Pedestrian Crossing Test Case top # A pedestrian begins crossing a crosswalk as the ego vehicle approaches the intersection. The ego vehicle will need to slow down or come to a stop to wait for the pedestrian to finish crossing. This test case runs on the Straight1LanePedestrianCrosswalk map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the Straight1LanePedestrianCrosswalk map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Pedestrian Crossing\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination at the end of the map. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled. Red Light Runner Test Case top # While the ego vehicle attempts to cross a 4-way intersection with a green traffic light, an NPC crossing the intersection from the right runs a red light and crosses the intersection. The Ego must react by braking, stopping, or turning in order to avoid a collision. This test case runs on the Borregas Ave map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the Borregas Ave map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Red Light Runner\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination on the other side of the intersection. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled. Sudden Braking Test Case top # The ego vehicle follows an NPC vehicle that is traveling at the speed limit. The NPC suddenly brakes and comes to a stop. The ego vehicle will need to greatly reduce its speed to avoid a collision. After a few seconds, the NPC speeds back up gradually to the speed limit. This test case runs on the SingleLaneRoad map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the Single Lane Road map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Sudden Braking\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination at the end of the map. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled. Known issues top # The NPC usually waits until the ego vehicle starts driving but in some cases it might start before a route has been requested for the ego. In that case wait for the simulation to finish and then run it again while leaving the modules enabled and the route to the destination already specified. If errors appear due to previously-downloaded (maps or vehicle) asset bundles from a previous version the Simulator, delete the Simulator persistent data folder located at /home/[username]/.config/unity3d/LGElectronics/SVLSimulator . If Straight 1 Lane Pedestrian Crosswalk is not available in the Dreamview map menu, try updating your apollo-5.0 sources (from the lgsvl fork ), then stop and re-start Apollo, and re-load Dreamview. The AD stack (e.g. Apollo) is not currently controlled by the Python Runner, so you will need to manually set a destination in Dreamview. You can control the AD stack through your Python Script using the python dreamview-API module. Dreamview-API module only supports Apollo at the moment. Using these APIs, you can configure hd_map and vehicle settings in Apollo Dreamview, enable/disable Apollo modules, set destination, etc..","title":"Python testcase"},{"location":"running-simulations/python-testcase/#video","text":"","title":"Video"},{"location":"running-simulations/python-testcase/#introduction","text":"Test Case mode is a new feature in the SVL Simulator which executes test cases written in Python using the LGSVL Python API . Python scripts can be used to implement many different kinds of tests to run in the SVL Simulator. Among the general use-cases of Python Test Case mode are: Generating and testing random variations of various driving test cases Performing automated (e.g. acceptance) tests based on a set of repeatable test cases Performing automated regressions tests Python Test Case mode provides the ability to configure and execute several sample test cases available under \"Available from Others\" view in Simulations, using Apollo for Autonomous Driving. Refer to Python Test Case Simulation for an illustrated step-by-step walk through on selecting and configuring a python test case simulation. This document describes how to set up and run these test cases with Apollo. For example, one provided example is a \"cut-in\" test case which is performed on the San Francisco map (shown below). Another provided example is a \"sudden braking\" test case which is performed on the SingleLaneRoad map (shown below). Additional examples demonstrate pedestrian-crossing test case which can be performed on the Straight1LanePedestrianCrosswalk map and the red-light runner test case which is performed on the Borregas Ave map. (Screenshot of the \"cut-in\" test case on the San Francisco map, above) (Screenshot of the \"sudden-braking\" test case on the SingleLandRoad map, above)","title":"Introduction"},{"location":"running-simulations/python-testcase/#requirements","text":"Python Test Case Mode has these dependencies: SVL Simulator -- the release tag of the simulator should match the release tag of Python Runner to ensure compatibility Apollo 5.0 lgsvl fork Docker and Docker-compose","title":"Requirements"},{"location":"running-simulations/python-testcase/#downloading-and-launching-svl-simulator","text":"The Simulator release can be downloaded as a prebuilt binary from the github release page . Download and extract the zip archive at the desired location. In the unzipped directory, run the executable file named simulator to launch the SVL Simulator. The main window of the simulator will open. Click on the \"Link to Cloud\" or \"Open Browser\" button to launch the simulator Web UI in a web browser. The Web UI controls all aspects of the simulator. You will need to create an account to login to the Web UI. Once logged in you will be able to link your simulator into a cluster, select maps, configure vehicles, and create simulations to run in the simulator. For more information on initial setup of the simulator, refer to the Installing Simuator and Link to Cloud sections of the Running SVL Simulator documentation for an illustrated step-by-step walk through of this process.","title":"Downloading and launching SVL Simulator"},{"location":"running-simulations/python-testcase/#installing-and-building-apollo-5.0","text":"Please follow the instructions on the SVL Simulator documentation website to for Running Apollo 5.0 with SVL Simulator if you have not already done so. This will involve installing Docker CE, Nvidia Docker and the LGSVL Docker image, then cloning the lgsvl fork of the Apollo 5.0 sources, and finally building Apollo and the bridge. Important : Confirm that Apollo is able to drive the car to a selected destination when running in a \"Random Interactive\" simulation before attempting to use Test Case Mode. Refer to Random Interactive Simulation for instructions on setting up a random interactive simulation you can use to test Apollo functionality with the simulator.","title":"Installing and Building Apollo 5.0"},{"location":"running-simulations/python-testcase/#python-test-case-mode-workflow","text":"Configure Test Case simulation in Web UI Launch Apollo, configure Dreamview, and enable modules Start SVL Simulator in Test Case mode Configure ego destination in Dreamview","title":"Python Test Case Mode workflow"},{"location":"running-simulations/python-testcase/#configure-test-case-simulation-in-web-ui-(if-needed)","text":"From the simulator Web UI, you will need to locate in the Store, a map and vehicle, and add each of them to your library. Refer to the Store for information on how to add the \"SanFrancisco\" map, \"Lincoln2017MKZ\" vehicle and sensors needed for \"Apollo 5.0 (full analysis)\" sensor configuration on this vehicle. Refer to Python Test Case Simulation for an illustrated step-by-step walk through on selecting and configuring a Python test case simulation. Note : The public \"Cut-in Scenario\" test case simulation should be ready to use if Apollo is running on the same machine as the simulator but if Apollo is running elsewhere, be sure to update the LGSVL__AUTOPILOT_0_HOST as described in the Python Test Case Simulation walk through.","title":"Configure Test Case simulation in Web UI (if needed)"},{"location":"running-simulations/python-testcase/#launch-apollo,-configure-dreamview,-and-enable-modules","text":"Open a terminal, change directory to the apollo-5.0 project (lgsvl fork: https://github.com/lgsvl/apollo-5.0), and type: ./docker/dev_start.sh to start apollo container ./docker/dev_into.sh to enter the container Make sure you have built Apollo with GPU enabled options; for more details, refer to Running Apollo 5.0 with SVL Simulator . bootstrap.sh && bridge.sh & to start the Apollo Dreamview Web UI and cyber_bridge. Open browser, enter address localhost:8888 to load Dreamview, and select Lincoln2017MKZ from the vehicle menu. Be sure to also select the correct map in Dreamview for the desired test case: For the Cut In scenario, select San Francisco from the map menu. For the Pedestrian Crossing scenario, select Straight 1 Lane Pedestrian Crosswalk from the map menu. For the Red Light Runner scenario, select Borregas Ave from the map menu. For the Sudden Braking scenario, select Single Lane Road from the map menu. Enable required modules in the Module Controller view before using the Python Runner including Camera , Localization , Perception , Planning , Prediction , Routing , Traffic Light , and Transform . Enable the Control module in the Module Controller view as well. Note : The ego vehicle will not drive until a destination is selected and a routing request is made in the Route Editing view. However, a destination cannot be selected until the simulation is started and Apollo is able to localize the ego on the map. Note : When changing the map selection in Dreamview, all modules will be disabled and will need to be re-enabled.","title":"Launch Apollo, Configure Dreamview, and Enable Modules"},{"location":"running-simulations/python-testcase/#start-svl-simulator-in-test-case-mode","text":"In the Simulations tab in the Web UI locate the Cut-in Scenario scenario simulation. Then click the red \"Run Simulation\" button under the Cut-in Scenario to start the simulation. Note : The ego vehicle will not drive until a destination is selected and a routing request is made in the Route Editing view. However, a destination cannot be selected until the simulation is started and Apollo is able to localize the ego on the map.","title":"Start SVL Simulator in Test Case mode"},{"location":"running-simulations/python-testcase/#configure-ego-destination-in-dreamview","text":"After starting the simulation, Apollo should receive sensor information and localize the car onto the selected map. In Dreamview, switch to Route Editing view, zoom out, scroll if needed, and click to set the destination in the same ego lane before the next intersection. Then click \"Send Routing Request\" to plan the route. In Module View make sure all required modules are still enabled. The ego vehicle should begin driving. The NPC should cut in front of the ego vehicle, and the ego vehicle should slow or stop to avoid hitting the NPC. This test case can be repeated by clicking the red \"Run\" (triangle/play) button in the Web UI. Since the destination and route plan are already configured, re-starting the simulation will reset the ego start location and Apollo should immediately being driving.","title":"Configure Ego Destination in Dreamview"},{"location":"running-simulations/python-testcase/#example-test-cases","text":"There are several example test cases provided in this preview release. Each test case has a required map, and a suggested destination, as summarized in the following table: Test Case Vehicle Sensor configuration Map Destination cut-in.py Lincoln2017MKZ Apollo 5.0 (full analysis) San Francisco same lane before next intersection ped-crossing.py Lincoln2017MKZ Apollo 5.0 (full analysis) Straight1LanePedestrianCrosswalk end of the map red-light-runner.py Lincoln2017MKZ Apollo 5.0 (full analysis) Borregas Ave other side of intersection sudden-braking.py Lincoln2017MKZ Apollo 5.0 (full analysis) SingleLaneRoad end of the map Refer to the Store for information on how to add the \"SanFrancisco\" map, \"Lincoln2017MKZ\" vehicle and sensors needed for \"Apollo 5.0 (full analysis)\" sensor configuration. Refer to Python Test Case Simulation for an illustrated step-by-step walk through on selecting and configuring a python test case simulation. Note that each map (environment) must be available in the Simulator Maps view, and the corresponding Apollo HD map must be selected in Dreamview. Also note that changing maps in Dreamview will disable the modules so you will need to re-enable them as described above. Details for each of the test cases are provided below.","title":"Example Test Cases"},{"location":"running-simulations/python-testcase/#cut-in-test-case","text":"On a two lane road, an NPC cuts in front of ego vehicle from an adjacent lane while maintaining previous speed, with a small enough distance in front of ego vehicle such that ego vehicle will need to react by either changing speed (braking) or turning or swerving away. This test case runs on the San Francisco map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the San Francisco map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Cut In\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination in the same ego lane before the next intersection. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled.","title":"Cut-in Test Case"},{"location":"running-simulations/python-testcase/#pedestrian-crossing-test-case","text":"A pedestrian begins crossing a crosswalk as the ego vehicle approaches the intersection. The ego vehicle will need to slow down or come to a stop to wait for the pedestrian to finish crossing. This test case runs on the Straight1LanePedestrianCrosswalk map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the Straight1LanePedestrianCrosswalk map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Pedestrian Crossing\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination at the end of the map. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled.","title":"Pedestrian Crossing Test Case"},{"location":"running-simulations/python-testcase/#red-light-runner-test-case","text":"While the ego vehicle attempts to cross a 4-way intersection with a green traffic light, an NPC crossing the intersection from the right runs a red light and crosses the intersection. The Ego must react by braking, stopping, or turning in order to avoid a collision. This test case runs on the Borregas Ave map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the Borregas Ave map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Red Light Runner\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination on the other side of the intersection. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled.","title":"Red Light Runner Test Case"},{"location":"running-simulations/python-testcase/#sudden-braking-test-case","text":"The ego vehicle follows an NPC vehicle that is traveling at the speed limit. The NPC suddenly brakes and comes to a stop. The ego vehicle will need to greatly reduce its speed to avoid a collision. After a few seconds, the NPC speeds back up gradually to the speed limit. This test case runs on the SingleLaneRoad map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the Single Lane Road map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Sudden Braking\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination at the end of the map. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled.","title":"Sudden Braking Test Case"},{"location":"running-simulations/python-testcase/#known-issues","text":"The NPC usually waits until the ego vehicle starts driving but in some cases it might start before a route has been requested for the ego. In that case wait for the simulation to finish and then run it again while leaving the modules enabled and the route to the destination already specified. If errors appear due to previously-downloaded (maps or vehicle) asset bundles from a previous version the Simulator, delete the Simulator persistent data folder located at /home/[username]/.config/unity3d/LGElectronics/SVLSimulator . If Straight 1 Lane Pedestrian Crosswalk is not available in the Dreamview map menu, try updating your apollo-5.0 sources (from the lgsvl fork ), then stop and re-start Apollo, and re-load Dreamview. The AD stack (e.g. Apollo) is not currently controlled by the Python Runner, so you will need to manually set a destination in Dreamview. You can control the AD stack through your Python Script using the python dreamview-API module. Dreamview-API module only supports Apollo at the moment. Using these APIs, you can configure hd_map and vehicle settings in Apollo Dreamview, enable/disable Apollo modules, set destination, etc..","title":"Known issues"},{"location":"running-simulations/running-simulator/","text":"Running SVL Simulator # Before running SVL Simulator make sure to go through the guide for installing and starting it for the first time. This guide assumes that SVL Simulator is installed and you are ready to run a simulation. This guide will walk you through the steps of adding assets from the Store to My Library , and creating and running following types of simulations: A random interactive simulation An API-only simulation A Python Test Case simulation A Visual scenario editor based simulation Table of Contents Video Add Assets from the Store Review Assets in your Library Create and run a random interactive simulation API Only Simulation Creating and running a Python Test Case Simulation Creating and running a Visual Scenario Editor based Simulation Video top # Add Assets from the Store top # Click tabs under Store to view available maps, vehicles, and sensors that are available for you to use. You'll need to add maps and vehicles from the Store to your own library (or upload maps and vehicles of your own) before you can use them in a simulation. Scroll through the list of all Free maps available, or click Trending to see most commonly downloaded maps or click Shared with me to see private maps shared to you by other users. Locate the BorregasAve map and click \"+\" to add it to your library if not already added (for Random Interactive Simulation below). Locate the SanFrancisco map and click \"+\" to add it to your library if not already added (for Python Test Case Simulation below). Click any tab under Store (if needed) to return to the Store view. Scroll through the list of all Free vehicles available, or click Trending to see most commonly downloaded maps or click Shared with me to see private maps shared to you by other users. Locate the Lincoln2017MKZ vehicle and click \"+\" to add it to your library if not already added (for Random Interactive Simulation or Python Test Case Simulation below). Locate the added vehicle in your library and click on the \"Apollo 5.0\" sensor configuration to confirm if there are any missing sensors in your library (for Random Interactive Simulation below). Locate the added vehicle in your library and click on the \"Apollo 5.0 (full analysis)\" sensor configuration to confirm if there are any missing sensors in your library (for Python Test Case Simulation below). If any sensors are missing, then locate them under \"Sensors\" tab under Store and add them to your library OR follow the instructions on the sensor configuration page. NOTE: All test scripts load map by name or UUID and vehicle's sensor configuration by UUID. So take care to add the specific vehicles, sensors and maps to your library that are needed for the scripts or test cases that you intend to run. Please use UUID instead of name in your Python Test Case scripts. You can get UUID from the url when you open the asset profile in your browser. Review Assets in your Library top # Click Library and then click Maps , Vehicles and Sensors to view the map, vehicle and sensors that you just added to your library. NOTE : You may notice some maps, vehicles and sensors in your library that you did not explicitly add; these were automatically added when your account was created. Create and run a random interactive simulation top # In this section we will create an Interactive Mode simulation which allows you to control the vehicle and interact with the environment while the simulation is running. Random Traffic Scenarios are simulations in which NPCs and pedestrians exhibit randomly generated behavior. This random behavior is deterministic, meaning that the same behavior will be repeated if the same random seed is used in the simulation set up. Click Simulations . Click Add New to create a new simulation. On the New Simulation General pane, enter a name for your new simulation (e.g. \"Random Interactive\"), and select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Interactive mode to enable interactive mode for this simulation, if you wish to be able to play/pause the simulation and change settings after the simulation has started. Click the switch next to Headless mode if you do not wish the simulation to be rendered on the screen. Click the switch next to Create test report to enable cloud to create a test report at the end of simulation run with analytics and event data from sensors on your vehicle. Click the Next button to advance to the Test Case pane. On the New Simulation Test Case pane, select Random Traffic from the Runtime Template list. Next select the BorregasAve map, the Lincoln2017MKZ vehicle and Apollo 5.0 sensor configuration from the dropdown lists. IMPORTANT: If you have not already added a Map, Vehicle or needed Sensors to your library, refer to the Store above for information on how to add them. (Optional) Click + Add another vehicle to add additional Ego vehicles. Note that each ego vehicle will need to be controlled by a separate AD stack and must have it's own bridge connection which would be specified in the Autopilot tab. You could also specify date, time, and environmental settings here but since we are creating an interactive simulation those can interactively be modified while the simulation is running. Click the switches next to Random Traffic and Random Pedestrians to enable these NPCs (non-player characters) in this random simulation. The Random Bicyclists feature is not yet supported and will be available in future releases. Enable Use Pre-defined Seed and manually enter a number to be used as the seed for all random behavior in the simulator. Using a pre-defined seed makes it possible to have deterministic and repeatable random simulations. Click the Next button to advance to the Autopilot pane. On the New Simulation Autopilot pane, select Apollo 5.0 from the Autopilot list. For all ego vehicles in your simulation, enter the details of where can simulator connect to Autopilot. In the Bridge IP, enter \"localhost:9090\" or you can replace \"localhost\" with the IP address of your autopilot system if it is different from the machine on which SVL Simulator is running. Click the Next button to advance to the Publish pane. On the New Simulation Publish pane, click Publish to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created Random Interactive simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button from drop-down menu on top-right corner to re-connect the simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the random interactive simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message indicating that any newly added maps, vehicles and sensors are downloading, after which the simulation should initialize. Since this is an interactive simulation, click the Play button at the bottom left corner of the screen to begin the simulation. You should see the random traffic driving and random pedestrians walking. Click the joystick icon at the bottom of the screen to get a list of keyboard controls. Click the settings icon to adjust the Environment settings. Click the eye icon to enable or disable individual sensor visualizations. If Apollo 5.0 is available and running, then SVL Simulator should connect to the Apollo bridge, and Apollo should be able to drive the ego vehicle in it. Refer to Running Apollo 5.0 with SVL Simulator for more information. To stop the simulation, switch back to the SVL Simulator browser window, click Simulations and click Stop Simulation . API Only Simulation top # In this section we will create an API Only simulation which allows you to control the simulation from the command line using a Python script or Python scenario runner. Click Simulations . Click Add New to create a new simulation. On the New Simulation General pane, enter a name for your new simulation (e.g. \"API Only Mode\"), and select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Create test report if you wish to generate a test report to review for each simulation. Click the Next button to advance to the Test Case pane. On the New Simulation Test Case pane, select API Only from the Runtime Template list. In API Only mode, there are no additional parameters to specify. Click the Next button to advance to the Autopilot pane. Click the Next button to advance to the Publish pane. On the New Simulation Publish pane, click Publish to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created API Only simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button in drop-down menu on top-right corner to re-connect SVL Simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the API Only simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message \"API ready!\" indicating that simulator is ready to connect to your API script. You may now control the simulation using any Python script. Refer to Python API for information on using the Python API to control simulation. To stop the simulation, switch back to the SVL Simulator browser window, click Simulations and click Stop Simulation . Creating and running a Python Test Case Simulation top # In this section we will create and run a Python Test Case simulation available under \"Available from Others\" view in Simulations tab. Note that Python Test Case simulation currently requires the binary build of the latest SVL Simulator running on Linux. Contact us if you need to use this with a self-built or editor build of SVL Simulator. Click Simulations tab and then switch to Available from Others view to see simulations that are available for you to use. Type test-case into the search field and hit enter to show available test-case simulations. Locate the \"Cut-in Scenario\" simulation and click on it to view the details. You could of course select any test-case or other simulation that matches the autopilot software and version that you might be using, but this test-case simulation will work fine to demonstrate how to create a test-case simulation. Click Customize and Add Simulation button to add it to your library. Known Issue : Currently, Customize and Add Simulation doesn't clone the settings of \"Create Test Report\" button and \"Sensor Configuration\" value under vehicle to the new simulation. On the simulation General pane, you may customize the name of the simulation, description, and tags (if desired) but it is not necessary to do so. Select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Create test report to generate a test report to review after running this simulation. Click the Next button to advance to the Test Case pane. On the simulation Test Case pane, you will be see the Python script that controls this test case. Review the Python script and take note of the map specified in the sim.load statement (e.g. SanFrancisco ) as well as the ego vehicles' sensor configurations specified in ego = sim.add_agent statements (e.g. Lincoln2017MKZ vehicle with Apollo 5.0 (full analysis) sensor configuration ). Make sure that these appear in the Map , Vehicle and Sensor configuration fields. If they are not present in the drop down menus, add them to your library following the instructions in Store above. For this cut-in scenario simulation, there are no additional parameters to specify. Click the Next button to advance to the Autopilot pane, select Apollo 5.0 from the Autopilot list. In the Bridge IP, enter \"localhost:9090\" or you can replace \"localhost\" with the IP address of your autopilot system if it is different from the machine on which SVL Simulator is running. Click the Next button to advance to the Publish pane. On the simulation Publish pane, click \"Publish\" to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created Cut-in Scenario simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button in the drop-down menu on top right corner to re-connect SVL Simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the Cut-in Scenario simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message indicating that any newly added maps, vehicles and sensors are downloading, after which the simulation should initialize. At this point you should configure Apollo in Dreamview for the correct map and vehicle, enable the required modules, set a destination point, and send a routing request. Refer to Python Test Cases and Apollo 5.0 or Apollo master if needed for information on executing test case scenarios like cut-in with Apollo. The simulation should stop automatically when finished (or if it times out). When the simulation is finished, switch back to the SVL Simulator browser window. Click Test Results to view results of simulation runs. Review the list of test results and click View to view test results from a specific simulation run. Troubleshooting tips for Python runtime template based simulations If python script throws errors in simulation, follow guide here . Many examples of pythonAPI runtime scenarios can be found in PythonAPI repository on github. You can also find a quick example script here . For Python API simulation to work properly, it should include proper environment variables . Creating and running a Visual Scenario Editor based Simulation top # In this section we will create and run a Visual Scenario Editor based simulation. Note that Visual Scenario Editor based simulation currently requires the binary build of the latest SVL Simulator running on Linux. Contact us if you need to use this with a self-built or editor build of SVL Simulator. Launch SVL Simulator binary and create a scenario using Visual Scenario Editor and save the scenario as \"sample-vse-test-case.json\". Now on the web UI, click Simulations tab in the left panel. Click Add New to create a new simulation. On the New Simulation General pane, enter a name for your new simulation (e.g. \"VSE Simulation\"), and select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Interactive mode to disable interactive mode for this simulation as we want the runner to control the simulation execution automatically. Click the switch next to Create test report to enable cloud to create a test report at the end of simulation run with analytics and event data from sensors on your vehicle. Click the Next button to advance to the Test Case pane. On the New Simulation Test Case pane, select Visual scenario editor from the Runtime Template list. Next select the cloud shaped upload button for \"Scenario\" field and upload the Visual scenario editor created json file \"sample-vse-test-case.json\" Next make sure that map is automatically set properly to what you used when creating the scenario in Visual scenario editor tool. eg: BorregasAve Next make sure that vehicle is automatically set to what you used when creating the scenario in Visual scenario editor tool. eg: Lincoln2017MKZ vehicle and Apollo 5.0 sensor configuration IMPORTANT: If you have not already added a Map, Vehicle or needed Sensors to your library, refer to the Store above for information on how to add them. You could also specify date, time, and environmental settings here. Click the Next button to advance to the Autopilot pane. On the New Simulation Autopilot pane, select Apollo 5.0 from the Autopilot list. In the Bridge IP, enter \"localhost:9090\" or you can replace \"localhost\" with the IP address of your autopilot system if it is different from the machine on which SVL Simulator is running. Click the Next button to advance to the Publish pane. On the New Simulation Publish pane, click Publish to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created Visual scenario editor based simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button from drop-down menu on top-right corner to re-connect the simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the Visual scenario editor based simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message indicating that any newly added maps, vehicles and sensors are downloading, after which the simulation should initialize. Since this is Visual scenario editor based simulation, \"TestCaseRunner\" in background will take care of configuring, enabling & disabling the Autopilot automatically. If Apollo 5.0 is available and running, then SVL Simulator should connect to the Apollo bridge, and Apollo should be able to drive the ego vehicle in it. Refer to Running Apollo 5.0 with SVL Simulator for more information. The simulation should stop automatically when finished (or if it times out or errors). When the simulation is finished, switch back to the SVL Simulator browser window. Click Test Results to view results of simulation runs. Review the list of test results and click View to view test results from a specific simulation run. Known issues with Visual scenario editor based simulations The TestCaseRunner for VSE won't turn off apollo modules when the scenario finishes. You need to disable the modules once you finish the testing. If your VSE based simulation is not working correctly, please make sure apollo has corresponding HD maps (HD map folder name should match the map name) and vehicle calibration files, and bridge is running. Visual scenario editor template simulations with Autopilot only work on Linux at the moment. We are working on adding the Windows support. VSE based simulations only support Apollo 5.0 and above at the moment as it is based on dreamview-python-api .","title":"Running Simulator"},{"location":"running-simulations/running-simulator/#video","text":"","title":"Video"},{"location":"running-simulations/running-simulator/#store","text":"Click tabs under Store to view available maps, vehicles, and sensors that are available for you to use. You'll need to add maps and vehicles from the Store to your own library (or upload maps and vehicles of your own) before you can use them in a simulation. Scroll through the list of all Free maps available, or click Trending to see most commonly downloaded maps or click Shared with me to see private maps shared to you by other users. Locate the BorregasAve map and click \"+\" to add it to your library if not already added (for Random Interactive Simulation below). Locate the SanFrancisco map and click \"+\" to add it to your library if not already added (for Python Test Case Simulation below). Click any tab under Store (if needed) to return to the Store view. Scroll through the list of all Free vehicles available, or click Trending to see most commonly downloaded maps or click Shared with me to see private maps shared to you by other users. Locate the Lincoln2017MKZ vehicle and click \"+\" to add it to your library if not already added (for Random Interactive Simulation or Python Test Case Simulation below). Locate the added vehicle in your library and click on the \"Apollo 5.0\" sensor configuration to confirm if there are any missing sensors in your library (for Random Interactive Simulation below). Locate the added vehicle in your library and click on the \"Apollo 5.0 (full analysis)\" sensor configuration to confirm if there are any missing sensors in your library (for Python Test Case Simulation below). If any sensors are missing, then locate them under \"Sensors\" tab under Store and add them to your library OR follow the instructions on the sensor configuration page. NOTE: All test scripts load map by name or UUID and vehicle's sensor configuration by UUID. So take care to add the specific vehicles, sensors and maps to your library that are needed for the scripts or test cases that you intend to run. Please use UUID instead of name in your Python Test Case scripts. You can get UUID from the url when you open the asset profile in your browser.","title":"Add Assets from the Store"},{"location":"running-simulations/running-simulator/#mylibrary","text":"Click Library and then click Maps , Vehicles and Sensors to view the map, vehicle and sensors that you just added to your library. NOTE : You may notice some maps, vehicles and sensors in your library that you did not explicitly add; these were automatically added when your account was created.","title":"Review Assets in your Library"},{"location":"running-simulations/running-simulator/#randominteractivesimulation","text":"In this section we will create an Interactive Mode simulation which allows you to control the vehicle and interact with the environment while the simulation is running. Random Traffic Scenarios are simulations in which NPCs and pedestrians exhibit randomly generated behavior. This random behavior is deterministic, meaning that the same behavior will be repeated if the same random seed is used in the simulation set up. Click Simulations . Click Add New to create a new simulation. On the New Simulation General pane, enter a name for your new simulation (e.g. \"Random Interactive\"), and select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Interactive mode to enable interactive mode for this simulation, if you wish to be able to play/pause the simulation and change settings after the simulation has started. Click the switch next to Headless mode if you do not wish the simulation to be rendered on the screen. Click the switch next to Create test report to enable cloud to create a test report at the end of simulation run with analytics and event data from sensors on your vehicle. Click the Next button to advance to the Test Case pane. On the New Simulation Test Case pane, select Random Traffic from the Runtime Template list. Next select the BorregasAve map, the Lincoln2017MKZ vehicle and Apollo 5.0 sensor configuration from the dropdown lists. IMPORTANT: If you have not already added a Map, Vehicle or needed Sensors to your library, refer to the Store above for information on how to add them. (Optional) Click + Add another vehicle to add additional Ego vehicles. Note that each ego vehicle will need to be controlled by a separate AD stack and must have it's own bridge connection which would be specified in the Autopilot tab. You could also specify date, time, and environmental settings here but since we are creating an interactive simulation those can interactively be modified while the simulation is running. Click the switches next to Random Traffic and Random Pedestrians to enable these NPCs (non-player characters) in this random simulation. The Random Bicyclists feature is not yet supported and will be available in future releases. Enable Use Pre-defined Seed and manually enter a number to be used as the seed for all random behavior in the simulator. Using a pre-defined seed makes it possible to have deterministic and repeatable random simulations. Click the Next button to advance to the Autopilot pane. On the New Simulation Autopilot pane, select Apollo 5.0 from the Autopilot list. For all ego vehicles in your simulation, enter the details of where can simulator connect to Autopilot. In the Bridge IP, enter \"localhost:9090\" or you can replace \"localhost\" with the IP address of your autopilot system if it is different from the machine on which SVL Simulator is running. Click the Next button to advance to the Publish pane. On the New Simulation Publish pane, click Publish to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created Random Interactive simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button from drop-down menu on top-right corner to re-connect the simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the random interactive simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message indicating that any newly added maps, vehicles and sensors are downloading, after which the simulation should initialize. Since this is an interactive simulation, click the Play button at the bottom left corner of the screen to begin the simulation. You should see the random traffic driving and random pedestrians walking. Click the joystick icon at the bottom of the screen to get a list of keyboard controls. Click the settings icon to adjust the Environment settings. Click the eye icon to enable or disable individual sensor visualizations. If Apollo 5.0 is available and running, then SVL Simulator should connect to the Apollo bridge, and Apollo should be able to drive the ego vehicle in it. Refer to Running Apollo 5.0 with SVL Simulator for more information. To stop the simulation, switch back to the SVL Simulator browser window, click Simulations and click Stop Simulation .","title":"Create and run a random interactive simulation"},{"location":"running-simulations/running-simulator/#apionlysimulation","text":"In this section we will create an API Only simulation which allows you to control the simulation from the command line using a Python script or Python scenario runner. Click Simulations . Click Add New to create a new simulation. On the New Simulation General pane, enter a name for your new simulation (e.g. \"API Only Mode\"), and select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Create test report if you wish to generate a test report to review for each simulation. Click the Next button to advance to the Test Case pane. On the New Simulation Test Case pane, select API Only from the Runtime Template list. In API Only mode, there are no additional parameters to specify. Click the Next button to advance to the Autopilot pane. Click the Next button to advance to the Publish pane. On the New Simulation Publish pane, click Publish to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created API Only simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button in drop-down menu on top-right corner to re-connect SVL Simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the API Only simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message \"API ready!\" indicating that simulator is ready to connect to your API script. You may now control the simulation using any Python script. Refer to Python API for information on using the Python API to control simulation. To stop the simulation, switch back to the SVL Simulator browser window, click Simulations and click Stop Simulation .","title":"API Only Simulation"},{"location":"running-simulations/running-simulator/#pythontestcasesimulation","text":"In this section we will create and run a Python Test Case simulation available under \"Available from Others\" view in Simulations tab. Note that Python Test Case simulation currently requires the binary build of the latest SVL Simulator running on Linux. Contact us if you need to use this with a self-built or editor build of SVL Simulator. Click Simulations tab and then switch to Available from Others view to see simulations that are available for you to use. Type test-case into the search field and hit enter to show available test-case simulations. Locate the \"Cut-in Scenario\" simulation and click on it to view the details. You could of course select any test-case or other simulation that matches the autopilot software and version that you might be using, but this test-case simulation will work fine to demonstrate how to create a test-case simulation. Click Customize and Add Simulation button to add it to your library. Known Issue : Currently, Customize and Add Simulation doesn't clone the settings of \"Create Test Report\" button and \"Sensor Configuration\" value under vehicle to the new simulation. On the simulation General pane, you may customize the name of the simulation, description, and tags (if desired) but it is not necessary to do so. Select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Create test report to generate a test report to review after running this simulation. Click the Next button to advance to the Test Case pane. On the simulation Test Case pane, you will be see the Python script that controls this test case. Review the Python script and take note of the map specified in the sim.load statement (e.g. SanFrancisco ) as well as the ego vehicles' sensor configurations specified in ego = sim.add_agent statements (e.g. Lincoln2017MKZ vehicle with Apollo 5.0 (full analysis) sensor configuration ). Make sure that these appear in the Map , Vehicle and Sensor configuration fields. If they are not present in the drop down menus, add them to your library following the instructions in Store above. For this cut-in scenario simulation, there are no additional parameters to specify. Click the Next button to advance to the Autopilot pane, select Apollo 5.0 from the Autopilot list. In the Bridge IP, enter \"localhost:9090\" or you can replace \"localhost\" with the IP address of your autopilot system if it is different from the machine on which SVL Simulator is running. Click the Next button to advance to the Publish pane. On the simulation Publish pane, click \"Publish\" to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created Cut-in Scenario simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button in the drop-down menu on top right corner to re-connect SVL Simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the Cut-in Scenario simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message indicating that any newly added maps, vehicles and sensors are downloading, after which the simulation should initialize. At this point you should configure Apollo in Dreamview for the correct map and vehicle, enable the required modules, set a destination point, and send a routing request. Refer to Python Test Cases and Apollo 5.0 or Apollo master if needed for information on executing test case scenarios like cut-in with Apollo. The simulation should stop automatically when finished (or if it times out). When the simulation is finished, switch back to the SVL Simulator browser window. Click Test Results to view results of simulation runs. Review the list of test results and click View to view test results from a specific simulation run. Troubleshooting tips for Python runtime template based simulations If python script throws errors in simulation, follow guide here . Many examples of pythonAPI runtime scenarios can be found in PythonAPI repository on github. You can also find a quick example script here . For Python API simulation to work properly, it should include proper environment variables .","title":"Creating and running a Python Test Case Simulation"},{"location":"running-simulations/running-simulator/#vsesimulation","text":"In this section we will create and run a Visual Scenario Editor based simulation. Note that Visual Scenario Editor based simulation currently requires the binary build of the latest SVL Simulator running on Linux. Contact us if you need to use this with a self-built or editor build of SVL Simulator. Launch SVL Simulator binary and create a scenario using Visual Scenario Editor and save the scenario as \"sample-vse-test-case.json\". Now on the web UI, click Simulations tab in the left panel. Click Add New to create a new simulation. On the New Simulation General pane, enter a name for your new simulation (e.g. \"VSE Simulation\"), and select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Interactive mode to disable interactive mode for this simulation as we want the runner to control the simulation execution automatically. Click the switch next to Create test report to enable cloud to create a test report at the end of simulation run with analytics and event data from sensors on your vehicle. Click the Next button to advance to the Test Case pane. On the New Simulation Test Case pane, select Visual scenario editor from the Runtime Template list. Next select the cloud shaped upload button for \"Scenario\" field and upload the Visual scenario editor created json file \"sample-vse-test-case.json\" Next make sure that map is automatically set properly to what you used when creating the scenario in Visual scenario editor tool. eg: BorregasAve Next make sure that vehicle is automatically set to what you used when creating the scenario in Visual scenario editor tool. eg: Lincoln2017MKZ vehicle and Apollo 5.0 sensor configuration IMPORTANT: If you have not already added a Map, Vehicle or needed Sensors to your library, refer to the Store above for information on how to add them. You could also specify date, time, and environmental settings here. Click the Next button to advance to the Autopilot pane. On the New Simulation Autopilot pane, select Apollo 5.0 from the Autopilot list. In the Bridge IP, enter \"localhost:9090\" or you can replace \"localhost\" with the IP address of your autopilot system if it is different from the machine on which SVL Simulator is running. Click the Next button to advance to the Publish pane. On the New Simulation Publish pane, click Publish to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created Visual scenario editor based simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button from drop-down menu on top-right corner to re-connect the simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the Visual scenario editor based simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message indicating that any newly added maps, vehicles and sensors are downloading, after which the simulation should initialize. Since this is Visual scenario editor based simulation, \"TestCaseRunner\" in background will take care of configuring, enabling & disabling the Autopilot automatically. If Apollo 5.0 is available and running, then SVL Simulator should connect to the Apollo bridge, and Apollo should be able to drive the ego vehicle in it. Refer to Running Apollo 5.0 with SVL Simulator for more information. The simulation should stop automatically when finished (or if it times out or errors). When the simulation is finished, switch back to the SVL Simulator browser window. Click Test Results to view results of simulation runs. Review the list of test results and click View to view test results from a specific simulation run. Known issues with Visual scenario editor based simulations The TestCaseRunner for VSE won't turn off apollo modules when the scenario finishes. You need to disable the modules once you finish the testing. If your VSE based simulation is not working correctly, please make sure apollo has corresponding HD maps (HD map folder name should match the map name) and vehicle calibration files, and bridge is running. Visual scenario editor template simulations with Autopilot only work on Linux at the moment. We are working on adding the Windows support. VSE based simulations only support Apollo 5.0 and above at the moment as it is based on dreamview-python-api .","title":"Creating and running a Visual Scenario Editor based Simulation"},{"location":"running-simulations/runtime-container/","text":"Runtime Container #","title":"Runtime Container"},{"location":"running-simulations/runtime-templates/","text":"Runtime Templates # The SVL Simulator has four Runtime Templates . When creating a simulation, a user must choose one of these templates to build their simulation on. The four runtime templates are: Random Traffic - Simulations where traffic (NPCs and pedestrians) behavior is determined by a random seed. Visual Scenario Editor - A template for running simulations created with the Visual Scenario Editor . The simulation is created by uploading a scenario file outputted by VSE directly to the Web UI. Visual Scenario Editor tool is supported on both Windows and Linux, but the end-to-end VSE runtime simulation from webUI is currently only supported on Linux. Python API - A template for running Python API simulations directly from the Web UI. Users can paste Python code directly into the Web UI to quickly create simulations/test cases. Python Api template is supported on both Windows and Linux; however, end-to-end integration with an AD stack such as Apollo using python dreamview-api is only supported on Linux. API-Only - A template for creating a simulation that is controlled by Python scripts located on the users machine.","title":"Runtime templates"},{"location":"simulation-content/add-destinations/","text":"Adding Ego Vehicle Destinations to a Map # It is possible to define fixed destination points in a map. These fixed destination points can be fetched by the Python API and passed on to the AD stack. This enables developers to run a quick test case on any given map. This guide provides instructions on how to properly define destination points. The guide is for users who will be using the SVL Simulator in Developer Mode , creating custom maps, or editing existing maps in the Unity Editor. Choosing the location for the destination point top # All locations on a given map are not valid destination points for an AD stack. Given a starting point and a destination, AD stacks will search through the HD map to find a route that will connect these two points. Although intuitively it may seem that any point on a road would be a valid destination, this is not necessarily the case since all roads in the maps are not necessarily connected, and even if they are connected they may require violation of traffic rules. To solve this problem, destination points are linked to spawn points in the simulator. Each spawn point maintains a list of destinations that are valid for that particular spawn point (details will be covered later). To confirm that a location is accessible from a particular starting point on the map: Open the map annotations tools by selecting Annotate HD Map under the Simulator menu in the Unity editor or use the keyboard shortcut Shift + Alt + M . In the new window, click the View All button to visualize all annotations. Trace a path between each spawn and destination point along the map annotations to confirm a path exists. Below is an example of map annotations in the simulator. The cyan colored markings are the lane center annotations with the arrows indicating the direction of travel. These markings can be traced to validate that a destination point is valid for a given spawn point. Bear in mind that arriving at a given destination may only be possible with one or more lane changes. Creating a destination object top # Once a location is selected, to create a destination point, and empty GameObject must be created at the desired location. Give the GameObject a name that identifies it as a destination object such as DestInfo . Some AD stacks take the orientation of destination points into account in addition to their position; therefore, it is important to make sure that the Z-axis of the GameObject is aligned to the direction the vehicle should be pointing when it arrives at the destination. You should also make sure that the Y-axis position (altitude) of the GameObject places it on the road surface. Below is a top-down view of a destination point (the blue and red axes) correctly aligned to the underlying map annotations. In the 'Inspector' tab, click 'Add Component' and select the Destinationinfo script as seen below. Repeat this step for any other destination point you wish to create on the map. Linking destinations to spawn points top # Spawn points are defined by the SpawnInfo GameObjects in the scene. Each SpawnInfo can hold a list of references to destination points that an ego vehicle should be able to drive to from the spawn point. A single destination point can be referenced by multiple spawn points. To add a destination to the list of destinations for a particular spawn point, select the spawn point and increment the size of its destinations list by one in the Inspector tab: An element will be added to the list ( Element 0 in the image above). Drag the desired destination object from the hierarchy tree to the new element: Repeat this step to assign any destination point to any spawn point. Once all destinations have been added and linked to their coorresponding spawn points, save the scene and move on to building the scene and uploading it to the cloud for use with the simulator. Accessing the destinations using Python API top # The Python API exposes the list of destinations for each Spawn object in Python. Each destination is represented as a transform object that has position and rotation attributes. Example: spawns = sim.get_spawn() # returns a list of spawn points destinations = spawns[0].destinations # list of destinations for the first spawn point","title":"Adding destinations to a map"},{"location":"simulation-content/add-destinations/#choosing-the-location-for-the-destination-point","text":"All locations on a given map are not valid destination points for an AD stack. Given a starting point and a destination, AD stacks will search through the HD map to find a route that will connect these two points. Although intuitively it may seem that any point on a road would be a valid destination, this is not necessarily the case since all roads in the maps are not necessarily connected, and even if they are connected they may require violation of traffic rules. To solve this problem, destination points are linked to spawn points in the simulator. Each spawn point maintains a list of destinations that are valid for that particular spawn point (details will be covered later). To confirm that a location is accessible from a particular starting point on the map: Open the map annotations tools by selecting Annotate HD Map under the Simulator menu in the Unity editor or use the keyboard shortcut Shift + Alt + M . In the new window, click the View All button to visualize all annotations. Trace a path between each spawn and destination point along the map annotations to confirm a path exists. Below is an example of map annotations in the simulator. The cyan colored markings are the lane center annotations with the arrows indicating the direction of travel. These markings can be traced to validate that a destination point is valid for a given spawn point. Bear in mind that arriving at a given destination may only be possible with one or more lane changes.","title":"Choosing the location for the destination point"},{"location":"simulation-content/add-destinations/#creating-a-destination-object","text":"Once a location is selected, to create a destination point, and empty GameObject must be created at the desired location. Give the GameObject a name that identifies it as a destination object such as DestInfo . Some AD stacks take the orientation of destination points into account in addition to their position; therefore, it is important to make sure that the Z-axis of the GameObject is aligned to the direction the vehicle should be pointing when it arrives at the destination. You should also make sure that the Y-axis position (altitude) of the GameObject places it on the road surface. Below is a top-down view of a destination point (the blue and red axes) correctly aligned to the underlying map annotations. In the 'Inspector' tab, click 'Add Component' and select the Destinationinfo script as seen below. Repeat this step for any other destination point you wish to create on the map.","title":"Creating a destination object"},{"location":"simulation-content/add-destinations/#linking-destinations-to-spawn-points","text":"Spawn points are defined by the SpawnInfo GameObjects in the scene. Each SpawnInfo can hold a list of references to destination points that an ego vehicle should be able to drive to from the spawn point. A single destination point can be referenced by multiple spawn points. To add a destination to the list of destinations for a particular spawn point, select the spawn point and increment the size of its destinations list by one in the Inspector tab: An element will be added to the list ( Element 0 in the image above). Drag the desired destination object from the hierarchy tree to the new element: Repeat this step to assign any destination point to any spawn point. Once all destinations have been added and linked to their coorresponding spawn points, save the scene and move on to building the scene and uploading it to the cloud for use with the simulator.","title":"Linking destinations to spawn points"},{"location":"simulation-content/add-destinations/#accessing-the-destinations-using-python-api","text":"The Python API exposes the list of destinations for each Spawn object in Python. Each destination is represented as a transform object that has position and rotation attributes. Example: spawns = sim.get_spawn() # returns a list of spawn points destinations = spawns[0].destinations # list of destinations for the first spawn point","title":"Accessing the destinations using Python API"},{"location":"simulation-content/add-new-ego-vehicle/","text":"How to Add a New Ego Vehicle # This document describes how to create a new ego vehicle in the SVL Simulator. Video top # ( Link ) Adding a new ego vehicle in SVL Simulator. Asset Preparation top # Open vehicle mesh in 3D modeling software such as Blender or Maya. Edit root node to origin z forward. Blender will need .fbx options changed to correct for different axis orientation. Unity is Z forward and Y up. Create a simple mesh from the main body mesh and name it collider. This needs to be less than 256 polygons. Name the body mesh 'Body'. Separate wheel meshes and name them FrontRightWheel, FrontLeftWheel, RearLeftWheel, RearRightWheel. Center pivots on each wheel mesh z forward. Create an empty node called 'LightsGroup' to hold all light emitting meshes, e.g., HeadLights, BrakeLights, RightTurnIndicator, LeftTurnIndicator, ReverseIndicator. These must be separate meshes with separate materials and named exactly the same. Export mesh as an .fbx. If you have applied textures and materials in the modeling software, check the embed media option so these files are included in the file and you can extract in Unity. Getting Started top # Launch SVL Simulator from the Unity Editor (as described here ). Open any scene to work in or create a new one. Create a folder for the new vehicle Assets/External/Vehicles/YourNewEgoVehicle/. Create two folders in this new folder Assets/External/Vehicles/YourNewEgoVehicle/Models/Materials/. Import the vehicle .fbx in Assets/External/Vehicles/YourNewEgoVehicle/Models. In the Unity mesh importer, toggle off all animations, rigs. Export materials and textures into Assets/External/Vehicles/YourNewEgoVehicle/Models/Materials. Convert all LightsGroup mesh materials shaders to Shader Graphs/VehicleLightsShader. You may need to adjust the emission color. Rename the LightsGroup mesh materials to HeadLightsMat, LeftIndicatorMat, RightIndicatorMat, ReverseIndicatorMat, BrakeLightsMat. Setup the Vehicle top # Create a new empty root game object for your vehicle and give it a name. YourNewEgoVehicle Reset transform. Unity can place the game object at random places so it is best to have the object at origin. Place vehicle meshes as a child of the root game object. Be sure that it is at local position origin. Drag YourNewEgoVehicle from the inspector panel into the project panel in Assets/External/Vehicles/YourNewEgoVehicle. This creates a prefab in Unity. Now if you make changes to the mesh, it will stay separate so you won't need to remake the prefab. Toggle off the mesh renderer Unity component for the Collider mesh. Add a mesh collider and toggle convex. Add the Jaguar2015XE open source vehicle to existing ego vehicle prefab to the hierarchy. This makes it easy to copy components to the new vehicle. Make sure the new vehicle prefab is at origin and overlaps with the Jaguar2015XE vehicle. This makes it easy to align light objects and wheel colliders. Unpack Jaguar2015XE prefab completely. We will use the needed components from this vehicle. Move the WheelColliderHolder object to the new vehicle as a child of the parent object. Move HeadLights, BrakeLights, IndicatorsLeft, IndicatorsRight, IndicatorsReverse and InteriorLights holder objects as a child of the new vehicle parent object. Copy all Unity components from the root object of the Jaguar2015XE to the root object of your new vehicle. Right click on each component on the reference prefab root, right click to copy and then right click on the root of the new vehicle to paste component as new. Do this for every component. Delete the Jaguar2015XE from the scene, it is no longer needed. DO NOT SAVE CHANGES in the Jaguar2015XE Reset transforms for WheelColliderHolder, HeadLights, BrakeLights, IndicatorsLeft, IndicatorsRight, IndicatorsReverse and InteriorLights objects so they align to the new vehicle. Align each wheel collider object to each wheel and align lights to the new vehicle light meshes. Create an empty object called BaseLink as a child of the root object of the new vehicle. Move BaseLink so it aligns with the center of the back axle of the new vehicle. Apply the component BaseLink to the BaseLink object. NOTE Not in video. Drag the BaseLink object to the BaseLink public reference in the VehicleSMI component. NOTE Be sure you have a MeshCollider component on the Collider mesh, the Collision mesh renderer component disabled and the collider has convex enabled. Apply the Correct References top # We need to apply the correct references to the vehicle scripts since the public variables are still referencing the other vehicle: Note: Be sure to click the Apply button in the Inspector after each component change. This saves the change to the prefab in the project. Check that no variable references are still bold after updating the references. VehicleSMI.cs top # For the VehicleSMI script, reference the following colliders and meshes in the Axles dropdown. Reference the FL WheelCollider in Axles Element 0 (Left) Reference the FR WheelCollider in Axles Element 0 (Right) Reference the RL WheelCollider in Axles Element 1 (Left) Reference the RR WheelCollider in Axles Element 1 (Right) Reference the FrontLeftWheel wheel mesh in Axles Element 0 (Left Visuals) Reference the FrontRightWheel wheel mesh in Axles Element 0 (Right Visuals) Reference the RearLeftWheel wheel mesh in Axles Element 1 (Left Visuals) Reference the RearRightWheel wheel mesh in Axles Element 1 (Right Visuals) VehicleActions.cs top # Duplicate the High and Low beam textures in the Jaguar2015XE and place in Assets/External/Vehicles/YourNewEgoVehicle/Models/Materials. Rename the prefix to the name of your vehicle, e.g., YourNewEgoVehicleHeadLightHighBeamCookie. NOTE Be sure the texture in set to 256 compression in the texture settings. Drag the textures into the VehicleActions.cs public references in the inspector panel on the new vehicle. Apply changes to the vehicle prefab. Final Steps top # Set the vehicle root and all child objects to the Agent layer. Set the vehicle root to the Player tag Apply changes to the vehicle prefab. Open Simulator -> Build... menu at the top of the Unity editor Be sure you have both Windows and Linux support to make asset bundles. Check the new vehicle and press build. When completed, in the root of the repo, YourNewEgoVehicle bundle will be in Simulator/AssetBundles/Vehicles/ Run simulator, open Library -> Vehicles tab in WebUI and select Add New . Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Create a new sensor configuration. Publish the NewEgoVehicle. Congratulations! You have successfully added a new ego vehicle! It can now be used with any simulation you create.","title":"Creating a new ego vehicle"},{"location":"simulation-content/add-new-ego-vehicle/#video","text":"( Link ) Adding a new ego vehicle in SVL Simulator.","title":"Video"},{"location":"simulation-content/add-new-ego-vehicle/#asset-preparation","text":"Open vehicle mesh in 3D modeling software such as Blender or Maya. Edit root node to origin z forward. Blender will need .fbx options changed to correct for different axis orientation. Unity is Z forward and Y up. Create a simple mesh from the main body mesh and name it collider. This needs to be less than 256 polygons. Name the body mesh 'Body'. Separate wheel meshes and name them FrontRightWheel, FrontLeftWheel, RearLeftWheel, RearRightWheel. Center pivots on each wheel mesh z forward. Create an empty node called 'LightsGroup' to hold all light emitting meshes, e.g., HeadLights, BrakeLights, RightTurnIndicator, LeftTurnIndicator, ReverseIndicator. These must be separate meshes with separate materials and named exactly the same. Export mesh as an .fbx. If you have applied textures and materials in the modeling software, check the embed media option so these files are included in the file and you can extract in Unity.","title":"Asset Preparation"},{"location":"simulation-content/add-new-ego-vehicle/#getting-started","text":"Launch SVL Simulator from the Unity Editor (as described here ). Open any scene to work in or create a new one. Create a folder for the new vehicle Assets/External/Vehicles/YourNewEgoVehicle/. Create two folders in this new folder Assets/External/Vehicles/YourNewEgoVehicle/Models/Materials/. Import the vehicle .fbx in Assets/External/Vehicles/YourNewEgoVehicle/Models. In the Unity mesh importer, toggle off all animations, rigs. Export materials and textures into Assets/External/Vehicles/YourNewEgoVehicle/Models/Materials. Convert all LightsGroup mesh materials shaders to Shader Graphs/VehicleLightsShader. You may need to adjust the emission color. Rename the LightsGroup mesh materials to HeadLightsMat, LeftIndicatorMat, RightIndicatorMat, ReverseIndicatorMat, BrakeLightsMat.","title":"Getting Started"},{"location":"simulation-content/add-new-ego-vehicle/#setup-the-vehicle","text":"Create a new empty root game object for your vehicle and give it a name. YourNewEgoVehicle Reset transform. Unity can place the game object at random places so it is best to have the object at origin. Place vehicle meshes as a child of the root game object. Be sure that it is at local position origin. Drag YourNewEgoVehicle from the inspector panel into the project panel in Assets/External/Vehicles/YourNewEgoVehicle. This creates a prefab in Unity. Now if you make changes to the mesh, it will stay separate so you won't need to remake the prefab. Toggle off the mesh renderer Unity component for the Collider mesh. Add a mesh collider and toggle convex. Add the Jaguar2015XE open source vehicle to existing ego vehicle prefab to the hierarchy. This makes it easy to copy components to the new vehicle. Make sure the new vehicle prefab is at origin and overlaps with the Jaguar2015XE vehicle. This makes it easy to align light objects and wheel colliders. Unpack Jaguar2015XE prefab completely. We will use the needed components from this vehicle. Move the WheelColliderHolder object to the new vehicle as a child of the parent object. Move HeadLights, BrakeLights, IndicatorsLeft, IndicatorsRight, IndicatorsReverse and InteriorLights holder objects as a child of the new vehicle parent object. Copy all Unity components from the root object of the Jaguar2015XE to the root object of your new vehicle. Right click on each component on the reference prefab root, right click to copy and then right click on the root of the new vehicle to paste component as new. Do this for every component. Delete the Jaguar2015XE from the scene, it is no longer needed. DO NOT SAVE CHANGES in the Jaguar2015XE Reset transforms for WheelColliderHolder, HeadLights, BrakeLights, IndicatorsLeft, IndicatorsRight, IndicatorsReverse and InteriorLights objects so they align to the new vehicle. Align each wheel collider object to each wheel and align lights to the new vehicle light meshes. Create an empty object called BaseLink as a child of the root object of the new vehicle. Move BaseLink so it aligns with the center of the back axle of the new vehicle. Apply the component BaseLink to the BaseLink object. NOTE Not in video. Drag the BaseLink object to the BaseLink public reference in the VehicleSMI component. NOTE Be sure you have a MeshCollider component on the Collider mesh, the Collision mesh renderer component disabled and the collider has convex enabled.","title":"Setup the Vehicle"},{"location":"simulation-content/add-new-ego-vehicle/#apply-the-correct-references","text":"We need to apply the correct references to the vehicle scripts since the public variables are still referencing the other vehicle: Note: Be sure to click the Apply button in the Inspector after each component change. This saves the change to the prefab in the project. Check that no variable references are still bold after updating the references.","title":"Apply the Correct References"},{"location":"simulation-content/add-new-ego-vehicle/#vehiclesmi.cs","text":"For the VehicleSMI script, reference the following colliders and meshes in the Axles dropdown. Reference the FL WheelCollider in Axles Element 0 (Left) Reference the FR WheelCollider in Axles Element 0 (Right) Reference the RL WheelCollider in Axles Element 1 (Left) Reference the RR WheelCollider in Axles Element 1 (Right) Reference the FrontLeftWheel wheel mesh in Axles Element 0 (Left Visuals) Reference the FrontRightWheel wheel mesh in Axles Element 0 (Right Visuals) Reference the RearLeftWheel wheel mesh in Axles Element 1 (Left Visuals) Reference the RearRightWheel wheel mesh in Axles Element 1 (Right Visuals)","title":"VehicleSMI.cs"},{"location":"simulation-content/add-new-ego-vehicle/#vehicleactions.cs","text":"Duplicate the High and Low beam textures in the Jaguar2015XE and place in Assets/External/Vehicles/YourNewEgoVehicle/Models/Materials. Rename the prefix to the name of your vehicle, e.g., YourNewEgoVehicleHeadLightHighBeamCookie. NOTE Be sure the texture in set to 256 compression in the texture settings. Drag the textures into the VehicleActions.cs public references in the inspector panel on the new vehicle. Apply changes to the vehicle prefab.","title":"VehicleActions.cs"},{"location":"simulation-content/add-new-ego-vehicle/#final-steps","text":"Set the vehicle root and all child objects to the Agent layer. Set the vehicle root to the Player tag Apply changes to the vehicle prefab. Open Simulator -> Build... menu at the top of the Unity editor Be sure you have both Windows and Linux support to make asset bundles. Check the new vehicle and press build. When completed, in the root of the repo, YourNewEgoVehicle bundle will be in Simulator/AssetBundles/Vehicles/ Run simulator, open Library -> Vehicles tab in WebUI and select Add New . Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Create a new sensor configuration. Publish the NewEgoVehicle. Congratulations! You have successfully added a new ego vehicle! It can now be used with any simulation you create.","title":"Final Steps"},{"location":"simulation-content/add-new-map/","text":"How to Add a New Map # This document will describe how to create a new map in the SVL Simulator. Video top # ( Link ) Adding a new map in SVL Simulator. Getting Started top # The following text is a list of the steps described in the above video. Launch SVL Simulator from the Unity Editor (as described here ). In the top menu, select File -> New Scene or Ctrl+N Create your new map folders in Assets/External/Environments/NewMap with /Models/Materials and /Prefabs in this root folder It is helpful to clone the CubeTown repository as an example scene to see how all Simulator features are setup and assets organized. Import Assets top # Import assets you created or obtained into the Unity Editor. Move models, textures, materials and any other assets you need into the /NewMap folders. The new map should have no asset dependencies from folders outside of /NewMap. Do not include script files or non-HDRP shaders. If you need scripts in your scene, you will need to place in the source files of the SVL Simulator and build your own binaries. If your assets have non-standard shaders you will need to change them to the Unity Standard shader before converting to HDRP. Once all non Standard shaders are changed, you can convert the materials to HDRP materials. Select all materials. Select Edit -> Render Pipeline -> Upgrade Selected Materials to High Definition Materials. If the materials are all using Standard shaders, all shader settings will be converted with little need for adjustment. Be sure all materials have GPU instancing checked. Now we can start building the new map. Creating the scene top # Delete the MapHolder object in the Hierarchy Panel. This will be added back with the annotation tool later. Create an Object to hold the mesh data for your map. Name it the same as your folder, e.g. NewMap. Zero out the transform position/rotation: 0,0,0 and scale: 1,1,1. Save the scene in the root of the /NewMap, naming exactly the same, NewMap.scene. Delete or move any other scene files out of this root folder. Open the Annotation Tool, select Simulator -> Annotation Tool, and click Create Map Holder. This will add the correct holder for annotation named already the same as your map scene name. Save the scene again. Do this often. You should always be hitting Ctrl+S during development. Create the scene by adding meshes and colliders to the NewMap mesh holder object as children. You can further organize by making secondary holder objects, e.g., Roads, Buildings, Props. Just be sure to zero transforms for holder objects. Once organized and positioned, select the root holder object and check static in the top right of the Inspector Panel. Then, open the static menu and uncheck Off Mesh Link Generation and Batching Static. HDRP supports GPU instancing materials OR Batching, not both. GPU instancing is more performant. Then select all mesh holder root objects that you do not want pedestrians to walk on, e.g., Props, Buildings, Signs, and uncheck Navigation Static. This will make Nav Meshes ignore these mesh types. Add the TimeOfDayLight.cs component to all meshes that will emit light, not building windows. Then add a light component object as a child of this light. Add TimeOfDayBuilding.cs to the building holder. Be sure you apply black textures to the emission component of the meshes you don't want to emit and create an emission map for the ones you do want to light during the night. Set semantic tags for the holder objects OR the objects themselves, not both. Code will search and apply tags to materials from parent, down to all child objects. Set physics layers for all meshes to default. Be sure to do this from the root object so all child objects get set. Purchased assets may have very odd tags and physics layers that you will need to fix. Change all road material shaders to ShaderGraphs -> EnvironmentSimulation for environmental effects to work on road surfaces. Add mesh colliders to all meshes that need physics collisions, especially roads. Extra Steps not in video top # NavMesh and occlusion culling top # NavMesh must be baked if you want pedestrians to be able to move in your environment. Occlusion Culling can help improve performance on complex scenes. Select Window -> AI -> Navigation. Select Object tab in this panel. Select groups of meshes, not their holders, e.g., road, sidewalk, grass, curb meshes. In the Object tab in Navigation, set the meshes to the types. Select the Bake tab. Click the Bake button. This creates a Nav mesh that gets saved in the /NewMap folder. Select Window -> Rendering -> Occlusion Culling. Select Bake tab. Click the Bake button. This creates an occlusion object that helps with frame rate. Adding rain colliders top # These steps have to be performed if you want rain to be stopped by obstacles (e.g. roofs). Locate all meshes that should stop rain. Review import settings for each mesh - in Model tab, the Read/Write Enabled option must be enabled. Select meshes one by one, or select their common hierarchy parent. Note: you can have multiple colliders on multiple objects on the scene. Note: if you choose to use the parent, all children meshes will be treated as single, merged collider. In the Inspector, select Add Component -> Rain Collider. In the newly added component, change Resolution based on mesh size - larger meshes should use higher resolution. Note: unless the mesh covers a large area, resolution of 128 or lower is usually enough. Click \"Calculate Size\" and make sure that calculated value does not exceed Memory Limit. After all colliders have been added, save the scene. Finalize other map features top # Select MapOrigin object in the Hierarchy Panel. Set Northing, Easting, UTM and TimeZone. Set NPC settings for the map. Edit SpawnInfo position and rotation. This object(s) will determine where the EGO vehicle(s) will spawn. If you want multiple spawn points, duplicate this object and move to another position on the map. If importing or creating HDMap annotations, rotate all map objects so map North faces in the -X coordinate in Unity Scene View (see here ). Use the simulator tool to rotate the scene to the correct position at Simulator -> Editor Tools -> Scene Type -> RotateSceneView and then the Run button. Make all scene objects a child of a empty game object at origin, rotate and then remove parent. Save the scene. Select Simulator -> Build... Select NewMap and click the Build button. Map Preview top # Map Preview is a feature in simulator to force a camera position and rotation for AssetBundle preview images in the web user interface. Create a empty game object and place it at the root of the scene. Add MapPreview.cs to the new object. Rotate and Position the object in the editor z forward and Y up at the position you want. You can view the scene from the object by selecting the menu GameObject -> Align View to Selected. Save the scene. Build the environment map bundle. Upload to your SVL Simulator account. See the preview image in the web user interface.","title":"Creating a new map"},{"location":"simulation-content/add-new-map/#video","text":"( Link ) Adding a new map in SVL Simulator.","title":"Video"},{"location":"simulation-content/add-new-map/#getting-started","text":"The following text is a list of the steps described in the above video. Launch SVL Simulator from the Unity Editor (as described here ). In the top menu, select File -> New Scene or Ctrl+N Create your new map folders in Assets/External/Environments/NewMap with /Models/Materials and /Prefabs in this root folder It is helpful to clone the CubeTown repository as an example scene to see how all Simulator features are setup and assets organized.","title":"Getting Started"},{"location":"simulation-content/add-new-map/#import-assets","text":"Import assets you created or obtained into the Unity Editor. Move models, textures, materials and any other assets you need into the /NewMap folders. The new map should have no asset dependencies from folders outside of /NewMap. Do not include script files or non-HDRP shaders. If you need scripts in your scene, you will need to place in the source files of the SVL Simulator and build your own binaries. If your assets have non-standard shaders you will need to change them to the Unity Standard shader before converting to HDRP. Once all non Standard shaders are changed, you can convert the materials to HDRP materials. Select all materials. Select Edit -> Render Pipeline -> Upgrade Selected Materials to High Definition Materials. If the materials are all using Standard shaders, all shader settings will be converted with little need for adjustment. Be sure all materials have GPU instancing checked. Now we can start building the new map.","title":"Import Assets"},{"location":"simulation-content/add-new-map/#creating-the-scene","text":"Delete the MapHolder object in the Hierarchy Panel. This will be added back with the annotation tool later. Create an Object to hold the mesh data for your map. Name it the same as your folder, e.g. NewMap. Zero out the transform position/rotation: 0,0,0 and scale: 1,1,1. Save the scene in the root of the /NewMap, naming exactly the same, NewMap.scene. Delete or move any other scene files out of this root folder. Open the Annotation Tool, select Simulator -> Annotation Tool, and click Create Map Holder. This will add the correct holder for annotation named already the same as your map scene name. Save the scene again. Do this often. You should always be hitting Ctrl+S during development. Create the scene by adding meshes and colliders to the NewMap mesh holder object as children. You can further organize by making secondary holder objects, e.g., Roads, Buildings, Props. Just be sure to zero transforms for holder objects. Once organized and positioned, select the root holder object and check static in the top right of the Inspector Panel. Then, open the static menu and uncheck Off Mesh Link Generation and Batching Static. HDRP supports GPU instancing materials OR Batching, not both. GPU instancing is more performant. Then select all mesh holder root objects that you do not want pedestrians to walk on, e.g., Props, Buildings, Signs, and uncheck Navigation Static. This will make Nav Meshes ignore these mesh types. Add the TimeOfDayLight.cs component to all meshes that will emit light, not building windows. Then add a light component object as a child of this light. Add TimeOfDayBuilding.cs to the building holder. Be sure you apply black textures to the emission component of the meshes you don't want to emit and create an emission map for the ones you do want to light during the night. Set semantic tags for the holder objects OR the objects themselves, not both. Code will search and apply tags to materials from parent, down to all child objects. Set physics layers for all meshes to default. Be sure to do this from the root object so all child objects get set. Purchased assets may have very odd tags and physics layers that you will need to fix. Change all road material shaders to ShaderGraphs -> EnvironmentSimulation for environmental effects to work on road surfaces. Add mesh colliders to all meshes that need physics collisions, especially roads.","title":"Creating the scene"},{"location":"simulation-content/add-new-map/#extra-steps-not-in-video","text":"","title":"Extra Steps not in video"},{"location":"simulation-content/add-new-map/#navmesh-and-occlusion-culling","text":"NavMesh must be baked if you want pedestrians to be able to move in your environment. Occlusion Culling can help improve performance on complex scenes. Select Window -> AI -> Navigation. Select Object tab in this panel. Select groups of meshes, not their holders, e.g., road, sidewalk, grass, curb meshes. In the Object tab in Navigation, set the meshes to the types. Select the Bake tab. Click the Bake button. This creates a Nav mesh that gets saved in the /NewMap folder. Select Window -> Rendering -> Occlusion Culling. Select Bake tab. Click the Bake button. This creates an occlusion object that helps with frame rate.","title":"NavMesh and occlusion culling"},{"location":"simulation-content/add-new-map/#adding-rain-colliders","text":"These steps have to be performed if you want rain to be stopped by obstacles (e.g. roofs). Locate all meshes that should stop rain. Review import settings for each mesh - in Model tab, the Read/Write Enabled option must be enabled. Select meshes one by one, or select their common hierarchy parent. Note: you can have multiple colliders on multiple objects on the scene. Note: if you choose to use the parent, all children meshes will be treated as single, merged collider. In the Inspector, select Add Component -> Rain Collider. In the newly added component, change Resolution based on mesh size - larger meshes should use higher resolution. Note: unless the mesh covers a large area, resolution of 128 or lower is usually enough. Click \"Calculate Size\" and make sure that calculated value does not exceed Memory Limit. After all colliders have been added, save the scene.","title":"Adding rain colliders"},{"location":"simulation-content/add-new-map/#finalize-other-map-features","text":"Select MapOrigin object in the Hierarchy Panel. Set Northing, Easting, UTM and TimeZone. Set NPC settings for the map. Edit SpawnInfo position and rotation. This object(s) will determine where the EGO vehicle(s) will spawn. If you want multiple spawn points, duplicate this object and move to another position on the map. If importing or creating HDMap annotations, rotate all map objects so map North faces in the -X coordinate in Unity Scene View (see here ). Use the simulator tool to rotate the scene to the correct position at Simulator -> Editor Tools -> Scene Type -> RotateSceneView and then the Run button. Make all scene objects a child of a empty game object at origin, rotate and then remove parent. Save the scene. Select Simulator -> Build... Select NewMap and click the Build button.","title":"Finalize other map features"},{"location":"simulation-content/add-new-map/#map-preview","text":"Map Preview is a feature in simulator to force a camera position and rotation for AssetBundle preview images in the web user interface. Create a empty game object and place it at the root of the scene. Add MapPreview.cs to the new object. Rotate and Position the object in the editor z forward and Y up at the position you want. You can view the scene from the object by selecting the menu GameObject -> Align View to Selected. Save the scene. Build the environment map bundle. Upload to your SVL Simulator account. See the preview image in the web user interface.","title":"Map Preview"},{"location":"simulation-content/assets/","text":"Adding Assets # The main repository for the SVL Simulator does not contain art assets. We have moved these assets to external repositories so users can easily add their own. Adding assets is only supported when cloning simulator source with Unity Editor in developer mode . Currently there are several open-source examples. Environments: CubeTown SingleLaneRoad Shalun SanFrancisco Vehicles: Jaguar2015XE NPCs: DefaultNPC Pedestrians: Walkers Table of Contents Adding an Asset Setup an Asset Building Assets Assets development Adding an Asset top # All assets have been removed from Simulator source code. When working in developer mode and building custom binaries, users must build assets locally for npcs and pedestrians. Assets need to be cloned into a specific location in the project: simulator/Assets/External/Environments for Environments simulator/Assets/External/Vehicles for Vehicles simulator/Assets/External/NPCs for NPCs and Bicycles simulator/Assets/External/Pedestrians for Walkers, Scooters and Animals Clone the desired asset into the appropriate folder. Do not change the name of the folder that the asset is cloned into, it must match the name of the asset. For environments: simulator/Assets/External/Environments/Mars must contain simulator/Assets/External/Environments/Mars/Mars.unity For vehicles: simulator/Assets/External/Vehicles/Rover must contain simulator/Assets/External/Vehicles/Rover/Rover.prefab For npcs: simulator/Assets/External/NPCs/DefaultNPC must contain simulator/Assets/External/NPCs/DefaultNPC/Jeep/Jeep.prefab For pedestrians: simulator/Assets/External/Pedestrians/Walkers must contain simulator/Assets/External/Pedestrians/Walkers/Bob/Bob.prefab Setup an Asset top # For map assets setup see here . For ego vehicle asset setup see here . For npc asset setup see here . For pedestrian asset setup see here . Building Assets top # Assets are built using the same build script as the simulator. Follow the build instructions through step 17. NPCs and Pedestrians must be built locally for custom binaries. They must be added to AssetBundles folder to be loaded by the simulator binary or in editor source in developer mode . IMPORTANT Windows and Linux support must be installed with Unity to build assetbundles Asset development top # In the Unity editor, the menu entry Simulator > Development Settings opens a panel of settings helpful while developing and testing assets. At the top you will find general information about the cluster instance your editor represents such as the web user interface instance it is linked to and under which id. Below there are convenience buttons to link the instance without opening the Loader scene, managing the cluster on web user interface and to refresh the displayed data. Version lets you choose or enter the version string that is used to connect with web user interface and influences linking and asset version matching as served by web user interface to the simulator in the Unity Editor. The following simulation settings only apply when you enter play mode from a Map scene instead of the Loader scene. You can configure list of simulation variables such as interactive, time of day that influcence the behaviour of the simulation as you hit play in the Unity Editor. Ego Vehicle Setup lets you choose and configure the Ego Vehicle used for development runs. You can choose between cloud and local vehicles. For local vehicles, this lists the vehicle prefabs found under simulator/Assets/External/Vehicles/ , and allows you to enter a json sensor configuration to add to the vehicle upon simulation start. The sensor json configuration supports web user interface provided sensors referenced by plugin Id - these will be downloaded as required. It also supports local source code of sensors found under simulator/Assets/External/Sensors/ if you enable the Debug Mode under Simulator > Developer Debug Mode and (as of 2021.2.2) also have an Assetbundle of your sensor built once. For cloud vehicles, it lists all Vehicles in your web user interface library and their sensor configurations, this is intended to quickly iterate on Map development with an already existing vehicle. Lastly, you can select which NPCs will be part of the simulation run.","title":"Building content"},{"location":"simulation-content/assets/#adding-an-asset","text":"All assets have been removed from Simulator source code. When working in developer mode and building custom binaries, users must build assets locally for npcs and pedestrians. Assets need to be cloned into a specific location in the project: simulator/Assets/External/Environments for Environments simulator/Assets/External/Vehicles for Vehicles simulator/Assets/External/NPCs for NPCs and Bicycles simulator/Assets/External/Pedestrians for Walkers, Scooters and Animals Clone the desired asset into the appropriate folder. Do not change the name of the folder that the asset is cloned into, it must match the name of the asset. For environments: simulator/Assets/External/Environments/Mars must contain simulator/Assets/External/Environments/Mars/Mars.unity For vehicles: simulator/Assets/External/Vehicles/Rover must contain simulator/Assets/External/Vehicles/Rover/Rover.prefab For npcs: simulator/Assets/External/NPCs/DefaultNPC must contain simulator/Assets/External/NPCs/DefaultNPC/Jeep/Jeep.prefab For pedestrians: simulator/Assets/External/Pedestrians/Walkers must contain simulator/Assets/External/Pedestrians/Walkers/Bob/Bob.prefab","title":"Adding an Asset"},{"location":"simulation-content/assets/#setup-an-asset","text":"For map assets setup see here . For ego vehicle asset setup see here . For npc asset setup see here . For pedestrian asset setup see here .","title":"Setup an Asset"},{"location":"simulation-content/assets/#building-assets","text":"Assets are built using the same build script as the simulator. Follow the build instructions through step 17. NPCs and Pedestrians must be built locally for custom binaries. They must be added to AssetBundles folder to be loaded by the simulator binary or in editor source in developer mode . IMPORTANT Windows and Linux support must be installed with Unity to build assetbundles","title":"Building Assets"},{"location":"simulation-content/assets/#asset-development","text":"In the Unity editor, the menu entry Simulator > Development Settings opens a panel of settings helpful while developing and testing assets. At the top you will find general information about the cluster instance your editor represents such as the web user interface instance it is linked to and under which id. Below there are convenience buttons to link the instance without opening the Loader scene, managing the cluster on web user interface and to refresh the displayed data. Version lets you choose or enter the version string that is used to connect with web user interface and influences linking and asset version matching as served by web user interface to the simulator in the Unity Editor. The following simulation settings only apply when you enter play mode from a Map scene instead of the Loader scene. You can configure list of simulation variables such as interactive, time of day that influcence the behaviour of the simulation as you hit play in the Unity Editor. Ego Vehicle Setup lets you choose and configure the Ego Vehicle used for development runs. You can choose between cloud and local vehicles. For local vehicles, this lists the vehicle prefabs found under simulator/Assets/External/Vehicles/ , and allows you to enter a json sensor configuration to add to the vehicle upon simulation start. The sensor json configuration supports web user interface provided sensors referenced by plugin Id - these will be downloaded as required. It also supports local source code of sensors found under simulator/Assets/External/Sensors/ if you enable the Debug Mode under Simulator > Developer Debug Mode and (as of 2021.2.2) also have an Assetbundle of your sensor built once. For cloud vehicles, it lists all Vehicles in your web user interface library and their sensor configurations, this is intended to quickly iterate on Map development with an already existing vehicle. Lastly, you can select which NPCs will be part of the simulation run.","title":"Assets development"},{"location":"simulation-content/control-calibration/","text":"How to Collect Data with Control Calibration Sensor # Control Calibration Sensor is for collecting control data to generate control calibration table which can be referred by control module to decide throttle, brake and steering command. Setup top # Check if there is Wide Flat Map ls /apollo/modules/map/data/wide_flat_map Instructions top # Add WideFlatMap into WebUI. The assetbundle is available on the content store Create a vehicle with a CyberRT bridge type and add total control calibration criteria into ego vehicle's sensor parameters . Start Apollo docker container /apollo/docker/scripts/dev_start.sh Go into Apollo docker container with /apollo/docker/scripts/dev_into.sh Run Cyber bridge /apollo/scripts/bridge.sh Run localization module mainboard -d /apollo/modules/localization/dag/dag_streaming_rtk_localization.dag Run dreamview bootstrap.sh In a browser, navigate to Dreamview at http://localhost:8888/ Choose vehicle model. Choose Map as Wide Flat Map. Choose setup mode to Vehicle Calibration. In Others tab, choose Data Collection Monitor In Modules tab, turn Recorder on. In the Simulator's WebUI, create a simulation with WideFlatMap and the vehicle created in Step 2 (with the bridge connection string to the computer running Apollo). In Simulator's WebUI, start simulation. You can see progress bar filled as Apollo collects data. Once all progress bars are filled, vehicle control calibration is done.","title":"How to Collect Data with Control Calibration Sensor"},{"location":"simulation-content/control-calibration/#setup","text":"Check if there is Wide Flat Map ls /apollo/modules/map/data/wide_flat_map","title":"Setup"},{"location":"simulation-content/control-calibration/#instructions","text":"Add WideFlatMap into WebUI. The assetbundle is available on the content store Create a vehicle with a CyberRT bridge type and add total control calibration criteria into ego vehicle's sensor parameters . Start Apollo docker container /apollo/docker/scripts/dev_start.sh Go into Apollo docker container with /apollo/docker/scripts/dev_into.sh Run Cyber bridge /apollo/scripts/bridge.sh Run localization module mainboard -d /apollo/modules/localization/dag/dag_streaming_rtk_localization.dag Run dreamview bootstrap.sh In a browser, navigate to Dreamview at http://localhost:8888/ Choose vehicle model. Choose Map as Wide Flat Map. Choose setup mode to Vehicle Calibration. In Others tab, choose Data Collection Monitor In Modules tab, turn Recorder on. In the Simulator's WebUI, create a simulation with WideFlatMap and the vehicle created in Step 2 (with the bridge connection string to the computer running Apollo). In Simulator's WebUI, start simulation. You can see progress bar filled as Apollo collects data. Once all progress bars are filled, vehicle control calibration is done.","title":"Instructions"},{"location":"simulation-content/differential-drive-control/","text":"Differential Drive Control Plugin # The Differential Drive Control plugin provides a means for driving differential drive robots/ego vehicles. Differential drive is a drive mechanism commonly used in mobile robots where two wheels are mounted on a common axis and each is capable of being driven forward or backward independently. Differential drive robots often have wheel encoders that precisely track the rotation of each wheel. The rotation of each wheel can be used both as feedback for a closed loop controller to precisely control the robot motion, and for dead reckoning. The differential drive control plugin of the SVL Simulator, subscribes to control velocity values in the form of geometry_msgs/Twist message types (see definition ), implements a PID controller to control each wheel, and publishes an odometric pose in the form of a nav_msgs/Odometry message (see definition ). Wheel Control # Setup # To be able to control the wheels on the robot/ego vehicle, the control plugin needs to be able to locate the GameObjects for each drive wheel in Unity. To maintain generality and avoid restricting the control plugin to a particular robot model the path from the main body to each wheel is defined as a customizable parameter which can be edited through the simulator's web user interface. The figure below shows the expanded hierarchy for the LG CLOi robot as seen in the Unity Editor. The path to each of the wheels can be followed through the tree and entered into the web user interface. Sensor configuration for the Differential Drive Control plugin in the web user interface is seen below. The path to the two wheels are defined under LeftWheelLinkPath and RightWheelLinkPath . Calculating wheel motion # The incoming control command expected by the plugin has a linear ( \\(V\\) ) and an angular velocity ( \\(\\omega\\) ) component. Using this input, as well as the wheel separation ( \\(l\\) ) and wheel radius ( \\(R\\) ) the rotation of each wheel can be computed as follows. \\[ \\omega_r = (V + \\omega . l/2)/R \\] \\[ \\omega_l = (V - \\omega . l/2)/R \\] The angular velocities \\(\\omega_l\\) and \\(\\omega_r\\) are the rotation rates of the left and right wheels which are applied to their respective articulated bodies in Unity. Note that the control plugin only expects an \\(x\\) component on the commanded linear velocity and a \\(z\\) component for the commanded angular velocity. Calculating the odometry pose # The odometry pose is calculated relative to the starting position of the robot. The yaw angle of the robot is calculated through an internal IMU, and is used alongside the rotation of each wheel as observed by the wheel odometers to calculate the pose of the robot/ego vehicle at each instance. The odometric pose of the robot is expected to drift over time as it does not account for wheel slippage which can be significant. The pose is published using a nav_msgs/Odometry message type. Since the odometry pose cannot estimate changes in elevation, the \\(z\\) component of the pose is not needed and is often left as 0. Here we are using this field to convey the yaw angle of the robot.","title":"Differential Drive Control sensor"},{"location":"simulation-content/differential-drive-control/#wheel-control","text":"","title":"Wheel Control"},{"location":"simulation-content/differential-drive-control/#setup","text":"To be able to control the wheels on the robot/ego vehicle, the control plugin needs to be able to locate the GameObjects for each drive wheel in Unity. To maintain generality and avoid restricting the control plugin to a particular robot model the path from the main body to each wheel is defined as a customizable parameter which can be edited through the simulator's web user interface. The figure below shows the expanded hierarchy for the LG CLOi robot as seen in the Unity Editor. The path to each of the wheels can be followed through the tree and entered into the web user interface. Sensor configuration for the Differential Drive Control plugin in the web user interface is seen below. The path to the two wheels are defined under LeftWheelLinkPath and RightWheelLinkPath .","title":"Setup"},{"location":"simulation-content/differential-drive-control/#calculating-wheel-motion","text":"The incoming control command expected by the plugin has a linear ( \\(V\\) ) and an angular velocity ( \\(\\omega\\) ) component. Using this input, as well as the wheel separation ( \\(l\\) ) and wheel radius ( \\(R\\) ) the rotation of each wheel can be computed as follows. \\[ \\omega_r = (V + \\omega . l/2)/R \\] \\[ \\omega_l = (V - \\omega . l/2)/R \\] The angular velocities \\(\\omega_l\\) and \\(\\omega_r\\) are the rotation rates of the left and right wheels which are applied to their respective articulated bodies in Unity. Note that the control plugin only expects an \\(x\\) component on the commanded linear velocity and a \\(z\\) component for the commanded angular velocity.","title":"Calculating wheel motion"},{"location":"simulation-content/differential-drive-control/#calculating-the-odometry-pose","text":"The odometry pose is calculated relative to the starting position of the robot. The yaw angle of the robot is calculated through an internal IMU, and is used alongside the rotation of each wheel as observed by the wheel odometers to calculate the pose of the robot/ego vehicle at each instance. The odometric pose of the robot is expected to drift over time as it does not account for wheel slippage which can be significant. The pose is published using a nav_msgs/Odometry message type. Since the odometry pose cannot estimate changes in elevation, the \\(z\\) component of the pose is not needed and is often left as 0. Here we are using this field to convey the yaw angle of the robot.","title":"Calculating the odometry pose"},{"location":"simulation-content/ego-vehicle-dynamics/","text":"EGO Vehicle Dynamics # SVL Simulator supports multiple dynamics models for EGO vehicles. The default dynamics model is a C# based model that uses Unity's PhysX physics engine and components. The model receives controller input and applies force to the Unity wheel colliders. For users who would like to change or replace vehicle dynamics with their own models, the simulator offers the following ways to do so. Simple Model Interface - pure C# dynamics Full Model Interface - FMI 2.0 supported dynamics Robotics - unique class extending IVehicleDynamics.cs and a unique class extending AgentController.cs Spectator - no dynamics class with a class extending AgentController.cs Simple Model Interface top # Simple Model Interface is our C# dynamic model. Most SVL Simulator vehicles that are provided, use the VehicleSMI class. It is located in Assets -> Scripts -> Dynamics -> Examples. This class inherits IVehicleDynamics and has required methods for simulation. Users can make any C# dynamic model class as long as it inherits IVehicleDynamics and Monobehavior. It can be compiled in SVL Simulator run-time executable or the vehicle bundle itself. Simple Model Interface Setup top # Create or select an EGO vehicle prefab in the Project panel only, not in scene Double click this EGO vehicle prefab. This opens the prefab editor scene Remove VehicleSMI.cs from the vehicle component list in the Inspector panel. This is the component that you are replacing with your own Add your new c# dynamics class component that inherits IVehicleDynamics If you plan to use Unity's physics engine, be sure to look at how VehicleSMI caches references to the wheel colliders and wheel meshes. This AxleInfo, in the C# example, enables you to apply force to the wheels and match the movement to the wheel models. Set any public references needed for your new dynamics model and save the prefab, Ctrl-S Open Simulator -> Build... to open the SVL Simulator bundle creation window Select Ego vehicle Select Build and after it is completed, in the root of the SVL Simulator you will have a folder called AssetBundles where the new bundles will be created. The new vehicle will be in AssetBundles/Vehicles folder. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser button in the Game panel view Under Library -> Vehicles select Add New Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Be sure to rename the vehicle so it isn't confused with other dynamics. Create a new sensor configuration. Publish the new vehicle. Full Model Interface top # Full Model Interface supports FMI2.0 Functional Mock-up Interface . Since user FMU's can vary greatly, we have provided an exampleFMU.fmu for testing in Windows only. It can be found in the source code in Assets -> Resources. ExampleVehicleFMU.cs is provided to see how the Full Model Interface system works. It is located in Assets -> Scripts -> Dynamics -> Examples. Users can create their own FMU class that inherits from IVehicleDynamics. Create or select an EGO vehicle prefab in the Project panel only, not in scene Double click this EGO vehicle prefab. This opens the prefab editor scene Remove VehicleSMI.cs from the vehicle component list in the Inspector panel. This is the component that you are replacing Add Component ExampleVehicleFMU.cs ExampleFMU.fmu requires Unity physics solver Toggle button Non Unity Physics to Unity Physics Set Axles size to 2 Setup Axles data. You will be dragging gameobjects from the Hierarchy panel to the Inspector panel Drag the correct wheel colliders and wheel meshes to public references from the prefab, see SMI setup Enable Motor and Steering for the front axle Set Brake Bias to 0.5f for front and back axles Import the ExampleFMU.fmu Toggle button Import FMU Choose ExampleFMU.fmu in Assets -> Resources -> ExampleFMU.fmu FMU will unpack in the repository vehicle folder in External -> Vehicles -> VehicleName -> FMUName folder FMUImporter.cs will parse the XML file into FMUData in the VehicleFMU class instance on prefab Model Variables will be listed so users can reference by index Each model variable has multiple values that are displayed in the scroll area Open Simulator -> Build... to open the SVL Simulator bundle creation window Select the new Ego vehicle to build Select Build and after it is completed, in the root of the SVL Simulator you will have a folder called AssetBundles/Vehicles where the new bundles will be created. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser... button in the Game panel view Under Library -> Vehicles select Add New Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Be sure to rename the vehicle so it isn't confused with other dynamics. Create a new sensor configuration. Publish the new vehicle. Full Model Interface Run-time behavior top # SVL Simulator only supports one FMU vehicle and for Windows only. Robotics top # Robotics is similar to SMI C# dynamic model. It requires a class that inherits IVehicleDynamics and implements the needed methods for simulation. Users can make any C# dynamic model class as long as it inherits IVehicleDynamics and Monobehavior. It can be compiled in SVL Simulator run-time executable or the vehicle bundle itself. Robotics also requires a Controller class that inherits AgentController and implements the needed methods for simulation. Robotics Setup top # Create an EGO robot prefab in the Project panel only, not in scene Double click this EGO robot prefab. This opens the prefab editor scene Robotics requires ArticulationBody components, not Rigidbody components. Set all ArticulationBody components in the Hierarchy Panel. Add your new c# dynamics class component that inherits IVehicleDynamics. Be sure to reference the ArticulationBody, not a rigidbody. Be sure BaseLink Transform is a child of the root prefab and set manually in the class. Add your new controller class component that inherits AgentController. Be sure Driver View Transform is a child of the root prefab and set manually in the class. Save the prefab, Ctrl-S. Open Simulator -> Build... to open the SVL Simulator bundle creation window Select Ego vehicle Select Build and after it is completed, in the root of the SVL Simulator you will have a folder called AssetBundles where the new bundles will be created. The new vehicle will be in AssetBundles/Vehicles folder. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser button in the Game panel view Under Library -> Vehicles select Add New Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Create a new sensor configuration. Publish the new vehicle. Spectator top # Spectator is not a vehicle at all. No dynamics class is needed and only a rigidbody with a class extending AgentController.cs. It can be compiled in SVL Simulator run-time executable or the vehicle bundle itself. Spectator vehicles that have no physics interaction just requires a Controller class that inherits AgentController and implements the needed methods for simulation. Spectator Setup top # Create an EGO robot prefab in the Project panel only, not in scene Double click this EGO robot prefab. This opens the prefab editor scene Add a RigidBody component and tick disable Use Gravity. Add your new controller class component that inherits AgentController. This can be mostly empty Be sure Driver View Transform is a child of the root prefab and set manually in the class. Save the prefab, Ctrl-S. Open Simulator -> Build... to open the SVL Simulator bundle creation window Select Ego vehicle Select Build and after it is completed, in the root of the SVL Simulator you will have a folder called AssetBundles where the new bundles will be created. The new vehicle will be in AssetBundles/Vehicles folder. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser button in the Game panel view Under Library -> Vehicles select Add New Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Create a new sensor configuration. Publish the new vehicle. Create a new simulation.","title":"Vehicle dynamics"},{"location":"simulation-content/ego-vehicle-dynamics/#simple-model-interface","text":"Simple Model Interface is our C# dynamic model. Most SVL Simulator vehicles that are provided, use the VehicleSMI class. It is located in Assets -> Scripts -> Dynamics -> Examples. This class inherits IVehicleDynamics and has required methods for simulation. Users can make any C# dynamic model class as long as it inherits IVehicleDynamics and Monobehavior. It can be compiled in SVL Simulator run-time executable or the vehicle bundle itself.","title":"Simple Model Interface"},{"location":"simulation-content/ego-vehicle-dynamics/#simple-model-interface-setup","text":"Create or select an EGO vehicle prefab in the Project panel only, not in scene Double click this EGO vehicle prefab. This opens the prefab editor scene Remove VehicleSMI.cs from the vehicle component list in the Inspector panel. This is the component that you are replacing with your own Add your new c# dynamics class component that inherits IVehicleDynamics If you plan to use Unity's physics engine, be sure to look at how VehicleSMI caches references to the wheel colliders and wheel meshes. This AxleInfo, in the C# example, enables you to apply force to the wheels and match the movement to the wheel models. Set any public references needed for your new dynamics model and save the prefab, Ctrl-S Open Simulator -> Build... to open the SVL Simulator bundle creation window Select Ego vehicle Select Build and after it is completed, in the root of the SVL Simulator you will have a folder called AssetBundles where the new bundles will be created. The new vehicle will be in AssetBundles/Vehicles folder. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser button in the Game panel view Under Library -> Vehicles select Add New Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Be sure to rename the vehicle so it isn't confused with other dynamics. Create a new sensor configuration. Publish the new vehicle.","title":"Simple Model Interface Setup"},{"location":"simulation-content/ego-vehicle-dynamics/#full-model-interface","text":"Full Model Interface supports FMI2.0 Functional Mock-up Interface . Since user FMU's can vary greatly, we have provided an exampleFMU.fmu for testing in Windows only. It can be found in the source code in Assets -> Resources. ExampleVehicleFMU.cs is provided to see how the Full Model Interface system works. It is located in Assets -> Scripts -> Dynamics -> Examples. Users can create their own FMU class that inherits from IVehicleDynamics. Create or select an EGO vehicle prefab in the Project panel only, not in scene Double click this EGO vehicle prefab. This opens the prefab editor scene Remove VehicleSMI.cs from the vehicle component list in the Inspector panel. This is the component that you are replacing Add Component ExampleVehicleFMU.cs ExampleFMU.fmu requires Unity physics solver Toggle button Non Unity Physics to Unity Physics Set Axles size to 2 Setup Axles data. You will be dragging gameobjects from the Hierarchy panel to the Inspector panel Drag the correct wheel colliders and wheel meshes to public references from the prefab, see SMI setup Enable Motor and Steering for the front axle Set Brake Bias to 0.5f for front and back axles Import the ExampleFMU.fmu Toggle button Import FMU Choose ExampleFMU.fmu in Assets -> Resources -> ExampleFMU.fmu FMU will unpack in the repository vehicle folder in External -> Vehicles -> VehicleName -> FMUName folder FMUImporter.cs will parse the XML file into FMUData in the VehicleFMU class instance on prefab Model Variables will be listed so users can reference by index Each model variable has multiple values that are displayed in the scroll area Open Simulator -> Build... to open the SVL Simulator bundle creation window Select the new Ego vehicle to build Select Build and after it is completed, in the root of the SVL Simulator you will have a folder called AssetBundles/Vehicles where the new bundles will be created. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser... button in the Game panel view Under Library -> Vehicles select Add New Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Be sure to rename the vehicle so it isn't confused with other dynamics. Create a new sensor configuration. Publish the new vehicle.","title":"Full Model Interface"},{"location":"simulation-content/ego-vehicle-dynamics/#full-model-interface-run-time-behavior","text":"SVL Simulator only supports one FMU vehicle and for Windows only.","title":"Full Model Interface Run-time behavior"},{"location":"simulation-content/ego-vehicle-dynamics/#robotics","text":"Robotics is similar to SMI C# dynamic model. It requires a class that inherits IVehicleDynamics and implements the needed methods for simulation. Users can make any C# dynamic model class as long as it inherits IVehicleDynamics and Monobehavior. It can be compiled in SVL Simulator run-time executable or the vehicle bundle itself. Robotics also requires a Controller class that inherits AgentController and implements the needed methods for simulation.","title":"Robotics"},{"location":"simulation-content/ego-vehicle-dynamics/#robotics-setup","text":"Create an EGO robot prefab in the Project panel only, not in scene Double click this EGO robot prefab. This opens the prefab editor scene Robotics requires ArticulationBody components, not Rigidbody components. Set all ArticulationBody components in the Hierarchy Panel. Add your new c# dynamics class component that inherits IVehicleDynamics. Be sure to reference the ArticulationBody, not a rigidbody. Be sure BaseLink Transform is a child of the root prefab and set manually in the class. Add your new controller class component that inherits AgentController. Be sure Driver View Transform is a child of the root prefab and set manually in the class. Save the prefab, Ctrl-S. Open Simulator -> Build... to open the SVL Simulator bundle creation window Select Ego vehicle Select Build and after it is completed, in the root of the SVL Simulator you will have a folder called AssetBundles where the new bundles will be created. The new vehicle will be in AssetBundles/Vehicles folder. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser button in the Game panel view Under Library -> Vehicles select Add New Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Create a new sensor configuration. Publish the new vehicle.","title":"Robotics Setup"},{"location":"simulation-content/ego-vehicle-dynamics/#spectator","text":"Spectator is not a vehicle at all. No dynamics class is needed and only a rigidbody with a class extending AgentController.cs. It can be compiled in SVL Simulator run-time executable or the vehicle bundle itself. Spectator vehicles that have no physics interaction just requires a Controller class that inherits AgentController and implements the needed methods for simulation.","title":"Spectator"},{"location":"simulation-content/ego-vehicle-dynamics/#spectator-setup","text":"Create an EGO robot prefab in the Project panel only, not in scene Double click this EGO robot prefab. This opens the prefab editor scene Add a RigidBody component and tick disable Use Gravity. Add your new controller class component that inherits AgentController. This can be mostly empty Be sure Driver View Transform is a child of the root prefab and set manually in the class. Save the prefab, Ctrl-S. Open Simulator -> Build... to open the SVL Simulator bundle creation window Select Ego vehicle Select Build and after it is completed, in the root of the SVL Simulator you will have a folder called AssetBundles where the new bundles will be created. The new vehicle will be in AssetBundles/Vehicles folder. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser button in the Game panel view Under Library -> Vehicles select Add New Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Create a new sensor configuration. Publish the new vehicle. Create a new simulation.","title":"Spectator Setup"},{"location":"simulation-content/hd-map-mesh-generation/","text":"HD Map Mesh Generation # Video top # Introduction top # SVL Simulator provides an option to automatically generate meshes for imported HD map data, allowing you to build and upload custom environments for your simulations. Depending on your needs this tool can prepare textured road meshes for rendering, mesh colliders, or both. The mesh builder does not use HD map files directly, but instead operates on Simulator's unified internal data structure for map annotations. Some of the most popular file formats can be imported through HD Map Importer tool. After import is finished, you can use that data to generate meshes. Accessing the mesh builder top # To access the mesh builder window, open Simulator project in Unity editor, then navigate into Simulator/Build HD Map Mesh on the menu bar. Mesh builder window will be opened. Builder settings top # Mesh builder window offers two non-exclusive options for mesh generation: colliders and renderers. Selecting colliders will create MeshCollider component for each of the roads. These colliders will be used by all entities in simulation, including agents and NPCs. Selecting renderers will create MeshRenderer component (along with MeshFilter ) for each of the roads. Meshes created this way will be visible in simulation, textured, and will include lane-lines. Mesh Settings top # Settings in this tab will affect general shape of the mesh, both for colliders and for rendering. Parameter Name Description Snap Lane Ends If this option is enabled, end vertices from subsequent lanes will be snapped together within distance threshold to remove holes. Snap Threshold Maximum distance (in meters) within which lane ends can be snapped together. Only available if Snap Lane Ends is enabled. Push Outer Verts If this option is enabled, external lane boundaries will be pushed out to create roadside. Separate Outer Mesh If this option is enabled, pushed out boundaries will be split into separate mesh. Only available if Push Outer Verts is enabled. Push Distance Distance (in meters) to push external lane boundaries out. Only available if Push Outer Verts is enabled. Snap Lane Ends top # Option for snapping lane ends can improve overall mesh quality if lanes in the original HD map file are not perfectly aligned. If you notice any holes like the one shown below, try enabling this option. If gaps are larger, you can try increasing snap threshold. Push Outer Verts top # This option is useful if you want to create additional roadside that was not defined in original HD map file. Lane-lines are not affected by this, and only outer lane boundaries are expanded. This can help prevent vehicles from falling off the road when even slightly crossing the side line. Separate Outer Mesh top # When this option is enabled, all roadsides created by using Push Outer Verts option will be split into separate meshes, instead of being an extension of the road. This will result in more complex geometry, but can be useful if you want to use different materials for roads and roadsides. Using separate meshes can also fix artifacts that rarely occur when tight corners are pushed out by a significant amount. Rendering Settings top # Settings in this tab will affect visuals of generated meshes. If Create Renderers option is disabled, this tab will not be available. Parameter Name Description Road UV Unit Distance (in meters) defining size of a single UV coordinates tile for roads. Increasing this value will stretch road texture over larger area. Line UV Unit Distance (in meters) defining size of a single UV coordinates tile for lane-lines. Increasing this value will stretch line texture over longer distance. Line Width Width (in meters) of a single lane-line. Double lines will be scaled proportionally. Line Bump Vertical distance (in meters) between road and lane-lines. This elevates lines slightly to avoid clipping. Material Settings top # Materials used on generated meshes are not exposed in mesh builder window. Instead, they are defined in an asset that can be found under Assets/Resources/Editor/HDMapMaterials.asset . Please note that even though it's possible to replace default materials with your own, it's generally not advised to use different shaders. Custom materials and shaders will render properly, but some additional features, like weather effects, might not be compatible and will not show. Building HD map environment top # It's possible to prepare a simple, usable environment for simulation from HD map data only. To create environment bundle from HD map data, follow the steps below. Launch SVL Simulator from the Unity Editor (as described here ). Create new scene through File/New Scene menu item (note: avoid creating scene through context menu - it will lack required components). Access HD Map Importer tool and import your HD map data. Access HD Map Mesh Builder tool and generate both colliders and renderers. Locate SpawnInfo object in scene hierarchy and move it to the desired EGO vehicle spawn point. Create new directory with your desired environment name under Assets/External/Environments/ . Save the scene under new directory. Final path of the newly created scene should be Assets/External/Environments/SceneName/SceneName.unity . (optional) Create NavMesh to enable pedestrians. See NavMesh section for details. Your environment is now ready to be built. Follow the build instructions for assets . Creating NavMesh for pedestrians top # If you plan to use pedestrians on generated environment, you'll have to bake navigation meshes for pathfinding. After road meshes are created and the scene have been saved, follow the steps below to prepare NavMesh. NOTE: Pedestrian annotations must be created for pedestrians to navigate the NavMesh. See Map Annotation . Access Navigation window through Window/AI/Navigation menu item. In the Object tab, select Mesh Renderers filtering option. Select all generated mesh renderers in scene hierarchy. Enable Navigation Static option for all selected mesh renderers and change their layer to Road . Note: If you separate outer meshes , you can assign different layer to them, e.g. Sidewalk . All of these objects have _roadside postfix. In the Bake tab, click Bake button to generate the NavMesh. Save the scene.","title":"Road network generation"},{"location":"simulation-content/hd-map-mesh-generation/#video","text":"","title":"Video"},{"location":"simulation-content/hd-map-mesh-generation/#introduction","text":"SVL Simulator provides an option to automatically generate meshes for imported HD map data, allowing you to build and upload custom environments for your simulations. Depending on your needs this tool can prepare textured road meshes for rendering, mesh colliders, or both. The mesh builder does not use HD map files directly, but instead operates on Simulator's unified internal data structure for map annotations. Some of the most popular file formats can be imported through HD Map Importer tool. After import is finished, you can use that data to generate meshes.","title":"Introduction"},{"location":"simulation-content/hd-map-mesh-generation/#accessing-the-mesh-builder","text":"To access the mesh builder window, open Simulator project in Unity editor, then navigate into Simulator/Build HD Map Mesh on the menu bar. Mesh builder window will be opened.","title":"Accessing the mesh builder"},{"location":"simulation-content/hd-map-mesh-generation/#builder-settings","text":"Mesh builder window offers two non-exclusive options for mesh generation: colliders and renderers. Selecting colliders will create MeshCollider component for each of the roads. These colliders will be used by all entities in simulation, including agents and NPCs. Selecting renderers will create MeshRenderer component (along with MeshFilter ) for each of the roads. Meshes created this way will be visible in simulation, textured, and will include lane-lines.","title":"Builder settings"},{"location":"simulation-content/hd-map-mesh-generation/#mesh-settings","text":"Settings in this tab will affect general shape of the mesh, both for colliders and for rendering. Parameter Name Description Snap Lane Ends If this option is enabled, end vertices from subsequent lanes will be snapped together within distance threshold to remove holes. Snap Threshold Maximum distance (in meters) within which lane ends can be snapped together. Only available if Snap Lane Ends is enabled. Push Outer Verts If this option is enabled, external lane boundaries will be pushed out to create roadside. Separate Outer Mesh If this option is enabled, pushed out boundaries will be split into separate mesh. Only available if Push Outer Verts is enabled. Push Distance Distance (in meters) to push external lane boundaries out. Only available if Push Outer Verts is enabled.","title":"Mesh Settings"},{"location":"simulation-content/hd-map-mesh-generation/#snap-lane-ends","text":"Option for snapping lane ends can improve overall mesh quality if lanes in the original HD map file are not perfectly aligned. If you notice any holes like the one shown below, try enabling this option. If gaps are larger, you can try increasing snap threshold.","title":"Snap Lane Ends"},{"location":"simulation-content/hd-map-mesh-generation/#push-outer-verts","text":"This option is useful if you want to create additional roadside that was not defined in original HD map file. Lane-lines are not affected by this, and only outer lane boundaries are expanded. This can help prevent vehicles from falling off the road when even slightly crossing the side line.","title":"Push Outer Verts"},{"location":"simulation-content/hd-map-mesh-generation/#separate-outer-mesh","text":"When this option is enabled, all roadsides created by using Push Outer Verts option will be split into separate meshes, instead of being an extension of the road. This will result in more complex geometry, but can be useful if you want to use different materials for roads and roadsides. Using separate meshes can also fix artifacts that rarely occur when tight corners are pushed out by a significant amount.","title":"Separate Outer Mesh"},{"location":"simulation-content/hd-map-mesh-generation/#rendering-settings","text":"Settings in this tab will affect visuals of generated meshes. If Create Renderers option is disabled, this tab will not be available. Parameter Name Description Road UV Unit Distance (in meters) defining size of a single UV coordinates tile for roads. Increasing this value will stretch road texture over larger area. Line UV Unit Distance (in meters) defining size of a single UV coordinates tile for lane-lines. Increasing this value will stretch line texture over longer distance. Line Width Width (in meters) of a single lane-line. Double lines will be scaled proportionally. Line Bump Vertical distance (in meters) between road and lane-lines. This elevates lines slightly to avoid clipping.","title":"Rendering Settings"},{"location":"simulation-content/hd-map-mesh-generation/#material-settings","text":"Materials used on generated meshes are not exposed in mesh builder window. Instead, they are defined in an asset that can be found under Assets/Resources/Editor/HDMapMaterials.asset . Please note that even though it's possible to replace default materials with your own, it's generally not advised to use different shaders. Custom materials and shaders will render properly, but some additional features, like weather effects, might not be compatible and will not show.","title":"Material Settings"},{"location":"simulation-content/hd-map-mesh-generation/#building-hd-map-environment","text":"It's possible to prepare a simple, usable environment for simulation from HD map data only. To create environment bundle from HD map data, follow the steps below. Launch SVL Simulator from the Unity Editor (as described here ). Create new scene through File/New Scene menu item (note: avoid creating scene through context menu - it will lack required components). Access HD Map Importer tool and import your HD map data. Access HD Map Mesh Builder tool and generate both colliders and renderers. Locate SpawnInfo object in scene hierarchy and move it to the desired EGO vehicle spawn point. Create new directory with your desired environment name under Assets/External/Environments/ . Save the scene under new directory. Final path of the newly created scene should be Assets/External/Environments/SceneName/SceneName.unity . (optional) Create NavMesh to enable pedestrians. See NavMesh section for details. Your environment is now ready to be built. Follow the build instructions for assets .","title":"Building HD map environment"},{"location":"simulation-content/hd-map-mesh-generation/#creating-navmesh-for-pedestrians","text":"If you plan to use pedestrians on generated environment, you'll have to bake navigation meshes for pathfinding. After road meshes are created and the scene have been saved, follow the steps below to prepare NavMesh. NOTE: Pedestrian annotations must be created for pedestrians to navigate the NavMesh. See Map Annotation . Access Navigation window through Window/AI/Navigation menu item. In the Object tab, select Mesh Renderers filtering option. Select all generated mesh renderers in scene hierarchy. Enable Navigation Static option for all selected mesh renderers and change their layer to Road . Note: If you separate outer meshes , you can assign different layer to them, e.g. Sidewalk . All of these objects have _roadside postfix. In the Bake tab, click Bake button to generate the NavMesh. Save the scene.","title":"Creating NavMesh for pedestrians"},{"location":"simulation-content/lane-line-detector/","text":"Lane-line Detector # Overview top # The lane-line detector tool can used to prepare lane-line data for the segmentation and lane-line sensors. Depending on the desired result, it can use data from map annotations and/or road intensity maps. If your annotation data is precise and aligns with the environment well, it's suggested to use it as the only input for this tool. On the other hand, if environment and annotations are slightly misaligned, you might want to try using the optional feature that will attempt to detect lines on intensity maps and correct the annotations based on this. Refer to the main options section for more details. The main output from this tool is a set of meshes matching lane-lines on the road. These meshes are not visible for normal cameras, but will be used by the segmentation sensor to differentiate between road and lane-lines. If Generate Line Sensor Data in main options is enabled, output will also include correction data for the lane-line sensor. You can find more details about this on lane-line sensor page. If line visibility for segmentation camera is unnecessary or undesired, you can safely ignore this tool. Comparison of segmentation image with and without lane-lines is shown below. Accessing the Detector top # To open the lane-line detector window, open a Simulator project in Unity editor, then select Simulator/Detect Lane-lines on the menu bar. Main Options top # There are two main options that must be set before any other, more detailed, settings are available. The Line Source dropdown describes which data should be used to generate lines that will be visible by segmentation sensor. Three options are available: - HD Map - only annotation data will be used and all generated lines will follow lines from the high-definition (HD) map. - Intensity Map - only road intensity maps will be used. This method is based on image processing and results may vary depending on the quality of the intensity maps. - Corrected HD Map - annotation data will be used, but the tool will attempt to correct any misaligned lines based on road intensity maps. The Generate Line Sensor Data checkbox determines whether the annotations should be corrected for the lane-line sensor . Correction is based on road intensity maps, similar to the Corrected HD Map option in the Line Source setting. If this option is disabled, raw annotation data will be used instead. Note that this setting is separate to Line Source , and both sensors can use differently processed data. After generation is done, a new object named LineData_generated will be created on the scene. When it is selected, the scene view will show outlines of the generated lines. If you're not satisfied with the result, try using a different mode, or changing the advanced settings. Line-detection Settings top # This section is only visible if any of the main options was configured to use line-detection based on intensity maps. Grouping Settings top # Parameter Name Description Line Distance Threshold The maximum distance between line segments that can be classified as a single lane-line when classifying lines detected on intensity maps. Line Angle Threshold The maximum angle between line segments that can be classified as a single lane-line when classifying lines detected on intensity maps. Max Line Segment Length The maximum length of a single detected line segment. Longer lines will be split into multiple segments. Worst Fit Threshold The maximum valid distance between a line center and the segments creating it. This is used to filter out large clusters of spread-parallel line-segments. Min Width Threshold The maximum distance between segments that should be considered parts of a single line. This is used to filter out large clusters of spread-parallel line-segments. Joint Line Threshold The minimum viable width of a lane-line. This is used to filter out linear artifacts from detected line-segments. Postprocessing Settings top # Parameter Name Description World Space Snap Distance The maximum distance between detected lines that could be considered parts of a single curve. Lines below this threshold will have their ends snap together to create a better curve approximation. World Dotted Line Distance The maximum distance between detected lines that could be considered separate parts of a single dotted line. World Space Snap Angle The maximum angle between detected lines that could be considered parts of a single curve (solid or dotted). Line Mesh Generation Settings top # This section is only visible if Line Source , in the main options, was configured to use map annotations. Parameter Name Description Snap Lane Ends If this option is enabled, end vertices from subsequent lanes will be snapped together, within the Snap Threshold, to remove holes. Snap Threshold The maximum distance (in meters) within which lane ends can be snapped together. This is only available if Snap Lane Ends is enabled. Line UV Unit The distance, in meters, defining the size of a single UV coordinates-tile for lane-lines. Increasing this value will stretch the line texture over a longer distance. Line Width The width, in meters, of a single lane-line. Double lines will be scaled proportionally. Line Bump The vertical distance, in meters, between road and lane-lines. This elevates lines slightly to avoid clipping.","title":"Lane-line detector"},{"location":"simulation-content/lane-line-detector/#overview","text":"The lane-line detector tool can used to prepare lane-line data for the segmentation and lane-line sensors. Depending on the desired result, it can use data from map annotations and/or road intensity maps. If your annotation data is precise and aligns with the environment well, it's suggested to use it as the only input for this tool. On the other hand, if environment and annotations are slightly misaligned, you might want to try using the optional feature that will attempt to detect lines on intensity maps and correct the annotations based on this. Refer to the main options section for more details. The main output from this tool is a set of meshes matching lane-lines on the road. These meshes are not visible for normal cameras, but will be used by the segmentation sensor to differentiate between road and lane-lines. If Generate Line Sensor Data in main options is enabled, output will also include correction data for the lane-line sensor. You can find more details about this on lane-line sensor page. If line visibility for segmentation camera is unnecessary or undesired, you can safely ignore this tool. Comparison of segmentation image with and without lane-lines is shown below.","title":"Overview"},{"location":"simulation-content/lane-line-detector/#accessing-the-detector","text":"To open the lane-line detector window, open a Simulator project in Unity editor, then select Simulator/Detect Lane-lines on the menu bar.","title":"Accessing the Detector"},{"location":"simulation-content/lane-line-detector/#main-options","text":"There are two main options that must be set before any other, more detailed, settings are available. The Line Source dropdown describes which data should be used to generate lines that will be visible by segmentation sensor. Three options are available: - HD Map - only annotation data will be used and all generated lines will follow lines from the high-definition (HD) map. - Intensity Map - only road intensity maps will be used. This method is based on image processing and results may vary depending on the quality of the intensity maps. - Corrected HD Map - annotation data will be used, but the tool will attempt to correct any misaligned lines based on road intensity maps. The Generate Line Sensor Data checkbox determines whether the annotations should be corrected for the lane-line sensor . Correction is based on road intensity maps, similar to the Corrected HD Map option in the Line Source setting. If this option is disabled, raw annotation data will be used instead. Note that this setting is separate to Line Source , and both sensors can use differently processed data. After generation is done, a new object named LineData_generated will be created on the scene. When it is selected, the scene view will show outlines of the generated lines. If you're not satisfied with the result, try using a different mode, or changing the advanced settings.","title":"Main Options"},{"location":"simulation-content/lane-line-detector/#line-detection-settings","text":"This section is only visible if any of the main options was configured to use line-detection based on intensity maps.","title":"Line-detection Settings"},{"location":"simulation-content/lane-line-detector/#grouping-settings","text":"Parameter Name Description Line Distance Threshold The maximum distance between line segments that can be classified as a single lane-line when classifying lines detected on intensity maps. Line Angle Threshold The maximum angle between line segments that can be classified as a single lane-line when classifying lines detected on intensity maps. Max Line Segment Length The maximum length of a single detected line segment. Longer lines will be split into multiple segments. Worst Fit Threshold The maximum valid distance between a line center and the segments creating it. This is used to filter out large clusters of spread-parallel line-segments. Min Width Threshold The maximum distance between segments that should be considered parts of a single line. This is used to filter out large clusters of spread-parallel line-segments. Joint Line Threshold The minimum viable width of a lane-line. This is used to filter out linear artifacts from detected line-segments.","title":"Grouping Settings"},{"location":"simulation-content/lane-line-detector/#postprocessing-settings","text":"Parameter Name Description World Space Snap Distance The maximum distance between detected lines that could be considered parts of a single curve. Lines below this threshold will have their ends snap together to create a better curve approximation. World Dotted Line Distance The maximum distance between detected lines that could be considered separate parts of a single dotted line. World Space Snap Angle The maximum angle between detected lines that could be considered parts of a single curve (solid or dotted).","title":"Postprocessing Settings"},{"location":"simulation-content/lane-line-detector/#line-mesh-generation-settings","text":"This section is only visible if Line Source , in the main options, was configured to use map annotations. Parameter Name Description Snap Lane Ends If this option is enabled, end vertices from subsequent lanes will be snapped together, within the Snap Threshold, to remove holes. Snap Threshold The maximum distance (in meters) within which lane ends can be snapped together. This is only available if Snap Lane Ends is enabled. Line UV Unit The distance, in meters, defining the size of a single UV coordinates-tile for lane-lines. Increasing this value will stretch the line texture over a longer distance. Line Width The width, in meters, of a single lane-line. Double lines will be scaled proportionally. Line Bump The vertical distance, in meters, between road and lane-lines. This elevates lines slightly to avoid clipping.","title":"Line Mesh Generation Settings"},{"location":"simulation-content/lane-line-sensor/","text":"Lane-line Sensor # Overview top # The lane-line sensor is used to extract and publish data describing the position and curvature of road lines on the lane which EGO vehicle currently occupies. It uses ground truth data from map annotations , which can be optionally corrected. You can read more about how annotation data is used in the input data section. Data format top # The lane-line sensor currently only supports the Cyber bridge, and publishes data in a format compatible with Apollo 5.0. Each message follows the perception_lane format and, aside from the header, contains data about one or more lines in a format compatible with CameraLaneLine . Fields populated and published by the simulator are described in the table below. Parameter Description type Describes the color and shape of the line (white/yellow, solid/dotted) pos_type Describes the position of the line in relation to the EGO vehicle (right/left, ego/adjacent/third/etc.) curve_camera_coord The line curve in sensor space defined as third-degree polynomial * * See Curve definition section for details Please note that even though lane-line sensor visualization in the simulator shows detected lines as an overlay for a color image, the image itself is not part of published data and is only shown as a visual aid. Curve Definition top # Each lane-curve is described as third-degree polynomial, with coordinate space being centered on the sensor, the x axis pointing towards its front, and the y axis pointing towards its right side. This coordinate space uses only two dimensions, ignoring altitude. Image below shows the described coordinate space. Each curve is defined by six values - a, b, c, d, longitude_min, longitude_max - as defined in LaneLineCubicCurve . On the referenced image, longitude_min and longitude_max are described as MinX and MaxX , respectively). Given these parameters, the curve function f(x) can be defined as: f(x) = a + b * x + c * x^2 + d * x^3 for x \u2208 <longitude_min, longitude_max> It's important to note that polynomial coefficients are calculated via polynomial regression, which means the final function is an approximation and might not match the input data (red dots) perfectly. This is mostly noticeable on steep curves. Input data for the polynomial regression is sampled directly from map annotations for the given environment, trimmed to the sensor's field of view ( FOV on the referenced image) and defined visibility range (see JSON parameters for more information). Details about how the points are sampled can be found in the input data section. Input Data top # By default, the lane-line sensor will use lines defined in the map's annotation data. All of the spatial data and metadata for published lines will be based on this, which eliminates issues related to image processing, but requires precise annotations. Since annotation data can be relatively scarce, each segment is resampled along its length before the polynomial regression step. Sampling density depends on SampleDelta parameter that can be defined in JSON parameters . An example of how the annotation data is resampled before final processing is shown on the image below. Points used for curve approximation (red dots) are based on white and yellow annotation lines. In some cases, annotation data imported from external sources might not match the environment perfectly. If you have no option to improve the alignment, you might want to try using the automated correction option. This uses image processing and will attempt to align annotation keypoints with lines detected on road intensity maps. Please note that the results may vary based on the intensity map's quality. To use automatic line-correction, open the lane-line detector tool and use it with the Generate Line Sensor Data option enabled. An example of how offset map annotation data can look before (blue and yellow lines) and after automated correction (green lines) is shown below. Note that this correction will only affect the sensor visibility and will not change the annotation data itself. Testing with Apollo top # Lane-line sensor currently only supports CyberRT message types. If you want to verify that lane-line data is properly detected and received by Apollo, follow the steps below. Follow the instructions for running Apollo 5.0 with SVL Simulator. Don't start any simulation, but make sure that bridge is running. Using the Web User Interface, add the map that you want to use for testing to your library. You can either choose a map from Store , or create and upload your own. Note: your map can use raw or corrected annotation data. See input data section for more details. Add Lane Line Sensor plugin to your library. Add any vehicle to your library. Create new sensor configuration for your vehicle. In the configuration options, add lane-line sensor to the list of used sensors. Make sure that Topic and Frame properties are not empty. Choose CyberRT as the bridge used by this configuration. Create new simulation using the map, vehicle and sensor configuration that you just created. On the Autopilot page, select Apollo 5.0 and provide the bridge IP address. Start the simulation. In Apollo's Docker environment, run cyber_monitor tool. Lane-line sensor topic should be reporting received data. You can inspect the details by selecting the topic name.","title":"Lane-line sensor"},{"location":"simulation-content/lane-line-sensor/#overview","text":"The lane-line sensor is used to extract and publish data describing the position and curvature of road lines on the lane which EGO vehicle currently occupies. It uses ground truth data from map annotations , which can be optionally corrected. You can read more about how annotation data is used in the input data section.","title":"Overview"},{"location":"simulation-content/lane-line-sensor/#data-format","text":"The lane-line sensor currently only supports the Cyber bridge, and publishes data in a format compatible with Apollo 5.0. Each message follows the perception_lane format and, aside from the header, contains data about one or more lines in a format compatible with CameraLaneLine . Fields populated and published by the simulator are described in the table below. Parameter Description type Describes the color and shape of the line (white/yellow, solid/dotted) pos_type Describes the position of the line in relation to the EGO vehicle (right/left, ego/adjacent/third/etc.) curve_camera_coord The line curve in sensor space defined as third-degree polynomial * * See Curve definition section for details Please note that even though lane-line sensor visualization in the simulator shows detected lines as an overlay for a color image, the image itself is not part of published data and is only shown as a visual aid.","title":"Data format"},{"location":"simulation-content/lane-line-sensor/#curve-definition","text":"Each lane-curve is described as third-degree polynomial, with coordinate space being centered on the sensor, the x axis pointing towards its front, and the y axis pointing towards its right side. This coordinate space uses only two dimensions, ignoring altitude. Image below shows the described coordinate space. Each curve is defined by six values - a, b, c, d, longitude_min, longitude_max - as defined in LaneLineCubicCurve . On the referenced image, longitude_min and longitude_max are described as MinX and MaxX , respectively). Given these parameters, the curve function f(x) can be defined as: f(x) = a + b * x + c * x^2 + d * x^3 for x \u2208 <longitude_min, longitude_max> It's important to note that polynomial coefficients are calculated via polynomial regression, which means the final function is an approximation and might not match the input data (red dots) perfectly. This is mostly noticeable on steep curves. Input data for the polynomial regression is sampled directly from map annotations for the given environment, trimmed to the sensor's field of view ( FOV on the referenced image) and defined visibility range (see JSON parameters for more information). Details about how the points are sampled can be found in the input data section.","title":"Curve Definition"},{"location":"simulation-content/lane-line-sensor/#input-data","text":"By default, the lane-line sensor will use lines defined in the map's annotation data. All of the spatial data and metadata for published lines will be based on this, which eliminates issues related to image processing, but requires precise annotations. Since annotation data can be relatively scarce, each segment is resampled along its length before the polynomial regression step. Sampling density depends on SampleDelta parameter that can be defined in JSON parameters . An example of how the annotation data is resampled before final processing is shown on the image below. Points used for curve approximation (red dots) are based on white and yellow annotation lines. In some cases, annotation data imported from external sources might not match the environment perfectly. If you have no option to improve the alignment, you might want to try using the automated correction option. This uses image processing and will attempt to align annotation keypoints with lines detected on road intensity maps. Please note that the results may vary based on the intensity map's quality. To use automatic line-correction, open the lane-line detector tool and use it with the Generate Line Sensor Data option enabled. An example of how offset map annotation data can look before (blue and yellow lines) and after automated correction (green lines) is shown below. Note that this correction will only affect the sensor visibility and will not change the annotation data itself.","title":"Input Data"},{"location":"simulation-content/lane-line-sensor/#testing-with-apollo","text":"Lane-line sensor currently only supports CyberRT message types. If you want to verify that lane-line data is properly detected and received by Apollo, follow the steps below. Follow the instructions for running Apollo 5.0 with SVL Simulator. Don't start any simulation, but make sure that bridge is running. Using the Web User Interface, add the map that you want to use for testing to your library. You can either choose a map from Store , or create and upload your own. Note: your map can use raw or corrected annotation data. See input data section for more details. Add Lane Line Sensor plugin to your library. Add any vehicle to your library. Create new sensor configuration for your vehicle. In the configuration options, add lane-line sensor to the list of used sensors. Make sure that Topic and Frame properties are not empty. Choose CyberRT as the bridge used by this configuration. Create new simulation using the map, vehicle and sensor configuration that you just created. On the Autopilot page, select Apollo 5.0 and provide the bridge IP address. Start the simulation. In Apollo's Docker environment, run cyber_monitor tool. Lane-line sensor topic should be reporting received data. You can inspect the details by selecting the topic name.","title":"Testing with Apollo"},{"location":"simulation-content/lighting-for-interiors/","text":"Lighting Set Up for Interior Environments # This document will describe how to set up lighting for environments that do not receive exterior lighting. Getting Started top # Light Layers: What are they ? # Unity provides us with tools that allow us to set up lights that only affect meshes specified to receive light from those light sources. This opens up the possibility to exclude exterior lighting from interiors and bake Mixed Lighting for higher quality visuals when running the simulator. This is especially useful when creating interior environments. Light Layers: How do we use them ? # The SVL simulator is already setup to employ Light Layers but you will need to do some extra preparation to use them. This includes proper setup of the interior post-processing volume, lights, meshes and then baking the lighting. To do this, you must be in Developer Mode . Interior Environment Setup top # Create an empty game object and name it LightingSetup (Right click in Hierarchy > Create Empty) then zero out the transforms (Click the 3 dots on the right of the Transform section > Reset). This is where you will place your rendering volumes, light probes and reflection probes and will help keep the hierarchy organized. ) Create a Box Volume (Right click in Hierarchy > Volume > Box Volume) and zero out the transforms. Adjust the position and size of the volume to contain your interior environment. Multiple volumes can be used. Place this inside your LightingSetup game object in the Hierarchy. Be sure to name it so that it is defined as an interior lighting post processing volume. Example: NameOfEnvironment PostProcessingVolumeInterior. ) Create a new Volume Profile (Assets > Create > Volume Profile). This Post-Processing Volume Profile will override the Simulator profile that is used at runtime for exterior lighting. Be sure to name it so that it is defined as an interior lighting post processing volume ex: NameOfEnvironment PostProcessingProfileInterior. ) Add your profile overrides for the interior (Select your NameOfEnvironment PostProcessingProfileInterior in the project view > Add Override > Select and adjust desired override). ) Add a Reflection Proxy Volume (Right click in Hierarchy > Volume > Select desired volume shape) and zero out the transforms. Be sure to name it so that it is defined as your interior environment reflection proxy volume. Example: NameOfEnvironment ReflectionProxyVolume. Add the Reflection Proxy Volume Script (Select your NameOfEnvironment ReflectionProxyVolume in the project view > Add Component > Search for and select Reflection Proxy Volume). Remove Box Collider and Volume components from the Reflection Proxy Volume asset. (Select your NameOfEnvironment ReflectionProxyVolume in the project view > click the 3 dots on the right for each section > Remove Component) Do not remove the Reflection Proxy Volume component. Move and adjust the size of the Reflection Proxy Volume asset to envelope the desired space in your interior. ) Add Reflection Probes to the scene (Right click in Hierarchy > Light > Reflection Probe). This is vital to proper lighting for interior spaces. For environments requiring multiple reflection probes create an empty game object inside of the LightingSetUp game object, reset transforms and place your probes inside. Naming for reflection probes and reflection probe group should be descriptive ex: NameOfEnvironment ReflectionProbe. Add the Reflection Proxy Volume asset to your Reflection Probes (Select your Reflection Probe > Projection Settings Section > Proxy Volume field > add your Proxy Volume). ) Adjust the size of each Reflection Probe to the desired size for your interior space. Add Light Probes to the scene (Right click in Hierarchy > Light > Light Probe Group). This step is optional but can improve the lighting fidelity for dynamic objects as they move through the environment. Place this inside your LightingSetup game object in the Hierarchy. Naming for the group should be descriptive ex: NameOfEnvironment LightProbeGroup. ) Setting Up the Lights top # Add lights to your scene if none already exist and adjust settings for the desired visuals (Right click in Hierarchy > Light > choose the desired light) Select the light in the inspector and in the General dropdown change Mode to Mixed. Next set the Light Layer to Light Layer 7 (Expand More Options in General drop down by clicking the gear icon). If Shadows are enabled make sure that Link Light Layer is also enabled (Expand More Options in General drop down by clicking the gear icon). ) Static Mesh Setup top # Set all meshes to Reflection Probe Static (Select mesh > Top right \"Static\" drop down > Reflection Probe Static). Set Rendering Layer Mask to Light Layer 7 (Select mesh > Expand Mesh Renderer > Expand Additional Settings > Rendering Layer Mask). ) Dynamic Mesh Setup top {: #dynamic-mesh-setup data-toc-label='Dynamic Mesh Setup'}} # Set Rendering Layer Mask to Light Layer 7 (See above). If you have a vehicle that includes lights, those lights will need to be set to Light Layer 7 and Link Light Layer enabled (See above). Bake Reflections and Lighting top {: #baking-the-lighting data-toc-label='Baking the Lighting'}} # Open the Lighting tab to access Unity's lighting options (Window > Rendering > Lighting). In the Scene section select your Lighting Settings asset. If you don't already have a Lighting Settings asset then create a new one using the button under the input field. ) Be sure that the Mixed Lighting Section has Baked Global Illumination checked and Lighting Mode is set to Baked Indirect. Adjust settings to match the quality expectations. For development purposes it's useful to have a draft version with lower quality settings and then a high quality version for final rendering. In the Environment section add the PostProcessingProfileInterior that you made earlier. ) Finally click the Generate Lighting. Once the lighting has finished processing click the Generate Lighting drop down and click Bake Reflection Probes. ) If you need to make changes to the lighting or the environment you will need to re-bake all lighting data.","title":"Lighting for indoor environments"},{"location":"simulation-content/lighting-for-interiors/#getting-started","text":"","title":"Getting Started"},{"location":"simulation-content/lighting-for-interiors/#light-layers-what-are-they","text":"Unity provides us with tools that allow us to set up lights that only affect meshes specified to receive light from those light sources. This opens up the possibility to exclude exterior lighting from interiors and bake Mixed Lighting for higher quality visuals when running the simulator. This is especially useful when creating interior environments.","title":"Light Layers: What are they ?"},{"location":"simulation-content/lighting-for-interiors/#light-layers-how-do-we-use-them","text":"The SVL simulator is already setup to employ Light Layers but you will need to do some extra preparation to use them. This includes proper setup of the interior post-processing volume, lights, meshes and then baking the lighting. To do this, you must be in Developer Mode .","title":"Light Layers: How do we use them ?"},{"location":"simulation-content/lighting-for-interiors/#interior-environment-setup","text":"Create an empty game object and name it LightingSetup (Right click in Hierarchy > Create Empty) then zero out the transforms (Click the 3 dots on the right of the Transform section > Reset). This is where you will place your rendering volumes, light probes and reflection probes and will help keep the hierarchy organized. ) Create a Box Volume (Right click in Hierarchy > Volume > Box Volume) and zero out the transforms. Adjust the position and size of the volume to contain your interior environment. Multiple volumes can be used. Place this inside your LightingSetup game object in the Hierarchy. Be sure to name it so that it is defined as an interior lighting post processing volume. Example: NameOfEnvironment PostProcessingVolumeInterior. ) Create a new Volume Profile (Assets > Create > Volume Profile). This Post-Processing Volume Profile will override the Simulator profile that is used at runtime for exterior lighting. Be sure to name it so that it is defined as an interior lighting post processing volume ex: NameOfEnvironment PostProcessingProfileInterior. ) Add your profile overrides for the interior (Select your NameOfEnvironment PostProcessingProfileInterior in the project view > Add Override > Select and adjust desired override). ) Add a Reflection Proxy Volume (Right click in Hierarchy > Volume > Select desired volume shape) and zero out the transforms. Be sure to name it so that it is defined as your interior environment reflection proxy volume. Example: NameOfEnvironment ReflectionProxyVolume. Add the Reflection Proxy Volume Script (Select your NameOfEnvironment ReflectionProxyVolume in the project view > Add Component > Search for and select Reflection Proxy Volume). Remove Box Collider and Volume components from the Reflection Proxy Volume asset. (Select your NameOfEnvironment ReflectionProxyVolume in the project view > click the 3 dots on the right for each section > Remove Component) Do not remove the Reflection Proxy Volume component. Move and adjust the size of the Reflection Proxy Volume asset to envelope the desired space in your interior. ) Add Reflection Probes to the scene (Right click in Hierarchy > Light > Reflection Probe). This is vital to proper lighting for interior spaces. For environments requiring multiple reflection probes create an empty game object inside of the LightingSetUp game object, reset transforms and place your probes inside. Naming for reflection probes and reflection probe group should be descriptive ex: NameOfEnvironment ReflectionProbe. Add the Reflection Proxy Volume asset to your Reflection Probes (Select your Reflection Probe > Projection Settings Section > Proxy Volume field > add your Proxy Volume). ) Adjust the size of each Reflection Probe to the desired size for your interior space. Add Light Probes to the scene (Right click in Hierarchy > Light > Light Probe Group). This step is optional but can improve the lighting fidelity for dynamic objects as they move through the environment. Place this inside your LightingSetup game object in the Hierarchy. Naming for the group should be descriptive ex: NameOfEnvironment LightProbeGroup. )","title":"Interior Environment Setup"},{"location":"simulation-content/lighting-for-interiors/#setting-up-the-lights","text":"Add lights to your scene if none already exist and adjust settings for the desired visuals (Right click in Hierarchy > Light > choose the desired light) Select the light in the inspector and in the General dropdown change Mode to Mixed. Next set the Light Layer to Light Layer 7 (Expand More Options in General drop down by clicking the gear icon). If Shadows are enabled make sure that Link Light Layer is also enabled (Expand More Options in General drop down by clicking the gear icon). )","title":"Setting Up the Lights"},{"location":"simulation-content/lighting-for-interiors/#static-mesh-setup","text":"Set all meshes to Reflection Probe Static (Select mesh > Top right \"Static\" drop down > Reflection Probe Static). Set Rendering Layer Mask to Light Layer 7 (Select mesh > Expand Mesh Renderer > Expand Additional Settings > Rendering Layer Mask). )","title":"Static Mesh Setup"},{"location":"simulation-content/lighting-for-interiors/#dynamic-mesh-setup-top-dynamic-mesh-setup-data-toc-labeldynamic-mesh-setup","text":"Set Rendering Layer Mask to Light Layer 7 (See above). If you have a vehicle that includes lights, those lights will need to be set to Light Layer 7 and Link Light Layer enabled (See above).","title":"Dynamic Mesh Setup top {: #dynamic-mesh-setup data-toc-label='Dynamic Mesh Setup'}}"},{"location":"simulation-content/lighting-for-interiors/#bake-reflections-and-lighting-top-baking-the-lighting-data-toc-labelbaking-the-lighting","text":"Open the Lighting tab to access Unity's lighting options (Window > Rendering > Lighting). In the Scene section select your Lighting Settings asset. If you don't already have a Lighting Settings asset then create a new one using the button under the input field. ) Be sure that the Mixed Lighting Section has Baked Global Illumination checked and Lighting Mode is set to Baked Indirect. Adjust settings to match the quality expectations. For development purposes it's useful to have a draft version with lower quality settings and then a high quality version for final rendering. In the Environment section add the PostProcessingProfileInterior that you made earlier. ) Finally click the Generate Lighting. Once the lighting has finished processing click the Generate Lighting drop down and click Bake Reflection Probes. ) If you need to make changes to the lighting or the environment you will need to re-bake all lighting data.","title":"Bake Reflections and Lighting top {: #baking-the-lighting data-toc-label='Baking the Lighting'}}"},{"location":"simulation-content/map-annotation/","text":"Map Annotation # SVL Simulator supports creating, editing, and exporting of HD maps of existing 3D environments (Unity scenes) in Developer Mode . The maps can be saved in the currently supported Apollo, Autoware or Lanelet2 formats. Map annotation objects contain a combination of data for all export formats and internal NPC navigation. The classes are organized in a specific way to better support NPC navigation. Not all data contained in an HD map import is kept in the annotation objects. To better understand map annotation objects and how they are organized, please review existing open-source maps before attempting to create complex map annotation scenes. Table of Contents Creating a New Map Annotate Lanes Create Parent Object Make Lanes Make Boundary Lines Annotate Intersections Create Parent Object Create Intersection Lanes Create Traffic Signals Create Traffic Signs Create Stop Lines Create Pole Annotate Self-Reversing Lanes Annotation Information Annotate Self-Reversing Lanes under Traffic Lanes Annotate Self-Reversing Lanes under Intersections Annotate Other Features Create Pedestrian NavMesh Create Pedestrian Path Sidewalk Create Pedestrian Path Crosswalk Create Junction Create Crosswalk Create Clear Area Create Parking Space Create Speed Bump Export Map Annotations Import Map Annotations Map Formats Creating a New Map top # Make sure your roads belong to the layer of Default since waypoints will be only created on this layer. Turn on Gizmos in Unity Editor Scene panel so annotation will be visible. Turn off scene lighting. Make sure your environment has Mesh Collider Unity components added. Open HD Map Annotation in Unity : Simulator -> Annotate HD Map By default, map annotation is not shown. Click View All under View Modes to show all existing map annotations. Or you can click Lock View Selected to only show the annotations you are selecting. NOTE Large maps with heavy annotation will slow the Unity Editor; be sure to only enable what you need to work with. You can also use the Unity Editor features to hide objects that are not needed in the Hierarchy panel. Gizmo Size enables larger or smaller gizmos for annotation objects. This value is saved per map, not globally. Before annotating, drag in the correct Parent Object , for example TrafficLanes . Then every new object you create will be under TrafficLanes object. The expected object hierarchy is as follows: Map - This prefab which will be the object containing all the HD Annotations. The MapHolder script should be added to this prefab TrafficLanes - This object will hold all of the MapLaneSection and MapLane which cannot be grouped under MapLaneSection MapLaneSection - This will hold all of the lanes in a section of road MapLane - A single annotated lane Intersections - This object will hold all of the MapIntersection MapIntersection - This object will hold all of the annotations for an intersection MapLane - A single annotated lane MapLine - A single annotated line (e.g. a stop line) MapSignal - A single annotated traffic signal MapSign - A single annotated traffic sign (e.g. a stop sign) BoundaryLines - This object will hold all of boundary lines More notes on HD Map Annotation window: Under Utilities , there are three Utility buttons Snap all which will snap all your annotations to ground layer. NOTE Colliders above the annotation will catch the raycast; be sure to turn off before using this tool. Remove lines which will remove all extra boundary lines in the map annotations. If two boundary lines' end points are both overlapping, one of the lines will be removed and the other will be shared. Create lines which will create fake boundary lines for all lanes which miss boundary lines using fixed width. To quick annotate a map, you can first annotate lanes, click this button to create boundary lines, and then adjust them. Under Create Intersection / Lane Section Holder , there are two buttons Intersection and Lane Section to create these two objects quickly and attach the corresponding scripts. To make Map a prefab, drag it from the scene Hierarchy into the Project folder After annotation is done, remember to save: select the Map prefab, in the Inspector click Overrides -> Apply All Annotate Lanes top # Create Parent Object top # In the Map prefab (if you don't have one, you can create an empty game object, attaching MapHolder script and make it a prefab), create a new object and name it \"TrafficLanes\" In the Inspector of the Map , drag the new TrafficLanes object into the Traffic Lanes holder In TrafficLanes , create a new object Add the MapLaneSection script to the object and position it close the section of road that will be annotated You can also create a MapLaneSection object using the Lane Section button in HD Map Annotation window. MapLaneSection script is needed for sections that contain more than 1 lane. For sections with only 1 lane, they can be left as a child object of the TrafficLanes object or grouped with the boundary lines under a different parent object without the MapLaneSection script. Each MapLaneSection will contain parallel lanes with similar lengths 1 MapLane per lane of road as well as 1 MapLine per boundary line (optional, you can use the utility button Create lines to generate fake boundary lines) If the annotations will be broken up into multiple MapLane (e.g. a straight section and a curved section), multiple MapLaneSection are required A MapLane cannot begin or end in the middle of another MapLane (e.g. a lane splits or merges). This situation would require a 2nd MapLaneSection to be created where the split/merge begins. The other lanes on the road will also need to be broken up into multiple MapLane to be included in the multiple MapLaneSection MapLaneSection is used to automate the computation of relation of neighboring lanes. Please make sure every lane under MapLaneSection has at least 3 waypoints. Example of single lane splitting into a right-turn only lane and a straight lane Make Lanes top # Select the Lane/Line option under Create Mode A large yellow TARGET_WAYPOINT will appear in the center of the scene. This is where the TEMP_WAYPOINT objects will be placed. The TARGET_WAYPOINT will not appear if your roads had no mesh collider attached. Drag in the appropriate MapLaneSection to be the Parent Object Click Waypoint button to create a new TEMP_WAYPOINT . This is where the lane will begin. Move the scene so that the TARGET_WAYPOINT is in the desired location of the next TEMP_WAYPOINT With 2 waypoints, the Create Straight function will connect the waypoints and use the number in Waypoint Count to determine how many waypoints to add in between More than 2 waypoints can be used with the Create Straight function and they will be connected in the order they were created, and the Waypoint Count value will not be used With 3 waypoints, the Create Curve function can be used to create a Bezier curve between the 1st and 3rd waypoints and the Waypoint Count value will be used to determine how many waypoints to add in between. Verify Lane is the selected Map Object Type Verify the selected Lane Turn Type is correct Select the appropriate lane boundry types Enter the speed limit of the lane (in m/s) Enter the desired number of waypoints in the lane (minimum 2 for a straight lane and 3 for a curved lane) Click the appropriate Connect button to create the lane To adjust the positions of the waypoints, with the MapLane selected, in the Inspector check Display Handles . Individual waypoints can now have their position adjusted You can also show each waypoint's handle by clicking Toggle Handles button in HD Map Annotation window. NOTE Be sure to toggle off when finished editing. Below you can also find a group of helper buttons to manipulate the lane's waypoints. Append - Adds a waypoint after the last index Prepend - Adds a waypoint before the first index Remove First - Removes the first waypoint Remove Last - Remove the the last waypoint Reverse - Reverses the waypoint order Clear - Removes all waypoints Double - Doubles the waypoint count Half - Reduces the waypoint count be approximately half Make Boundary Lines top # Drag in the appropriate MapLaneSection to be the Parent Object The same process as for lanes can be used to create boundary lines, but the Map Object Type will be BoundaryLine It is better to have the direction of a boundary line match the direction of the lane if possible For every boundary line, you also need to drag it into the corresponding field in the lane object. Boundary lines can be combined for adjacent lanes - no need to make duplicates for the same line. It is best to name boundary lines left and right so it is easier to tell which line goes into the public references on the map lane data. Annotate Intersections top # Create Parent Object top # In the Map prefab, create a new object and name it \"Intersections\" In the Inspector of the Map , drag the new Intersections object into the Intersections holder In Intersections , create a new object Add the MapIntersection script to the new object and position it in the center of the intersection that will be annotated You can also create an MapIntersection object using the Intersection button in HD Map Annotation window You need to adjust the Trigger Bounds of the MapIntersection so that the box covers the space of the intersection The MapIntersection will contain all lanes, traffic signals, stop lines, and traffic signs in the intersection Create Intersection Lanes top # Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object Place the intersection object in the center of the intersection. The same process for creating normal MapLane is used here If the Lane involves changing directions, verify the correct Lane Turn Type is selected If you are annotating boundary lines for each lane, remember to set VIRTUAL as the Line Type for the line objects If there is a stop line in the intersection, the start of the intersection lanes should be after the stop line Be sure the trigger bounds size covers most of the intersection, in X,Y, and Z axes, but does not overlap with any stop lines. This trigger volume counts NPCs that enter the intersection and will cause errors if an NPC is counted before it reaches the intersection itself. For NPCs to properly navigate an intersection, the Yield To Lanes list of a lane (usually a turning lane) must be manually filled in. With View Selected toggled in the HD Map Annotation window, the lanes in the Yield To Lanes list will be highlighted in yellow to make it easier to verify that the correct lanes are in the list. Before an NPC enters an intersection lane, it will check that there are no NPCs on the lanes in the Yield To Lanes before continuing. Enter the number of lanes that the current lanes yields to as the size of Yield To Lanes For each element in the list, drag in a lane that takes priority over the selected lane For example, generally when turning left the NPC will yield to the oncoming traffic going straight so the straight lanes should be in the left turn lane's Yield To Lanes list Create Traffic Signals top # Select the Signal option under Create Mode Drag in the MapIntersection as the Parent Object Select the correct Signal Type Traffic Signals are created on top of existing meshes. The signal annotation directions will be the same as the directions of the mesh. Please make sure the directions of the mesh is correct. Select the mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Click Create Signal to annotate the traffic signal Make sure the bounds of the map signal match the mesh object bounds Create Traffic Signs top # Select the Sign option under Create Mode Drag in the MapIntersection as the Parent Object Select the Sign mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Change the Sign Type to the desired type Click Create Sign to create the annotation Find the stop line MapLine that is associated with the created MapSign Select the MapSign and drag the MapLine into the Stop Line box Verify the annotation is created in the correct orientation. The Z-axis (blue) should be facing the same way the sign faces. Move the Bounding Box so that is matches the sign Bound Offsets adjusts the location of the box Bound Scale adjusts the size of the box Create Stop Lines top # Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object A similar process to boundary lines and traffic lanes is used for stop lines Change the Map Object Type to StopLine Forward Right vs Forward Left depend on the direction of the lane related to this stop line and the order that the waypoints were created in. The \"Forward\" direction should match the direction of the lane Example: In a right-hand drive map (cars are on the right side of the road), if the waypoints are created from the outside of the road inwards, then Forward Right should be selected. To verify if the correct direction was selected, Toggle Tool Handle Rotation so that the tool handles are in the active object's rotation. The Z-axis of the selected Stop Line should point to the intersection. A StopLine needs to be with the lanes that approach it. The last waypoint of approaching lanes should be perpendicular and intersect with the stop line Stop lines should be checked IsStopSign for stop sign intersections and left false for signal light intersections For intersections that do not have signals, stop lines are not needed. Create Pole top # Poles are required for Autoware Vector maps. In an intersection with traffic lights, there is 1 MapPole on each corner, next to a stop line. The MapPole holds references to all traffic lights that signal the closest stop line. Select the Pole option under Create Mode Drag in the MapIntersection as the Parent Object A TARGET_WAYPOINT will appear in the center of the scene. This is where the pole annotation will be created. Position the TARGET_WAYPOINT on the corner of the intersection near the stop line Click Create Pole to create the annotation Find the traffic light MapSignal that are associated with the closest stop line Select the MapPole In the Inspector, change the Signal Lights size to be the number of MapSignal that signal the stop line Drag 1 MapSignal into each element of Signal Lights Annotate Self-Reversing Lanes top # These types of lanes are only supported on Apollo 5.0 Annotation Information top # There are TrafficLanes and Intersections objects. TrafficLanes object is for straight lane without any branch. Intersections object is for branching lanes, which has same predecessor lane. Annotate Self-Reversing Lanes under Traffic Lanes top # Add MapLaneSection game object under TrafficLanes. MapLaneSection has one pair of lanes, which are forward and reverse lanes. Annotate lane given name to MapLane_forward and move it under MapLaneSection. Use straight line connect menu with way point count and move each point to shape lane. Use Display Lane option to show width of lane. In order for parking to work, there should have some space between parking space and lane. Duplicate the MapLane_forward and rename it to MapLane_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane_forward, MapLane_reverse) and change properties like the following: Is Self Reverse Lane: True Lane Turn Type: NO_TURN Left Bound Type: SOLID_WHITE Right Bound Type: SOLID_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map) Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way. Annotate Self-Reversing Lanes under Intersections top # Add MapIntersection game object under Intersections. MapIntersection has several pair of lanes. Annotate lane given name to MapLane_forward move it under MapIntersection. Use curve connect menu with way point count, 5. Duplicate the MapLane_forward and rename it to MapLane1_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane_forward, MapLane_reverse) and change properties like the following: Is Self Reverse Lane: True Left Bound Type: DOTTED_WHITE Right Bound Type: DOTTED_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map). Choose each lane and decide its turn type considering its direction. Lane Turn Type: NO_TURN or LEFT_TURN or RIGHT_TURN Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way. Annotate Other Features top # Other annotation features may be included in the TrafficLanes or Intersections objects or they may be sorted into other parent objects. Create Pedestrian NavMesh top # To support pedestrians, the map must include a NavMesh created with the the Unity Editor. Be sure map scene is saved in the correct folder in Assets/External/Environments/YourMap Mark all walkable ground meshes as Navigation Static in the Inspector panel Open Window -> AI -> Navigation Select meshes and mark Navigation Area in the Object menu of the Navigation panel Road and Sidewalk MUST be separate meshes and layered for Pedestrian crosswalk to work (see second image below) Click Bake button in the Bake menu of the Navigation panel. NavMesh will be created in a folder named from the scene file. Create Pedestrian Path Sidewalk top # This annotation controls where pedestrians will pass on sidewalks. Pedestrians can walk anywhere but will stay on annotated areas if possible. Select the Pedestrian option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center for the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT on the desired path Click Waypoint to create a TEMP_WAYPOINT Repeat to create a trail of TEMP_WAYPOINT along the desired path Click Connect to create the MapPedestrian object Create Pedestrian Path Crosswalk top # This annotation controls where pedestrians will walk on crosswalks. Pedestrians can walk anywhere but will stay on annotated areas if possible. When annotated correctly, pedestrians will only walk when signal lights are red. Select the Pedestrian option under Create Mode Drag in the MapIntersection Parent Object . This MUST be the MapIntersection object for the pedestrian to know what signal states are. Again, MapIntersection holder MUST be the parent for crosswalk map pedestrian annotation. A TARGET_WAYPOINT will appear in the center for the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT on the desired path Click Waypoint to create a TEMP_WAYPOINT start Click Waypoint to create a TEMP_WAYPOINT end Select Crosswalk type and select Z forward orientation. MapPedestrian object MUST be Z-forward INTO intersection. Click Connect to create the MapPedestrian object Create Junction top # Junction annotations need to be annotated for Apollo 5.0 for Map Exporter to work correctly. It converts the boundary of the intersection. Select the Junction option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one vertex of the junction Click Waypoint to create a TEMP_WAYPOINT Create the desired number of TEMP_WAYPOINTS Click Connect to create the MapJunction Create Crosswalk top # Select the CrossWalk option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the crosswalk Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapCrossWalk Create Clear Area top # Select the ClearArea option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the clear area Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapClearArea Create Parking Space top # Select the ParkingSpace option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to the corner of the parking space closest to an approaching vehicle Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapParkingSpace NOTE: Apollo 5.0 requires some space between the edge of a lane and the parking space. To verify that there is space, select the MapLane that goes past the MapParkingSpace and click Display Lane . This will show the width of the lane facilitate adjustment of the lane to have a gap between the lane and parking space. Create Speed Bump top # Select the SpeedBump option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one end of the speed bump Click Waypoint to create a TEMP_WAYPOINT Create 2 TEMP_WAYPOINT in the order shown below The TEMP_WAYPOINT should be wide enough to be outside the lane Click Connect to create the MapSpeedBump To verify that the MapSpeedBump is wide enough, select the MapLane that the MapSpeedBump crosses. Enable Display Lane to visualize the width of the lane. If necesary, select the MapSpeedBump and enable Display Handles to adjust the positions of the individual waypoints. Export Map Annotations top # HD Map Annotations may be exported in a variety of formats. Current supported formats are: Apollo 5.0 HD Map Autoware Vector Map Lanelet2 Map OpenDrive Map To export a map: Open the HD Map Export tool in Unity : Simulator -> Export HD Map Select the desired format from the dropdown Export Format Enter the desired save location of the exported map Click Export to create the exported map Note the exporter will fail if any lane has no boundary lines annotated - you need to either manually annotate them or use the Create lines button in HD Map Annotation window to create fake boundary lines. Import Map Annotations top # The simulator can import a variety of formats of annotated map. NOTE After importing an HD map, you will be required to check each annotation to make sure import is correct. Simulator will provide logs if there are issues that need resolved. If the importer cannot parse a HD map, it will make holder objects that will need manually edited. Note you must delete any MapHolder existing in a scene before the importer tool will work. Current supported formats are: Lanelet2 Map Apollo 5.0 HD Map OpenDRIVE Map To import a map: Open the HD Map Export tool in Unity : Simulator -> Import HD Map Select the format of the input map from the dropdown Import Format Select the file or folder that will be imported Click Import to import the map annotations into the simulator Distance Threshold is used to down-sample straight lines/lanes and Delta Threshold is used to down-sample curved lines/lanes. Changing both of them to 0 will keep all points, but for large maps displaying all annotations may lead to Unity crash. Lanelet2 map importer Notes: Lanes will be automatically imported and grouped as MapLaneSection if possible Intersections with four-way traffic lights / stop signs or two-way stop signs can be imported and grouped under MapIntersection Left-turn lanes are automatically found and their corresponding lanes to yield are also obtained automatically to get NPCs working correctly For each MapIntersection , you need to manually adjust X and Z for the Trigger Bounds as explained in annotate intersection part . Remember to check objects under Intersections are grouped correctly. Map Formats top # For more information on the map formats, please see the links below: Apollo HD Autoware Vector Lanelet2 OpenDRIVE","title":"Map annotation"},{"location":"simulation-content/map-annotation/#creating-a-new-map","text":"Make sure your roads belong to the layer of Default since waypoints will be only created on this layer. Turn on Gizmos in Unity Editor Scene panel so annotation will be visible. Turn off scene lighting. Make sure your environment has Mesh Collider Unity components added. Open HD Map Annotation in Unity : Simulator -> Annotate HD Map By default, map annotation is not shown. Click View All under View Modes to show all existing map annotations. Or you can click Lock View Selected to only show the annotations you are selecting. NOTE Large maps with heavy annotation will slow the Unity Editor; be sure to only enable what you need to work with. You can also use the Unity Editor features to hide objects that are not needed in the Hierarchy panel. Gizmo Size enables larger or smaller gizmos for annotation objects. This value is saved per map, not globally. Before annotating, drag in the correct Parent Object , for example TrafficLanes . Then every new object you create will be under TrafficLanes object. The expected object hierarchy is as follows: Map - This prefab which will be the object containing all the HD Annotations. The MapHolder script should be added to this prefab TrafficLanes - This object will hold all of the MapLaneSection and MapLane which cannot be grouped under MapLaneSection MapLaneSection - This will hold all of the lanes in a section of road MapLane - A single annotated lane Intersections - This object will hold all of the MapIntersection MapIntersection - This object will hold all of the annotations for an intersection MapLane - A single annotated lane MapLine - A single annotated line (e.g. a stop line) MapSignal - A single annotated traffic signal MapSign - A single annotated traffic sign (e.g. a stop sign) BoundaryLines - This object will hold all of boundary lines More notes on HD Map Annotation window: Under Utilities , there are three Utility buttons Snap all which will snap all your annotations to ground layer. NOTE Colliders above the annotation will catch the raycast; be sure to turn off before using this tool. Remove lines which will remove all extra boundary lines in the map annotations. If two boundary lines' end points are both overlapping, one of the lines will be removed and the other will be shared. Create lines which will create fake boundary lines for all lanes which miss boundary lines using fixed width. To quick annotate a map, you can first annotate lanes, click this button to create boundary lines, and then adjust them. Under Create Intersection / Lane Section Holder , there are two buttons Intersection and Lane Section to create these two objects quickly and attach the corresponding scripts. To make Map a prefab, drag it from the scene Hierarchy into the Project folder After annotation is done, remember to save: select the Map prefab, in the Inspector click Overrides -> Apply All","title":"Creating a New Map"},{"location":"simulation-content/map-annotation/#annotate-lanes","text":"","title":"Annotate Lanes"},{"location":"simulation-content/map-annotation/#create-parent-object-lanes","text":"In the Map prefab (if you don't have one, you can create an empty game object, attaching MapHolder script and make it a prefab), create a new object and name it \"TrafficLanes\" In the Inspector of the Map , drag the new TrafficLanes object into the Traffic Lanes holder In TrafficLanes , create a new object Add the MapLaneSection script to the object and position it close the section of road that will be annotated You can also create a MapLaneSection object using the Lane Section button in HD Map Annotation window. MapLaneSection script is needed for sections that contain more than 1 lane. For sections with only 1 lane, they can be left as a child object of the TrafficLanes object or grouped with the boundary lines under a different parent object without the MapLaneSection script. Each MapLaneSection will contain parallel lanes with similar lengths 1 MapLane per lane of road as well as 1 MapLine per boundary line (optional, you can use the utility button Create lines to generate fake boundary lines) If the annotations will be broken up into multiple MapLane (e.g. a straight section and a curved section), multiple MapLaneSection are required A MapLane cannot begin or end in the middle of another MapLane (e.g. a lane splits or merges). This situation would require a 2nd MapLaneSection to be created where the split/merge begins. The other lanes on the road will also need to be broken up into multiple MapLane to be included in the multiple MapLaneSection MapLaneSection is used to automate the computation of relation of neighboring lanes. Please make sure every lane under MapLaneSection has at least 3 waypoints. Example of single lane splitting into a right-turn only lane and a straight lane","title":"Create Parent Object"},{"location":"simulation-content/map-annotation/#make-lanes","text":"Select the Lane/Line option under Create Mode A large yellow TARGET_WAYPOINT will appear in the center of the scene. This is where the TEMP_WAYPOINT objects will be placed. The TARGET_WAYPOINT will not appear if your roads had no mesh collider attached. Drag in the appropriate MapLaneSection to be the Parent Object Click Waypoint button to create a new TEMP_WAYPOINT . This is where the lane will begin. Move the scene so that the TARGET_WAYPOINT is in the desired location of the next TEMP_WAYPOINT With 2 waypoints, the Create Straight function will connect the waypoints and use the number in Waypoint Count to determine how many waypoints to add in between More than 2 waypoints can be used with the Create Straight function and they will be connected in the order they were created, and the Waypoint Count value will not be used With 3 waypoints, the Create Curve function can be used to create a Bezier curve between the 1st and 3rd waypoints and the Waypoint Count value will be used to determine how many waypoints to add in between. Verify Lane is the selected Map Object Type Verify the selected Lane Turn Type is correct Select the appropriate lane boundry types Enter the speed limit of the lane (in m/s) Enter the desired number of waypoints in the lane (minimum 2 for a straight lane and 3 for a curved lane) Click the appropriate Connect button to create the lane To adjust the positions of the waypoints, with the MapLane selected, in the Inspector check Display Handles . Individual waypoints can now have their position adjusted You can also show each waypoint's handle by clicking Toggle Handles button in HD Map Annotation window. NOTE Be sure to toggle off when finished editing. Below you can also find a group of helper buttons to manipulate the lane's waypoints. Append - Adds a waypoint after the last index Prepend - Adds a waypoint before the first index Remove First - Removes the first waypoint Remove Last - Remove the the last waypoint Reverse - Reverses the waypoint order Clear - Removes all waypoints Double - Doubles the waypoint count Half - Reduces the waypoint count be approximately half","title":"Make Lanes"},{"location":"simulation-content/map-annotation/#make-boundary-lines","text":"Drag in the appropriate MapLaneSection to be the Parent Object The same process as for lanes can be used to create boundary lines, but the Map Object Type will be BoundaryLine It is better to have the direction of a boundary line match the direction of the lane if possible For every boundary line, you also need to drag it into the corresponding field in the lane object. Boundary lines can be combined for adjacent lanes - no need to make duplicates for the same line. It is best to name boundary lines left and right so it is easier to tell which line goes into the public references on the map lane data.","title":"Make Boundary Lines"},{"location":"simulation-content/map-annotation/#annotate-intersections","text":"","title":"Annotate Intersections"},{"location":"simulation-content/map-annotation/#create-parent-object-intersection","text":"In the Map prefab, create a new object and name it \"Intersections\" In the Inspector of the Map , drag the new Intersections object into the Intersections holder In Intersections , create a new object Add the MapIntersection script to the new object and position it in the center of the intersection that will be annotated You can also create an MapIntersection object using the Intersection button in HD Map Annotation window You need to adjust the Trigger Bounds of the MapIntersection so that the box covers the space of the intersection The MapIntersection will contain all lanes, traffic signals, stop lines, and traffic signs in the intersection","title":"Create Parent Object"},{"location":"simulation-content/map-annotation/#create-intersection-lanes","text":"Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object Place the intersection object in the center of the intersection. The same process for creating normal MapLane is used here If the Lane involves changing directions, verify the correct Lane Turn Type is selected If you are annotating boundary lines for each lane, remember to set VIRTUAL as the Line Type for the line objects If there is a stop line in the intersection, the start of the intersection lanes should be after the stop line Be sure the trigger bounds size covers most of the intersection, in X,Y, and Z axes, but does not overlap with any stop lines. This trigger volume counts NPCs that enter the intersection and will cause errors if an NPC is counted before it reaches the intersection itself. For NPCs to properly navigate an intersection, the Yield To Lanes list of a lane (usually a turning lane) must be manually filled in. With View Selected toggled in the HD Map Annotation window, the lanes in the Yield To Lanes list will be highlighted in yellow to make it easier to verify that the correct lanes are in the list. Before an NPC enters an intersection lane, it will check that there are no NPCs on the lanes in the Yield To Lanes before continuing. Enter the number of lanes that the current lanes yields to as the size of Yield To Lanes For each element in the list, drag in a lane that takes priority over the selected lane For example, generally when turning left the NPC will yield to the oncoming traffic going straight so the straight lanes should be in the left turn lane's Yield To Lanes list","title":"Create Intersection Lanes"},{"location":"simulation-content/map-annotation/#create-traffic-signals","text":"Select the Signal option under Create Mode Drag in the MapIntersection as the Parent Object Select the correct Signal Type Traffic Signals are created on top of existing meshes. The signal annotation directions will be the same as the directions of the mesh. Please make sure the directions of the mesh is correct. Select the mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Click Create Signal to annotate the traffic signal Make sure the bounds of the map signal match the mesh object bounds","title":"Create Traffic Signals"},{"location":"simulation-content/map-annotation/#create-traffic-signs","text":"Select the Sign option under Create Mode Drag in the MapIntersection as the Parent Object Select the Sign mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Change the Sign Type to the desired type Click Create Sign to create the annotation Find the stop line MapLine that is associated with the created MapSign Select the MapSign and drag the MapLine into the Stop Line box Verify the annotation is created in the correct orientation. The Z-axis (blue) should be facing the same way the sign faces. Move the Bounding Box so that is matches the sign Bound Offsets adjusts the location of the box Bound Scale adjusts the size of the box","title":"Create Traffic Signs"},{"location":"simulation-content/map-annotation/#create-stop-lines","text":"Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object A similar process to boundary lines and traffic lanes is used for stop lines Change the Map Object Type to StopLine Forward Right vs Forward Left depend on the direction of the lane related to this stop line and the order that the waypoints were created in. The \"Forward\" direction should match the direction of the lane Example: In a right-hand drive map (cars are on the right side of the road), if the waypoints are created from the outside of the road inwards, then Forward Right should be selected. To verify if the correct direction was selected, Toggle Tool Handle Rotation so that the tool handles are in the active object's rotation. The Z-axis of the selected Stop Line should point to the intersection. A StopLine needs to be with the lanes that approach it. The last waypoint of approaching lanes should be perpendicular and intersect with the stop line Stop lines should be checked IsStopSign for stop sign intersections and left false for signal light intersections For intersections that do not have signals, stop lines are not needed.","title":"Create Stop Lines"},{"location":"simulation-content/map-annotation/#create-pole","text":"Poles are required for Autoware Vector maps. In an intersection with traffic lights, there is 1 MapPole on each corner, next to a stop line. The MapPole holds references to all traffic lights that signal the closest stop line. Select the Pole option under Create Mode Drag in the MapIntersection as the Parent Object A TARGET_WAYPOINT will appear in the center of the scene. This is where the pole annotation will be created. Position the TARGET_WAYPOINT on the corner of the intersection near the stop line Click Create Pole to create the annotation Find the traffic light MapSignal that are associated with the closest stop line Select the MapPole In the Inspector, change the Signal Lights size to be the number of MapSignal that signal the stop line Drag 1 MapSignal into each element of Signal Lights","title":"Create Pole"},{"location":"simulation-content/map-annotation/#annotate-self-reversing-lanes","text":"These types of lanes are only supported on Apollo 5.0","title":"Annotate Self-Reversing Lanes"},{"location":"simulation-content/map-annotation/#annotation-information","text":"There are TrafficLanes and Intersections objects. TrafficLanes object is for straight lane without any branch. Intersections object is for branching lanes, which has same predecessor lane.","title":"Annotation Information"},{"location":"simulation-content/map-annotation/#annotate-self-reversing-lanes-under-traffic-lanes","text":"Add MapLaneSection game object under TrafficLanes. MapLaneSection has one pair of lanes, which are forward and reverse lanes. Annotate lane given name to MapLane_forward and move it under MapLaneSection. Use straight line connect menu with way point count and move each point to shape lane. Use Display Lane option to show width of lane. In order for parking to work, there should have some space between parking space and lane. Duplicate the MapLane_forward and rename it to MapLane_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane_forward, MapLane_reverse) and change properties like the following: Is Self Reverse Lane: True Lane Turn Type: NO_TURN Left Bound Type: SOLID_WHITE Right Bound Type: SOLID_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map) Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way.","title":"Annotate Self-Reversing Lanes under Traffic Lanes"},{"location":"simulation-content/map-annotation/#annotate-self-reversing-lanes-under-intersections","text":"Add MapIntersection game object under Intersections. MapIntersection has several pair of lanes. Annotate lane given name to MapLane_forward move it under MapIntersection. Use curve connect menu with way point count, 5. Duplicate the MapLane_forward and rename it to MapLane1_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane_forward, MapLane_reverse) and change properties like the following: Is Self Reverse Lane: True Left Bound Type: DOTTED_WHITE Right Bound Type: DOTTED_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map). Choose each lane and decide its turn type considering its direction. Lane Turn Type: NO_TURN or LEFT_TURN or RIGHT_TURN Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way.","title":"Annotate Self-Reversing Lanes under Intersections"},{"location":"simulation-content/map-annotation/#annotate-other-features","text":"Other annotation features may be included in the TrafficLanes or Intersections objects or they may be sorted into other parent objects.","title":"Annotate Other Features"},{"location":"simulation-content/map-annotation/#create-pedestrian-navmesh","text":"To support pedestrians, the map must include a NavMesh created with the the Unity Editor. Be sure map scene is saved in the correct folder in Assets/External/Environments/YourMap Mark all walkable ground meshes as Navigation Static in the Inspector panel Open Window -> AI -> Navigation Select meshes and mark Navigation Area in the Object menu of the Navigation panel Road and Sidewalk MUST be separate meshes and layered for Pedestrian crosswalk to work (see second image below) Click Bake button in the Bake menu of the Navigation panel. NavMesh will be created in a folder named from the scene file.","title":"Create Pedestrian NavMesh"},{"location":"simulation-content/map-annotation/#create-pedestrian-path-sidewalk","text":"This annotation controls where pedestrians will pass on sidewalks. Pedestrians can walk anywhere but will stay on annotated areas if possible. Select the Pedestrian option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center for the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT on the desired path Click Waypoint to create a TEMP_WAYPOINT Repeat to create a trail of TEMP_WAYPOINT along the desired path Click Connect to create the MapPedestrian object","title":"Create Pedestrian Path Sidewalk"},{"location":"simulation-content/map-annotation/#create-pedestrian-path-crosswalk","text":"This annotation controls where pedestrians will walk on crosswalks. Pedestrians can walk anywhere but will stay on annotated areas if possible. When annotated correctly, pedestrians will only walk when signal lights are red. Select the Pedestrian option under Create Mode Drag in the MapIntersection Parent Object . This MUST be the MapIntersection object for the pedestrian to know what signal states are. Again, MapIntersection holder MUST be the parent for crosswalk map pedestrian annotation. A TARGET_WAYPOINT will appear in the center for the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT on the desired path Click Waypoint to create a TEMP_WAYPOINT start Click Waypoint to create a TEMP_WAYPOINT end Select Crosswalk type and select Z forward orientation. MapPedestrian object MUST be Z-forward INTO intersection. Click Connect to create the MapPedestrian object","title":"Create Pedestrian Path Crosswalk"},{"location":"simulation-content/map-annotation/#create-junction","text":"Junction annotations need to be annotated for Apollo 5.0 for Map Exporter to work correctly. It converts the boundary of the intersection. Select the Junction option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one vertex of the junction Click Waypoint to create a TEMP_WAYPOINT Create the desired number of TEMP_WAYPOINTS Click Connect to create the MapJunction","title":"Create Junction"},{"location":"simulation-content/map-annotation/#create-crosswalk","text":"Select the CrossWalk option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the crosswalk Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapCrossWalk","title":"Create Crosswalk"},{"location":"simulation-content/map-annotation/#create-clear-area","text":"Select the ClearArea option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the clear area Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapClearArea","title":"Create Clear Area"},{"location":"simulation-content/map-annotation/#create-parking-space","text":"Select the ParkingSpace option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to the corner of the parking space closest to an approaching vehicle Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapParkingSpace NOTE: Apollo 5.0 requires some space between the edge of a lane and the parking space. To verify that there is space, select the MapLane that goes past the MapParkingSpace and click Display Lane . This will show the width of the lane facilitate adjustment of the lane to have a gap between the lane and parking space.","title":"Create Parking Space"},{"location":"simulation-content/map-annotation/#create-speed-bump","text":"Select the SpeedBump option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one end of the speed bump Click Waypoint to create a TEMP_WAYPOINT Create 2 TEMP_WAYPOINT in the order shown below The TEMP_WAYPOINT should be wide enough to be outside the lane Click Connect to create the MapSpeedBump To verify that the MapSpeedBump is wide enough, select the MapLane that the MapSpeedBump crosses. Enable Display Lane to visualize the width of the lane. If necesary, select the MapSpeedBump and enable Display Handles to adjust the positions of the individual waypoints.","title":"Create Speed Bump"},{"location":"simulation-content/map-annotation/#export-map-annotations","text":"HD Map Annotations may be exported in a variety of formats. Current supported formats are: Apollo 5.0 HD Map Autoware Vector Map Lanelet2 Map OpenDrive Map To export a map: Open the HD Map Export tool in Unity : Simulator -> Export HD Map Select the desired format from the dropdown Export Format Enter the desired save location of the exported map Click Export to create the exported map Note the exporter will fail if any lane has no boundary lines annotated - you need to either manually annotate them or use the Create lines button in HD Map Annotation window to create fake boundary lines.","title":"Export Map Annotations"},{"location":"simulation-content/map-annotation/#import-map-annotations","text":"The simulator can import a variety of formats of annotated map. NOTE After importing an HD map, you will be required to check each annotation to make sure import is correct. Simulator will provide logs if there are issues that need resolved. If the importer cannot parse a HD map, it will make holder objects that will need manually edited. Note you must delete any MapHolder existing in a scene before the importer tool will work. Current supported formats are: Lanelet2 Map Apollo 5.0 HD Map OpenDRIVE Map To import a map: Open the HD Map Export tool in Unity : Simulator -> Import HD Map Select the format of the input map from the dropdown Import Format Select the file or folder that will be imported Click Import to import the map annotations into the simulator Distance Threshold is used to down-sample straight lines/lanes and Delta Threshold is used to down-sample curved lines/lanes. Changing both of them to 0 will keep all points, but for large maps displaying all annotations may lead to Unity crash. Lanelet2 map importer Notes: Lanes will be automatically imported and grouped as MapLaneSection if possible Intersections with four-way traffic lights / stop signs or two-way stop signs can be imported and grouped under MapIntersection Left-turn lanes are automatically found and their corresponding lanes to yield are also obtained automatically to get NPCs working correctly For each MapIntersection , you need to manually adjust X and Z for the Trigger Bounds as explained in annotate intersection part . Remember to check objects under Intersections are grouped correctly.","title":"Import Map Annotations"},{"location":"simulation-content/map-annotation/#map-formats","text":"For more information on the map formats, please see the links below: Apollo HD Autoware Vector Lanelet2 OpenDRIVE","title":"Map Formats"},{"location":"simulation-content/pointcloud-export/","text":"Point Cloud Export # Point cloud exporter tool can be used to generate point clouds from the environments and save them in binary PCD format. Exporter will only work on properly prepared environments - you can either use one of the existing environment assets or create a new environment . Exported data contains position (4 bytes per axis per point, float type) and intensity (1 byte per point, uint8 type) for each point. Points are generated precisely on meshes, with no noise. For more information about point distribution and density, see generation process section. To use the exporter, you need to have at least one point cloud generating sensor in your project. You can use lidar sensor for this - simply make sure that the sensor repository is cloned into Assets/External/Sensors in the Simulator project. Accessing the exporter top # To access point cloud exporter window, open Simulator project in Unity editor, then navigate into Simulator/Export Point Cloud on the menu bar. Point cloud exporter window will be opened. Exporter settings top # Parameter Name Description Generator Class Class that will be used for generating points. See generator class section for details. Template Settings template for the generator class. This assumes that lidar is used, see notes below. Height Height above the ground at which generator will be placed. See generation process section for details. Distance Step distance between subsequent generations. See generation process section for details. Ratio Fraction of randomly selected points that will be kept after each generation. Output path Path to the PCD file that will be created. Note: Template assumes that lidar sensor will be used as the generator class . If you use another generator class or create your own, some of the values in template may not be applicable. Templates with higher numbers generate more points per scan. Generation process top # Generation process follows a simple loop: generator is moved along all the roads in the environment with movement step defined by Distance . For each position it's placed at a defined Height , then a scan happens. Scan generates a number of points. Fraction of them (defined by Ratio ) is randomly picked and appended to the point cloud, the rest is discarded. Assuming that the lidar sensor is used as generator, this approach has a few implications: - For each scan, points are more dense closer to the sensor position - In the final point cloud, points are more dense near the roads - Points are not uniformly distributed - Elements invisible from the road are not included in the point cloud - Circular patterns can be visible if Distance is too high While this approach creates data sets similar to real world scenario of scanning environment through sensors attached to a vehicle, it might not be suitable for some use cases that require uniform distribution or inclusion of occluded areas. Custom generator class can address these issues. Generator class top # Generator class is responsible for providing a set of points for each scan position during generation process. Its behavior is not defined beyond that, so if you have a particular case that is not covered by default implementation (e.g. you need uniform sampling or no occlusion), you can create a custom generator class. Default implementation of the point cloud generator can be found in the lidar sensor repository. If you want to use any other class for generating point cloud, it has to be a valid sensor plugin that implements IPointCloudGenerator interface. All sensor plugins implementing this interface will be listed under the Generator Class section in settings . IPointCloudGenerator defines 3 methods that have to be implemented: - void ApplySettings(LidarTemplate template) - called once before first scan. Use this for initialization. Settings for selected lidar template are available here. - Vector4[] GeneratePoints(Vector3 position) - called for each scan. This implementation should return an array of points ( x , y , z , intensity ) in world space generated for given position . - void Cleanup() - called once after last scan. Use this to clean up any used resources.","title":"Point cloud export"},{"location":"simulation-content/pointcloud-export/#accessing-the-exporter","text":"To access point cloud exporter window, open Simulator project in Unity editor, then navigate into Simulator/Export Point Cloud on the menu bar. Point cloud exporter window will be opened.","title":"Accessing the exporter"},{"location":"simulation-content/pointcloud-export/#exporter-settings","text":"Parameter Name Description Generator Class Class that will be used for generating points. See generator class section for details. Template Settings template for the generator class. This assumes that lidar is used, see notes below. Height Height above the ground at which generator will be placed. See generation process section for details. Distance Step distance between subsequent generations. See generation process section for details. Ratio Fraction of randomly selected points that will be kept after each generation. Output path Path to the PCD file that will be created. Note: Template assumes that lidar sensor will be used as the generator class . If you use another generator class or create your own, some of the values in template may not be applicable. Templates with higher numbers generate more points per scan.","title":"Exporter settings"},{"location":"simulation-content/pointcloud-export/#generation-process","text":"Generation process follows a simple loop: generator is moved along all the roads in the environment with movement step defined by Distance . For each position it's placed at a defined Height , then a scan happens. Scan generates a number of points. Fraction of them (defined by Ratio ) is randomly picked and appended to the point cloud, the rest is discarded. Assuming that the lidar sensor is used as generator, this approach has a few implications: - For each scan, points are more dense closer to the sensor position - In the final point cloud, points are more dense near the roads - Points are not uniformly distributed - Elements invisible from the road are not included in the point cloud - Circular patterns can be visible if Distance is too high While this approach creates data sets similar to real world scenario of scanning environment through sensors attached to a vehicle, it might not be suitable for some use cases that require uniform distribution or inclusion of occluded areas. Custom generator class can address these issues.","title":"Generation process"},{"location":"simulation-content/pointcloud-export/#generator-class","text":"Generator class is responsible for providing a set of points for each scan position during generation process. Its behavior is not defined beyond that, so if you have a particular case that is not covered by default implementation (e.g. you need uniform sampling or no occlusion), you can create a custom generator class. Default implementation of the point cloud generator can be found in the lidar sensor repository. If you want to use any other class for generating point cloud, it has to be a valid sensor plugin that implements IPointCloudGenerator interface. All sensor plugins implementing this interface will be listed under the Generator Class section in settings . IPointCloudGenerator defines 3 methods that have to be implemented: - void ApplySettings(LidarTemplate template) - called once before first scan. Use this for initialization. Settings for selected lidar template are available here. - Vector4[] GeneratePoints(Vector3 position) - called for each scan. This implementation should return an array of points ( x , y , z , intensity ) in world space generated for given position . - void Cleanup() - called once after last scan. Use this to clean up any used resources.","title":"Generator class"},{"location":"simulation-content/pointcloud-import/","text":"Point Cloud Import # SVL Simulator uses its own format to store all of the point cloud data for rendering. To make any point cloud usable within the simulator, it has to be processed first. Built-in point cloud importer tool provides all the functionality required to convert most popular point cloud file formats (PCD, PLY, LAS, LAZ) into data usable for simulation. It is also able to create approximated mesh collider based on imported data. If you're using pre-built point cloud environments, you don't have to use this tool. Its main purpose is to create new environments compatible with Simulator from raw point cloud data. Accessing the importer top # To access point cloud importer window, open Simulator project in Unity editor, then navigate into Simulator/Import Point Cloud on the menu bar. Point cloud importer window will be opened. Import process overview top # Importer window requires you to at least specity input and output files, but also gives you access to additional settings. All of these are described in more detail in their respective sections. To start the import process, just click \"Import\" button at the bottom of the window. Depending on the point cloud size and chosen settings, this process might take a while. You'll see multiple progress bars during this time for preprocessing steps and the main import process itself. If you cancel the import process, partially processed data is not usable. Processing points can be performed on multiple threads, but first pass is always single-threaded. Progress bar reporting that only one thread is used during this time is normal. After the first pass, import process speeds up severely depending on amount of threads available. Importer will validate some of your import settings before the import process. There is a chance that \"Import\" button will be disabled. Error or warning message is always displayed in this case, with an information about the reason. Most convenient way of using the importer is with target environment scene open in the background - in this case the tool will offer to setup open scene with all the components required for rendering after the import process succeeds. You can also decide to skip this step and set the components yourself. Details about this are available on the point cloud rendering page. Importer settings top # Importer window is split into multiple sections, each with settings related to different subject. Input Files top # This tab is used to select point cloud files that should be converted into combatible format. Multiple files to import can be added at once. All of the files listed in this tab will be merged into single point cloud after importing. If you prefer to keep them separated, import each one of them individually (rendering multiple point clouds is supported, but decreases performance). All of the input files must be of one of following formats: PCD , PLY , LAS , LAZ . Mixing multiple formats is supported. Add Folder button will add all of the compatible files from selected folder into the list. This does not include subdirectories. Output Files top # This tab expects you to specify a directory that will be used to store all of the generated files. This directory does not have to be inside of the project's folder and can be located in any accessible place on the drive. If you decide to use directory inside the project, path relative to Assets folder fill be used instead of absolute path to help with cross-machine migration. Using directory with processed point cloud data already present will overwrite all of previous data. Warning about this is displayed in such case. Please note that output directory will contain a lot of files after import process. If this directory is located under Assets folder, Unity will generate metadata for each of the files. This process takes a long time and the metadata is completely unused, so it's recommended to skip meta file generation altogether. This can be done by using Unity's special folder names - just make sure that folder name ends with ~ . Tree Settings top # This tab describes data structure and layout of the imported point cloud. Default settings are working well with most of the standard data sets, so changing them is usually not required. Parameter Name Description Tree Type Tree data structure used to subdivide point cloud. Octree and Quadtree variants are available. Sampling Sampling used on each level of the tree structure, determining distribution of points in the final cloud. Cell Center uses voxel-based, uniform sampling. Poisson Disk offers more natural-looking distribution, but is slower and does not support mesh generation. Root Node Subdivision Determines amount of per-axis samples for each node. Higher values will create larger nodes, while lower values will create deeper tree structures. Node Branch Threshold Minimum amount of points in the node required for splitting it further. Prevents creating mostly empty nodes and overly deep tree structure. Max Tree Depth Maximum amount of levels in the tree structure. Levels deeper than the specified value will be discarded. Min Point Distance Minimum distance between points in a node. Points below this threshold will be discarded. Mesh Settings top # This tab contains options related to collider mesh generation. Note that mesh generation will only work for Cell Center sampling. Parameter Name Description Generate Mesh If this option is enabled, mesh collider will be generated and additional settings will be available. Road Only Mesh If this option is enabled, mesh generation will attempt to only create mesh for the road, ignoring other structures. This option is not suitable for all data sets - if you experience problems, try again with this feature disabled. Mesh Detail Level Level of the tree that should be used for mesh generation. This should corelate with represented area size - for single intersection level 2 is usually enough, while larger areas with multiple roads might require level 5 or higher. Erosion Passes Number of erosion passes that will be performed after mesh generation. Erosion will remove and flatten steep surfaces. See erosion for details. Erosion Angle Threshold Surfaces angled steeper than this threshold will be processed during erosion passes. Remove Small Surfaces If this option is enabled, small, secluded surfaces that are not attached to larger clusters will be removed after mesh generation. Erosion top # Depending on the mesh detail and point cloud quality, you might get some unwanted obstacles included in the mesh generation process. If you prefer a flat surface that's easier to drive on, you can include one or more erosion passes during import. This will remove steep sections of the mesh and flatten the underlying geometry. You can see how the erosion process affects the final mesh in the image below. Build Settings top # This tab contains options related to import process and initial point cloud transformations. Some options under this tab will affect amount of system memory required during the import process. You will see the warning if you're exceeding your system's limits - lower one of related settings values in this case. Parameter Name Description Thread Count Amount of threads that the import process should be split across. It's not guaranteed that all of the threads will be utilized fully, but higher thread count reduces import time. Affects memory requirements. Chunk Size Maximum amount of points loaded into memory at once, per thread. This only affects import process and has no effect on output data. Affects memory requirements. Center If this option is enabled, local space origin of the output point cloud will be moved to its bounds center. Normalize If this option is enabled, all points positions will be rescaled to fit normalized coordinates. LAS RGB 8bit Workaround If this option is enabled, additional 8 bit offset is used when reading color and intensity from LAS files. Use this if your data imported from LAS files seems to have corrupted color and/or intensity values. Axes Coordinate system axes convention used in original files. Setting this to correct value ensures that output point cloud will be compatible with Unity's coordinate system.","title":"Point cloud import"},{"location":"simulation-content/pointcloud-import/#accessing-the-importer","text":"To access point cloud importer window, open Simulator project in Unity editor, then navigate into Simulator/Import Point Cloud on the menu bar. Point cloud importer window will be opened.","title":"Accessing the importer"},{"location":"simulation-content/pointcloud-import/#import-process-overview","text":"Importer window requires you to at least specity input and output files, but also gives you access to additional settings. All of these are described in more detail in their respective sections. To start the import process, just click \"Import\" button at the bottom of the window. Depending on the point cloud size and chosen settings, this process might take a while. You'll see multiple progress bars during this time for preprocessing steps and the main import process itself. If you cancel the import process, partially processed data is not usable. Processing points can be performed on multiple threads, but first pass is always single-threaded. Progress bar reporting that only one thread is used during this time is normal. After the first pass, import process speeds up severely depending on amount of threads available. Importer will validate some of your import settings before the import process. There is a chance that \"Import\" button will be disabled. Error or warning message is always displayed in this case, with an information about the reason. Most convenient way of using the importer is with target environment scene open in the background - in this case the tool will offer to setup open scene with all the components required for rendering after the import process succeeds. You can also decide to skip this step and set the components yourself. Details about this are available on the point cloud rendering page.","title":"Import process overview"},{"location":"simulation-content/pointcloud-import/#importer-settings","text":"Importer window is split into multiple sections, each with settings related to different subject.","title":"Importer settings"},{"location":"simulation-content/pointcloud-import/#input-files","text":"This tab is used to select point cloud files that should be converted into combatible format. Multiple files to import can be added at once. All of the files listed in this tab will be merged into single point cloud after importing. If you prefer to keep them separated, import each one of them individually (rendering multiple point clouds is supported, but decreases performance). All of the input files must be of one of following formats: PCD , PLY , LAS , LAZ . Mixing multiple formats is supported. Add Folder button will add all of the compatible files from selected folder into the list. This does not include subdirectories.","title":"Input Files"},{"location":"simulation-content/pointcloud-import/#output-files","text":"This tab expects you to specify a directory that will be used to store all of the generated files. This directory does not have to be inside of the project's folder and can be located in any accessible place on the drive. If you decide to use directory inside the project, path relative to Assets folder fill be used instead of absolute path to help with cross-machine migration. Using directory with processed point cloud data already present will overwrite all of previous data. Warning about this is displayed in such case. Please note that output directory will contain a lot of files after import process. If this directory is located under Assets folder, Unity will generate metadata for each of the files. This process takes a long time and the metadata is completely unused, so it's recommended to skip meta file generation altogether. This can be done by using Unity's special folder names - just make sure that folder name ends with ~ .","title":"Output Files"},{"location":"simulation-content/pointcloud-import/#tree-settings","text":"This tab describes data structure and layout of the imported point cloud. Default settings are working well with most of the standard data sets, so changing them is usually not required. Parameter Name Description Tree Type Tree data structure used to subdivide point cloud. Octree and Quadtree variants are available. Sampling Sampling used on each level of the tree structure, determining distribution of points in the final cloud. Cell Center uses voxel-based, uniform sampling. Poisson Disk offers more natural-looking distribution, but is slower and does not support mesh generation. Root Node Subdivision Determines amount of per-axis samples for each node. Higher values will create larger nodes, while lower values will create deeper tree structures. Node Branch Threshold Minimum amount of points in the node required for splitting it further. Prevents creating mostly empty nodes and overly deep tree structure. Max Tree Depth Maximum amount of levels in the tree structure. Levels deeper than the specified value will be discarded. Min Point Distance Minimum distance between points in a node. Points below this threshold will be discarded.","title":"Tree Settings"},{"location":"simulation-content/pointcloud-import/#mesh-settings","text":"This tab contains options related to collider mesh generation. Note that mesh generation will only work for Cell Center sampling. Parameter Name Description Generate Mesh If this option is enabled, mesh collider will be generated and additional settings will be available. Road Only Mesh If this option is enabled, mesh generation will attempt to only create mesh for the road, ignoring other structures. This option is not suitable for all data sets - if you experience problems, try again with this feature disabled. Mesh Detail Level Level of the tree that should be used for mesh generation. This should corelate with represented area size - for single intersection level 2 is usually enough, while larger areas with multiple roads might require level 5 or higher. Erosion Passes Number of erosion passes that will be performed after mesh generation. Erosion will remove and flatten steep surfaces. See erosion for details. Erosion Angle Threshold Surfaces angled steeper than this threshold will be processed during erosion passes. Remove Small Surfaces If this option is enabled, small, secluded surfaces that are not attached to larger clusters will be removed after mesh generation.","title":"Mesh Settings"},{"location":"simulation-content/pointcloud-import/#erosion","text":"Depending on the mesh detail and point cloud quality, you might get some unwanted obstacles included in the mesh generation process. If you prefer a flat surface that's easier to drive on, you can include one or more erosion passes during import. This will remove steep sections of the mesh and flatten the underlying geometry. You can see how the erosion process affects the final mesh in the image below.","title":"Erosion"},{"location":"simulation-content/pointcloud-import/#build-settings","text":"This tab contains options related to import process and initial point cloud transformations. Some options under this tab will affect amount of system memory required during the import process. You will see the warning if you're exceeding your system's limits - lower one of related settings values in this case. Parameter Name Description Thread Count Amount of threads that the import process should be split across. It's not guaranteed that all of the threads will be utilized fully, but higher thread count reduces import time. Affects memory requirements. Chunk Size Maximum amount of points loaded into memory at once, per thread. This only affects import process and has no effect on output data. Affects memory requirements. Center If this option is enabled, local space origin of the output point cloud will be moved to its bounds center. Normalize If this option is enabled, all points positions will be rescaled to fit normalized coordinates. LAS RGB 8bit Workaround If this option is enabled, additional 8 bit offset is used when reading color and intensity from LAS files. Use this if your data imported from LAS files seems to have corrupted color and/or intensity values. Axes Coordinate system axes convention used in original files. Setting this to correct value ensures that output point cloud will be compatible with Unity's coordinate system.","title":"Build Settings"},{"location":"simulation-content/pointcloud-rendering/","text":"Point Cloud Rendering # SVL Simulator supports scenes containing point cloud data. Point clouds can be rendered alongside other geometry like meshes or particles, are combatible with most of the features available in high definition render pipeline and can be detected by multiple sensors, including LiDAR. Point cloud rendering in Simulator can handle large point clouds (hundreds of millions of points) thanks to octree-based structure and selective, frustum-based culling. If you want to create your own point cloud based environment, you should start with importing your point cloud data into simulator. This step is not necessary for pre-built point cloud environments - they do not require any additional setup. Rendering Features top # Most of the features available for point cloud renderers are optional and can be toggled based on individual use cases. Lighting, for example, can be turned off if point cloud has light data already baked in, or calculated fully based on scene settings like time of day or weather. Most notable features include: Full deferred lighting compatible with HDRP's lighting system Filling holes between points Rendering multiple point clouds at once Normals estimation Shadow casting/receiving Point streaming Point budget (for memory and render buffer) Camera frustum or distance-based culling Mesh collider generation Support for color/depth camera sensors and LiDAR Setup top # The easiest way to set up point cloud rendering is to perform an import with the target scene open in the background. When importing is done, you will be prompted with a choice to automatically add all of the required components to the open scene. If accepted, point cloud with default rendering settings will immediately show up in scene view. Alternatively, a prefab located in Assets/Prefabs/PointCloudRenderer.prefab can be added to any scene for a quick setup. This requires to specify point cloud data path in NodeTreeLoader component. More complex setups with multiple point clouds or masked rendering require setting up components manually. Details can be found in Components Overview section. Components Overview top # In the most common scenario with a single point cloud, one of each of the components described below will be present on a scene. For more complex cases (e.g. multiple point clouds or separate color and intensity data) it's possible to add multiple instances of NodeTreeLoader and/or NodeTreeRenderer to one scene. Only one instance of PointCloudManager should ever be present. PointCloudManager # This component is responsible for keeping track of shared point cloud resources and orchestrating rendering for one or more point cloud renderers. It will also automatically set up HDRP passes and enable or disable them depending on all renderers' settings. There are no settings available for this component. Only one instance should be active at a time. Disabling this component will disable rendering of all point cloud data. NodeTreeLoader # This component is responsible for managing single point cloud's data in memory. It will load chunks of point cloud data from mass storage based on each renderer's requests (performed on a separate thread) and will keep track of total amount of points loaded into memory. When defined point budget is exceeded, older points will be unloaded. Single instance of this component can only operate on one point cloud, but multiple renderers can use data stored in a single NodeTreeLoader . This component can also optionally load mesh colliders for a point cloud if they were generated during import process. NodeTreeRenderer # This component represents a single point cloud to render and stores all related settings. Transform to which this component is tied affects translation, rotation and scale of the point cloud. By default, point cloud rendered by a single renderer will be visible in all cameras and compatible sensors, but not in shadow caster pass. This can be controlled on a per-renderer basis. Note that this component will not actively trigger any rendering. It's used mostly as a container for data that will be used by PointCloudManager . NodeTreeRenderer is default implementation that includes camera-based culling, which will make load requests to NodeTreeLoader . Adding other culling methods can be done by extending the base class, PointCloudRenderer . Rendering Modes Overview top # Depending on the requirements, there are multiple combinations of rendering modes available. Sensors supported in point cloud rendering are compatible with all of them. The most simple one ( Points ) will render points with size dependent on distance from the camera. This image is stable, but does not support lighting and will show holes if point cloud is not dense enough. Improved version of this mode ( Cones ) will use 3D cones instead of 2D circles, which creates better transitions between very close points, but has a slight performance cost. More advanced rendering mode, Solid , will remove points that should be invisible from observer's position and interpolate color and depth data between leftover points. It can also reconstruct normals data from depth, and subsequently calculate lighting for point cloud. Filters used here are executed in screen space, which means that camera position and rotation affect rendering output. This might introduce some artifacts around edges, especially around large depth differences. It's suggested to test available modes and choose the one best suited for given point cloud and use case. If lighting is required, solid rendering mode is the only one currently supporting it. Settings top # Loader Settings top # Parameter Name Type Description Data Path string Directory in which imported point cloud data is stored. Paths starting with .../ will be treated as relative to Assets folder in the project, others are treated as absolute. Point Limit int Maximum amount of points that can be loaded into memory at once. These points are treated as cache between render buffer and hard drive. When point limit is exceeded, older points will be unloaded. Load Meshes bool If this option is enabled, collider meshes will loaded alongside the point cloud data. This requires Generate Mesh option in point cloud import settings to be enabled. Renderer Settings top # Settings visible for renderer depend on currently selected rendering mode. Irrelevant settings will be hidden in inspector. Enabling additional features will sometimes show new settings related to them. Shared Rendering Settings top # Parameter Name Type Description Colorize enum Colorization mode to use for point cloud. If selected data is not present in source point cloud, result will be black. Available modes: Colors , Intensity , Rainbow Intensity and Rainbow Height . Render Mode enum Describes how points should be represented. Available modes: Points (most simple one), Cones (similar to points, with interpolation) and Solid (fills holes, required for lighting). Mask enum Enum flags used to enable or disable renderer's visibility. Point cloud will be visible only in selected passes. Available options: Camera (all color and depth cameras), Shadowcaster (enables shadow casting by point cloud), LiDAR (makes point cloud detectable by LiDAR sensor). Shadow Point Size float Only visible if Shadowcaster is enabled under Mask . Determines size of a point for shadow casting purposes. Shadow Bias float Only visible if Shadowcaster is enabled under Mask . Determines shadow bias for each point. Value is relative to Shadow Point Size . Point/Cones Rendering Settings top # Parameter Name Type Description Constant Size bool If this option is enabled, point size will not depend on distance from camera. Pixel Size float Only visible with Constant Size enabled. Determines size of a point. Absolute Size float Only visible with Constant Size disabled. Determines size of a point at a near plane. Points further away will be smaller. Min Pixel Size float Only visible with Constant Size disabled. Determines minimum size of a point, irrelevant from distance. Solid Rendering Mode top # Cascades options for Remove Hidden and Smooth Normals stages affect filter window size for both these effects and its scaling with distance for camera. They can be visualized by toggling Show Preview option on. Cascade options for Remove Hidden stage will only be visible if Hidden Point Removal option is set to Screen Space . For Depth Prepass , they will be replaced with point rendering settings . Parameter Name Type Description Lighting Mode enum Determines how light will affect the point cloud. Available modes: Unlit (no lighting, no shadows), Shadow Receiver (no lighting, receives shadows), Full Deferred (receives lighting and shadows) Hidden Point Removal enum Method used for removing obscured points. Screen Space is the default, faster option. Depth Prepass reduces flickering on edges, but requires more precise setup and can have impact on performance. Calculate Normals bool If enabled, normals approximation stage will be performed and world-space normals will be available. Required for lighting. Smooth Normals bool If enabled, additional normals smoothing stage will be performed to reduce noise. Results will replace normals data from Calculate Normals option. FoV Reprojection bool If enabled, wider image of point cloud will be rendered internally and it will be reprojected to desired camera FOV. This option reduces artifacts near the edges of the image. Does not affect final FOV. Reprojection Ratio float Multiplier determining ratio between FOV used internally for FoV Reprojection option and original FOV of the camera. Solid rendering also has some debug options under separate tab. These are mostly directly affecting separate rendering stages, but some of them can me modified to affect final image. Parameter Name Type Description Linear depth bool If enabled, linear depth will be used throughout all compute shaders instead of depth buffer values. Force fill bool If enabled, holes below horizon line will be filled regardless of their size. Can create blurred artifacts for large holes, but removes all holes in road for low density point clouds. Fill threshold float Sets height threshold under which Force fill option will be used if it's enabled. Blend sky bool If enabled, point cloud color and intensity will be blended to sky color instead of black. Removes black color around edges, but requires additional sky pre-render for lit mode. Culling Settings top # Parameter Name Type Description Node Tree Loader NodeTreeLoader Reference to loader that will be queried for point cache updates. Cull Camera Camera Reference to camera used for culling. If this field is left empty, main simulator camera will be used after simulation is started. Cull Mode enum Determines how points are evaluated for visibility. Available modes: Camera Frustum (both view frustum and distance from camera are considered), Distance (only distance from camera is considered - recommended if LiDAR is used). Point Limit int Number of points that should be pushed to render buffer. This value determines maximum amount of points that are visible at once and directly affects buffer size. Min Projection float Minimum screen projection size along single axis (in pixels) that octree node has to occupy to be traversed further. Nodes below this threshold will not be visible. Rebuild Steps int Number of frames to spend for render buffer rebuild. Previous fully completed buffer will be used until new one is ready. Values higher than 1 reduce per-frame cost of buffer rebuild, but can introduce popping.","title":"Point cloud rendering"},{"location":"simulation-content/pointcloud-rendering/#rendering-features","text":"Most of the features available for point cloud renderers are optional and can be toggled based on individual use cases. Lighting, for example, can be turned off if point cloud has light data already baked in, or calculated fully based on scene settings like time of day or weather. Most notable features include: Full deferred lighting compatible with HDRP's lighting system Filling holes between points Rendering multiple point clouds at once Normals estimation Shadow casting/receiving Point streaming Point budget (for memory and render buffer) Camera frustum or distance-based culling Mesh collider generation Support for color/depth camera sensors and LiDAR","title":"Rendering Features"},{"location":"simulation-content/pointcloud-rendering/#setup","text":"The easiest way to set up point cloud rendering is to perform an import with the target scene open in the background. When importing is done, you will be prompted with a choice to automatically add all of the required components to the open scene. If accepted, point cloud with default rendering settings will immediately show up in scene view. Alternatively, a prefab located in Assets/Prefabs/PointCloudRenderer.prefab can be added to any scene for a quick setup. This requires to specify point cloud data path in NodeTreeLoader component. More complex setups with multiple point clouds or masked rendering require setting up components manually. Details can be found in Components Overview section.","title":"Setup"},{"location":"simulation-content/pointcloud-rendering/#components-overview","text":"In the most common scenario with a single point cloud, one of each of the components described below will be present on a scene. For more complex cases (e.g. multiple point clouds or separate color and intensity data) it's possible to add multiple instances of NodeTreeLoader and/or NodeTreeRenderer to one scene. Only one instance of PointCloudManager should ever be present.","title":"Components Overview"},{"location":"simulation-content/pointcloud-rendering/#pointcloudmanager","text":"This component is responsible for keeping track of shared point cloud resources and orchestrating rendering for one or more point cloud renderers. It will also automatically set up HDRP passes and enable or disable them depending on all renderers' settings. There are no settings available for this component. Only one instance should be active at a time. Disabling this component will disable rendering of all point cloud data.","title":"PointCloudManager"},{"location":"simulation-content/pointcloud-rendering/#nodetreeloader","text":"This component is responsible for managing single point cloud's data in memory. It will load chunks of point cloud data from mass storage based on each renderer's requests (performed on a separate thread) and will keep track of total amount of points loaded into memory. When defined point budget is exceeded, older points will be unloaded. Single instance of this component can only operate on one point cloud, but multiple renderers can use data stored in a single NodeTreeLoader . This component can also optionally load mesh colliders for a point cloud if they were generated during import process.","title":"NodeTreeLoader"},{"location":"simulation-content/pointcloud-rendering/#nodetreerenderer","text":"This component represents a single point cloud to render and stores all related settings. Transform to which this component is tied affects translation, rotation and scale of the point cloud. By default, point cloud rendered by a single renderer will be visible in all cameras and compatible sensors, but not in shadow caster pass. This can be controlled on a per-renderer basis. Note that this component will not actively trigger any rendering. It's used mostly as a container for data that will be used by PointCloudManager . NodeTreeRenderer is default implementation that includes camera-based culling, which will make load requests to NodeTreeLoader . Adding other culling methods can be done by extending the base class, PointCloudRenderer .","title":"NodeTreeRenderer"},{"location":"simulation-content/pointcloud-rendering/#rendering-modes-overview","text":"Depending on the requirements, there are multiple combinations of rendering modes available. Sensors supported in point cloud rendering are compatible with all of them. The most simple one ( Points ) will render points with size dependent on distance from the camera. This image is stable, but does not support lighting and will show holes if point cloud is not dense enough. Improved version of this mode ( Cones ) will use 3D cones instead of 2D circles, which creates better transitions between very close points, but has a slight performance cost. More advanced rendering mode, Solid , will remove points that should be invisible from observer's position and interpolate color and depth data between leftover points. It can also reconstruct normals data from depth, and subsequently calculate lighting for point cloud. Filters used here are executed in screen space, which means that camera position and rotation affect rendering output. This might introduce some artifacts around edges, especially around large depth differences. It's suggested to test available modes and choose the one best suited for given point cloud and use case. If lighting is required, solid rendering mode is the only one currently supporting it.","title":"Rendering Modes Overview"},{"location":"simulation-content/pointcloud-rendering/#settings","text":"","title":"Settings"},{"location":"simulation-content/pointcloud-rendering/#loader-settings","text":"Parameter Name Type Description Data Path string Directory in which imported point cloud data is stored. Paths starting with .../ will be treated as relative to Assets folder in the project, others are treated as absolute. Point Limit int Maximum amount of points that can be loaded into memory at once. These points are treated as cache between render buffer and hard drive. When point limit is exceeded, older points will be unloaded. Load Meshes bool If this option is enabled, collider meshes will loaded alongside the point cloud data. This requires Generate Mesh option in point cloud import settings to be enabled.","title":"Loader Settings"},{"location":"simulation-content/pointcloud-rendering/#renderer-settings","text":"Settings visible for renderer depend on currently selected rendering mode. Irrelevant settings will be hidden in inspector. Enabling additional features will sometimes show new settings related to them.","title":"Renderer Settings"},{"location":"simulation-content/pointcloud-rendering/#shared-rendering-settings","text":"Parameter Name Type Description Colorize enum Colorization mode to use for point cloud. If selected data is not present in source point cloud, result will be black. Available modes: Colors , Intensity , Rainbow Intensity and Rainbow Height . Render Mode enum Describes how points should be represented. Available modes: Points (most simple one), Cones (similar to points, with interpolation) and Solid (fills holes, required for lighting). Mask enum Enum flags used to enable or disable renderer's visibility. Point cloud will be visible only in selected passes. Available options: Camera (all color and depth cameras), Shadowcaster (enables shadow casting by point cloud), LiDAR (makes point cloud detectable by LiDAR sensor). Shadow Point Size float Only visible if Shadowcaster is enabled under Mask . Determines size of a point for shadow casting purposes. Shadow Bias float Only visible if Shadowcaster is enabled under Mask . Determines shadow bias for each point. Value is relative to Shadow Point Size .","title":"Shared Rendering Settings"},{"location":"simulation-content/pointcloud-rendering/#point/cones-rendering-settings","text":"Parameter Name Type Description Constant Size bool If this option is enabled, point size will not depend on distance from camera. Pixel Size float Only visible with Constant Size enabled. Determines size of a point. Absolute Size float Only visible with Constant Size disabled. Determines size of a point at a near plane. Points further away will be smaller. Min Pixel Size float Only visible with Constant Size disabled. Determines minimum size of a point, irrelevant from distance.","title":"Point/Cones Rendering Settings"},{"location":"simulation-content/pointcloud-rendering/#solid-rendering-mode","text":"Cascades options for Remove Hidden and Smooth Normals stages affect filter window size for both these effects and its scaling with distance for camera. They can be visualized by toggling Show Preview option on. Cascade options for Remove Hidden stage will only be visible if Hidden Point Removal option is set to Screen Space . For Depth Prepass , they will be replaced with point rendering settings . Parameter Name Type Description Lighting Mode enum Determines how light will affect the point cloud. Available modes: Unlit (no lighting, no shadows), Shadow Receiver (no lighting, receives shadows), Full Deferred (receives lighting and shadows) Hidden Point Removal enum Method used for removing obscured points. Screen Space is the default, faster option. Depth Prepass reduces flickering on edges, but requires more precise setup and can have impact on performance. Calculate Normals bool If enabled, normals approximation stage will be performed and world-space normals will be available. Required for lighting. Smooth Normals bool If enabled, additional normals smoothing stage will be performed to reduce noise. Results will replace normals data from Calculate Normals option. FoV Reprojection bool If enabled, wider image of point cloud will be rendered internally and it will be reprojected to desired camera FOV. This option reduces artifacts near the edges of the image. Does not affect final FOV. Reprojection Ratio float Multiplier determining ratio between FOV used internally for FoV Reprojection option and original FOV of the camera. Solid rendering also has some debug options under separate tab. These are mostly directly affecting separate rendering stages, but some of them can me modified to affect final image. Parameter Name Type Description Linear depth bool If enabled, linear depth will be used throughout all compute shaders instead of depth buffer values. Force fill bool If enabled, holes below horizon line will be filled regardless of their size. Can create blurred artifacts for large holes, but removes all holes in road for low density point clouds. Fill threshold float Sets height threshold under which Force fill option will be used if it's enabled. Blend sky bool If enabled, point cloud color and intensity will be blended to sky color instead of black. Removes black color around edges, but requires additional sky pre-render for lit mode.","title":"Solid Rendering Mode"},{"location":"simulation-content/pointcloud-rendering/#culling-settings","text":"Parameter Name Type Description Node Tree Loader NodeTreeLoader Reference to loader that will be queried for point cache updates. Cull Camera Camera Reference to camera used for culling. If this field is left empty, main simulator camera will be used after simulation is started. Cull Mode enum Determines how points are evaluated for visibility. Available modes: Camera Frustum (both view frustum and distance from camera are considered), Distance (only distance from camera is considered - recommended if LiDAR is used). Point Limit int Number of points that should be pushed to render buffer. This value determines maximum amount of points that are visible at once and directly affects buffer size. Min Projection float Minimum screen projection size along single axis (in pixels) that octree node has to occupy to be traversed further. Nodes below this threshold will not be visible. Rebuild Steps int Number of frames to spend for render buffer rebuild. Previous fully completed buffer will be used until new one is ready. Values higher than 1 reduce per-frame cost of buffer rebuild, but can introduce popping.","title":"Culling Settings"},{"location":"simulation-content/sdf-import/","text":"SDF Import # Since version 2021.3 Simulator supports importing of SDF world files (v1.6) into a Unity Editor scene. From this import you can export AssetBundles for Maps and Vehicles to upload on web user interface. Limitations # SDF support excludes certain attributes that can't easily be implemented within Unity, such as changing the physics engine, certain physics properties that are not exposed by Unity's PhysicMaterial (e.g. max_contacts, mu2, fdir1, slip1, slip2...) or rendering material properties that do not apply to HDRP (e.g. ambient term, specular color). SDF import only generates ArticulationBody physics components and SDF links that share joints are placed in a parent-child relationship to enable the joint even if the original links were siblings before. As such, circular joint setups are not supported. MeshColliders are convex unless the link is static. Plugins are only supported as stubs where Simulator will try to find corresponding c# classes to implement the plugin at runtime . Preparation for importing a SDF world file # Place the sdf file and associated assets under Simulator/Assets/External/Maps/YourMapName . Return to Unity and wait for the import of textures and meshes to finish. Open the SDF import panel by clicking Simulator > Import SDF in the Unity Editor menu. Your world file(s) should now be listed in the drop down selection box. The import script tries a couple of default locations to figure out where sdf model files and their media are stored in relation to the world file. If they fail to be found, you can set it manually using the SDF model include path field. Note that media files are expected to live under the Simulator/Assets/External/Maps directory. Select your world file and click Import World to start the import process. The import script will now walk the SDF xml tree and generate a new scene corresponding to the world definition. Vehicle export # The import process will show unique models that are not static as candidates for vehicle export. To generate a vehicle prefab, select the candidate from the drop down, enter a meaningful name and description and press create prefab . The new prefab will be created under Simulator/Assets/External/Vehicles/YourVehicleName Add Vehicle Controller and Agent Controller Components to the root GameObject by selecting them from the dropdowns and configure them using the Inspector view. You can develop custom agent controllers and dynamics implementations by inheriting from IAgentController and IVehicleDynamics and placing the scripts under Simulator/Assets/External/Vehicles/YourVehicleName . Saving Map scenes # Before you want to save your scene you will want to remove the models that represent your agent from the imported scene so that they can later be placed during simulation initialization. To do this, select the models that represent agents from the selection list under Convert models to spawn locations . These models will be replaced with empty SpawnInfo GameObjects which SpawnManager uses during Simulation startup to place Ego Agents into the scene. Building asset bundles # Finally, after you have exported agent prefabs and have saved the map without the agent models, you can generate AssetBundles from your assets as described in the building asset section . Plugins # To implement a sdf plugin, you should develop a Controllable that inherits from SDFParserBase and place the SDFPluginParser attribute on its class definition to denote the plugin name this parser implements. Example implementation: using System; using System.Xml.Linq; using System.Linq; using UnityEngine; using Simulator.Controllable; [SDFPluginParser(\"ElevatorSystem\")] public class SDFElevatorParser : SDFParserBase { public SDFElevatorParser() { } /* <!-- plugin parameters example --> <plugin name=\"ElevatorSystem\" filename=\"libElevatorSystem.so\"> <system_name>ElevatorSystem_00</system_name> <elevator prefix_name=\"Elevator_\" speed=\"2\"> <floor>floor_collision</floor> <doors speed=\"0.6\" closing_timer=\"10.0\"> <inside open_offset=\"0.567\"> <door name=\"left\">seocho_EV_door_L_link</door> <door name=\"right\">seocho_EV_door_R_link</door> </inside> <!-- ... --> </doors> </elevator> </plugin> */ public override GameObject Parse(XElement pluginElement, GameObject parentModel) { var elevatorSystem = parentModel.AddComponent<ElevatorSystem>(); elevatorSystem.elevatorSystemName = pluginElement.Element(\"system_name\").Value; var elevator = pluginElement.Element(\"elevator\"); var elevatorPrefixName = elevator.Attribute(\"prefix_name\").Value; var elevatorSpeed = ParseSingle(elevator.Attribute(\"speed\"), 2.0f); var elevatorFloor = elevator.Element(\"floor\").Value; var doors = elevator.Element(\"doors\"); var inside = doors.Element(\"inside\"); var elevatorInsideDoorNameLeft = inside.Elements(\"door\") .Where(d => d.Attribute(\"name\").Value == \"left\").First().Value; // ... // work with collected parameters to generate GameObjects // instantiate Custom Behaviours etc. // find gameobjects by name at runtime return elevatorSystem.gameObject; } Place your controllable under Simulator/Assets/External/Controllables/YourControllableName and follow the development instructions as laid out in the controllable plugins chapter .","title":"Importing SDF world files"},{"location":"simulation-content/sdf-import/#limitations","text":"SDF support excludes certain attributes that can't easily be implemented within Unity, such as changing the physics engine, certain physics properties that are not exposed by Unity's PhysicMaterial (e.g. max_contacts, mu2, fdir1, slip1, slip2...) or rendering material properties that do not apply to HDRP (e.g. ambient term, specular color). SDF import only generates ArticulationBody physics components and SDF links that share joints are placed in a parent-child relationship to enable the joint even if the original links were siblings before. As such, circular joint setups are not supported. MeshColliders are convex unless the link is static. Plugins are only supported as stubs where Simulator will try to find corresponding c# classes to implement the plugin at runtime .","title":"Limitations"},{"location":"simulation-content/sdf-import/#preparation-for-importing-a-sdf-world-file","text":"Place the sdf file and associated assets under Simulator/Assets/External/Maps/YourMapName . Return to Unity and wait for the import of textures and meshes to finish. Open the SDF import panel by clicking Simulator > Import SDF in the Unity Editor menu. Your world file(s) should now be listed in the drop down selection box. The import script tries a couple of default locations to figure out where sdf model files and their media are stored in relation to the world file. If they fail to be found, you can set it manually using the SDF model include path field. Note that media files are expected to live under the Simulator/Assets/External/Maps directory. Select your world file and click Import World to start the import process. The import script will now walk the SDF xml tree and generate a new scene corresponding to the world definition.","title":"Preparation for importing a SDF world file"},{"location":"simulation-content/sdf-import/#vehicle-export","text":"The import process will show unique models that are not static as candidates for vehicle export. To generate a vehicle prefab, select the candidate from the drop down, enter a meaningful name and description and press create prefab . The new prefab will be created under Simulator/Assets/External/Vehicles/YourVehicleName Add Vehicle Controller and Agent Controller Components to the root GameObject by selecting them from the dropdowns and configure them using the Inspector view. You can develop custom agent controllers and dynamics implementations by inheriting from IAgentController and IVehicleDynamics and placing the scripts under Simulator/Assets/External/Vehicles/YourVehicleName .","title":"Vehicle export"},{"location":"simulation-content/sdf-import/#saving-map-scenes","text":"Before you want to save your scene you will want to remove the models that represent your agent from the imported scene so that they can later be placed during simulation initialization. To do this, select the models that represent agents from the selection list under Convert models to spawn locations . These models will be replaced with empty SpawnInfo GameObjects which SpawnManager uses during Simulation startup to place Ego Agents into the scene.","title":"Saving Map scenes"},{"location":"simulation-content/sdf-import/#building-asset-bundles","text":"Finally, after you have exported agent prefabs and have saved the map without the agent models, you can generate AssetBundles from your assets as described in the building asset section .","title":"Building asset bundles"},{"location":"simulation-content/sdf-import/#plugins","text":"To implement a sdf plugin, you should develop a Controllable that inherits from SDFParserBase and place the SDFPluginParser attribute on its class definition to denote the plugin name this parser implements. Example implementation: using System; using System.Xml.Linq; using System.Linq; using UnityEngine; using Simulator.Controllable; [SDFPluginParser(\"ElevatorSystem\")] public class SDFElevatorParser : SDFParserBase { public SDFElevatorParser() { } /* <!-- plugin parameters example --> <plugin name=\"ElevatorSystem\" filename=\"libElevatorSystem.so\"> <system_name>ElevatorSystem_00</system_name> <elevator prefix_name=\"Elevator_\" speed=\"2\"> <floor>floor_collision</floor> <doors speed=\"0.6\" closing_timer=\"10.0\"> <inside open_offset=\"0.567\"> <door name=\"left\">seocho_EV_door_L_link</door> <door name=\"right\">seocho_EV_door_R_link</door> </inside> <!-- ... --> </doors> </elevator> </plugin> */ public override GameObject Parse(XElement pluginElement, GameObject parentModel) { var elevatorSystem = parentModel.AddComponent<ElevatorSystem>(); elevatorSystem.elevatorSystemName = pluginElement.Element(\"system_name\").Value; var elevator = pluginElement.Element(\"elevator\"); var elevatorPrefixName = elevator.Attribute(\"prefix_name\").Value; var elevatorSpeed = ParseSingle(elevator.Attribute(\"speed\"), 2.0f); var elevatorFloor = elevator.Element(\"floor\").Value; var doors = elevator.Element(\"doors\"); var inside = doors.Element(\"inside\"); var elevatorInsideDoorNameLeft = inside.Elements(\"door\") .Where(d => d.Attribute(\"name\").Value == \"left\").First().Value; // ... // work with collected parameters to generate GameObjects // instantiate Custom Behaviours etc. // find gameobjects by name at runtime return elevatorSystem.gameObject; } Place your controllable under Simulator/Assets/External/Controllables/YourControllableName and follow the development instructions as laid out in the controllable plugins chapter .","title":"Plugins"},{"location":"simulation-content/sensors-list/","text":"Sensors List # This page details the different available sensors and the configuration options possible. For more on how to build sensor plugins, please see Sensor Plugins . Table of Contents Examples How to Specify a Sensor Clock Color Camera Sensor Effects Depth Camera Segmentation Camera LiDAR LiDAR2D top 3D Ground Truth Apollo Perception Visualizer 3D CAN-Bus GPS Device GPS Odometry GPS-INS Status LGSVL Control Apollo Control Differential Drive Control Keyboard Control Wheel Control Cruise Control IMU 2D Ground Truth Radar Ultrasonic Control Calibration Transform Sensor Signal Sensor Video Recording Sensor Comfort Sensor Stop Line Sensor Vehicle Odometry Vehicle State HUD Keyboard Control AutowareAI Control Lane-line Sensor Lane Following Sensor Destination Sensor Examples top # Example JSON configurations are available here: Apollo 5.0 JSON Autoware.AI Autoware.Auto Some LiDAR sensor JSONs: Velodyne VLP-16 Velodyne VLP-32C Velodyne VLS-128 How to Specify a Sensor top # A vehicle configuration is in the following format: [ SENSOR, SENSOR, SENSOR ] A SENSOR is defined in the JSON configuration in the following format: { \"name\": STRING, \"params\": {PARAMS}, \"parent\": STRING, \"transform\": { \"x\": FLOAT, \"y\": FLOAT, \"z\": FLOAT, \"pitch\": FLOAT, \"yaw\": FLOAT, \"roll\": FLOAT, } \"plugin\": { \"id\": STRING } } name is the name of the sensor. This is how the sensor will be identified. params are the explicitly specified parameters. If a parameter is not set, the Default Value in the sensor definition will be used. ex. {\"Width\": 1920, \"Height\": 1080} There are 2 parameters that all sensors have Parameter Description Default Value Topic defines the topic that the sensor will subscribe/publish to null Frame defines the frame_id if the sensor publishes a ROS message. See ROS Header Message for more information null parent (OPTIONAL) a sensor's transform can be relative to another sensor. STRING is the name of the base sensor transform to which this sensor is relative. If omitted, the transform is relative to the origin of the vehicle. transform is the location and rotation of the sensor relative to the local position of the vehicle. The Unity left-hand coordinate system is used (+x right, +y up, +z forward, +pitch tilts the front down, +yaw rotates clockwise when viewed from above, +roll tilts the left side down). x is the position of the sensor along the x-axis y is the position of the sensor along the y-axis z is the position of the sensor along the z-axis pitch is the rotation around the x-axis yaw is the rotation around the y-axis roll is the rotation around the z-axis id is the unique ID for the sensor plugin Clock top # AssetBundle This sensor outputs simulated time to ROS as rosgraph_msgs/Clock message, or to CyberRT as clock message. The only parameter to use is topic/channel name. For ROS, you can add <param name=\"/use_sim_time\" value=\"true\"> to their ROS launch file, or use rosparam set /use_sim_time true in command line, to have a ROS node use simulation time according to the /clock topic. For more details please refer to this page . For CyberRT, you can set clock_mode in cyber.pb.conf as MODE_MOCK to have CyberRT use simulation time according to the /clock topic. { \"name\": \"Clock Sensor\", \"parent\": null, \"params\": { \"Topic\": \"/clock\" }, \"plugin\": {\"id\": \"968f386f-dc0b-485d-ba33-5bb71bff93ef\"} } Color Camera top # AssetBundle This is the type of sensor that would be used for the Main Camera in Apollo or other AD stacks. (For questions about camera matrix, please refer to this FAQ .) Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 JpegQuality defines the quality if the image output % Int 75 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion * List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 CubemapSize size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 * If Distorted is true , DistortionParameters must be an empty list or a list of FOUR floats. The values in this list should come from calibration result of real camera. Setting arbitrary values may cause undefined result. If Distorted is false , DistortionParameters , Fisheye and Xi are ignored. ** If Fisheye is true , Xi should be a value from calibration result of real camera. Setting arbitrary value may cause undefined result. If Fisheye is false , Xi is ignored. *** CubemapSize should only be 512, 1024, or 2048. { \"name\": \"Color Camera\", \"parent\": null, \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ], \"Postprocessing\": [ ] }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"3d4f1e08-4c62-4e9f-b859-b26d4910b85e\"} } Sensor Effects top # Color Camera has multiple post processing sensor effects that can be added to the Postprocessing field in params . Effects can be combined with an array of Postprocessing fields but order is hard coded. SunFlare - creates a sun flare effect Parameter Description Type Default Value Min Max type postprocessing sensor effect name String sunIntensity defines the intensity of the sun flare Float 1 0 10 haloIntensity defines the intensity of the sun flare halo Float 1 0 10 ghostingIntensity defines the intensity of the mirror effect Float 1 0 10 \"Postprocessing\": [ { \"type\": \"SunFlare\", \"sunIntensity\": 1, \"haloIntensity\": 1, \"ghostingIntensity\": 1 } ] Rain - creates rain drops on the lens Parameter Description Type Default Value Min Max type postprocessing sensor effect name String size defines the size of the drops Float 1 0 1 \"Postprocessing\": [ { \"type\": \"Rain\", \"size\": 1 } ] GreyScale - converts color to grey scale Parameter Description Type Default Value Min Max type postprocessing sensor effect name String intensity defines the intensity of the grey scale Float 1 0 1 \"Postprocessing\": [ { \"type\": \"GreyScale\", \"intensity\": 1 } ] VideoArtifacts - creates jpeg compression artifacts Parameter Description Type Default Value Min Max type postprocessing sensor effect name String intensity defines the intensity of the effect Float 0.25 0 1 blockSize defines the size of the affected block Int 32 1 128 \"Postprocessing\": [ { \"type\": \"VideoArtifacts\", \"intensity\": 0.25, \"blockSize\": 32 } ] Depth Camera top # AssetBundle This sensor returns an image where the shades on the grey-scale correspond to the depth of objects. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 5 1 100 JpegQuality defines the quality if the image output % Int 100 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion* List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 CubemapSize size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 * See notes on DistortionParameters for Color Camera. ** See notes on Xi for Color Camera. *** See notes on CubemapSize for Color Camera. { \"name\": \"Depth Camera\", \"parent\": null, \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/depth_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"290af702-b42a-4dce-800e-0999a07d49d2\"} } Segmentation Camera top # AssetBundle This sensor returns an image where objects are colored corresponding to their tag: Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F If a tag is included in \"InstanceSegmentationTags\", each instance of objects with that tag is colored differently, but all will have same hue (e.g. all cars will be bluish and all pedestrians will be reddish). Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion* List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 CubemapSize size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 InstanceSegmentationTags define tags with instance segmentation List of String empty list * See notes on DistortionParameters for Color Camera. ** See notes on Xi for Color Camera. *** See notes on CubemapSize for Color Camera. { \"name\": \"Segmentation Camera\", \"parent\": null, \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"InstanceSegmentationTags\": [ ], \"Topic\": \"/simulator/segmentation_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"dbd171ab-66bc-4c7a-a523-d4e72afe1d0f\"} } Example of Instance Segmentation Tags. Be aware that this effects performance greatly. \"InstanceSegmentationTags\": [ \"Car\", \"Pedestrian\", \"Obstacle\", \"Building\" ], LiDAR top # AssetBundle This sensor returns a point cloud after 1 revolution. Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF * If VerticalRayAngles is not empty, LaserCount will be automatically set to the length of VerticalRayAngles , and FieldOfView and CenterAngle will be ignored. A sample of uniformly distributed angles: { \"name\": \"Lidar-Uniform\", \"parent\": null, \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"b30d0478-8c7b-4687-bfc2-b3cdb3f5faff\"} } A sample of non-uniformly distributed angles: { \"name\": \"Lidar-NonUniform\", \"parent\": null, \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"b30d0478-8c7b-4687-bfc2-b3cdb3f5faff\"} } LiDAR2D top # AssetBundle This sensor returns a LaserScan message for a planar (2D) lidar. Parameter Description Unit Type Default Value Minimum Maximum CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF CubemapSize defines the size of each face of the cubemap used to model the lidar pixels Float 1024 ForwardAngle defines the angle between forward axis of the car and forward axis of the lidar degrees Float 0 0 360 HorizontalAngle defines the delta view angle degrees Float 360 0 360 performanceLoad defines the weight for distributed simulation Float 1 A sample json: { \"name\": \"LiDAR 2D Sensor\", \"parent\": null, \"params\": { \"CenterAngle\": 0, \"RotationFrequency\": 10, \"Topic\": \"/scan\", \"Frame\": \"base_scan\", \"Compensated\": false }, \"transform\": { \"x\": 0, \"y\": 0.1125, \"z\": 0.27, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"2392cabf-47b4-4410-bd81-37545b78feca\"} }, 3D Ground Truth top # AssetBundle This sensor returns 3D ground truth data for training and creates bounding boxes around the detected objects. The color of the object corresponds to the object's type: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta This sensor detects objects in all directions and includes occlusion. If any part of an object is visible from the sensor's position, a bounding box for the whole object is created - partial visibility does not affect bounds size. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines the how close an object must be to the sensor to be detected meters Float 100 1 1000 { \"name\": \"3D Ground Truth\", \"parent\": null, \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"e3fef197-9724-48ec-b98e-c5a0892d09c4\"} } Apollo Perception Visualizer 3D top # AssetBundle This sensor will visualize 3D bounding boxes on objects as detected by Apollo. It does not publish any data and instead subscribes to the perception topic from Apollo. The color of the boxes are: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta { \"name\": \"Apollo Perception Visualizer 3D Sensor\", \"parent\": null, \"params\": { \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"ab5d574b-4b32-490f-bc71-f2631d43d5df\"} } CAN-Bus top # AssetBundle This sensor sends data about the vehicle chassis. The data includes: Speed [m/s] Throttle [%] Braking [%] Steering [+/- %] Parking Brake Status [bool] High Beam Status [bool] Low Beam Status [bool] Hazard Light Status [bool] Fog Light Status [bool] Left Turn Signal Status [bool] Right Turn Signal Status [bool] Wiper Status [bool] Reverse Gear Status [bool] Selected Gear [Int] Engine Status [bool] Engine RPM [RPM] GPS Latitude [Latitude] GPS Longitude [Longitude] Altitude [m] Orientation [3D Vector of Euler angles] Velocity [3D Vector of m/s] Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 { \"name\": \"CAN Bus\", \"parent\": null, \"params\": { \"Frequency\": 10, \"Topic\": \"/canbus\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"b0bdc474-ac09-4355-901d-d7012a8c57a8\"} } GPS Device top # AssetBundle This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"name\": \"GPS\", \"parent\": null, \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"75bbcbfa-fdca-4703-8e82-abf8078f7991\"} } GPS Odometry top # AssetBundle This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates and the vehicle velocity. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 ChildFrame used by Autoware IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"name\": \"GPS Odometry\", \"parent\": null, \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_odometry\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"483f7b90-2f76-42ee-82c1-22f02f25f924\"} } GPS-INS Status top # AssetBundle This sensor outputs the status of the GPS correction due to INS. The Simulator is an ideal environment in which GPS is always corrected. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published [Hertz] Float 12.5 1 100 { \"name\": \"GPS INS Status\", \"parent\": null, \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"77250db2-82c9-47dd-9785-c38cc4cb566d\"} } LGSVL Control top # AssetBundle This sensor is required for a vehicle to subscribe to a control topic published in ROS or ROS2 with message type lgsvl_msgs/VehicleControlData . { \"name\": \"LGSVL Control Sensor\", \"parent\": null, \"params\": { \"Topic\": \"/vehicle_cmd\" }, \"plugin\": {\"id\": \"a5696615-d3e2-45e9-afa3-9b5ae7359e02\"} } Apollo Control top # AssetBundle This sensor is required for a vehicle to subscribe to the control topic from Apollo using the CyberRT bridge. { \"name\": \"Apollo Car Control\", \"parent\": null, \"params\": { \"Topic\": \"/apollo/control\" }, \"plugin\": {\"id\": \"03ea1a65-03ae-4f2e-b670-d7244e48deb4\"} } The sensor also demonstrates how a user can check when SVL Simulator receives the first control message from the AD stack like Apollo. Based on this time, you can start the rest of the simulation or do some other interesting things. To wait for control message from apollo in a Python script, you can call on_custom() of the ego agent. For example, the following code set isControlReceived of agent self.ego to be True when the simulator first receives a message of the /apollo/control topic. def on_control_received(agent, kind, context): if kind == \"checkControl\": agent.isControlReceived = True log.info(\"Control message recieved\") self.ego.on_custom(on_control_received) Differential Drive Control top # AssetBundle The sensor allows an ego vehicle (robot) with a differential drive setup to subscribe to control commands using the Ros2 bridge to drive the robot. Internally, the sensor track wheel movement and calculates an odometry pose based on wheel motion which can be published to a topic. The wheel motion is also used as feedback in a PID controller internal to the sensor which can be tuned using sensor parameters. This sensor requires a path to be defined to each of the two drive wheels under RightWheelLinkPath and LeftWheelLinkPath as seen below. The Topic parameter is the name of the topic which the sensor subscribes to in order to receive control commands. The OdometryTopic provides the name of the topic odometry is to be published to and Frame and OdometryChildFrame populate the frame and odometry frame of the odometry message. For more information on the odometry frame in ROS see REP 105 . { \"name\": \"Differential Drive Control Sensor\", \"parent\": null, \"params\": { \"OdometryTopic\": \"/odom\", \"Topic\": \"/cmd_vel\", \"OdometryChildFrame\": \"odom\", \"Frame\": \"base_footprint\", \"RightWheelLinkPath\": \"link_MainBody/SuspensionRight/link/wheel_right/link\", \"LeftWheelLinkPath\": \"link_MainBody/SuspensionLeft/link/wheel_left/link\" }, \"pluin\": {\"id\": \"6bee1cb8-1be9-417a-9cbb-7fcf2eed9600\"} } Keyboard Control top # AssetBundle This sensor is required for a vehicle to accept keyboard control commands. Parameters are not required but you can invert the inputs. { \"name\": \"Keyboard Car Control\", \"parent\": null, \"params\": { \"InvertAccel\": false, \"InvertSteer\": false }, \"plugin\": {\"id\": \"a2ff904a-ff06-4f06-9e45-cb58217a7142\"} } Wheel Control top # AssetBundle This sensor is required for a vehicle to accept Logitech G920 wheel control commands. Parameters are not required. { \"name\": \"Wheel Car Control\", \"parent\": null, \"plugin\": {\"id\": \"3d5dea8d-232a-46ea-9a16-57ef4c99df03\"} } Cruise Control top # AssetBundle This sensor causes the vehicle to accelerate to the desired speed and then maintain the desired speed. Parameter Description Unit Type Default Value Minimum Maximum CruiseSpeed defines the desired speed meters/second Float 0 0 200 { \"name\": \"Cruise Control Sensor\", \"parent\": null, \"params\": { \"CruiseSpeed\": 10 }, \"plugin\": {\"id\": \"1726cbc8-85f2-4e29-8c61-0ca93948b8e3\"} } IMU top # AssetBundle This sensor output at a fixed rate of 100 Hz. IMU publishes data on topics where the 2nd topic has corrected IMU data. Parameter Description CorectedTopic defines the 2nd topic that the data is published to CorrectedFrame defines the 2nd frame for the ROS header { \"name\": \"IMU\", \"parent\": null, \"params\": { \"Topic\": \"/imu\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"9c34ec28-627d-4e73-9bb0-9c2aa7e978f5\"} } 2D Ground Truth top # AssetBundle This sensor outputs an image where objects are encased in a box. The color of the box depends on the type of object. Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Bounding boxes for objects tightly encase all visible pixels of an object on the 2D image. Partially occluded objects will have only their visible part detected and encased. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 Width defines the width of the image pixels Int 1920 1 1920 Height defines the height of the iamge pixels Int 1080 1 1080 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor to be in the image meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor to be in the image meters Float 2000 0.01 2000 DetectionRange defines how close an object must be to be given a bounding box meters Float 100 0.01 2000 { \"name\": \"2D Ground Truth\", \"parent\": null, \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"e0bfb6cc-a8f0-425e-9b6c-a643ca1e255a\"} } Radar top # AssetBundle This sensor outputs the objects detected by the radar. Detected objects are visualized with a box colored by their type: Type Color Car Green Agent Magenta Bicycle Cyan Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 13.4 1 100 { \"name\": \"Radar\", \"parent\": null, \"params\": { \"Frequency\": 13.4, \"Topic\": \"/radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"215b7d1e-7ad3-4fe6-b3d9-db0e229eb0a4\"} } Ultrasonic top # AssetBundle This sensor outputs the distance (to the center of the sensor) of the closest point within the sensor's FOV. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 400 1 1920 Height defines the height of the image output pixels Int 160 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 JpegQuality defines the quality if the image output % Int 75 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 40 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.3 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2.0 0.01 2000 { \"name\": \"Ultrasonic Sensor\", \"parent\": null, \"params\": { \"Width\": 400, \"Height\": 160, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.3, \"MaxDistance\": 2, \"Topic\": \"/simulator/ultrasonic\", \"Frame\": \"ultrasonic\" }, \"transform\": { \"x\": 0, \"y\": 0.5, \"z\": 2.5, \"pitch\": -13, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"fb1d98b9-fb45-431a-893d-1634f760bde1\"} } Control Calibration top # AssetBundle This sensor outputs control calibration criteria collected by AD Stacks (Apollo, Autoware). It generates steering, throttle or brakes with gear commands between minimum and maximum of velocity during duration. Parameter Description Unit Type Minimum Maximum min_velocity defines the minimum velocity when criterion is executed meters/second Float 0 50.0 max_velocity defines the maximum velocity when criterion is executed meters/second Float 0 50.0 throttle defines the throttle which makes acceleration Percent Float 0 100.0 brakes defines the brakes which make deceleration Percent Float 0 100.0 steering defines ego vehicle's steering Percent Float -100.0 100.0 gear defines ego vehicle's direction (forward or reverse) String duration defines criterion's execution time second Float 0 { \"name\": \"Control Calibration\", \"parent\": null, \"params\": { \"states\": [{ \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 } ] }, \"plugin\": {\"id\": \"ef77d2d3-32ff-4627-9f0c-8eca68c18f71\"} } Total Control Calibration Criteria: Transform Sensor top # AssetBundle This sensor is specifically used to parent other sensors. For example, if there is a cluster of sensors a Transform sensor can be added at the location of the cluster and then the individual sensors can have a transform that is relative to the location of the Transform sensor`. Example usage { \"name\": \"Cluster Reference\", \"parent\": null, \"transform\": { \"x\": 0.75, \"y\": 1.7, \"z\": 1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ] }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0.1, \"y\": 0, \"z\": -0.1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0, \"y\": 0.2, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Signal Sensor top # AssetBundle This sensor returns ground truth data for traffic light signals connected to the current lane of ego vehicle and creates bounding boxes around the detected signals. The color of the bounding box corresponds to the signal's type: Bounding Box Signal Green Green Yellow Yellow Red Red Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines how close a traffic light must be to the sensor to be detected meters Float 100 1 1000 { \"name\": \"Signal Sensor\", \"parent\": null, \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/simulator/ground_truth/signals\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"c4ba9b81-b274-4c05-b78e-636e61b4590e\"} } Video Recording Sensor top # AssetBundle This sensor records a video for test cases. For local simulations, the path to the recorded video will be shown in the Video Recording Sensor section of the Test Results for each simulation. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the video pixels Int 1920 1 1920 Height defines the width of the video pixels Int 1080 1 1080 Framerate defines the number of frames per second of the video fps Int 15 1 15 Bitrate defines the average number of bits per second Kbps Int 3000 1000 6000 MaxBitrate defines the maximum number of bits per second Kbps Int 6000 1000 6000 Quality defines the target constant quality level for VBR rate control (0 to 51, 0 means automatic) Int 22 0 51 { \"name\": \"Video Recording Sensor\", \"parent\": null, \"params\": { \"Width\": 1920, \"Height\": 1080, \"Framerate\": 15, \"Bitrate\": 3000, \"MaxBitrate\": 6000, \"Quality\": 22 }, \"transform\": { \"x\": 0, \"y\": 10.0, \"z\": -10.0, \"pitch\": 30, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"06aef741-23e6-444b-b6d9-18f84f9ace06\"} } Comfort Sensor top # AssetBundle Comfort Sensor will detect whether a vehicle's acceleration, rotation or other values are out of acceptable ranges. For more details, check github . Parameter Description Unit Type Default Value Minimum Maximum maxAccelAllowed Maximum acceleration allowed m/s^2 Int maxJerkAllowed Maximum jerk allowed m/s^3 Int maxAngularVelocityAllowed Maximum angular velocity allowed deg/s Int maxAngularAccelerationAllowed Maximum angular acceleration allowed deg/s^2 Int rollTolerance Maximum deg rotation on the x axis deg Int slipTolerance Maximum deg difference between vehicle's velocity and vehicle's forward deg Int { \"name\" : \"Comfort Sensor\", \"parent\": null, \"params\": { \"maxAccelAllowed\": 8, \"maxJerkAllowed\": 4, \"maxAngularVelocityAllowed\": 200, \"maxAngularAccelerationAllowed\": 100, \"rollTolerance\": 10, \"slipTolerance\": 15 }, \"plugin\": {\"id\": \"f96ddc31-02b9-48d9-add5-a920a738236a\"} } Stop Line Sensor top # AssetBundle The Stop Line sensor is a sensor used purely for analyzing the results of a simulation when the Create test report option is enabled for the simulation (See here for more information on test reports). The sensor will allow the simulator to detect and report stop line violations. { \"name\": \"Stop Line Sensor\", \"parent\": null, \"plugin\": {\"id\": \"1b94ebcd-0057-4ef5-a9de-23e7f0628e19\"} } Vehicle Odometry top # AssetBundle The Vehicle Odometry sensor publishes information on the vehicle velocity and front and rear angles in ROS and ROS2 using the lgsvl_msgs/VehicleOdometry message type. { \"name\": \"Vehicle Odometry Sensor\", \"parent\": null, \"params\": { \"Topic\": \"lgsvl/vehicle_odom\" }, \"plugin\": {\"id\": \"662072f8-8123-4e18-9b45-ee801e0bc020\"} } Vehicle State top # AssetBundle The Vehicle State sensor subscribes to information about the state of the vehicle that is not captured in the Can Bus sensor, such as the state of the headlights, blinkers, and wipers, and reflects that state in the simulated ego vehicle. The sensor subscribes in ROS and ROS2 to a topic with the lgsvl_msgs/VehicleStateData message type. See here for all the fields subscribed to by the sensor. { \"name\": \"Vehicle State Sensor\", \"parent\": null, \"params\": { \"Topic\": \"lgsvl/vehicle_state\" }, \"plugin\": {\"id\": \"8aff00f2-a5e4-4bd3-a778-517f9307fa99\"} } HUD Keyboard Control top # AssetBundle The HUD Keyboard Control sensor is a Keyboard Control sensor with an additional Heads Up Display (HUD) that displays the following information: Speed (mph) Current gear Engine speed (rpm) Ignition status indicator Parking brake indicator The HUD is shown by enabling the sensor visualization for the HUD Keyboard Sensor. { \"name\": \"HUD Keyboard Control Sensor\", \"parent\": null, \"plugin\": {\"id\": \"41fdc7b2-5e2b-4f78-ba1f-e195a5604ad4\"} } AutowareAI Control top # AssetBundle The AutowareAI Control sensor subscribes to the vehicle control topic from Autoware AI in ROS and allows the ego vehicle to react to control commands. The sensor subscribes to autoware_msgs/VehicleCmd message type. { \"name\": \"Vehicle State Sensor\", \"parent\": null, \"params\": { \"Topic\": \"lgsvl/vehicle_state\" }, \"plugin\": {\"id\": \"76ff7bd1-81bf-4284-b9bf-e8173dc6053e\"} } Lane-line Sensor top # AssetBundle This sensor outputs lines data for the current lane in third-degree polynomial format, along with line type and color. More details are available on the lane-line sensor page. Parameter Description Unit Type Default Value Minimum Maximum Width The width of the image output * pixels Int 1920 1 1920 Height The height of the image output * pixels Int 1080 1 1080 Frequency The maximum rate that messages will be published Hertz Int 15 1 100 FieldOfView The vertical angle that the camera sees degrees Float 60 1 90 MinDistance The near plane of the preview camera * meters Float 0.1 0.01 1000 MaxDistance The far plane of the preview camera * meters Float 2000 0.01 2000 DetectionRange Defines how far from the sensor line will be sampled meters Float 50 10 200 SampleDelta The distance between discrete line samples meters Float 0.5 0.05 1 * These parameters only affect the preview, not the output data itself. { \"name\": \"LaneLineSensor\", \"parent\": null, \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 10, \"FieldOfView\": 60, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"DetectionRange\": 50, \"SampleDelta\": 0.5, \"Topic\": \"/simulator/lane_line\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": 3.0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"5ae0c053-92e5-46e0-b22b-142a3d433dde\"} } Lane Following Sensor top # AssetBundle The Lane Following sensor is used to steer an ego vehicle using a deep learning model. The sensor subscribes to steering commands from the model which are sent as lgsvl_msgs/VehicleControlData messages in ROS2 and applies them to the ego vehicle. Read this tutorial to learn more. { \"name\": \"Lane Following Sensor\", \"parent\": null, \"params\": { \"Topic\": \"/lanefollowing/steering_cmd\" }, \"plugin\": {\"id\": \"111c4f5f-12a3-48b6-b3df-7d87e158e7d9\"} } Destination Sensor top # AssetBundle The Destination Sensor is used to set an initial pose or a destination pose of an attached agent for Navigation2 stack via python API scripts. The sensor publishes to ROS and ROS2 using the geometry_msgs/PoseStamped message type. Parameter Description Unit Type Default Value Minimum Maximum InitPoseTopic Topic name for initial pose String InitPoseFrame Frame name for initial pose String DestinationCheckRadius Defines how close a destination must be from the sensor in meters to check whether it has been reached Meters Float 1.0 1.0 10.0 DestinationCheckAngle Defines how close a destination must be from the sensor in degrees to check whether it has been reached Degrees Float 10.0 1.0 360.0 { \"name\": \"Destination Sensor\", \"parent\": null, \"params\": { \"Topic\": \"/goal_pose\", \"Frame\": \"map\", \"InitPoseTopic\": \"/initialpose\", \"InitPoseFrame\": \"map\", \"DestinationCheckRadius\": 1.0, \"DestinationCheckAngle\": 10.0 }, \"plugin\": {\"id\": \"087803de-1d56-4db3-93cf-f0018d099b96\"} }","title":"List of sensors"},{"location":"simulation-content/sensors-list/#examples","text":"Example JSON configurations are available here: Apollo 5.0 JSON Autoware.AI Autoware.Auto Some LiDAR sensor JSONs: Velodyne VLP-16 Velodyne VLP-32C Velodyne VLS-128","title":"Examples"},{"location":"simulation-content/sensors-list/#how-to-specify-a-sensor","text":"A vehicle configuration is in the following format: [ SENSOR, SENSOR, SENSOR ] A SENSOR is defined in the JSON configuration in the following format: { \"name\": STRING, \"params\": {PARAMS}, \"parent\": STRING, \"transform\": { \"x\": FLOAT, \"y\": FLOAT, \"z\": FLOAT, \"pitch\": FLOAT, \"yaw\": FLOAT, \"roll\": FLOAT, } \"plugin\": { \"id\": STRING } } name is the name of the sensor. This is how the sensor will be identified. params are the explicitly specified parameters. If a parameter is not set, the Default Value in the sensor definition will be used. ex. {\"Width\": 1920, \"Height\": 1080} There are 2 parameters that all sensors have Parameter Description Default Value Topic defines the topic that the sensor will subscribe/publish to null Frame defines the frame_id if the sensor publishes a ROS message. See ROS Header Message for more information null parent (OPTIONAL) a sensor's transform can be relative to another sensor. STRING is the name of the base sensor transform to which this sensor is relative. If omitted, the transform is relative to the origin of the vehicle. transform is the location and rotation of the sensor relative to the local position of the vehicle. The Unity left-hand coordinate system is used (+x right, +y up, +z forward, +pitch tilts the front down, +yaw rotates clockwise when viewed from above, +roll tilts the left side down). x is the position of the sensor along the x-axis y is the position of the sensor along the y-axis z is the position of the sensor along the z-axis pitch is the rotation around the x-axis yaw is the rotation around the y-axis roll is the rotation around the z-axis id is the unique ID for the sensor plugin","title":"How to Specify a Sensor"},{"location":"simulation-content/sensors-list/#clock","text":"AssetBundle This sensor outputs simulated time to ROS as rosgraph_msgs/Clock message, or to CyberRT as clock message. The only parameter to use is topic/channel name. For ROS, you can add <param name=\"/use_sim_time\" value=\"true\"> to their ROS launch file, or use rosparam set /use_sim_time true in command line, to have a ROS node use simulation time according to the /clock topic. For more details please refer to this page . For CyberRT, you can set clock_mode in cyber.pb.conf as MODE_MOCK to have CyberRT use simulation time according to the /clock topic. { \"name\": \"Clock Sensor\", \"parent\": null, \"params\": { \"Topic\": \"/clock\" }, \"plugin\": {\"id\": \"968f386f-dc0b-485d-ba33-5bb71bff93ef\"} }","title":"Clock"},{"location":"simulation-content/sensors-list/#color-camera","text":"AssetBundle This is the type of sensor that would be used for the Main Camera in Apollo or other AD stacks. (For questions about camera matrix, please refer to this FAQ .) Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 JpegQuality defines the quality if the image output % Int 75 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion * List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 CubemapSize size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 * If Distorted is true , DistortionParameters must be an empty list or a list of FOUR floats. The values in this list should come from calibration result of real camera. Setting arbitrary values may cause undefined result. If Distorted is false , DistortionParameters , Fisheye and Xi are ignored. ** If Fisheye is true , Xi should be a value from calibration result of real camera. Setting arbitrary value may cause undefined result. If Fisheye is false , Xi is ignored. *** CubemapSize should only be 512, 1024, or 2048. { \"name\": \"Color Camera\", \"parent\": null, \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ], \"Postprocessing\": [ ] }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"3d4f1e08-4c62-4e9f-b859-b26d4910b85e\"} }","title":"Color Camera"},{"location":"simulation-content/sensors-list/#sensor-effects","text":"Color Camera has multiple post processing sensor effects that can be added to the Postprocessing field in params . Effects can be combined with an array of Postprocessing fields but order is hard coded. SunFlare - creates a sun flare effect Parameter Description Type Default Value Min Max type postprocessing sensor effect name String sunIntensity defines the intensity of the sun flare Float 1 0 10 haloIntensity defines the intensity of the sun flare halo Float 1 0 10 ghostingIntensity defines the intensity of the mirror effect Float 1 0 10 \"Postprocessing\": [ { \"type\": \"SunFlare\", \"sunIntensity\": 1, \"haloIntensity\": 1, \"ghostingIntensity\": 1 } ] Rain - creates rain drops on the lens Parameter Description Type Default Value Min Max type postprocessing sensor effect name String size defines the size of the drops Float 1 0 1 \"Postprocessing\": [ { \"type\": \"Rain\", \"size\": 1 } ] GreyScale - converts color to grey scale Parameter Description Type Default Value Min Max type postprocessing sensor effect name String intensity defines the intensity of the grey scale Float 1 0 1 \"Postprocessing\": [ { \"type\": \"GreyScale\", \"intensity\": 1 } ] VideoArtifacts - creates jpeg compression artifacts Parameter Description Type Default Value Min Max type postprocessing sensor effect name String intensity defines the intensity of the effect Float 0.25 0 1 blockSize defines the size of the affected block Int 32 1 128 \"Postprocessing\": [ { \"type\": \"VideoArtifacts\", \"intensity\": 0.25, \"blockSize\": 32 } ]","title":"Sensor Effects"},{"location":"simulation-content/sensors-list/#depth-camera","text":"AssetBundle This sensor returns an image where the shades on the grey-scale correspond to the depth of objects. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 5 1 100 JpegQuality defines the quality if the image output % Int 100 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion* List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 CubemapSize size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 * See notes on DistortionParameters for Color Camera. ** See notes on Xi for Color Camera. *** See notes on CubemapSize for Color Camera. { \"name\": \"Depth Camera\", \"parent\": null, \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/depth_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"290af702-b42a-4dce-800e-0999a07d49d2\"} }","title":"Depth Camera"},{"location":"simulation-content/sensors-list/#segmentation-camera","text":"AssetBundle This sensor returns an image where objects are colored corresponding to their tag: Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F If a tag is included in \"InstanceSegmentationTags\", each instance of objects with that tag is colored differently, but all will have same hue (e.g. all cars will be bluish and all pedestrians will be reddish). Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion* List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 CubemapSize size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 InstanceSegmentationTags define tags with instance segmentation List of String empty list * See notes on DistortionParameters for Color Camera. ** See notes on Xi for Color Camera. *** See notes on CubemapSize for Color Camera. { \"name\": \"Segmentation Camera\", \"parent\": null, \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"InstanceSegmentationTags\": [ ], \"Topic\": \"/simulator/segmentation_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"dbd171ab-66bc-4c7a-a523-d4e72afe1d0f\"} } Example of Instance Segmentation Tags. Be aware that this effects performance greatly. \"InstanceSegmentationTags\": [ \"Car\", \"Pedestrian\", \"Obstacle\", \"Building\" ],","title":"Segmentation Camera"},{"location":"simulation-content/sensors-list/#lidar","text":"AssetBundle This sensor returns a point cloud after 1 revolution. Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF * If VerticalRayAngles is not empty, LaserCount will be automatically set to the length of VerticalRayAngles , and FieldOfView and CenterAngle will be ignored. A sample of uniformly distributed angles: { \"name\": \"Lidar-Uniform\", \"parent\": null, \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"b30d0478-8c7b-4687-bfc2-b3cdb3f5faff\"} } A sample of non-uniformly distributed angles: { \"name\": \"Lidar-NonUniform\", \"parent\": null, \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"b30d0478-8c7b-4687-bfc2-b3cdb3f5faff\"} }","title":"LiDAR"},{"location":"simulation-content/sensors-list/#lidar2d","text":"AssetBundle This sensor returns a LaserScan message for a planar (2D) lidar. Parameter Description Unit Type Default Value Minimum Maximum CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF CubemapSize defines the size of each face of the cubemap used to model the lidar pixels Float 1024 ForwardAngle defines the angle between forward axis of the car and forward axis of the lidar degrees Float 0 0 360 HorizontalAngle defines the delta view angle degrees Float 360 0 360 performanceLoad defines the weight for distributed simulation Float 1 A sample json: { \"name\": \"LiDAR 2D Sensor\", \"parent\": null, \"params\": { \"CenterAngle\": 0, \"RotationFrequency\": 10, \"Topic\": \"/scan\", \"Frame\": \"base_scan\", \"Compensated\": false }, \"transform\": { \"x\": 0, \"y\": 0.1125, \"z\": 0.27, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"2392cabf-47b4-4410-bd81-37545b78feca\"} },","title":"LiDAR2D top"},{"location":"simulation-content/sensors-list/#3d-ground-truth","text":"AssetBundle This sensor returns 3D ground truth data for training and creates bounding boxes around the detected objects. The color of the object corresponds to the object's type: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta This sensor detects objects in all directions and includes occlusion. If any part of an object is visible from the sensor's position, a bounding box for the whole object is created - partial visibility does not affect bounds size. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines the how close an object must be to the sensor to be detected meters Float 100 1 1000 { \"name\": \"3D Ground Truth\", \"parent\": null, \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"e3fef197-9724-48ec-b98e-c5a0892d09c4\"} }","title":"3D Ground Truth"},{"location":"simulation-content/sensors-list/#apollo-perception-visualizer-3d","text":"AssetBundle This sensor will visualize 3D bounding boxes on objects as detected by Apollo. It does not publish any data and instead subscribes to the perception topic from Apollo. The color of the boxes are: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta { \"name\": \"Apollo Perception Visualizer 3D Sensor\", \"parent\": null, \"params\": { \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"ab5d574b-4b32-490f-bc71-f2631d43d5df\"} }","title":"Apollo Perception Visualizer 3D"},{"location":"simulation-content/sensors-list/#can-bus","text":"AssetBundle This sensor sends data about the vehicle chassis. The data includes: Speed [m/s] Throttle [%] Braking [%] Steering [+/- %] Parking Brake Status [bool] High Beam Status [bool] Low Beam Status [bool] Hazard Light Status [bool] Fog Light Status [bool] Left Turn Signal Status [bool] Right Turn Signal Status [bool] Wiper Status [bool] Reverse Gear Status [bool] Selected Gear [Int] Engine Status [bool] Engine RPM [RPM] GPS Latitude [Latitude] GPS Longitude [Longitude] Altitude [m] Orientation [3D Vector of Euler angles] Velocity [3D Vector of m/s] Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 { \"name\": \"CAN Bus\", \"parent\": null, \"params\": { \"Frequency\": 10, \"Topic\": \"/canbus\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"b0bdc474-ac09-4355-901d-d7012a8c57a8\"} }","title":"CAN-Bus"},{"location":"simulation-content/sensors-list/#gps-device","text":"AssetBundle This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"name\": \"GPS\", \"parent\": null, \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"75bbcbfa-fdca-4703-8e82-abf8078f7991\"} }","title":"GPS Device"},{"location":"simulation-content/sensors-list/#gps-odometry","text":"AssetBundle This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates and the vehicle velocity. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 ChildFrame used by Autoware IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"name\": \"GPS Odometry\", \"parent\": null, \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_odometry\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"483f7b90-2f76-42ee-82c1-22f02f25f924\"} }","title":"GPS Odometry"},{"location":"simulation-content/sensors-list/#gps-ins-status","text":"AssetBundle This sensor outputs the status of the GPS correction due to INS. The Simulator is an ideal environment in which GPS is always corrected. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published [Hertz] Float 12.5 1 100 { \"name\": \"GPS INS Status\", \"parent\": null, \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"77250db2-82c9-47dd-9785-c38cc4cb566d\"} }","title":"GPS-INS Status"},{"location":"simulation-content/sensors-list/#vehicle-control","text":"AssetBundle This sensor is required for a vehicle to subscribe to a control topic published in ROS or ROS2 with message type lgsvl_msgs/VehicleControlData . { \"name\": \"LGSVL Control Sensor\", \"parent\": null, \"params\": { \"Topic\": \"/vehicle_cmd\" }, \"plugin\": {\"id\": \"a5696615-d3e2-45e9-afa3-9b5ae7359e02\"} }","title":"LGSVL Control"},{"location":"simulation-content/sensors-list/#apollo-control","text":"AssetBundle This sensor is required for a vehicle to subscribe to the control topic from Apollo using the CyberRT bridge. { \"name\": \"Apollo Car Control\", \"parent\": null, \"params\": { \"Topic\": \"/apollo/control\" }, \"plugin\": {\"id\": \"03ea1a65-03ae-4f2e-b670-d7244e48deb4\"} } The sensor also demonstrates how a user can check when SVL Simulator receives the first control message from the AD stack like Apollo. Based on this time, you can start the rest of the simulation or do some other interesting things. To wait for control message from apollo in a Python script, you can call on_custom() of the ego agent. For example, the following code set isControlReceived of agent self.ego to be True when the simulator first receives a message of the /apollo/control topic. def on_control_received(agent, kind, context): if kind == \"checkControl\": agent.isControlReceived = True log.info(\"Control message recieved\") self.ego.on_custom(on_control_received)","title":"Apollo Control"},{"location":"simulation-content/sensors-list/#differential-drive-control","text":"AssetBundle The sensor allows an ego vehicle (robot) with a differential drive setup to subscribe to control commands using the Ros2 bridge to drive the robot. Internally, the sensor track wheel movement and calculates an odometry pose based on wheel motion which can be published to a topic. The wheel motion is also used as feedback in a PID controller internal to the sensor which can be tuned using sensor parameters. This sensor requires a path to be defined to each of the two drive wheels under RightWheelLinkPath and LeftWheelLinkPath as seen below. The Topic parameter is the name of the topic which the sensor subscribes to in order to receive control commands. The OdometryTopic provides the name of the topic odometry is to be published to and Frame and OdometryChildFrame populate the frame and odometry frame of the odometry message. For more information on the odometry frame in ROS see REP 105 . { \"name\": \"Differential Drive Control Sensor\", \"parent\": null, \"params\": { \"OdometryTopic\": \"/odom\", \"Topic\": \"/cmd_vel\", \"OdometryChildFrame\": \"odom\", \"Frame\": \"base_footprint\", \"RightWheelLinkPath\": \"link_MainBody/SuspensionRight/link/wheel_right/link\", \"LeftWheelLinkPath\": \"link_MainBody/SuspensionLeft/link/wheel_left/link\" }, \"pluin\": {\"id\": \"6bee1cb8-1be9-417a-9cbb-7fcf2eed9600\"} }","title":"Differential Drive Control"},{"location":"simulation-content/sensors-list/#keyboard-control","text":"AssetBundle This sensor is required for a vehicle to accept keyboard control commands. Parameters are not required but you can invert the inputs. { \"name\": \"Keyboard Car Control\", \"parent\": null, \"params\": { \"InvertAccel\": false, \"InvertSteer\": false }, \"plugin\": {\"id\": \"a2ff904a-ff06-4f06-9e45-cb58217a7142\"} }","title":"Keyboard Control"},{"location":"simulation-content/sensors-list/#wheel-control","text":"AssetBundle This sensor is required for a vehicle to accept Logitech G920 wheel control commands. Parameters are not required. { \"name\": \"Wheel Car Control\", \"parent\": null, \"plugin\": {\"id\": \"3d5dea8d-232a-46ea-9a16-57ef4c99df03\"} }","title":"Wheel Control"},{"location":"simulation-content/sensors-list/#cruise-control","text":"AssetBundle This sensor causes the vehicle to accelerate to the desired speed and then maintain the desired speed. Parameter Description Unit Type Default Value Minimum Maximum CruiseSpeed defines the desired speed meters/second Float 0 0 200 { \"name\": \"Cruise Control Sensor\", \"parent\": null, \"params\": { \"CruiseSpeed\": 10 }, \"plugin\": {\"id\": \"1726cbc8-85f2-4e29-8c61-0ca93948b8e3\"} }","title":"Cruise Control"},{"location":"simulation-content/sensors-list/#imu","text":"AssetBundle This sensor output at a fixed rate of 100 Hz. IMU publishes data on topics where the 2nd topic has corrected IMU data. Parameter Description CorectedTopic defines the 2nd topic that the data is published to CorrectedFrame defines the 2nd frame for the ROS header { \"name\": \"IMU\", \"parent\": null, \"params\": { \"Topic\": \"/imu\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"9c34ec28-627d-4e73-9bb0-9c2aa7e978f5\"} }","title":"IMU"},{"location":"simulation-content/sensors-list/#2d-ground-truth","text":"AssetBundle This sensor outputs an image where objects are encased in a box. The color of the box depends on the type of object. Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Bounding boxes for objects tightly encase all visible pixels of an object on the 2D image. Partially occluded objects will have only their visible part detected and encased. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 Width defines the width of the image pixels Int 1920 1 1920 Height defines the height of the iamge pixels Int 1080 1 1080 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor to be in the image meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor to be in the image meters Float 2000 0.01 2000 DetectionRange defines how close an object must be to be given a bounding box meters Float 100 0.01 2000 { \"name\": \"2D Ground Truth\", \"parent\": null, \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"e0bfb6cc-a8f0-425e-9b6c-a643ca1e255a\"} }","title":"2D Ground Truth"},{"location":"simulation-content/sensors-list/#radar","text":"AssetBundle This sensor outputs the objects detected by the radar. Detected objects are visualized with a box colored by their type: Type Color Car Green Agent Magenta Bicycle Cyan Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 13.4 1 100 { \"name\": \"Radar\", \"parent\": null, \"params\": { \"Frequency\": 13.4, \"Topic\": \"/radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"215b7d1e-7ad3-4fe6-b3d9-db0e229eb0a4\"} }","title":"Radar"},{"location":"simulation-content/sensors-list/#ultrasonic","text":"AssetBundle This sensor outputs the distance (to the center of the sensor) of the closest point within the sensor's FOV. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 400 1 1920 Height defines the height of the image output pixels Int 160 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 JpegQuality defines the quality if the image output % Int 75 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 40 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.3 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2.0 0.01 2000 { \"name\": \"Ultrasonic Sensor\", \"parent\": null, \"params\": { \"Width\": 400, \"Height\": 160, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.3, \"MaxDistance\": 2, \"Topic\": \"/simulator/ultrasonic\", \"Frame\": \"ultrasonic\" }, \"transform\": { \"x\": 0, \"y\": 0.5, \"z\": 2.5, \"pitch\": -13, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"fb1d98b9-fb45-431a-893d-1634f760bde1\"} }","title":"Ultrasonic"},{"location":"simulation-content/sensors-list/#control-calibration","text":"AssetBundle This sensor outputs control calibration criteria collected by AD Stacks (Apollo, Autoware). It generates steering, throttle or brakes with gear commands between minimum and maximum of velocity during duration. Parameter Description Unit Type Minimum Maximum min_velocity defines the minimum velocity when criterion is executed meters/second Float 0 50.0 max_velocity defines the maximum velocity when criterion is executed meters/second Float 0 50.0 throttle defines the throttle which makes acceleration Percent Float 0 100.0 brakes defines the brakes which make deceleration Percent Float 0 100.0 steering defines ego vehicle's steering Percent Float -100.0 100.0 gear defines ego vehicle's direction (forward or reverse) String duration defines criterion's execution time second Float 0 { \"name\": \"Control Calibration\", \"parent\": null, \"params\": { \"states\": [{ \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 } ] }, \"plugin\": {\"id\": \"ef77d2d3-32ff-4627-9f0c-8eca68c18f71\"} } Total Control Calibration Criteria:","title":"Control Calibration"},{"location":"simulation-content/sensors-list/#transform-sensor","text":"AssetBundle This sensor is specifically used to parent other sensors. For example, if there is a cluster of sensors a Transform sensor can be added at the location of the cluster and then the individual sensors can have a transform that is relative to the location of the Transform sensor`. Example usage { \"name\": \"Cluster Reference\", \"parent\": null, \"transform\": { \"x\": 0.75, \"y\": 1.7, \"z\": 1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ] }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0.1, \"y\": 0, \"z\": -0.1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0, \"y\": 0.2, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Transform Sensor"},{"location":"simulation-content/sensors-list/#signal-sensor","text":"AssetBundle This sensor returns ground truth data for traffic light signals connected to the current lane of ego vehicle and creates bounding boxes around the detected signals. The color of the bounding box corresponds to the signal's type: Bounding Box Signal Green Green Yellow Yellow Red Red Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines how close a traffic light must be to the sensor to be detected meters Float 100 1 1000 { \"name\": \"Signal Sensor\", \"parent\": null, \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/simulator/ground_truth/signals\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"c4ba9b81-b274-4c05-b78e-636e61b4590e\"} }","title":"Signal Sensor"},{"location":"simulation-content/sensors-list/#video-recording-sensor","text":"AssetBundle This sensor records a video for test cases. For local simulations, the path to the recorded video will be shown in the Video Recording Sensor section of the Test Results for each simulation. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the video pixels Int 1920 1 1920 Height defines the width of the video pixels Int 1080 1 1080 Framerate defines the number of frames per second of the video fps Int 15 1 15 Bitrate defines the average number of bits per second Kbps Int 3000 1000 6000 MaxBitrate defines the maximum number of bits per second Kbps Int 6000 1000 6000 Quality defines the target constant quality level for VBR rate control (0 to 51, 0 means automatic) Int 22 0 51 { \"name\": \"Video Recording Sensor\", \"parent\": null, \"params\": { \"Width\": 1920, \"Height\": 1080, \"Framerate\": 15, \"Bitrate\": 3000, \"MaxBitrate\": 6000, \"Quality\": 22 }, \"transform\": { \"x\": 0, \"y\": 10.0, \"z\": -10.0, \"pitch\": 30, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"06aef741-23e6-444b-b6d9-18f84f9ace06\"} }","title":"Video Recording Sensor"},{"location":"simulation-content/sensors-list/#comfort-sensor","text":"AssetBundle Comfort Sensor will detect whether a vehicle's acceleration, rotation or other values are out of acceptable ranges. For more details, check github . Parameter Description Unit Type Default Value Minimum Maximum maxAccelAllowed Maximum acceleration allowed m/s^2 Int maxJerkAllowed Maximum jerk allowed m/s^3 Int maxAngularVelocityAllowed Maximum angular velocity allowed deg/s Int maxAngularAccelerationAllowed Maximum angular acceleration allowed deg/s^2 Int rollTolerance Maximum deg rotation on the x axis deg Int slipTolerance Maximum deg difference between vehicle's velocity and vehicle's forward deg Int { \"name\" : \"Comfort Sensor\", \"parent\": null, \"params\": { \"maxAccelAllowed\": 8, \"maxJerkAllowed\": 4, \"maxAngularVelocityAllowed\": 200, \"maxAngularAccelerationAllowed\": 100, \"rollTolerance\": 10, \"slipTolerance\": 15 }, \"plugin\": {\"id\": \"f96ddc31-02b9-48d9-add5-a920a738236a\"} }","title":"Comfort Sensor"},{"location":"simulation-content/sensors-list/#stopline-sensor","text":"AssetBundle The Stop Line sensor is a sensor used purely for analyzing the results of a simulation when the Create test report option is enabled for the simulation (See here for more information on test reports). The sensor will allow the simulator to detect and report stop line violations. { \"name\": \"Stop Line Sensor\", \"parent\": null, \"plugin\": {\"id\": \"1b94ebcd-0057-4ef5-a9de-23e7f0628e19\"} }","title":"Stop Line Sensor"},{"location":"simulation-content/sensors-list/#vehicle-odometry","text":"AssetBundle The Vehicle Odometry sensor publishes information on the vehicle velocity and front and rear angles in ROS and ROS2 using the lgsvl_msgs/VehicleOdometry message type. { \"name\": \"Vehicle Odometry Sensor\", \"parent\": null, \"params\": { \"Topic\": \"lgsvl/vehicle_odom\" }, \"plugin\": {\"id\": \"662072f8-8123-4e18-9b45-ee801e0bc020\"} }","title":"Vehicle Odometry"},{"location":"simulation-content/sensors-list/#vehicle-state","text":"AssetBundle The Vehicle State sensor subscribes to information about the state of the vehicle that is not captured in the Can Bus sensor, such as the state of the headlights, blinkers, and wipers, and reflects that state in the simulated ego vehicle. The sensor subscribes in ROS and ROS2 to a topic with the lgsvl_msgs/VehicleStateData message type. See here for all the fields subscribed to by the sensor. { \"name\": \"Vehicle State Sensor\", \"parent\": null, \"params\": { \"Topic\": \"lgsvl/vehicle_state\" }, \"plugin\": {\"id\": \"8aff00f2-a5e4-4bd3-a778-517f9307fa99\"} }","title":"Vehicle State"},{"location":"simulation-content/sensors-list/#hud-keyboard-control","text":"AssetBundle The HUD Keyboard Control sensor is a Keyboard Control sensor with an additional Heads Up Display (HUD) that displays the following information: Speed (mph) Current gear Engine speed (rpm) Ignition status indicator Parking brake indicator The HUD is shown by enabling the sensor visualization for the HUD Keyboard Sensor. { \"name\": \"HUD Keyboard Control Sensor\", \"parent\": null, \"plugin\": {\"id\": \"41fdc7b2-5e2b-4f78-ba1f-e195a5604ad4\"} }","title":"HUD Keyboard Control"},{"location":"simulation-content/sensors-list/#autowareai-control","text":"AssetBundle The AutowareAI Control sensor subscribes to the vehicle control topic from Autoware AI in ROS and allows the ego vehicle to react to control commands. The sensor subscribes to autoware_msgs/VehicleCmd message type. { \"name\": \"Vehicle State Sensor\", \"parent\": null, \"params\": { \"Topic\": \"lgsvl/vehicle_state\" }, \"plugin\": {\"id\": \"76ff7bd1-81bf-4284-b9bf-e8173dc6053e\"} }","title":"AutowareAI Control"},{"location":"simulation-content/sensors-list/#lane-line-sensor","text":"AssetBundle This sensor outputs lines data for the current lane in third-degree polynomial format, along with line type and color. More details are available on the lane-line sensor page. Parameter Description Unit Type Default Value Minimum Maximum Width The width of the image output * pixels Int 1920 1 1920 Height The height of the image output * pixels Int 1080 1 1080 Frequency The maximum rate that messages will be published Hertz Int 15 1 100 FieldOfView The vertical angle that the camera sees degrees Float 60 1 90 MinDistance The near plane of the preview camera * meters Float 0.1 0.01 1000 MaxDistance The far plane of the preview camera * meters Float 2000 0.01 2000 DetectionRange Defines how far from the sensor line will be sampled meters Float 50 10 200 SampleDelta The distance between discrete line samples meters Float 0.5 0.05 1 * These parameters only affect the preview, not the output data itself. { \"name\": \"LaneLineSensor\", \"parent\": null, \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 10, \"FieldOfView\": 60, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"DetectionRange\": 50, \"SampleDelta\": 0.5, \"Topic\": \"/simulator/lane_line\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": 3.0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 }, \"plugin\": {\"id\": \"5ae0c053-92e5-46e0-b22b-142a3d433dde\"} }","title":"Lane-line Sensor"},{"location":"simulation-content/sensors-list/#lane-following-sensor","text":"AssetBundle The Lane Following sensor is used to steer an ego vehicle using a deep learning model. The sensor subscribes to steering commands from the model which are sent as lgsvl_msgs/VehicleControlData messages in ROS2 and applies them to the ego vehicle. Read this tutorial to learn more. { \"name\": \"Lane Following Sensor\", \"parent\": null, \"params\": { \"Topic\": \"/lanefollowing/steering_cmd\" }, \"plugin\": {\"id\": \"111c4f5f-12a3-48b6-b3df-7d87e158e7d9\"} }","title":"Lane Following Sensor"},{"location":"simulation-content/sensors-list/#destination-sensor","text":"AssetBundle The Destination Sensor is used to set an initial pose or a destination pose of an attached agent for Navigation2 stack via python API scripts. The sensor publishes to ROS and ROS2 using the geometry_msgs/PoseStamped message type. Parameter Description Unit Type Default Value Minimum Maximum InitPoseTopic Topic name for initial pose String InitPoseFrame Frame name for initial pose String DestinationCheckRadius Defines how close a destination must be from the sensor in meters to check whether it has been reached Meters Float 1.0 1.0 10.0 DestinationCheckAngle Defines how close a destination must be from the sensor in degrees to check whether it has been reached Degrees Float 10.0 1.0 360.0 { \"name\": \"Destination Sensor\", \"parent\": null, \"params\": { \"Topic\": \"/goal_pose\", \"Frame\": \"map\", \"InitPoseTopic\": \"/initialpose\", \"InitPoseFrame\": \"map\", \"DestinationCheckRadius\": 1.0, \"DestinationCheckAngle\": 10.0 }, \"plugin\": {\"id\": \"087803de-1d56-4db3-93cf-f0018d099b96\"} }","title":"Destination Sensor"},{"location":"simulation-content/sharing/","text":"Sharing Assets # The sharing feature allows you to share any private library assets and simulations with new and existing users in a controlled manner, without having to publish them publicly. Sharing an asset top # Sharing assets is very straightforward. At the top right of every private owned library asset and simulation there is a \"Share\" button. This will bring up a popup, which you can enter one or more email addresses to share the item. Email addresses for registered users will automatically add the item to the recipient's library. Conversely, unregistered users will be sent an email inviting them to sign up and accept the shared item. Viewing shared assets top # Once a private asset has been shared with you, it will automatically be added to your library. These assets will have a yellow circle indicating it was shared. The \"Shared with Me\" filter can also be used to easily view all shared assets. Unsharing top # Owners of shared assets and simulations can revoke access at any time. By bringing up the \"Share\" popup once more, a list of all shared users will be visible. By pressing the trash can next to a user's name, the item will be revoked from that user.","title":"Sharing assets"},{"location":"simulation-content/sharing/#sharing-an-asset","text":"Sharing assets is very straightforward. At the top right of every private owned library asset and simulation there is a \"Share\" button. This will bring up a popup, which you can enter one or more email addresses to share the item. Email addresses for registered users will automatically add the item to the recipient's library. Conversely, unregistered users will be sent an email inviting them to sign up and accept the shared item.","title":"Sharing an asset"},{"location":"simulation-content/sharing/#viewing-shared-assets","text":"Once a private asset has been shared with you, it will automatically be added to your library. These assets will have a yellow circle indicating it was shared. The \"Shared with Me\" filter can also be used to easily view all shared assets.","title":"Viewing shared assets"},{"location":"simulation-content/sharing/#unsharing","text":"Owners of shared assets and simulations can revoke access at any time. By bringing up the \"Share\" popup once more, a list of all shared users will be visible. By pressing the trash can next to a user's name, the item will be revoked from that user.","title":"Unsharing"},{"location":"simulation-content/total-control-calibration-criteria/","text":"Total Control Calibration Criteria # back This page has control calibration criteria JSON. { \"type\": \"ControlCalibrationSensor\", \"name\": \"Control Calibration\", \"params\": { \"states\": [{ \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 20, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 24, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 20, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 24, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 20, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 27, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 18, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 10, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 18, \"brakes\": 0, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": -10, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 22, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 45, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": -45, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 15, \"steering\": -65, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 } ] } }","title":"Total Control Calibration Criteria"},{"location":"simulation-content/velodyne-json-examples/","text":"Velodyne VLP-16 # [ { \"type\": \"LidarSensor\", \"name\": \"Velodyne VLP-16\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"FieldOfView\": 30, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ] Velodyne VLP-32C # [ { \"type\": \"LidarSensor\", \"name\": \"Velodyne VLP-32C\", \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ] Velodyne VLS-128 # [ { \"type\": \"LidarSensor\", \"name\": \"Velodyne VLS-128\", \"params\": { \"VerticalRayAngles\": [ -11.742, -1.99, 3.4, -5.29, -0.78, 4.61, -4.08, 1.31, -6.5, -1.11, 4.28, -4.41, 0.1, 6.48, -3.2, 2.19, -3.86, 1.53, -9.244, -1.77, 2.74, -5.95, -0.56, 4.83, -2.98, 2.41, -6.28, -0.89, 3.62, -5.07, 0.32, 7.58, -0.34, 5.18, -3.64, 1.75, -25, -2.43, 2.96, -5.73, 0.54, 9.7, -2.76, 2.63, -7.65, -1.55, 3.84, -4.85, 3.188, -5.51, -0.12, 5.73, -4.3, 1.09, -16.042, -2.21, 4.06, -4.63, 0.76, 15, -3.42, 1.97, -6.85, -1.33, -5.62, -0.23, 5.43, -3.53, 0.98, -19.582, -2.32, 3.07, -4.74, 0.65, 11.75, -2.65, 1.86, -7.15, -1.44, 3.95, -2.1, 3.29, -5.4, -0.01, 4.5, -4.19, 1.2, -13.565, -1.22, 4.17, -4.52, 0.87, 6.08, -3.31, 2.08, -6.65, 1.42, -10.346, -1.88, 3.51, -6.06, -0.67, 4.72, -3.97, 2.3, -6.39, -1, 4.39, -5.18, 0.21, 6.98, -3.09, 4.98, -3.75, 1.64, -8.352, -2.54, 2.85, -5.84, -0.45, 8.43, -2.87, 2.52, -6.17, -1.66, 3.73, -4.96, 0.43 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ]","title":"Velodyne json examples"},{"location":"simulation-content/velodyne-json-examples/#velodyne-vlp-16","text":"[ { \"type\": \"LidarSensor\", \"name\": \"Velodyne VLP-16\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"FieldOfView\": 30, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ]","title":"Velodyne VLP-16"},{"location":"simulation-content/velodyne-json-examples/#velodyne-vlp-32c","text":"[ { \"type\": \"LidarSensor\", \"name\": \"Velodyne VLP-32C\", \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ]","title":"Velodyne VLP-32C"},{"location":"simulation-content/velodyne-json-examples/#velodyne-vls-128","text":"[ { \"type\": \"LidarSensor\", \"name\": \"Velodyne VLS-128\", \"params\": { \"VerticalRayAngles\": [ -11.742, -1.99, 3.4, -5.29, -0.78, 4.61, -4.08, 1.31, -6.5, -1.11, 4.28, -4.41, 0.1, 6.48, -3.2, 2.19, -3.86, 1.53, -9.244, -1.77, 2.74, -5.95, -0.56, 4.83, -2.98, 2.41, -6.28, -0.89, 3.62, -5.07, 0.32, 7.58, -0.34, 5.18, -3.64, 1.75, -25, -2.43, 2.96, -5.73, 0.54, 9.7, -2.76, 2.63, -7.65, -1.55, 3.84, -4.85, 3.188, -5.51, -0.12, 5.73, -4.3, 1.09, -16.042, -2.21, 4.06, -4.63, 0.76, 15, -3.42, 1.97, -6.85, -1.33, -5.62, -0.23, 5.43, -3.53, 0.98, -19.582, -2.32, 3.07, -4.74, 0.65, 11.75, -2.65, 1.86, -7.15, -1.44, 3.95, -2.1, 3.29, -5.4, -0.01, 4.5, -4.19, 1.2, -13.565, -1.22, 4.17, -4.52, 0.87, 6.08, -3.31, 2.08, -6.65, 1.42, -10.346, -1.88, 3.51, -6.06, -0.67, 4.72, -3.97, 2.3, -6.39, -1, 4.39, -5.18, 0.21, 6.98, -3.09, 4.98, -3.75, 1.64, -8.352, -2.54, 2.85, -5.84, -0.45, 8.43, -2.87, 2.52, -6.17, -1.66, 3.73, -4.96, 0.43 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ]","title":"Velodyne VLS-128"},{"location":"support/contributing/","text":"Contributing to SVL Simulator # As an open project and community, we welcome and highly encourage contributions back to SVL Simulator. Through feedback, questions, bug reports/issues, new features, documentation, or demonstrations showing your use case of SVL Simulator, you can help contribute to the SVL Simulator project in several different ways. Feedback, questions, bug reports, and issues top # The best way to give feedback, raise an issue, or ask a question about SVL Simulator is to submit a Github issue to our main Github repository: https://www.github.com/lgsvl/simulator . Please also check first if your question has already been answered in an existing issue or on our Frequently Asked Questions page. We will try to respond to open issues as quickly as possible. If you would like to get in touch about partnerships, licensing, or larger collaborations, please email us at contact@svlsimulator.com . Submitting a Pull Request top # We welcome pull requests for new features or bug fixes to SVL Simulator. You can submit a pull request here . After submitting the initial pull request, you will be prompted to agree to our Contributor License Agreement . Your pull request will then be ready for review and merging. Below are a few notes/guidelines on submitting pull requests and working on our project. While we are an open project, please be aware that we have a custom license . You must agree to the terms of our license to use and contribute back to our project. Please make sure that any preexisting licenses/terms of your contributions are compatible with our terms. Under the terms of the Unity Asset Store Terms of Service and EULA , you cannot contribute back to our project anything which you have obtained from the Unity Asset Store (unless you have separate agreements in place with Unity). Our team synchronizes the lgsvl/simulator repository internally first, then pushes to Github approximately once a week. When we merge pull requests, it will first be merged into our internal source, then pushed to Github, so there may be a slight delay for your merged PR to show up in the public repository. We always rebase and do not merge branches - when submitting a pull request, please make sure your code is rebased on the latest version of the master branch. If you are working on a large feature or a big change to how the simulator works, please submit a Github issue first or reach out to us to discuss the idea before you start working on it or submit a pull request. SVL Simulator is cross platform. Please do not write platform-specific code, or if you do, please make sure you do so while supporting all of our supported platforms. Documentation top # We currently use mkdocs version 1.0.4 to build our documentation website . To submit a piece of documentation, please submit a PR to the lgsvl/simulator repository, placing your materials in the Docs directory. After merging, our team will build and upload to the public website. Demonstrations top # If you are using the SVL Simulator for your development, we would like to hear from you. We are also always excited to see any demonstration videos, blog posts, or articles showing use cases for the SVL Simulator. Please reach out to us at contact@svlsimulator.com to let us know about your application.","title":"Contributing"},{"location":"support/contributing/#feedback,-questions,-bug-reports,-and-issues","text":"The best way to give feedback, raise an issue, or ask a question about SVL Simulator is to submit a Github issue to our main Github repository: https://www.github.com/lgsvl/simulator . Please also check first if your question has already been answered in an existing issue or on our Frequently Asked Questions page. We will try to respond to open issues as quickly as possible. If you would like to get in touch about partnerships, licensing, or larger collaborations, please email us at contact@svlsimulator.com .","title":"Feedback, questions, bug reports, and issues"},{"location":"support/contributing/#submitting-a-pull-request","text":"We welcome pull requests for new features or bug fixes to SVL Simulator. You can submit a pull request here . After submitting the initial pull request, you will be prompted to agree to our Contributor License Agreement . Your pull request will then be ready for review and merging. Below are a few notes/guidelines on submitting pull requests and working on our project. While we are an open project, please be aware that we have a custom license . You must agree to the terms of our license to use and contribute back to our project. Please make sure that any preexisting licenses/terms of your contributions are compatible with our terms. Under the terms of the Unity Asset Store Terms of Service and EULA , you cannot contribute back to our project anything which you have obtained from the Unity Asset Store (unless you have separate agreements in place with Unity). Our team synchronizes the lgsvl/simulator repository internally first, then pushes to Github approximately once a week. When we merge pull requests, it will first be merged into our internal source, then pushed to Github, so there may be a slight delay for your merged PR to show up in the public repository. We always rebase and do not merge branches - when submitting a pull request, please make sure your code is rebased on the latest version of the master branch. If you are working on a large feature or a big change to how the simulator works, please submit a Github issue first or reach out to us to discuss the idea before you start working on it or submit a pull request. SVL Simulator is cross platform. Please do not write platform-specific code, or if you do, please make sure you do so while supporting all of our supported platforms.","title":"Submitting a Pull Request"},{"location":"support/contributing/#documentation","text":"We currently use mkdocs version 1.0.4 to build our documentation website . To submit a piece of documentation, please submit a PR to the lgsvl/simulator repository, placing your materials in the Docs directory. After merging, our team will build and upload to the public website.","title":"Documentation"},{"location":"support/contributing/#demonstrations","text":"If you are using the SVL Simulator for your development, we would like to hear from you. We are also always excited to see any demonstration videos, blog posts, or articles showing use cases for the SVL Simulator. Please reach out to us at contact@svlsimulator.com to let us know about your application.","title":"Demonstrations"},{"location":"support/faq/","text":"SVL Simulator FAQ # What are the recommended system specs? What are the minimum REQUIRED system specs? Does the simulator run on Windows/Mac/Linux? Why does the simulator not open on Linux? Which Unity version is required and how do I get it? Why does my Simulator say \"Invalid: Out of date Assetbundle\"? How do I setup development environment for Unity on Ubuntu? Where are Unity log files located Why are assets/scenes missing/empty after cloning from git? Why do I get an error saying some files (e.g. rosbridge_websocket.launch) are missing in Apollo? ROS Bridge won How do I control the ego vehicle (my vehicle) spawn position? How can I add a custom ego vehicle to SVL Simulator? How can I add extra sensors to vehicles in SVL Simulator? How do I get parameters in camera matrix? How can I add a custom map to SVL Simulator? How can I create or edit map annotations? Why are pedestrians not spawning when annotated correctly? Why can't I find catkin_make command when building Apollo? Why is Apollo perception module turning on and off all the time? Why does the Apollo vehicle stop at stop line and not cross intersections? Dreamview in Apollo shows \"Hardware GPS triggers safety mode. No GNSS status message.\" Why does Rviz not load the Autoware vector map? Why are there no maps when I make a local build? Why is the TARGET_WAYPOINT missing when using the Map Annotation Tool Why does the simulator start and then say the simulation is \"Invalid\"? Why are there no assets when building the simulator from Unity Editor? Why are there libraries missing when running a PythonAPI script? How to fix the \"RuntimeError: The current Numpy installation\" error? Other questions? What are the recommended system specs? What are the minimum REQUIRED system specs? top # For optimal performance, we recommend that you run the simulator on a system with at least a 4 GHz Quad core CPU, NVIDIA GTX 1080 graphics card (8GB memory), and 16GB memory or higher, running on Windows 10. While you can run on a lower-spec system, the performance of the simulator will be impacted and you will probably see much lower frame rates. The minimum specification to run is a 3GHz dual core CPU, NVIDIA graphics card, and 8 GB memory system. Note that simulator runs better on Windows due to fact that Unity and NVIDIA drivers provide better performance on Windows than on Linux. If Apollo or Autoware will be running on the same system, upgrading to a GPU with at least 10GB memory is recommended. Does the simulator run on Windows/Mac/Linux? top # Officially, you can run SVL Simulator on Windows 10 and Ubuntu 18.04 (or later). We do not support macOS at this time. Why does the simulator not open on Linux? top # The Simulator requires the vulkan libraries to be installed on Linux: sudo apt install libvulkan1 Which Unity version is required and how do I get it? top # SVL Simulator is currently on Unity version 2020.3.3f1, and can be downloaded from the Unity Download Archive. You can download the Windows version here: https://unity3d.com/get-unity/download/archive You can download the Linux version (2020.3.3f1) here: https://beta.unity3d.com/download/76626098c1c4/UnitySetup-2020.3.3f1 We are constantly working to ensure that SVL Simulator runs on the latest version of Unity which supports all of our required functionality. Why does my Simulator say \"Invalid: Out of date Assetbundle\"? top # Assetbundle versions change as we add new features. To get the latest assetbundles, add assets uploaded by SVL Admin on content asset store to your Library and restart your local simulator. How do I setup development environment for Unity on Ubuntu? top # Install Unity Editor dependencies: sudo apt install \\ gconf-service lib32gcc1 lib32stdc++6 libasound2 libc6 libc6-i386 libcairo2 libcap2 libcups2 \\ libdbus-1-3 libexpat1 libfontconfig1 libfreetype6 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 \\ libgl1 libglib2.0-0 libglu1 libgtk2.0-0 libgtk-3-0 libnspr4 libnss3 libpango1.0-0 libstdc++6 \\ libx11-6 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 \\ libxrender1 libxtst6 zlib1g debconf libgtk2.0-0 libsoup2.4-1 libarchive13 libpng16-16 Download and install Unity 2020.3.3f1: curl -fLo UnitySetup https://beta.unity3d.com/download/76626098c1c4/UnitySetup-2020.3.3f1 chmod +x UnitySetup ./UnitySetup --unattended --install-location=/opt/Unity --components=Unity,Windows-Mono,Mac-Mono Install .NET Core SDK, available from https://dotnet.microsoft.com/download On Ubuntu 18.04 run following commands: wget -q https://packages.microsoft.com/config/ubuntu/18.04/packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get install apt-transport-https sudo apt-get update sudo apt-get install dotnet-sdk-2.2 Install Mono, available from https://www.mono-project.com/download/stable/#download-lin On Ubuntu 18.04 run following commands: sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF sudo apt install apt-transport-https ca-certificates echo \"deb https://download.mono-project.com/repo/ubuntu stable-xenial main\" | sudo tee /etc/apt/sources.list.d/mono-official-stable.list sudo apt update sudo apt install mono-devel Install Visual Studio Code, available in Ubuntu Software or from https://code.visualstudio.com/docs/setup/linux Open VS Code and install C# extension Press Ctrl+Shift+X Search for C# Install extension C# for Visual Studio Code (powered by OmniSharp) Install Unity Debug Extension, available here: https://marketplace.visualstudio.com/items?itemName=Unity.unity-debug Set Unity preferences to use VS Code. See instructions here: https://code.visualstudio.com/docs/other/unity#_setup-vs-code-as-unity-script-editor To find out where Code is installed use which code Where are Unity log files located? top # Version Location Windows Binary %APPDATA%\\..\\LocalLow\\LGElectronics\\SVLSimulator\\Player.log Windows Editor %APPDATA%\\..\\Local\\Unity\\Editor\\Editor.log Linux Binary ~/.config/unity3d/LGElectronics/SVLSimulator/Player.log Linux Editor ~/.config/unity3d/Editor.log Why are assets/scenes missing/empty after cloning from git? top # We use Git LFS for large file storage to improve performance of cloning. Before cloning, install and run git lfs install . Then repeat the git clone process. You can find the Git LFS installation instructions here: https://github.com/git-lfs/git-lfs/wiki/Installation Typically if you do not have Git LFS installed or configured then you will see the following error when opening Unity project: error CS026: The type or namespace name \"WebSocketSharp\" could not be found. Are you missing a using directive or an assembly reference? Why do I get an error saying some files (e.g. rosbridge_websocket.launch) are missing in Apollo? top # If you see that some files are missing from ros_pkgs folder in Apollo repository, you need to make sure that you are cloning all submodules: git clone --recurse-submodules https://github.com/lgsvl/apollo.git ROS Bridge won't connect? top # First make sure you are running rosbridge. If using our Apollo docker image, run: rosbridge.sh For standalone ROS environments run: roslaunch rosbridge_server rosbridge_websocket.launch If you are running ROS bridge on different machine, verify that simulator can connect to it and you do not have firewall blocking ports. How do I control the ego vehicle (my vehicle) spawn position? top # Find the \"spawn_transform\" game objects in scene and adjust their transform position. If you are creating new maps make sure you add \"SpawnInfo\" component to empty game object. The Simulator will use location of first game object that has SpawnInfo component. How can I add a custom ego vehicle to SVL Simulator? top # Please see our tutorial on how to add a new ego vehicle to SVL Simulator here . How can I add extra sensors to vehicles in SVL Simulator? top # Adding sensors to a vehicle is done by editing the configuration JSON in the WebUI. See Sensor JSON Options for details on all the available sensors. How do I get parameters in camera matrix like the following? top # [fx 0 cx 0 fy cy 0 0 1] Our reference camera sensors use the pinhole camera model, where all of these parameters can be calculated from other parameters. For focal lengths, i.e. fx and fy , the pinhole camera has same focal lengths in both horizontal and vertical directions as: fx = fy = Height / 2 / Mathf.Tan(FieldOfView / 2.0f * Mathf.Deg2Rad) . Note that since FieldOfView is the vertical FOV, we use half of Height in the calculation. For optical center, i.e. cx and cy , since Unity uses symmetric view frustum by default, the optical center is always at the center of the image. So we have cx = Width / 2.0f and cy = Height / 2.0f How can I add a custom map to SVL Simulator? top # See Maps for details. How can I create or edit map annotations? top # Please see our tutorial on how to add map annotations in SVL Simulator here . Why are pedestrians not spawning when annotated correctly? top # SVL Simulator uses Unity's NavMesh API to work correctly. In Unity Editor, select Window -> AI -> Navigation and bake the NavMesh. Why can't I find catkin_make command when building Apollo? top # Make sure you are not running Apollo dev_start/into.sh scripts as root. The will not work as root. You need to run them as non-root user, without sudo. Why is Apollo perception module turning on and off all the time? top # This means that Apollo perception process is exiting with error. Check apollo/data/log/perception.ERROR file for error messages. Typically you will see following error: Check failed: error == cudaSuccess (8 vs. 0) invalid device function This means one of two things: 1) GPU you are using is not supported by Apollo. Apollo requires CUDA 8 compatible hardware, it won't work if GPU is too old or too new. Apollo officially supports only GTX 1080. RTX 2080 will not work. 2) Other option is that CUDA driver is broken. To fix this you will need to restart your computer. Check that CUDA works on your host system by running one of CUDA examples before running Apollo Why does the Apollo vehicle stop at stop line and not cross intersections? top # Apollo vehicle continues over intersection only when traffic light is green. If perception module does not see traffic light, the vehicle won't move. Check previous question to verify that perception module is running and Apollo is seeing traffic light (top left of dreamview should say GREEN or RED). Dreamview in Apollo shows \"Hardware GPS triggers safety mode. No GNSS status message.\" top # This is expected behavior. SVL Simulator does simulation on software level. It sends only ROS messages to Apollo. Dreamview in Apollo has extra checks that tries to verify if hardware devices are working correctly and are not disconnected. This error message means that Apollo does not see GPS hardware working (as it is not present). It it safe to ignore it. Why does Rviz not load the Autoware vector map? top # Loading SanFrancisco map in Rviz for Autoware is a very slow process, because SanFrancisco map has many annotations and Rviz cannot handle them efficiently. It will either crash or will take many minutes if not hours. You can checkout older commit of autoware-data repository that has annotations only for smaller part of SanFrancisco. git checkout e3cfe709e4af32ad2ea8ea4de85579b9916fe516 Why are there no maps when I make a local build? top # See Build Instructions . It is not required to build the whole simulator using this tool. Why is the TARGET_WAYPOINT missing when using the Map Annotation Tool ? top # Make sure the meshes that make up the road have the Default layer assigned to them and they have a Mesh Collider added. Why does the simulator start and then say the simulation is \"Invalid\"? top # If the vehicle(s) selected for the simulation have a bridge, then a Bridge Connection String is required. The format of the string is IP:port (e.g. localhost:9090 ). The simulator does not assume a port so it must be specified. Why are there no assets when building the simulator from Unity Editor? top # Assets (environments and vehicles) are not included in the main simulator repository to reduce it's size. Maps and vehicles can get large because of 3D assets and textures. See Adding Assets for instructions on how to add assets to the project. Why are there libraries missing when running a PythonAPI quickstart script? top # PythonAPI quickstart scripts use Python libraries that are available publicly. To install all the required libraries, execute the command below inside the PythonAPI directory. pip3 install --user -e . How to fix the \"RuntimeError: The current Numpy installation\" error? top # There is a known issue with Numpy on Windows with the newest updates. See this issue for more information: https://tinyurl.com/y3dm3h86 . To fix this issue, execute the command below. pip install numpy==1.19.3 Other questions? top # See our Github issues page, or email us at contact@svlsimulator.com .","title":"Frequently asked questions"},{"location":"support/faq/#what-are-the-recommended-system-specs-what-are-the-minimum-required-system-specs","text":"For optimal performance, we recommend that you run the simulator on a system with at least a 4 GHz Quad core CPU, NVIDIA GTX 1080 graphics card (8GB memory), and 16GB memory or higher, running on Windows 10. While you can run on a lower-spec system, the performance of the simulator will be impacted and you will probably see much lower frame rates. The minimum specification to run is a 3GHz dual core CPU, NVIDIA graphics card, and 8 GB memory system. Note that simulator runs better on Windows due to fact that Unity and NVIDIA drivers provide better performance on Windows than on Linux. If Apollo or Autoware will be running on the same system, upgrading to a GPU with at least 10GB memory is recommended.","title":"What are the recommended system specs? What are the minimum REQUIRED system specs?"},{"location":"support/faq/#does-the-simulator-run-on-windows-mac-linux","text":"Officially, you can run SVL Simulator on Windows 10 and Ubuntu 18.04 (or later). We do not support macOS at this time.","title":"Does the simulator run on Windows/Mac/Linux?"},{"location":"support/faq/#why-does-the-simulator-not-open-on-linux","text":"The Simulator requires the vulkan libraries to be installed on Linux: sudo apt install libvulkan1","title":"Why does the simulator not open on Linux?"},{"location":"support/faq/#which-unity-version-is-required-and-how-do-i-get-it","text":"SVL Simulator is currently on Unity version 2020.3.3f1, and can be downloaded from the Unity Download Archive. You can download the Windows version here: https://unity3d.com/get-unity/download/archive You can download the Linux version (2020.3.3f1) here: https://beta.unity3d.com/download/76626098c1c4/UnitySetup-2020.3.3f1 We are constantly working to ensure that SVL Simulator runs on the latest version of Unity which supports all of our required functionality.","title":"Which Unity version is required and how do I get it?"},{"location":"support/faq/#why-does-my-simulator-say-invalid-out-of-date-assetbundle","text":"Assetbundle versions change as we add new features. To get the latest assetbundles, add assets uploaded by SVL Admin on content asset store to your Library and restart your local simulator.","title":"Why does my Simulator say \"Invalid: Out of date Assetbundle\"?"},{"location":"support/faq/#how-do-i-setup-development-environment-for-unity-on-ubuntu","text":"Install Unity Editor dependencies: sudo apt install \\ gconf-service lib32gcc1 lib32stdc++6 libasound2 libc6 libc6-i386 libcairo2 libcap2 libcups2 \\ libdbus-1-3 libexpat1 libfontconfig1 libfreetype6 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 \\ libgl1 libglib2.0-0 libglu1 libgtk2.0-0 libgtk-3-0 libnspr4 libnss3 libpango1.0-0 libstdc++6 \\ libx11-6 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 \\ libxrender1 libxtst6 zlib1g debconf libgtk2.0-0 libsoup2.4-1 libarchive13 libpng16-16 Download and install Unity 2020.3.3f1: curl -fLo UnitySetup https://beta.unity3d.com/download/76626098c1c4/UnitySetup-2020.3.3f1 chmod +x UnitySetup ./UnitySetup --unattended --install-location=/opt/Unity --components=Unity,Windows-Mono,Mac-Mono Install .NET Core SDK, available from https://dotnet.microsoft.com/download On Ubuntu 18.04 run following commands: wget -q https://packages.microsoft.com/config/ubuntu/18.04/packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get install apt-transport-https sudo apt-get update sudo apt-get install dotnet-sdk-2.2 Install Mono, available from https://www.mono-project.com/download/stable/#download-lin On Ubuntu 18.04 run following commands: sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF sudo apt install apt-transport-https ca-certificates echo \"deb https://download.mono-project.com/repo/ubuntu stable-xenial main\" | sudo tee /etc/apt/sources.list.d/mono-official-stable.list sudo apt update sudo apt install mono-devel Install Visual Studio Code, available in Ubuntu Software or from https://code.visualstudio.com/docs/setup/linux Open VS Code and install C# extension Press Ctrl+Shift+X Search for C# Install extension C# for Visual Studio Code (powered by OmniSharp) Install Unity Debug Extension, available here: https://marketplace.visualstudio.com/items?itemName=Unity.unity-debug Set Unity preferences to use VS Code. See instructions here: https://code.visualstudio.com/docs/other/unity#_setup-vs-code-as-unity-script-editor To find out where Code is installed use which code","title":"How do I setup development environment for Unity on Ubuntu?"},{"location":"support/faq/#where-are-unity-log-files-located","text":"Version Location Windows Binary %APPDATA%\\..\\LocalLow\\LGElectronics\\SVLSimulator\\Player.log Windows Editor %APPDATA%\\..\\Local\\Unity\\Editor\\Editor.log Linux Binary ~/.config/unity3d/LGElectronics/SVLSimulator/Player.log Linux Editor ~/.config/unity3d/Editor.log","title":"Where are Unity log files located"},{"location":"support/faq/#why-are-assets-scenes-missing-empty-after-cloning-from-git","text":"We use Git LFS for large file storage to improve performance of cloning. Before cloning, install and run git lfs install . Then repeat the git clone process. You can find the Git LFS installation instructions here: https://github.com/git-lfs/git-lfs/wiki/Installation Typically if you do not have Git LFS installed or configured then you will see the following error when opening Unity project: error CS026: The type or namespace name \"WebSocketSharp\" could not be found. Are you missing a using directive or an assembly reference?","title":"Why are assets/scenes missing/empty after cloning from git?"},{"location":"support/faq/#why-do-i-get-an-error-saying-som-files-e-g-rosbridge-websocket-launch-are-missing-in-apollo","text":"If you see that some files are missing from ros_pkgs folder in Apollo repository, you need to make sure that you are cloning all submodules: git clone --recurse-submodules https://github.com/lgsvl/apollo.git","title":"Why do I get an error saying some files (e.g. rosbridge_websocket.launch) are missing in Apollo?"},{"location":"support/faq/#ros-bridge-won-t-connect","text":"First make sure you are running rosbridge. If using our Apollo docker image, run: rosbridge.sh For standalone ROS environments run: roslaunch rosbridge_server rosbridge_websocket.launch If you are running ROS bridge on different machine, verify that simulator can connect to it and you do not have firewall blocking ports.","title":"ROS Bridge won"},{"location":"support/faq/#how-do-i-control-the-ego-vehicle-my-vehicle-spawn-position","text":"Find the \"spawn_transform\" game objects in scene and adjust their transform position. If you are creating new maps make sure you add \"SpawnInfo\" component to empty game object. The Simulator will use location of first game object that has SpawnInfo component.","title":"How do I control the ego vehicle (my vehicle) spawn position?"},{"location":"support/faq/#how-can-i-add-a-custom-ego-vehicle-to-lgsvl-simulator","text":"Please see our tutorial on how to add a new ego vehicle to SVL Simulator here .","title":"How can I add a custom ego vehicle to SVL Simulator?"},{"location":"support/faq/#how-can-i-add-extra-sensors-to-vehicles-in-lgsvl-simulator","text":"Adding sensors to a vehicle is done by editing the configuration JSON in the WebUI. See Sensor JSON Options for details on all the available sensors.","title":"How can I add extra sensors to vehicles in SVL Simulator?"},{"location":"support/faq/#how-do-i-get-parameters-in-camera-matrix","text":"[fx 0 cx 0 fy cy 0 0 1] Our reference camera sensors use the pinhole camera model, where all of these parameters can be calculated from other parameters. For focal lengths, i.e. fx and fy , the pinhole camera has same focal lengths in both horizontal and vertical directions as: fx = fy = Height / 2 / Mathf.Tan(FieldOfView / 2.0f * Mathf.Deg2Rad) . Note that since FieldOfView is the vertical FOV, we use half of Height in the calculation. For optical center, i.e. cx and cy , since Unity uses symmetric view frustum by default, the optical center is always at the center of the image. So we have cx = Width / 2.0f and cy = Height / 2.0f","title":"How do I get parameters in camera matrix?"},{"location":"support/faq/#how-can-i-add-a-custom-map-to-lgsvl-simulator","text":"See Maps for details.","title":"How can I add a custom map to SVL Simulator?"},{"location":"support/faq/#how-can-i-create-or-edit-map-annotations","text":"Please see our tutorial on how to add map annotations in SVL Simulator here .","title":"How can I create or edit map annotations?"},{"location":"support/faq/#why-are-pedestrians-not-spawning-when-annotated-correctly","text":"SVL Simulator uses Unity's NavMesh API to work correctly. In Unity Editor, select Window -> AI -> Navigation and bake the NavMesh.","title":"Why are pedestrians not spawning when annotated correctly?"},{"location":"support/faq/#why-can-t-i-find-catkin-make-command-when-building-apollo","text":"Make sure you are not running Apollo dev_start/into.sh scripts as root. The will not work as root. You need to run them as non-root user, without sudo.","title":"Why can't I find catkin_make command when building Apollo?"},{"location":"support/faq/#why-is-apollo-perception-module-turning-on-and-off-all-the-time","text":"This means that Apollo perception process is exiting with error. Check apollo/data/log/perception.ERROR file for error messages. Typically you will see following error: Check failed: error == cudaSuccess (8 vs. 0) invalid device function This means one of two things: 1) GPU you are using is not supported by Apollo. Apollo requires CUDA 8 compatible hardware, it won't work if GPU is too old or too new. Apollo officially supports only GTX 1080. RTX 2080 will not work. 2) Other option is that CUDA driver is broken. To fix this you will need to restart your computer. Check that CUDA works on your host system by running one of CUDA examples before running Apollo","title":"Why is Apollo perception module turning on and off all the time?"},{"location":"support/faq/#why-does-the-apollo-vehicle-stop-at-stop-line-and-not-cross-intersections","text":"Apollo vehicle continues over intersection only when traffic light is green. If perception module does not see traffic light, the vehicle won't move. Check previous question to verify that perception module is running and Apollo is seeing traffic light (top left of dreamview should say GREEN or RED).","title":"Why does the Apollo vehicle stop at stop line and not cross intersections?"},{"location":"support/faq/#dreamview-in-apollo-shows-hardware-gps-triggers-safety-mode-no-gnss-status-message","text":"This is expected behavior. SVL Simulator does simulation on software level. It sends only ROS messages to Apollo. Dreamview in Apollo has extra checks that tries to verify if hardware devices are working correctly and are not disconnected. This error message means that Apollo does not see GPS hardware working (as it is not present). It it safe to ignore it.","title":"Dreamview in Apollo shows \"Hardware GPS triggers safety mode. No GNSS status message.\""},{"location":"support/faq/#why-does-rviz-not-load-the-autoware-vector-map","text":"Loading SanFrancisco map in Rviz for Autoware is a very slow process, because SanFrancisco map has many annotations and Rviz cannot handle them efficiently. It will either crash or will take many minutes if not hours. You can checkout older commit of autoware-data repository that has annotations only for smaller part of SanFrancisco. git checkout e3cfe709e4af32ad2ea8ea4de85579b9916fe516","title":"Why does Rviz not load the Autoware vector map?"},{"location":"support/faq/#why-are-there-no-maps-when-i-make-a-local-build","text":"See Build Instructions . It is not required to build the whole simulator using this tool.","title":"Why are there no maps when I make a local build?"},{"location":"support/faq/#what-is-the-target-waypoint-missing-when-using-the-map-annotation-tool","text":"Make sure the meshes that make up the road have the Default layer assigned to them and they have a Mesh Collider added.","title":"Why is the TARGET_WAYPOINT missing when using the Map Annotation Tool"},{"location":"support/faq/#why-does-the-simulator-start-and-then-say-the-simulation-is-invalid","text":"If the vehicle(s) selected for the simulation have a bridge, then a Bridge Connection String is required. The format of the string is IP:port (e.g. localhost:9090 ). The simulator does not assume a port so it must be specified.","title":"Why does the simulator start and then say the simulation is \"Invalid\"?"},{"location":"support/faq/#why-are-there-no-assets-when-building-the-simulator-from-unity-editor","text":"Assets (environments and vehicles) are not included in the main simulator repository to reduce it's size. Maps and vehicles can get large because of 3D assets and textures. See Adding Assets for instructions on how to add assets to the project.","title":"Why are there no assets when building the simulator from Unity Editor?"},{"location":"support/faq/#why-are-there-libraries-missing-when-running-a-pythonapi-script","text":"PythonAPI quickstart scripts use Python libraries that are available publicly. To install all the required libraries, execute the command below inside the PythonAPI directory. pip3 install --user -e .","title":"Why are there libraries missing when running a PythonAPI script?"},{"location":"support/faq/#how-to-fix-the-runtime-the-current-numpy-installation-error","text":"There is a known issue with Numpy on Windows with the newest updates. See this issue for more information: https://tinyurl.com/y3dm3h86 . To fix this issue, execute the command below. pip install numpy==1.19.3","title":"How to fix the \"RuntimeError: The current Numpy installation\" error?"},{"location":"support/faq/#other-questions","text":"See our Github issues page, or email us at contact@svlsimulator.com .","title":"Other questions?"},{"location":"support/support-requests/","text":"Support Requests and Reporting Problems #","title":"Support Requests and Reporting Problems"},{"location":"support/troubleshooting/","text":"Troubleshooting # Important: if you have run a previous version (2020.06 or older) of SVL Simulator on your machine, then your assets like maps, vehicles, clusters, simulations will not be automatically migrated to the new version 2021.1 or above. To solve this issue, you will have to create a new account on the new cloud-based web-ui , build your assets in editor using SVL Simulator 2021.1 or greater, upload the newly built asset bundles to your library and create new simulations . If you have any other issues with downloaded data, that need all local data to be wiped, you can delete the folder \u201cLGElectronics\u201d, in the location below: Platform Filepath Windows %APPDATA%\\..\\LocalLow\\LGElectronics Linux ~/.config/unity3d/LGElectronics NOTE: This will delete all existing simulation configurations and log files related to SVL Simulator. You can then restart the SVL Simulator binary executable, and the folder will be created again and you will have to link to a new cluster . Logs top # Logs can be found at the following locations: Version Location Windows Binary %APPDATA%\\..\\LocalLow\\LGElectronics\\SVLSimulator\\Player.log Windows Editor %APPDATA%\\..\\Local\\Unity\\Editor\\Editor.log Linux Binary ~/.config/unity3d/LGElectronics/SVLSimulator/Player.log Linux Editor ~/.config/unity3d/Unity/Editor/Editor.log Unity Help top # Please see our Unity Help document for additional tips and troubleshooting guides when working with Unity Editor in Developer Mode. Frequently Asked Questions (FAQ) top # You can find our FAQ here , which may be helpful in answering common questions or issues that may arise.","title":"Troubleshooting"},{"location":"support/troubleshooting/#logs","text":"Logs can be found at the following locations: Version Location Windows Binary %APPDATA%\\..\\LocalLow\\LGElectronics\\SVLSimulator\\Player.log Windows Editor %APPDATA%\\..\\Local\\Unity\\Editor\\Editor.log Linux Binary ~/.config/unity3d/LGElectronics/SVLSimulator/Player.log Linux Editor ~/.config/unity3d/Unity/Editor/Editor.log","title":"Logs"},{"location":"support/troubleshooting/#unity-help","text":"Please see our Unity Help document for additional tips and troubleshooting guides when working with Unity Editor in Developer Mode.","title":"Unity Help"},{"location":"support/troubleshooting/#frequently-asked-questions-(faq)","text":"You can find our FAQ here , which may be helpful in answering common questions or issues that may arise.","title":"Frequently Asked Questions (FAQ)"},{"location":"support/unity-help/","text":"Unity Help # SVL Simulator was created with Unity Real-Time Development Platform and its High-Definition Render Pipeline (HDRP). HDRP is Unity Technologies' scriptable pipeline that is used to create software applications that prioritize graphics quality for realism in demanding scenarios utilizing GPU hardware. Since this technology is relatively new, there can be some issues and solutions that are not documented. This document goes over some of our own tips, guides, and recommended best practices for working inside the Unity Editor. HDRP Requirements top # HDRP targets high-end PCs and prioritizes high definition visuals but has specific system requirements. Target supported APIs are D3D11, D3D12, Metal, and Vulkan Architecture x86, x86_64 We highly recommend GPU's with at least 8GB of memory - HDRP and SVL Simulator are very GPU-intensive. HDRP Asset Conversion top # If you are converting non-HDRP assets to HDRP, there are a few things to watch out for. Before importing assets into HDRP, be sure to remove all custom shaders Change materials to use Unity's standard shaders Use Unity's tools and documentation to properly convert assets HDRP shader issues top # After pulling major changes to SVL Simulator, or a crash, HDRP may get corrupted. This is a known issue but it can be fixed easily: Close Unity Navigate to the root folder of SVL Simulator that you cloned Find Library folder Delete Library folder Open Unity. Wait while Unity creates the Library folder Close Unity and open again Unity Editor Tips top # There are many ways to fix, prevent issues, and improve workflows with Unity Editor. Turn off scene lighting in Scene View Panel. This adds lighting to the scene view camera because SVL Simulator adds lights at runtime When opening a prefab or a environment scene you may see a cyan texture on meshes; this is fine. This is Unity compiling a shader - DO NOT close Unity or try to do anything until it is finished. Before making a bundle build, be sure to open assets in the editor so Unity can do this first. Set Script Changes While Playing to Stop Playing and Recompile in Edit -> Preferences -> General window. This prevents bad states after changing code and leaving the Editor in play mode. It is also recommended to toggle off Auto Refresh and use Ctrl+R to recompile but it is up to your preferences. Change the default Unity Editor Layout by moving the Console View Panel next to the Project View Panel. This allows you to see any errors easily and make better screen captures for any issues you report. If you have issues with an asset, you can right click and choose Reimport. This helps solve many issues with individual assets. SVL Simulator uses many Editor Gizmos to visualize tools. Be sure to enable this button in the Scene View Panel. Be sure to install support for both Windows and Linux. This is needed to make any asset bundle because SVL Simulator supports both of these targets. This image is an example of a Windows target but with Linux support.","title":"Unity help"},{"location":"support/unity-help/#hdrp-requirements","text":"HDRP targets high-end PCs and prioritizes high definition visuals but has specific system requirements. Target supported APIs are D3D11, D3D12, Metal, and Vulkan Architecture x86, x86_64 We highly recommend GPU's with at least 8GB of memory - HDRP and SVL Simulator are very GPU-intensive.","title":"HDRP Requirements"},{"location":"support/unity-help/#hdrp-asset-conversion","text":"If you are converting non-HDRP assets to HDRP, there are a few things to watch out for. Before importing assets into HDRP, be sure to remove all custom shaders Change materials to use Unity's standard shaders Use Unity's tools and documentation to properly convert assets","title":"HDRP Asset Conversion"},{"location":"support/unity-help/#hdrp-shader-issues","text":"After pulling major changes to SVL Simulator, or a crash, HDRP may get corrupted. This is a known issue but it can be fixed easily: Close Unity Navigate to the root folder of SVL Simulator that you cloned Find Library folder Delete Library folder Open Unity. Wait while Unity creates the Library folder Close Unity and open again","title":"HDRP shader issues"},{"location":"support/unity-help/#unity-editor-tips","text":"There are many ways to fix, prevent issues, and improve workflows with Unity Editor. Turn off scene lighting in Scene View Panel. This adds lighting to the scene view camera because SVL Simulator adds lights at runtime When opening a prefab or a environment scene you may see a cyan texture on meshes; this is fine. This is Unity compiling a shader - DO NOT close Unity or try to do anything until it is finished. Before making a bundle build, be sure to open assets in the editor so Unity can do this first. Set Script Changes While Playing to Stop Playing and Recompile in Edit -> Preferences -> General window. This prevents bad states after changing code and leaving the Editor in play mode. It is also recommended to toggle off Auto Refresh and use Ctrl+R to recompile but it is up to your preferences. Change the default Unity Editor Layout by moving the Console View Panel next to the Project View Panel. This allows you to see any errors easily and make better screen captures for any issues you report. If you have issues with an asset, you can right click and choose Reimport. This helps solve many issues with individual assets. SVL Simulator uses many Editor Gizmos to visualize tools. Be sure to enable this button in the Scene View Panel. Be sure to install support for both Windows and Linux. This is needed to make any asset bundle because SVL Simulator supports both of these targets. This image is an example of a Windows target but with Linux support.","title":"Unity Editor Tips"},{"location":"system-under-test/apollo-master-instructions/","text":"Running latest Apollo with SVL Simulator # These instructions are tested after the last commit enabling SVL Simulator with latest Apollo master. Commits after that are assumed to work as well, but not guaranteed. Big changes have recently been introduced in Apollo master; camera perception may not be working yet, and LiDAR perception is recently able to build but still unstable (and uses a lot of GPU memory making it challenging for Apollo to share an 8GB GPU with the SVL Simulator). Because of this we will be using the modular testing features of the SVL Simulator which allows the simulator to directly publish perception and traffic light messages. For those who used our fork of Apollo 5.0 before: please note the new step to select the correct setup mode in Dreamview . Table of Contents Getting Started Prerequisites Setup Docker Installing Docker CE Installing the NVIDIA Container Toolkit Cloning the Repository Building Apollo and bridge Adding a Vehicle Adding an HD Map Setting Clock Mode in Apollo Launching Apollo alongisde the Simulator Adding a Vehicle Adding an HD Map Getting Started top # This guide outlines the steps required to setup Apollo for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator . Prerequisites top # Ubuntu 18.04 or later NVIDIA graphics card (required for Perception) NVIDIA proprietary driver (>=410.48) must be installed Setup top # Docker top # Apollo is designed to run out of Docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made. Installing Docker CE top # To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps . Installing the NVIDIA Container Toolkit top # Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.39 Driver Version: 460.39 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:02:00.0 On | N/A | | 35% 54C P0 44W / 180W | 3048MiB / 8116MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available here . Cloning the Repository top # Clone latest Apollo using the following command: git clone https://github.com/ApolloAuto/apollo Building Apollo and bridge top # Now everything should be in place to build Apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE The Apollo build may fail on machines with less than 1GB of RAM per CPU core due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, either re-start it until it succeeds, but if it continues to fail (especially when running the linker) then you'll need to address the low memory situation by either adding more memory to your build machine or enabling or increasing available swap space . If your Apollo build is crashing on a 16GB machine with little or no swap, try setting it to 16GB. Adding a Vehicle to Apollo top # In order to run Apollo with the simulator, the calibration files for the vehicle you use in the simulator must be available in Apollo for perception module to work correctly. Currently, only calibration files for Lincoln 2017 MKZ are included in the latest Apollo. More calibration files may be added later. To add a new vehicle to Apollo: Create a new folder with the new vehicle's name under APOLLO_ROOT/modules/calibration/data . Copy all calibration files of Lincoln 2017 MKZ to the folder. Update vehicle parameters in vehicle_param.pb.txt if your new vehicle is different from Lincoln 2017 MKZ. Update LiDAR calibration files in velodyne_params folder. If you have different configuration for LiDAR with respect to novatel, you need to update LiDAR's height and extrinsics. Update camera calibration files in camera_params folder. You need to update camera intrinsics and extrinsics for your own camera and sensor configurations. You may also need to update other sensor calibration files if you are using them on your vehicle. After you add the new vehicle, you need to restart dreamview to refresh the vehicle list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh Adding an HD Map to Apollo top # An HD map in Apollo format corresponding to the environment you use in the simulator must be available in Apollo for modules like routing , planning , etc. to work. Currently, only HD map files for Borregas Ave are included in the the latest Apollo at this time. More HD maps may be added later. You can also download HD Maps for any map available in the Web UI Store ( for example ), and manually add them to /apollo/modules/map/data/ . Or you can export your own map file following Export Map Annotations , then generate routing_map and sim_map following these instructions . In our Apollo 5.0 fork, we have a script generate_map.sh to help users generate map files. You can copy that script to APOLLO_ROOT/scripts/ folder to use it. For details, please follow here . After you add the new map, you need to restart Dreamview to refresh the map list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh Alternatively, you can analyze top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 . Setting Clock Mode in Apollo (Optional) top # SVL Simulator has a clock sensor which outputs a simulated time that can be used as the reference time for Apollo. When used, message lag which may result from transporting large amounts of data between the Simulator and Apollo will not affect Apollo performance. In addition, the clock sensor allows Apollo to run with \"simulator time\" when used in \"stepped simulation\" mode as shown in Python API Quickstart Script 33 . To be able to use the clock sensor (which is part of the Apollo 6.0 (modular testing) sensor configuration), clock_mode must be set to MODE_MOCK in cyber.pb.conf . Launching Apollo alongside the Simulator top # Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch Apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about Dreamview not being build if you do not run the script from the /apollo directory. ./scripts/bootstrap_lgsvl.sh Launch bridge (inside docker container): ./scripts/bridge.sh Run the SVL Simulator outside of docker. See instructions for Running the Simulator Create a Simulation using the Random Traffic runtime template and selecting the BorregasAve map and Lincoln2017MKZ vehicle with the Apollo 6.0 (modular testing) sensor configuration Enter localhost:9090 as the Bridge Connection String (if apollo and the simulator are running on separate machines you will need to enter the IPv4 address of the machine running the bridge instead of localhost ) (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Run Simulation\" Open Apollo Dreamview in a browser by navigating to: localhost:8888 NEW for Apollo Master : Select the Mkz Lgsvl setup mode, from the menu to the left of the vehicle menu. Select the Lincoln2017MKZ LGSVL vehicle and Borregas Ave map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Planning , Prediction , Routing , and Control (there is no need to enable Perception or Traffic Light because the modular testing sensors directly publish perception and traffic light results). Navigate to the Route Editing tab. Select a destination by clicking on a lane-line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). ./dev_start.sh stop If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090 Adding a Vehicle top # Only calibration files for Lincoln 2017 MKZ is included in the latest Apollo at this time. More calibration files map be added later. Adding an HD Map top # Only HD map files for Borregas Ave is included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map from our map annotation tool , name it base_map.bin and then generate routing_map and sim_map following these Apollo map instructions .","title":"Latest Apollo"},{"location":"system-under-test/apollo-master-instructions/#getting-started","text":"This guide outlines the steps required to setup Apollo for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator .","title":"Getting Started"},{"location":"system-under-test/apollo-master-instructions/#prerequisites","text":"Ubuntu 18.04 or later NVIDIA graphics card (required for Perception) NVIDIA proprietary driver (>=410.48) must be installed","title":"Prerequisites"},{"location":"system-under-test/apollo-master-instructions/#setup","text":"","title":"Setup"},{"location":"system-under-test/apollo-master-instructions/#docker","text":"Apollo is designed to run out of Docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made.","title":"Docker"},{"location":"system-under-test/apollo-master-instructions/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"system-under-test/apollo-master-instructions/#installing-nvidia-docker","text":"Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.39 Driver Version: 460.39 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:02:00.0 On | N/A | | 35% 54C P0 44W / 180W | 3048MiB / 8116MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available here .","title":"Installing the NVIDIA Container Toolkit"},{"location":"system-under-test/apollo-master-instructions/#cloning-the-repository","text":"Clone latest Apollo using the following command: git clone https://github.com/ApolloAuto/apollo","title":"Cloning the Repository"},{"location":"system-under-test/apollo-master-instructions/#building-apollo-and-bridge","text":"Now everything should be in place to build Apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE The Apollo build may fail on machines with less than 1GB of RAM per CPU core due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, either re-start it until it succeeds, but if it continues to fail (especially when running the linker) then you'll need to address the low memory situation by either adding more memory to your build machine or enabling or increasing available swap space . If your Apollo build is crashing on a 16GB machine with little or no swap, try setting it to 16GB.","title":"Building Apollo and bridge"},{"location":"system-under-test/apollo-master-instructions/#adding-a-vehicle","text":"In order to run Apollo with the simulator, the calibration files for the vehicle you use in the simulator must be available in Apollo for perception module to work correctly. Currently, only calibration files for Lincoln 2017 MKZ are included in the latest Apollo. More calibration files may be added later. To add a new vehicle to Apollo: Create a new folder with the new vehicle's name under APOLLO_ROOT/modules/calibration/data . Copy all calibration files of Lincoln 2017 MKZ to the folder. Update vehicle parameters in vehicle_param.pb.txt if your new vehicle is different from Lincoln 2017 MKZ. Update LiDAR calibration files in velodyne_params folder. If you have different configuration for LiDAR with respect to novatel, you need to update LiDAR's height and extrinsics. Update camera calibration files in camera_params folder. You need to update camera intrinsics and extrinsics for your own camera and sensor configurations. You may also need to update other sensor calibration files if you are using them on your vehicle. After you add the new vehicle, you need to restart dreamview to refresh the vehicle list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh","title":"Adding a Vehicle"},{"location":"system-under-test/apollo-master-instructions/#adding-an-hd-map","text":"An HD map in Apollo format corresponding to the environment you use in the simulator must be available in Apollo for modules like routing , planning , etc. to work. Currently, only HD map files for Borregas Ave are included in the the latest Apollo at this time. More HD maps may be added later. You can also download HD Maps for any map available in the Web UI Store ( for example ), and manually add them to /apollo/modules/map/data/ . Or you can export your own map file following Export Map Annotations , then generate routing_map and sim_map following these instructions . In our Apollo 5.0 fork, we have a script generate_map.sh to help users generate map files. You can copy that script to APOLLO_ROOT/scripts/ folder to use it. For details, please follow here . After you add the new map, you need to restart Dreamview to refresh the map list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh Alternatively, you can analyze top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 .","title":"Adding an HD Map"},{"location":"system-under-test/apollo-master-instructions/#setting-clock-mode","text":"SVL Simulator has a clock sensor which outputs a simulated time that can be used as the reference time for Apollo. When used, message lag which may result from transporting large amounts of data between the Simulator and Apollo will not affect Apollo performance. In addition, the clock sensor allows Apollo to run with \"simulator time\" when used in \"stepped simulation\" mode as shown in Python API Quickstart Script 33 . To be able to use the clock sensor (which is part of the Apollo 6.0 (modular testing) sensor configuration), clock_mode must be set to MODE_MOCK in cyber.pb.conf .","title":"Setting Clock Mode in Apollo"},{"location":"system-under-test/apollo-master-instructions/#launching-apollo-alongide-the-simulator","text":"Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch Apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about Dreamview not being build if you do not run the script from the /apollo directory. ./scripts/bootstrap_lgsvl.sh Launch bridge (inside docker container): ./scripts/bridge.sh Run the SVL Simulator outside of docker. See instructions for Running the Simulator Create a Simulation using the Random Traffic runtime template and selecting the BorregasAve map and Lincoln2017MKZ vehicle with the Apollo 6.0 (modular testing) sensor configuration Enter localhost:9090 as the Bridge Connection String (if apollo and the simulator are running on separate machines you will need to enter the IPv4 address of the machine running the bridge instead of localhost ) (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Run Simulation\" Open Apollo Dreamview in a browser by navigating to: localhost:8888 NEW for Apollo Master : Select the Mkz Lgsvl setup mode, from the menu to the left of the vehicle menu. Select the Lincoln2017MKZ LGSVL vehicle and Borregas Ave map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Planning , Prediction , Routing , and Control (there is no need to enable Perception or Traffic Light because the modular testing sensors directly publish perception and traffic light results). Navigate to the Route Editing tab. Select a destination by clicking on a lane-line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). ./dev_start.sh stop If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090","title":"Launching Apollo alongisde the Simulator"},{"location":"system-under-test/apollo-master-instructions/#adding-a-vehicle","text":"Only calibration files for Lincoln 2017 MKZ is included in the latest Apollo at this time. More calibration files map be added later.","title":"Adding a Vehicle"},{"location":"system-under-test/apollo-master-instructions/#adding-an-hd-map","text":"Only HD map files for Borregas Ave is included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map from our map annotation tool , name it base_map.bin and then generate routing_map and sim_map following these Apollo map instructions .","title":"Adding an HD Map"},{"location":"system-under-test/apollo5-0-instructions/","text":"Running Apollo 5.0 with SVL Simulator # Table of Contents Getting Started Prerequisites Setup Docker Installing Docker CE Installing NVIDIA Container Toolkit Pulling LGSVL Docker image Cloning the Repository Building Apollo and bridge Launching Apollo alongisde the Simulator Supported Vehicles Adding an HD Map Getting Started top # This guide outlines the steps required to setup Apollo 5.0 for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator . Prerequisites top # Linux operating system (preferably Ubuntu 18.04 or later) NVIDIA graphics card (required for Perception) NVIDIA proprietary drivers must be installed. Apollo 5.0 does not support Volta, Turing, or Ampere architectures (this includes Titan V, GTX 16xx, and RTX GPUs). Setup top # Docker top # Apollo 5.0 is designed to be run inside of a Docker container. The working directory of this repository will be mounted as a volume when starting the container so the image will not need to be rebuilt each time a modification is made. Installing Docker CE top # To install Docker CE please refer to the official documentation . NOTE: Apollo 5.0 does not work if the Docker is started with sudo . We suggest following through with the post installation steps . Installing NVIDIA Container Toolkit top # Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed, enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.102.04 Driver Version: 450.102.04 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:02:00.0 On | N/A | | 41% 31C P8 21W / 260W | 270MiB / 11016MiB | 3% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1404 G /usr/lib/xorg/Xorg 20MiB | | 0 N/A N/A 1467 G /usr/bin/gnome-shell 57MiB | | 0 N/A N/A 2491 G /usr/lib/xorg/Xorg 131MiB | | 0 N/A N/A 2630 G /usr/bin/gnome-shell 44MiB | | 0 N/A N/A 3423 G ...AAAAAAAA== --shared-files 12MiB | +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available from the official documentation . Pulling LGSVL Docker image top # LGSVL maintains a Docker image to be used alongside this repository. The Docker image is available from Docker Hub . To pull the image use the following command: docker pull lgsvl/apollo-5.0 Cloning the Repository top # This repository includes the protobuf branch of lgsvl_msgs as a submodule. To make sure that the submodule is also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo-5.0.git Building Apollo and bridge top # Now everything should be in place to build Apollo. Apollo must be built from inside the container. To launch the container, navigate to the directory where the repository was cloned and enter: docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. Enter the container: docker/scripts/dev_into.sh Build Apollo 5.0 (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE: The Apollo 5.0 build may run out of memory and crash on some machines due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, either re-start it until it succeeds, or try changing line ~150 of function build() in apollo.sh as follows: Replace this: JOB_ARG=\"--jobs=$(nproc) --ram_utilization_factor 80\" With this, which will use all but two cores for the build: JOB_ARG=\"--jobs=$(expr $(nproc) - 2) --ram_utilization_factor 70\" Alternatively, you can analyze with top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 . If it continues to fail (especially when running the linker) then you'll need to address the low memory situation by either adding more memory to your build machine or enabling or increasing available swap space . If your Apollo 5.0 build is crashing on a 16GB machine with little or no swap, try setting it to 16GB. Launching Apollo alongside the Simulator top # Here we only describe only a simple case of driving from point A to point B using Apollo 5.0 and the SVL Simulator. Launch Apollo 5.0 Enter the container: docker/scripts/dev_into.sh Start Apollo 5.0: bootstrap.sh Note: You may receive errors about Dreamview not being built if you do not run the script from the /apollo directory. Start the bridge: bridge.sh Run the SVL Simulator (outside of Docker). See these instructions . Create a Simulation from the Random Traffic runtime template selecting the BorregasAve Map, the Lincoln2017MKZ Vehicle, and the Apollo 5.0 Sensor Configuration. Enter localhost:9090 as the Bridge IP . (Optional) Enable Traffic and Pedestrians. (Optional) Set the Time of Day and weather settings. Publish the simulation. Select the created simulation and click Run Simulation . Open Apollo 5.0 Dreamview in a browser by navigating to: localhost:8888 Select the Lincoln2017MKZ vehicle and BorregasAve map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Traffic Light , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane-line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the Docker container, run this in a new terminal (not inside the Docker container): docker/scripts/dev_start.sh stop If you are using ufw , it is easiest to completely disable the firewall to allow connections: sudo ufw disable If that is not possible, add the following rules: sudo ufw allow 8888 sudo ufw allow 9090 These are required even if running the simulator and Apollo on the same machine. Supported Vehicles top # Only the calibration files for Lincoln 2017 MKZ, Jaguar 2015 XE, and Hyundai 2018 Nexo are included in the the simulator branch of the LGSVL fork of Apollo 5.0 at this time. More calibration files may be added later. Adding an HD Map top # Some default maps in your Maps library have their HD map files included in the simulator branch of the LGSVL fork of Apollo 5.0. You can find other maps are in the Maps store . If you want to add a new HD map to Apollo 5.0, follow these steps: Either select a map in your Maps library and download the apollo50 HD map or export your own map from our map annotation tool as base_map.bin . Create a new map folder under APOLLO_ROOT/modules/map/data/ and put base_map.bin in the folder. Inside Apollo 5.0 Docker container: Generate map files required by Apollo 5.0: cd /apollo generate_map.sh YOUR_MAP_FOLDER_NAME You need to restart Dreamview to refresh the map list: bootstrap.sh stop && bootstrap.sh","title":"Apollo 5.0"},{"location":"system-under-test/apollo5-0-instructions/#getting-started","text":"This guide outlines the steps required to setup Apollo 5.0 for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator .","title":"Getting Started"},{"location":"system-under-test/apollo5-0-instructions/#prerequisites","text":"Linux operating system (preferably Ubuntu 18.04 or later) NVIDIA graphics card (required for Perception) NVIDIA proprietary drivers must be installed. Apollo 5.0 does not support Volta, Turing, or Ampere architectures (this includes Titan V, GTX 16xx, and RTX GPUs).","title":"Prerequisites"},{"location":"system-under-test/apollo5-0-instructions/#setup","text":"","title":"Setup"},{"location":"system-under-test/apollo5-0-instructions/#docker","text":"Apollo 5.0 is designed to be run inside of a Docker container. The working directory of this repository will be mounted as a volume when starting the container so the image will not need to be rebuilt each time a modification is made.","title":"Docker"},{"location":"system-under-test/apollo5-0-instructions/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . NOTE: Apollo 5.0 does not work if the Docker is started with sudo . We suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"system-under-test/apollo5-0-instructions/#installing-nvidia-container-toolkit","text":"Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed, enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.102.04 Driver Version: 450.102.04 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:02:00.0 On | N/A | | 41% 31C P8 21W / 260W | 270MiB / 11016MiB | 3% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1404 G /usr/lib/xorg/Xorg 20MiB | | 0 N/A N/A 1467 G /usr/bin/gnome-shell 57MiB | | 0 N/A N/A 2491 G /usr/lib/xorg/Xorg 131MiB | | 0 N/A N/A 2630 G /usr/bin/gnome-shell 44MiB | | 0 N/A N/A 3423 G ...AAAAAAAA== --shared-files 12MiB | +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available from the official documentation .","title":"Installing NVIDIA Container Toolkit"},{"location":"system-under-test/apollo5-0-instructions/#pulling-lgsvl-docker-image","text":"LGSVL maintains a Docker image to be used alongside this repository. The Docker image is available from Docker Hub . To pull the image use the following command: docker pull lgsvl/apollo-5.0","title":"Pulling LGSVL Docker image"},{"location":"system-under-test/apollo5-0-instructions/#cloning-the-repository","text":"This repository includes the protobuf branch of lgsvl_msgs as a submodule. To make sure that the submodule is also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo-5.0.git","title":"Cloning the Repository"},{"location":"system-under-test/apollo5-0-instructions/#building-apollo-and-bridge","text":"Now everything should be in place to build Apollo. Apollo must be built from inside the container. To launch the container, navigate to the directory where the repository was cloned and enter: docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. Enter the container: docker/scripts/dev_into.sh Build Apollo 5.0 (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE: The Apollo 5.0 build may run out of memory and crash on some machines due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, either re-start it until it succeeds, or try changing line ~150 of function build() in apollo.sh as follows: Replace this: JOB_ARG=\"--jobs=$(nproc) --ram_utilization_factor 80\" With this, which will use all but two cores for the build: JOB_ARG=\"--jobs=$(expr $(nproc) - 2) --ram_utilization_factor 70\" Alternatively, you can analyze with top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 . If it continues to fail (especially when running the linker) then you'll need to address the low memory situation by either adding more memory to your build machine or enabling or increasing available swap space . If your Apollo 5.0 build is crashing on a 16GB machine with little or no swap, try setting it to 16GB.","title":"Building Apollo and bridge"},{"location":"system-under-test/apollo5-0-instructions/#launching-apollo-alongide-the-simulator","text":"Here we only describe only a simple case of driving from point A to point B using Apollo 5.0 and the SVL Simulator. Launch Apollo 5.0 Enter the container: docker/scripts/dev_into.sh Start Apollo 5.0: bootstrap.sh Note: You may receive errors about Dreamview not being built if you do not run the script from the /apollo directory. Start the bridge: bridge.sh Run the SVL Simulator (outside of Docker). See these instructions . Create a Simulation from the Random Traffic runtime template selecting the BorregasAve Map, the Lincoln2017MKZ Vehicle, and the Apollo 5.0 Sensor Configuration. Enter localhost:9090 as the Bridge IP . (Optional) Enable Traffic and Pedestrians. (Optional) Set the Time of Day and weather settings. Publish the simulation. Select the created simulation and click Run Simulation . Open Apollo 5.0 Dreamview in a browser by navigating to: localhost:8888 Select the Lincoln2017MKZ vehicle and BorregasAve map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Traffic Light , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane-line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the Docker container, run this in a new terminal (not inside the Docker container): docker/scripts/dev_start.sh stop If you are using ufw , it is easiest to completely disable the firewall to allow connections: sudo ufw disable If that is not possible, add the following rules: sudo ufw allow 8888 sudo ufw allow 9090 These are required even if running the simulator and Apollo on the same machine.","title":"Launching Apollo alongisde the Simulator"},{"location":"system-under-test/apollo5-0-instructions/#supported-vehicles","text":"Only the calibration files for Lincoln 2017 MKZ, Jaguar 2015 XE, and Hyundai 2018 Nexo are included in the the simulator branch of the LGSVL fork of Apollo 5.0 at this time. More calibration files may be added later.","title":"Supported Vehicles"},{"location":"system-under-test/apollo5-0-instructions/#adding-an-hd-map","text":"Some default maps in your Maps library have their HD map files included in the simulator branch of the LGSVL fork of Apollo 5.0. You can find other maps are in the Maps store . If you want to add a new HD map to Apollo 5.0, follow these steps: Either select a map in your Maps library and download the apollo50 HD map or export your own map from our map annotation tool as base_map.bin . Create a new map folder under APOLLO_ROOT/modules/map/data/ and put base_map.bin in the folder. Inside Apollo 5.0 Docker container: Generate map files required by Apollo 5.0: cd /apollo generate_map.sh YOUR_MAP_FOLDER_NAME You need to restart Dreamview to refresh the map list: bootstrap.sh stop && bootstrap.sh","title":"Adding an HD Map"},{"location":"system-under-test/apollo5-0-json-example/","text":"Example JSON Configuration for an Apollo 5.0 Vehicle # Bridge Type top # CyberRT Published Topics top # Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/ins_stat GPS INS Status /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/lidar128/compensator/PointCloud2 Lidar /apollo/sensor/camera/front_6mm/image/compressed Main Camera /apollo/sensor/camera/front_12mm/image/compressed Telephoto Camera Subscribed Topics top # Topic Sensor Name /apollo/control Apollo Car Control Complete JSON Configuration top # [ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"RadarSensor\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.337, \"z\": 3.691, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/lidar128/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_12mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Example JSON Configuration for an Apollo 5.0 Vehicle [](#top)"},{"location":"system-under-test/apollo5-0-json-example/#bridge-type","text":"CyberRT","title":"Bridge Type"},{"location":"system-under-test/apollo5-0-json-example/#published-topics","text":"Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/ins_stat GPS INS Status /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/lidar128/compensator/PointCloud2 Lidar /apollo/sensor/camera/front_6mm/image/compressed Main Camera /apollo/sensor/camera/front_12mm/image/compressed Telephoto Camera","title":"Published Topics"},{"location":"system-under-test/apollo5-0-json-example/#subscribed-topics","text":"Topic Sensor Name /apollo/control Apollo Car Control","title":"Subscribed Topics"},{"location":"system-under-test/apollo5-0-json-example/#complete-json-configuration","text":"[ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"RadarSensor\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.337, \"z\": 3.691, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/lidar128/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_12mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Complete JSON Configuration"},{"location":"system-under-test/apollo6-0-instructions/","text":"Running Apollo 6.0 with SVL Simulator # These instructions have been tested with the r6.0.0 branch from the Apollo repository ( latest commit at the time of writing). Apollo 6.0 introduced major changes to the perception stack and as a result camera perception is not yet working, and LiDAR perception is unstable. Because of this, this guide will focus on using the modular testing feature of the SVL Simulator to simulate perception instead of using Apollo's perception module. For those who used our fork of Apollo 5.0 before: please note the new step to select the correct setup mode in Dreamview . Table of Contents Getting Started Prerequisites Setup Docker Installing Docker CE Installing NVIDIA Docker Cloning the Repository Building Apollo and the bridge Adding a Vehicle Adding an HD Map Launching Apollo alongisde the Simulator Adding a Vehicle Adding an HD Map Getting Started top # This guide outlines the steps required to setup Apollo for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator . Prerequisites top # Ubuntu 18.04 or later NVIDIA graphics card (required for perception and prediction) NVIDIA proprietary driver (>=410.48) must be installed Setup top # Docker top # Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made. Installing Docker CE top # To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps . Installing NVIDIA Docker top # Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.64 Driver Version: 440.64 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:01:00.0 On | N/A | | 27% 29C P8 7W / 180W | 579MiB / 8117MiB | 1% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1745 G /usr/lib/xorg/Xorg 40MiB | | 0 1862 G /usr/bin/gnome-shell 49MiB | | 0 4409 G /usr/lib/xorg/Xorg 223MiB | | 0 4545 G /usr/bin/gnome-shell 140MiB | | 0 4962 G ...uest-channel-token=10798087356903621100 27MiB | | 0 9570 G /proc/self/exe 50MiB | | 0 17619 G ...uest-channel-token=14399957398263092148 40MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Cloning the Repository top # Clone the Apollo using the following command: git clone https://github.com/ApolloAuto/apollo Checkout the r6.0.0 branch: cd apollo git checkout r6.0.0 Do not checkout the v6.0.0 tag as it does not include the latest changes. Building Apollo and the bridge top # Now everything should be in place to build Apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE The Apollo build may fail on machines with less than 1GB of RAM per CPU core due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, you can try re-start it until it succeeds, but if it continues to fail (especially when running the linker) then you'll need to address the low memory situation by either adding more memory to your build machine or enabling or increasing available swap space . If your Apollo build is crashing on a 16GB machine with little or no swap, try setting it to 16GB. Adding a Vehicle to Apollo top # In order to run Apollo with the simulator, the calibration files for the vehicle you use in the simulator must be available in Apollo for perception module to work correctly. Currently, only calibration files for Lincoln 2017 MKZ are included in the latest Apollo. More calibration files may be added later. To add a new vehicle to Apollo: Create a new folder with the new vehicle's name under APOLLO_ROOT/modules/calibration/data . Copy all calibration files of Lincoln 2017 MKZ to the folder. Update vehicle parameters in vehicle_param.pb.txt if your new vehicle is different from Lincoln 2017 MKZ. Update LiDAR calibration files in velodyne_params folder. If you have different configuration for LiDAR with respect to novatel, you need to update LiDAR's height and extrinsics. Update camera calibration files in camera_params folder. You need to update camera intrinsics and extrinsics for your own camera and sensor configurations. You may also need to update other sensor calibration files if you are using them on your vehicle. After you add the new vehicle, you need to restart Dreamview to refresh the vehicle list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh Adding an HD Map to Apollo top # An HD map in Apollo format corresponding to the environment you use in the simulator must be available in Apollo for modules like routing , planning , etc. to work. Currently, only HD map files for Borregas Ave are included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map file following Export Map Annotations , then generate routing_map and sim_map following these instructions . In our Apollo 5.0 fork, we have a script generate_map.sh ( link ) to help users generate map files. You can copy that script to APOLLO_ROOT/scripts/ folder to use it. For details, please follow here . After you add the new map, you need to restart Dreamview to refresh the map list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh Alternatively, you can analyze top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 . Launching Apollo alongside the Simulator top # Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch Apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about Dreamview not being built if you do not run the script from the /apollo directory. ./scripts/bootstrap_lgsvl.sh Launch bridge (inside docker container): ./scripts/bridge.sh Run the SVL Simulator outside of docker. See instructions for Running the Simulator Create a Simulation the BorregasAve map and Lincoln2017MKZ (Apollo 5.0) vehicle Enter localhost:9090 as the Bridge Connection String (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Play\" Open Apollo Dreamview in a browser by navigating to: localhost:8888 NEW for Apollo Master : Select the Mkz Lgsvl setup mode, from the menu to the left of the vehicle menu. Select the Lincoln2017MKZ LGSVL vehicle and Borregas Ave map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane-line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090 Adding a Vehicle top # Only calibration files for Lincoln 2017 MKZ is included in the latest Apollo at this time. More calibration files map be added later. Adding an HD Map top # Only HD map files for Borregas Ave is included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map from our map annotation tool , name it base_map.bin and then generate routing_map and sim_map following these instructions .","title":"Running Apollo 6.0 with SVL Simulator [](#top)"},{"location":"system-under-test/apollo6-0-instructions/#getting-started","text":"This guide outlines the steps required to setup Apollo for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator .","title":"Getting Started"},{"location":"system-under-test/apollo6-0-instructions/#prerequisites","text":"Ubuntu 18.04 or later NVIDIA graphics card (required for perception and prediction) NVIDIA proprietary driver (>=410.48) must be installed","title":"Prerequisites"},{"location":"system-under-test/apollo6-0-instructions/#setup","text":"","title":"Setup"},{"location":"system-under-test/apollo6-0-instructions/#docker","text":"Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made.","title":"Docker"},{"location":"system-under-test/apollo6-0-instructions/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"system-under-test/apollo6-0-instructions/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.64 Driver Version: 440.64 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:01:00.0 On | N/A | | 27% 29C P8 7W / 180W | 579MiB / 8117MiB | 1% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1745 G /usr/lib/xorg/Xorg 40MiB | | 0 1862 G /usr/bin/gnome-shell 49MiB | | 0 4409 G /usr/lib/xorg/Xorg 223MiB | | 0 4545 G /usr/bin/gnome-shell 140MiB | | 0 4962 G ...uest-channel-token=10798087356903621100 27MiB | | 0 9570 G /proc/self/exe 50MiB | | 0 17619 G ...uest-channel-token=14399957398263092148 40MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing NVIDIA Docker"},{"location":"system-under-test/apollo6-0-instructions/#cloning-the-repository","text":"Clone the Apollo using the following command: git clone https://github.com/ApolloAuto/apollo Checkout the r6.0.0 branch: cd apollo git checkout r6.0.0 Do not checkout the v6.0.0 tag as it does not include the latest changes.","title":"Cloning the Repository"},{"location":"system-under-test/apollo6-0-instructions/#building-apollo-and-bridge","text":"Now everything should be in place to build Apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE The Apollo build may fail on machines with less than 1GB of RAM per CPU core due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, you can try re-start it until it succeeds, but if it continues to fail (especially when running the linker) then you'll need to address the low memory situation by either adding more memory to your build machine or enabling or increasing available swap space . If your Apollo build is crashing on a 16GB machine with little or no swap, try setting it to 16GB.","title":"Building Apollo and the bridge"},{"location":"system-under-test/apollo6-0-instructions/#adding-a-vehicle","text":"In order to run Apollo with the simulator, the calibration files for the vehicle you use in the simulator must be available in Apollo for perception module to work correctly. Currently, only calibration files for Lincoln 2017 MKZ are included in the latest Apollo. More calibration files may be added later. To add a new vehicle to Apollo: Create a new folder with the new vehicle's name under APOLLO_ROOT/modules/calibration/data . Copy all calibration files of Lincoln 2017 MKZ to the folder. Update vehicle parameters in vehicle_param.pb.txt if your new vehicle is different from Lincoln 2017 MKZ. Update LiDAR calibration files in velodyne_params folder. If you have different configuration for LiDAR with respect to novatel, you need to update LiDAR's height and extrinsics. Update camera calibration files in camera_params folder. You need to update camera intrinsics and extrinsics for your own camera and sensor configurations. You may also need to update other sensor calibration files if you are using them on your vehicle. After you add the new vehicle, you need to restart Dreamview to refresh the vehicle list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh","title":"Adding a Vehicle"},{"location":"system-under-test/apollo6-0-instructions/#adding-an-hd-map","text":"An HD map in Apollo format corresponding to the environment you use in the simulator must be available in Apollo for modules like routing , planning , etc. to work. Currently, only HD map files for Borregas Ave are included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map file following Export Map Annotations , then generate routing_map and sim_map following these instructions . In our Apollo 5.0 fork, we have a script generate_map.sh ( link ) to help users generate map files. You can copy that script to APOLLO_ROOT/scripts/ folder to use it. For details, please follow here . After you add the new map, you need to restart Dreamview to refresh the map list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh Alternatively, you can analyze top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 .","title":"Adding an HD Map"},{"location":"system-under-test/apollo6-0-instructions/#launching-apollo-alongide-the-simulator","text":"Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch Apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about Dreamview not being built if you do not run the script from the /apollo directory. ./scripts/bootstrap_lgsvl.sh Launch bridge (inside docker container): ./scripts/bridge.sh Run the SVL Simulator outside of docker. See instructions for Running the Simulator Create a Simulation the BorregasAve map and Lincoln2017MKZ (Apollo 5.0) vehicle Enter localhost:9090 as the Bridge Connection String (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Play\" Open Apollo Dreamview in a browser by navigating to: localhost:8888 NEW for Apollo Master : Select the Mkz Lgsvl setup mode, from the menu to the left of the vehicle menu. Select the Lincoln2017MKZ LGSVL vehicle and Borregas Ave map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane-line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090","title":"Launching Apollo alongisde the Simulator"},{"location":"system-under-test/apollo6-0-instructions/#adding-a-vehicle","text":"Only calibration files for Lincoln 2017 MKZ is included in the latest Apollo at this time. More calibration files map be added later.","title":"Adding a Vehicle"},{"location":"system-under-test/apollo6-0-instructions/#adding-an-hd-map","text":"Only HD map files for Borregas Ave is included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map from our map annotation tool , name it base_map.bin and then generate routing_map and sim_map following these instructions .","title":"Adding an HD Map"},{"location":"system-under-test/apollo6-0-json-example/","text":"Example JSON Configuration for an Apollo 6.0 Vehicle # Bridge Type top # CyberRT Published Topics top # Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/ins_stat GPS INS Status /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/perception/obstacles 3D Ground Truth Sensor /apollo/perception/traffic_light Signal Sensor /apollo/sensor/conti_radar Radar (optional) /apollo/sensor/lidar128/compensator/PointCloud2 Lidar (optional) /apollo/sensor/camera/front_6mm/image/compressed Main Camera (optional) /apollo/sensor/camera/front_12mm/image/compressed Telephoto Camera (optional) Note: The Radar , LiDAR , and Cameras are not needed for modular testing since the perception module in Apollo will not be running. We suggest against including them unless their data is needed for other reasons. Subscribed Topics top # Topic Sensor Name /apollo/control Apollo Car Control Complete JSON Configuration excluding optional sensors top # [ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/traffic_light\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ] Complete JSON Configuration including optional sensors top # [ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/traffic_light\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"RadarSensor\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.337, \"z\": 3.691, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/lidar128/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_12mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Example JSON Configuration for an Apollo 6.0 Vehicle [](#top)"},{"location":"system-under-test/apollo6-0-json-example/#bridge-type","text":"CyberRT","title":"Bridge Type"},{"location":"system-under-test/apollo6-0-json-example/#published-topics","text":"Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/ins_stat GPS INS Status /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/perception/obstacles 3D Ground Truth Sensor /apollo/perception/traffic_light Signal Sensor /apollo/sensor/conti_radar Radar (optional) /apollo/sensor/lidar128/compensator/PointCloud2 Lidar (optional) /apollo/sensor/camera/front_6mm/image/compressed Main Camera (optional) /apollo/sensor/camera/front_12mm/image/compressed Telephoto Camera (optional) Note: The Radar , LiDAR , and Cameras are not needed for modular testing since the perception module in Apollo will not be running. We suggest against including them unless their data is needed for other reasons.","title":"Published Topics"},{"location":"system-under-test/apollo6-0-json-example/#subscribed-topics","text":"Topic Sensor Name /apollo/control Apollo Car Control","title":"Subscribed Topics"},{"location":"system-under-test/apollo6-0-json-example/#complete-json-configuration","text":"[ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/traffic_light\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Complete JSON Configuration excluding optional sensors"},{"location":"system-under-test/apollo6-0-json-example/#complete-json-configuration","text":"[ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/traffic_light\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"RadarSensor\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.337, \"z\": 3.691, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/lidar128/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_12mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Complete JSON Configuration excluding optional sensors"},{"location":"system-under-test/autoware-auto-instructions/","text":"Autoware.Auto with SVL Simulator # Table of Contents Overview Setup Requirements Install Docker CE Install NVIDIA Container Toolkit Simulator Installation Install Autoware.auto Install ADE Download Autoware.Auto top Run ADE container Install ROS 2 LGSVL Bridge 1. Using the Package Manager (preferred) 2. Building from source code Install ROS 2 LGSVL Messages Run Simulator alongside Autoware.Auto Start the Autoware.Auto containers without NVIDIA setup: Start the Autoware.Auto containers with NVIDIA setup: Build and start RViz in the running ADE container Start the SVL Simulator Launch ROS 2 LGSVL bridge Configure RViz to see the LIDAR point cloud Run Simulator alongside Autoware.Auto in ADE docker container 1. Build ADE docker container 2. Build and start ADE docker container 3. Launch Simulator in ADE docker container Run AVP Parking Demo in Autoware.Auto 1. Run ROS 2 LGSVL Bridge 2. Run AVP Sim launch file 3. Run RViz2 4. Run the Simulator 5. Setting Pose Estimate and Goal Pose Overview top # This guide describes setting up and using Autoware.Auto with the SVL Simulator. As Autoware.Auto is still under-development, full self-driving is not yet possible. This guide will focus on running individual modules which have been implemented. Setup top # Requirements top # Linux operating system NVIDIA graphics card Install Docker CE top # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps to run docker as a non-root user. Verify that you can run docker commands without sudo : # In a desktop shell docker run hello-world This command downloads a test image and runs it in a container. When the container runs, it prints a message and exits. Install NVIDIA Container Toolkit top # Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed, enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.59 Driver Version: 440.59 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available from the official documentation . Verify that NVIDIA Container Toolkit is working by running nvidia-smi inside a base CUDA container: # In a desktop shell docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi This will verify that Docker CE and NVIDIA Container Toolkit are installed and working properly. Simulator installation top # Download and extract the latest simulator release . (Optional) Download the latest PythonAPI release (make sure the release version matches the simulator) and install it using pip: # In a desktop shell cd PythonAPI pip3 install --user . Install Autoware.auto top # Install ADE top # # In a desktop shell # (from https://ade-cli.readthedocs.io/en/latest/install.html) cd ~/.local/bin wget https://gitlab.com/ApexAI/ade-cli/uploads/f6c47dc34cffbe90ca197e00098bdd3f/ade+x86_64 mv ade+x86_64 ade chmod +x ade ./ade --version # 4.0.0 ./ade update-cli ./ade --version # <latest-version> PATH=$PATH:~/.local/bin mkdir -p ~/adehome cd ~/adehome touch .adehome Download Autoware.Auto top # Download Autoware.Auto under the ~/adehome folder. # In a desktop shell cd ~/adehome git clone https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto.git Installation and Development: Setup guide guide for Autoware.auto. Each of the external dependency repos (ROS2 bridge, messages, etc.) must be cloned to the external folder under the source directory: mkdir -p ~/adehome/AutowareAuto/src/external/ cd ~/adehome/AutowareAuto/src/external/ Downloading the AutowareAuto \"messages\" module # This module is necessary for the further build progress. # in ~/adehome/AutowareAuto/src/external/ git clone https://gitlab.com/autowarefoundation/autoware.auto/autoware_auto_msgs.git Run ADE container # # in ~/adehome/AutowareAuto/ ade start --update To stop the ADE container run ade stop NOTE All the next build steps assume an ADE container is already running. Install ROS 2 LGSVL Bridge top # There are two ways of installing ros2-lgsvl-bridge: Using the Package Manager Building source code 1. Using the Package Manager (preferred) # # In the ade container sudo apt update sudo apt install ros-foxy-lgsvl-bridge # Test the bridge (then ctrl-c to stop the bridge): lgsvl_bridge Note: If sudo apt update returns apt update: signatures were invalid in ADE, try updating the repo key with curl http://repo.ros2.org/repos.key | sudo apt-key add - as noted in answers.ros.org . 2. Building from source code # External modules folder # Downloading the ROS2 bridge # # in ~/adehome/AutowareAuto/src/external/ git clone https://github.com/lgsvl/ros2-lgsvl-bridge.git cd ros2-lgsvl-bridge git checkout foxy-devel Building the ROS2 bridge # Refer to README.md in the repo. ade enter # In the ade container cd ~/AutowareAuto colcon build --packages-select lgsvl_bridge --cmake-args '-DCMAKE_BUILD_TYPE=Release' Running the ROS2 bridge # Refer to README.md in the repo. ade enter # In the ade container source ~/AutowareAuto/install/setup.bash lgsvl_bridge Install ROS 2 LGSVL Messages top # Downloading # # in ~/adehome/AutowareAuto/src/external/ git clone https://github.com/lgsvl/lgsvl_msgs.git Building # ade enter # In the ade container source ~/AutowareAuto/src/external/ros2-lgsvl-bridge/install/setup.bash lgsvl_bridge cd ~/AutowareAuto colcon build --cmake-args '-DCMAKE_BUILD_TYPE=Release' As an example of a successful build, observe in the console output something like: ... Summary: 115 packages finished [8min 5s] ... Note: You may want to build only lgsvl_msgs package with the following command: colcon build --packages-select lgsvl_msgs --cmake-args '-DCMAKE_BUILD_TYPE=Release' Testing # Note: the ROS2 Foxy has deprecated the ros2 msg list command, use the ros2 interface list instead. ade enter # In the ade container cd ~/AutowareAuto source install/setup.bash ros2 interface list | grep lgsvl_msgs # If you can see the list of lgsvl_msgs, they're ready to be used. # Example: ... lgsvl_msgs/CanBusData lgsvl_msgs/VehicleControlData lgsvl_msgs/VehicleStateData ... Run Simulator alongside Autoware.Auto top # The ROS 2 web bridge allows the simulator and Autoware.auto to communicate. To test this connection we can visualize sensor data from the simulator in rviz2 (running in the Autoware.auto container). Start the Autoware.Auto containers without NVIDIA setup: # NOTE Stop any ADE container if it is already running: $ ade stop # In a desktop shell cd ~/adehome/AutowareAuto source .aderc-amd64-foxy-lgsvl ade start Start the Autoware.Auto containers with NVIDIA setup: # NOTE Stop any ADE container if it is already running: $ ade stop Create a aderc file which has nvidia setup: # In a desktop shell vim ~/adehome/AutowareAuto/.aderc-amd64-foxy-lgsvl-nvidia Paste the next block into the .aderc-amd64-foxy-lgsvl-nvidia file: export ADE_DOCKER_RUN_ARGS=\"--cap-add=SYS_PTRACE --net=host --privileged --add-host ade:127.0.0.1 -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --runtime=nvidia -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e NVIDIA_VISIBLE_DEVICES=all \\ -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,display\" export ADE_GITLAB=gitlab.com export ADE_REGISTRY=registry.gitlab.com export ADE_DISABLE_NVIDIA_DOCKER=false export ADE_IMAGES=\" registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/ade-foxy:master registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/binary-foxy:master registry.gitlab.com/autowarefoundation/autoware.auto/ade-lgsvl/foxy:2020.06 nvidia/cuda:11.0-base \" (Stop and re-) start the ADE container: # In a desktop shell, first stop ADE container if it was previously running ade stop cd ~/adehome/AutowareAuto source .aderc-amd64-foxy-lgsvl-nvidia ade start ade enter Build and start RViz in the running ADE container # ade enter # In the ADE container cd ~/AutowareAuto colcon build --cmake-args '-DCMAKE_BUILD_TYPE=Release' source ~/AutowareAuto/install/setup.bash Start rviz2: # In the ade container rviz2 -d /home/\"${USER}\"/AutowareAuto/install/autoware_auto_examples/share/autoware_auto_examples/rviz2/autoware.rviz Start the SVL Simulator # Outside of the ADE container launch the executable and click on the OPEN BROWSER button to open the web UI. bash $ (path\\to\\downloaded\\simulator)/svlsimulator-linux64-2021.3/simulator In the Vehicles tab under Library look for Lexus2016RXHybrid . If not available, see the Library page to add it. Make sure that Autoware.Auto sensor configuration has the ROS2 bridge and all of sensors are added. Click Vehicles under Library in the left side and click Lexus2016RXHybrid and click Autoware.Auto in Sensor Configurations. If you can see i mark next to sensor name, click Add to Library button to add sensor plugins into library. Switch to the Simulations tab and click the Add new button: Enter a Simulation Name and click Next. Select Random Traffic in Runtime Template. Select a map from the drop down menu. If none are available follow this guide to get a map. Select the Lexus2016RXHybrid from the drop down menu of Vehicle. Select the Autoware.Auto in Sensor Configuration and click Next. Select Autoware.Auto (Apex.AI) in Autopilot and enter the bridge address (default: localhost:9090 ) in Bridge IP box and click Next. Click Publish. Press Run Simulation button. Launch ROS 2 LGSVL bridge # (in a new terminal window): ade enter # In the ADE container source ~/AutowareAuto/install/setup.bash lgsvl_bridge Configure RViz to see the LIDAR point cloud # In the RViz Displays panel (top left on the screenshot below): Set the Fixed Frame (under Global Options) to lidar_front . Verify either a PointCloud2 or a Transformed Points message is added. Set the message's Topic field to the /lidar_front/points_raw topic. Run Simulator alongside Autoware.Auto in ADE docker container top # Those who wish to run SVL Simulator inside ADE might find the following instructions helpful, but it is recommended that users instead run SVL Simulator on the host (not from inside ADE). 1. Build ADE docker container # Copy the following Dockerfile into ~/adehome/AutowareAuto/tools/ade_image folder. FROM ubuntu:20.04 CMD [\"bash\"] ARG ROS_DISTRO=foxy # https://docs.ros.org/en/foxy/Installation/Ubuntu-Development-Setup.html RUN apt update && apt install locales RUN locale-gen en_US en_US.UTF-8 RUN update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8 RUN export LANG=en_US.UTF-8 # tz America/Los_Angeles ENV TZ=America/Los_Angeles RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone RUN apt-get install -y tzdata RUN apt update && apt install -y curl gnupg2 lsb-release python3-pip gettext RUN curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg RUN echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" | tee /etc/apt/sources.list.d/ros2.list > /dev/null # /https://docs.ros.org/en/foxy/Installation/Ubuntu-Development-Setup.html RUN apt-get update && \\ apt-get install -y \\ python3-vcstool \\ ros-$ROS_DISTRO-cyclonedds \\ ros-$ROS_DISTRO-rmw-cyclonedds-cpp && \\ rm -rf /var/lib/apt/lists/* #/tmp/ros-deps COPY apt-packages /tmp/ RUN apt-get update && \\ apt-get install -y \\ $(cat /tmp/apt-packages | cut -d# -f1 | envsubst) \\ && rm -rf /var/lib/apt/lists/* /tmp/apt-packages RUN echo 'ALL ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers RUN echo 'Defaults env_keep += \"DEBUG ROS_DISTRO\"' >> /etc/sudoers COPY pip3-packages /tmp/ RUN pip3 install -U \\ $(cut -d# -f1 </tmp/pip3-packages) \\ && rm -rf /root/.cache /tmp/pip-* /tmp/pip3-packages RUN git clone https://github.com/rigtorp/udpreplay && mkdir -p udpreplay/build \\ && cd udpreplay/build && cmake .. && make && make install \\ && cd - && rm -rf udpreplay/ COPY bashrc-git-prompt / RUN cat /bashrc-git-prompt >> /etc/skel/.bashrc && \\ rm /bashrc-git-prompt COPY gdbinit /etc/gdb/ # ===================== CLEAN UP ZONE ===================== # # Commands in the cleanup zone will be cleaned up before every release # and put into the correct place. RUN apt-get update \\ && apt-get install -y \\ unzip \\ ca-certificates \\ libx11-6 \\ libxau6 \\ libxcb1 \\ libxdmcp6 \\ libxext6 \\ libvulkan1 \\ libgl1 \\ libgtk2.0-0 \\ vulkan-utils \\ docker.io \\ xdg-utils \\ && apt-get clean ADD \"https://gitlab.com/nvidia/container-images/vulkan/raw/master/nvidia_icd.json\" /etc/vulkan/icd.d/nvidia_icd.json RUN chmod 644 /etc/vulkan/icd.d/nvidia_icd.json # ===================== END OF CLEAN UP ZONE ===================== # # Add print url into tty part. COPY print-tty.desktop /usr/share/applications/print-tty.desktop COPY print-tty.sh /usr/bin/print-tty.sh RUN chmod 755 /usr/bin/print-tty.sh RUN apt-get update \\ && apt-get install -y \\ ros-$ROS_DISTRO-lgsvl-bridge \\ && apt-get clean # /Add print url into tty part. # Do full package upgrade as last step # to avoid disrupting layer caching RUN apt-get update && \\ apt-get -y dist-upgrade && \\ rm -rf /var/lib/apt/lists/* COPY env.sh /etc/profile.d/ade_env.sh COPY gitconfig /etc/gitconfig COPY entrypoint /ade_entrypoint COPY colcon-defaults.yaml /usr/local/etc/colcon-defaults.yaml RUN echo \"export COLCON_DEFAULTS_FILE=/usr/local/etc/colcon-defaults.yaml\" >> \\ /etc/skel/.bashrc ENTRYPOINT [\"/ade_entrypoint\"] CMD [\"/bin/sh\", \"-c\", \"trap 'exit 147' TERM; tail -f /dev/null & wait ${!}\"] Copy the following print-tty.desktop file into ~/adehome/AutowareAuto/tools/ade_image folder. [Desktop Entry] Name=PrintTty GenericName=Print On Tty Comment=Prints text on first tty TryExec=echo Exec=print-tty.sh %F Terminal=true Type=Application MimeType=text/english;text/plain;text/x-makefile;text/x-c++hdr;text/x-c++src;text/x-chdr;text/x-csrc;text/x-java;text/x-moc;text/x-pascal;text/x-tcl;text/x-tex;application/x-shellscript;text/x-c;text/x-c++;application/pdf;application/rdf+xml;application/rss+xml;application/xhtml+xml;application/xhtml_xml;application/xml;image/gif;image/jpeg;image/png;image/webp;text/html;text/xml;x-scheme-handler/ftp;x-scheme-handler/http;x-scheme-handler/https; Copy the following print-tty.sh file into ~/adehome/AutowareAuto/tools/ade_image folder. #!/bin/sh echo \"$@\" >/dev/tty Build docker container and tag it: cd ~/adehome/AutowareAuto/tools/ade_image docker build -t registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/ade-foxy:local . 2. Build and start ADE docker container # Copy the following .aderc-amd64-foxy-lgsvl-nvidia into ~/adehome/AutowareAuto folder: export ADE_DOCKER_RUN_ARGS=\"--cap-add=SYS_PTRACE --net=host --privileged --add-host ade:127.0.0.1 -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --runtime=nvidia -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e NVIDIA_VISIBLE_DEVICES=all \\ -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,display\" export ADE_GITLAB=gitlab.com export ADE_REGISTRY=registry.gitlab.com export ADE_DISABLE_NVIDIA_DOCKER=false export ADE_IMAGES=\" registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/ade-foxy:local registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/binary-foxy:master registry.gitlab.com/apexai/ade-lgsvl:2021.2.1 nvidia/cuda:11.0-base \" Note: If you have registry.gitlab.com/apexai/ade-lgsvl:2021.2.1 simulator docker container, you may include it in .aderc-amd64-foxy-lgsvl-nvidia file (as in the example above); but if you don't you can mount local simulator into /opt/lgsvl in ade docker container when you start ade container. Source the .aderc-amd64-foxy-lgsvl-nvidia: source .aderc-amd64-foxy-lgsvl-nvidia Unzip Simulator in desktop shell: unzip svlsimulator-linux64-2021.2.1.zip -d ~ Start ade: ade start -- -e DISPLAY -e XAUTHORITY=/tmp/.Xauthority \\ -v ${XAUTHORITY}:/tmp/.Xauthority -v /tmp/.X11-unix:/tmp/.X11-unix \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /tmp/LGElectronics/:/tmp/LGElectronics/ \\ -v ~/svlsimulator-linux64-2021.2.1:/opt/lgsvl 3. Launch Simulator in ADE docker container # ade enter lgsvl_bridge & /opt/lgsvl/simulator Click link to cluster and URL is printed on the terminal. Copy and paste the URL on your web browser to open the SVL Simulator user interface. Run AVP Parking Demo in Autoware.Auto # This explains running the pre-built Autoware.Auto from the ADE docker container. If you want to run Autoware.Auto built from source code, refer to How to build the code . 1. Run ROS 2 LGSVL Bridge # Check if you installed ROS 2 LGSVL Bridge. If not, Install the ROS 2 LGSVL Bridge . # In the ade container source /opt/AutowareAuto/setup.bash lgsvl_bridge 2. Run AVP Sim launch file # # In the ade container source /opt/AutowareAuto/setup.bash ros2 launch /opt/AutowareAuto/src/launch/autoware_demos/launch/avp_sim.launch.py 3. Run RViz2 # # In the ade container source /opt/AutowareAuto/setup.bash rviz2 -d /opt/AutowareAuto/share/autoware_auto_launch/config/avp.rviz rviz2 will load the AutonomouStuff map for the parking demo. 4. Run the Simulator # # In a desktop shell, launch SVL Simulator (path\\to\\downloaded\\simulator)/svlsimulator-linux64-2021.3/simulator Simulation setup should be as follows: Runtime Template: Random Traffic Maps: AutonomouStuff Vehicle Asset: Lexus2016RXHybrid Sensor configuration: Autoware.Auto Bridge: ROS2 Autopilot: Autoware.Auto (Apex.AI) Connection: localhost:9090 (In the case of the Simulator and Autoware.Auto running on the same machine.) 5. Setting Pose Estimate and Goal Pose # Click 2D Pose Estimate Click to the initial position and drag downwards to set ego's direction. Click 2D Goal Pose Click on the desired parking spot and drag perpendicular to the lane. Depending on the parking spot, the planner may or may not succeed in finding a route.","title":"Autoware.Auto"},{"location":"system-under-test/autoware-auto-instructions/#general","text":"This guide describes setting up and using Autoware.Auto with the SVL Simulator. As Autoware.Auto is still under-development, full self-driving is not yet possible. This guide will focus on running individual modules which have been implemented.","title":"Overview"},{"location":"system-under-test/autoware-auto-instructions/#setup","text":"","title":"Setup"},{"location":"system-under-test/autoware-auto-instructions/#requirements","text":"Linux operating system NVIDIA graphics card","title":"Requirements"},{"location":"system-under-test/autoware-auto-instructions/#install-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps to run docker as a non-root user. Verify that you can run docker commands without sudo : # In a desktop shell docker run hello-world This command downloads a test image and runs it in a container. When the container runs, it prints a message and exits.","title":"Install Docker CE"},{"location":"system-under-test/autoware-auto-instructions/#install-nvidia-container-toolkit","text":"Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed, enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.59 Driver Version: 440.59 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available from the official documentation . Verify that NVIDIA Container Toolkit is working by running nvidia-smi inside a base CUDA container: # In a desktop shell docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi This will verify that Docker CE and NVIDIA Container Toolkit are installed and working properly.","title":"Install NVIDIA Container Toolkit"},{"location":"system-under-test/autoware-auto-instructions/#simulator-installation","text":"Download and extract the latest simulator release . (Optional) Download the latest PythonAPI release (make sure the release version matches the simulator) and install it using pip: # In a desktop shell cd PythonAPI pip3 install --user .","title":"Simulator Installation"},{"location":"system-under-test/autoware-auto-instructions/#install-autoware-auto","text":"","title":"Install Autoware.auto"},{"location":"system-under-test/autoware-auto-instructions/#Install-ADE","text":"# In a desktop shell # (from https://ade-cli.readthedocs.io/en/latest/install.html) cd ~/.local/bin wget https://gitlab.com/ApexAI/ade-cli/uploads/f6c47dc34cffbe90ca197e00098bdd3f/ade+x86_64 mv ade+x86_64 ade chmod +x ade ./ade --version # 4.0.0 ./ade update-cli ./ade --version # <latest-version> PATH=$PATH:~/.local/bin mkdir -p ~/adehome cd ~/adehome touch .adehome","title":"Install ADE"},{"location":"system-under-test/autoware-auto-instructions/#download-autowareauto-top","text":"Download Autoware.Auto under the ~/adehome folder. # In a desktop shell cd ~/adehome git clone https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto.git Installation and Development: Setup guide guide for Autoware.auto. Each of the external dependency repos (ROS2 bridge, messages, etc.) must be cloned to the external folder under the source directory: mkdir -p ~/adehome/AutowareAuto/src/external/ cd ~/adehome/AutowareAuto/src/external/","title":"Download Autoware.Auto top"},{"location":"system-under-test/autoware-auto-instructions/#run-ade-container","text":"# in ~/adehome/AutowareAuto/ ade start --update To stop the ADE container run ade stop NOTE All the next build steps assume an ADE container is already running.","title":"Run ADE container"},{"location":"system-under-test/autoware-auto-instructions/#install-ros2-lgsvl-bridge","text":"There are two ways of installing ros2-lgsvl-bridge: Using the Package Manager Building source code","title":"Install ROS 2 LGSVL Bridge"},{"location":"system-under-test/autoware-auto-instructions/#1-using-the-package-manager-preferred","text":"# In the ade container sudo apt update sudo apt install ros-foxy-lgsvl-bridge # Test the bridge (then ctrl-c to stop the bridge): lgsvl_bridge Note: If sudo apt update returns apt update: signatures were invalid in ADE, try updating the repo key with curl http://repo.ros2.org/repos.key | sudo apt-key add - as noted in answers.ros.org .","title":"1. Using the Package Manager (preferred)"},{"location":"system-under-test/autoware-auto-instructions/#2-building-from-source-code","text":"","title":"2. Building from source code"},{"location":"system-under-test/autoware-auto-instructions/#install-ros2-lgsvl-messages","text":"","title":"Install ROS 2 LGSVL Messages"},{"location":"system-under-test/autoware-auto-instructions/#run-simulator-alongside-autoware-auto","text":"The ROS 2 web bridge allows the simulator and Autoware.auto to communicate. To test this connection we can visualize sensor data from the simulator in rviz2 (running in the Autoware.auto container).","title":"Run Simulator alongside Autoware.Auto"},{"location":"system-under-test/autoware-auto-instructions/#start-the-autowareauto-containers-without-nvidia-setup","text":"NOTE Stop any ADE container if it is already running: $ ade stop # In a desktop shell cd ~/adehome/AutowareAuto source .aderc-amd64-foxy-lgsvl ade start","title":"Start the Autoware.Auto containers without NVIDIA setup:"},{"location":"system-under-test/autoware-auto-instructions/#start-the-autowareauto-containers-with-nvidia-setup","text":"NOTE Stop any ADE container if it is already running: $ ade stop Create a aderc file which has nvidia setup: # In a desktop shell vim ~/adehome/AutowareAuto/.aderc-amd64-foxy-lgsvl-nvidia Paste the next block into the .aderc-amd64-foxy-lgsvl-nvidia file: export ADE_DOCKER_RUN_ARGS=\"--cap-add=SYS_PTRACE --net=host --privileged --add-host ade:127.0.0.1 -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --runtime=nvidia -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e NVIDIA_VISIBLE_DEVICES=all \\ -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,display\" export ADE_GITLAB=gitlab.com export ADE_REGISTRY=registry.gitlab.com export ADE_DISABLE_NVIDIA_DOCKER=false export ADE_IMAGES=\" registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/ade-foxy:master registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/binary-foxy:master registry.gitlab.com/autowarefoundation/autoware.auto/ade-lgsvl/foxy:2020.06 nvidia/cuda:11.0-base \" (Stop and re-) start the ADE container: # In a desktop shell, first stop ADE container if it was previously running ade stop cd ~/adehome/AutowareAuto source .aderc-amd64-foxy-lgsvl-nvidia ade start ade enter","title":"Start the Autoware.Auto containers with NVIDIA setup:"},{"location":"system-under-test/autoware-auto-instructions/#build-and-start-rviz-in-the-running-ade-container","text":"ade enter # In the ADE container cd ~/AutowareAuto colcon build --cmake-args '-DCMAKE_BUILD_TYPE=Release' source ~/AutowareAuto/install/setup.bash Start rviz2: # In the ade container rviz2 -d /home/\"${USER}\"/AutowareAuto/install/autoware_auto_examples/share/autoware_auto_examples/rviz2/autoware.rviz","title":"Build and start RViz in the running ADE container"},{"location":"system-under-test/autoware-auto-instructions/#start-the-svl-simulator","text":"Outside of the ADE container launch the executable and click on the OPEN BROWSER button to open the web UI. bash $ (path\\to\\downloaded\\simulator)/svlsimulator-linux64-2021.3/simulator In the Vehicles tab under Library look for Lexus2016RXHybrid . If not available, see the Library page to add it. Make sure that Autoware.Auto sensor configuration has the ROS2 bridge and all of sensors are added. Click Vehicles under Library in the left side and click Lexus2016RXHybrid and click Autoware.Auto in Sensor Configurations. If you can see i mark next to sensor name, click Add to Library button to add sensor plugins into library. Switch to the Simulations tab and click the Add new button: Enter a Simulation Name and click Next. Select Random Traffic in Runtime Template. Select a map from the drop down menu. If none are available follow this guide to get a map. Select the Lexus2016RXHybrid from the drop down menu of Vehicle. Select the Autoware.Auto in Sensor Configuration and click Next. Select Autoware.Auto (Apex.AI) in Autopilot and enter the bridge address (default: localhost:9090 ) in Bridge IP box and click Next. Click Publish. Press Run Simulation button.","title":"Start the SVL Simulator"},{"location":"system-under-test/autoware-auto-instructions/#launch-ros-2-lgsvl-bridge","text":"(in a new terminal window): ade enter # In the ADE container source ~/AutowareAuto/install/setup.bash lgsvl_bridge","title":"Launch ROS 2 LGSVL bridge"},{"location":"system-under-test/autoware-auto-instructions/#configure-rviz-to-see-the-lidar-point-cloud","text":"In the RViz Displays panel (top left on the screenshot below): Set the Fixed Frame (under Global Options) to lidar_front . Verify either a PointCloud2 or a Transformed Points message is added. Set the message's Topic field to the /lidar_front/points_raw topic.","title":"Configure RViz to see the LIDAR point cloud"},{"location":"system-under-test/autoware-auto-instructions/#run-simulator-alongside-autoware-auto-in-ade-docker-container","text":"Those who wish to run SVL Simulator inside ADE might find the following instructions helpful, but it is recommended that users instead run SVL Simulator on the host (not from inside ADE).","title":"Run Simulator alongside Autoware.Auto in ADE docker container"},{"location":"system-under-test/autoware-auto-instructions/#1-build-ade-docker-container","text":"Copy the following Dockerfile into ~/adehome/AutowareAuto/tools/ade_image folder. FROM ubuntu:20.04 CMD [\"bash\"] ARG ROS_DISTRO=foxy # https://docs.ros.org/en/foxy/Installation/Ubuntu-Development-Setup.html RUN apt update && apt install locales RUN locale-gen en_US en_US.UTF-8 RUN update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8 RUN export LANG=en_US.UTF-8 # tz America/Los_Angeles ENV TZ=America/Los_Angeles RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone RUN apt-get install -y tzdata RUN apt update && apt install -y curl gnupg2 lsb-release python3-pip gettext RUN curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg RUN echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" | tee /etc/apt/sources.list.d/ros2.list > /dev/null # /https://docs.ros.org/en/foxy/Installation/Ubuntu-Development-Setup.html RUN apt-get update && \\ apt-get install -y \\ python3-vcstool \\ ros-$ROS_DISTRO-cyclonedds \\ ros-$ROS_DISTRO-rmw-cyclonedds-cpp && \\ rm -rf /var/lib/apt/lists/* #/tmp/ros-deps COPY apt-packages /tmp/ RUN apt-get update && \\ apt-get install -y \\ $(cat /tmp/apt-packages | cut -d# -f1 | envsubst) \\ && rm -rf /var/lib/apt/lists/* /tmp/apt-packages RUN echo 'ALL ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers RUN echo 'Defaults env_keep += \"DEBUG ROS_DISTRO\"' >> /etc/sudoers COPY pip3-packages /tmp/ RUN pip3 install -U \\ $(cut -d# -f1 </tmp/pip3-packages) \\ && rm -rf /root/.cache /tmp/pip-* /tmp/pip3-packages RUN git clone https://github.com/rigtorp/udpreplay && mkdir -p udpreplay/build \\ && cd udpreplay/build && cmake .. && make && make install \\ && cd - && rm -rf udpreplay/ COPY bashrc-git-prompt / RUN cat /bashrc-git-prompt >> /etc/skel/.bashrc && \\ rm /bashrc-git-prompt COPY gdbinit /etc/gdb/ # ===================== CLEAN UP ZONE ===================== # # Commands in the cleanup zone will be cleaned up before every release # and put into the correct place. RUN apt-get update \\ && apt-get install -y \\ unzip \\ ca-certificates \\ libx11-6 \\ libxau6 \\ libxcb1 \\ libxdmcp6 \\ libxext6 \\ libvulkan1 \\ libgl1 \\ libgtk2.0-0 \\ vulkan-utils \\ docker.io \\ xdg-utils \\ && apt-get clean ADD \"https://gitlab.com/nvidia/container-images/vulkan/raw/master/nvidia_icd.json\" /etc/vulkan/icd.d/nvidia_icd.json RUN chmod 644 /etc/vulkan/icd.d/nvidia_icd.json # ===================== END OF CLEAN UP ZONE ===================== # # Add print url into tty part. COPY print-tty.desktop /usr/share/applications/print-tty.desktop COPY print-tty.sh /usr/bin/print-tty.sh RUN chmod 755 /usr/bin/print-tty.sh RUN apt-get update \\ && apt-get install -y \\ ros-$ROS_DISTRO-lgsvl-bridge \\ && apt-get clean # /Add print url into tty part. # Do full package upgrade as last step # to avoid disrupting layer caching RUN apt-get update && \\ apt-get -y dist-upgrade && \\ rm -rf /var/lib/apt/lists/* COPY env.sh /etc/profile.d/ade_env.sh COPY gitconfig /etc/gitconfig COPY entrypoint /ade_entrypoint COPY colcon-defaults.yaml /usr/local/etc/colcon-defaults.yaml RUN echo \"export COLCON_DEFAULTS_FILE=/usr/local/etc/colcon-defaults.yaml\" >> \\ /etc/skel/.bashrc ENTRYPOINT [\"/ade_entrypoint\"] CMD [\"/bin/sh\", \"-c\", \"trap 'exit 147' TERM; tail -f /dev/null & wait ${!}\"] Copy the following print-tty.desktop file into ~/adehome/AutowareAuto/tools/ade_image folder. [Desktop Entry] Name=PrintTty GenericName=Print On Tty Comment=Prints text on first tty TryExec=echo Exec=print-tty.sh %F Terminal=true Type=Application MimeType=text/english;text/plain;text/x-makefile;text/x-c++hdr;text/x-c++src;text/x-chdr;text/x-csrc;text/x-java;text/x-moc;text/x-pascal;text/x-tcl;text/x-tex;application/x-shellscript;text/x-c;text/x-c++;application/pdf;application/rdf+xml;application/rss+xml;application/xhtml+xml;application/xhtml_xml;application/xml;image/gif;image/jpeg;image/png;image/webp;text/html;text/xml;x-scheme-handler/ftp;x-scheme-handler/http;x-scheme-handler/https; Copy the following print-tty.sh file into ~/adehome/AutowareAuto/tools/ade_image folder. #!/bin/sh echo \"$@\" >/dev/tty Build docker container and tag it: cd ~/adehome/AutowareAuto/tools/ade_image docker build -t registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/ade-foxy:local .","title":"1. Build ADE docker container"},{"location":"system-under-test/autoware-auto-instructions/#2-build-and-start-ade-docker-container","text":"Copy the following .aderc-amd64-foxy-lgsvl-nvidia into ~/adehome/AutowareAuto folder: export ADE_DOCKER_RUN_ARGS=\"--cap-add=SYS_PTRACE --net=host --privileged --add-host ade:127.0.0.1 -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --runtime=nvidia -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e NVIDIA_VISIBLE_DEVICES=all \\ -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,display\" export ADE_GITLAB=gitlab.com export ADE_REGISTRY=registry.gitlab.com export ADE_DISABLE_NVIDIA_DOCKER=false export ADE_IMAGES=\" registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/ade-foxy:local registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/binary-foxy:master registry.gitlab.com/apexai/ade-lgsvl:2021.2.1 nvidia/cuda:11.0-base \" Note: If you have registry.gitlab.com/apexai/ade-lgsvl:2021.2.1 simulator docker container, you may include it in .aderc-amd64-foxy-lgsvl-nvidia file (as in the example above); but if you don't you can mount local simulator into /opt/lgsvl in ade docker container when you start ade container. Source the .aderc-amd64-foxy-lgsvl-nvidia: source .aderc-amd64-foxy-lgsvl-nvidia Unzip Simulator in desktop shell: unzip svlsimulator-linux64-2021.2.1.zip -d ~ Start ade: ade start -- -e DISPLAY -e XAUTHORITY=/tmp/.Xauthority \\ -v ${XAUTHORITY}:/tmp/.Xauthority -v /tmp/.X11-unix:/tmp/.X11-unix \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /tmp/LGElectronics/:/tmp/LGElectronics/ \\ -v ~/svlsimulator-linux64-2021.2.1:/opt/lgsvl","title":"2. Build and start ADE docker container"},{"location":"system-under-test/autoware-auto-instructions/#3-launch-simulator-in-ade-docker-container","text":"ade enter lgsvl_bridge & /opt/lgsvl/simulator Click link to cluster and URL is printed on the terminal. Copy and paste the URL on your web browser to open the SVL Simulator user interface.","title":"3. Launch Simulator in ADE docker container"},{"location":"system-under-test/autoware-auto-instructions/#run-avp-parking-demo-in-autowareauto","text":"This explains running the pre-built Autoware.Auto from the ADE docker container. If you want to run Autoware.Auto built from source code, refer to How to build the code .","title":"Run AVP Parking Demo in Autoware.Auto"},{"location":"system-under-test/autoware-auto-instructions/#1-run-ros-2-lgsvl-bridge","text":"Check if you installed ROS 2 LGSVL Bridge. If not, Install the ROS 2 LGSVL Bridge . # In the ade container source /opt/AutowareAuto/setup.bash lgsvl_bridge","title":"1. Run ROS 2 LGSVL Bridge"},{"location":"system-under-test/autoware-auto-instructions/#2-run-avp-sim-launch-file","text":"# In the ade container source /opt/AutowareAuto/setup.bash ros2 launch /opt/AutowareAuto/src/launch/autoware_demos/launch/avp_sim.launch.py","title":"2. Run AVP Sim launch file"},{"location":"system-under-test/autoware-auto-instructions/#3-run-rviz2","text":"# In the ade container source /opt/AutowareAuto/setup.bash rviz2 -d /opt/AutowareAuto/share/autoware_auto_launch/config/avp.rviz rviz2 will load the AutonomouStuff map for the parking demo.","title":"3. Run RViz2"},{"location":"system-under-test/autoware-auto-instructions/#4-run-the-simulator","text":"# In a desktop shell, launch SVL Simulator (path\\to\\downloaded\\simulator)/svlsimulator-linux64-2021.3/simulator Simulation setup should be as follows: Runtime Template: Random Traffic Maps: AutonomouStuff Vehicle Asset: Lexus2016RXHybrid Sensor configuration: Autoware.Auto Bridge: ROS2 Autopilot: Autoware.Auto (Apex.AI) Connection: localhost:9090 (In the case of the Simulator and Autoware.Auto running on the same machine.)","title":"4. Run the Simulator"},{"location":"system-under-test/autoware-auto-instructions/#5-setting-pose-estimate-and-goal-pose","text":"Click 2D Pose Estimate Click to the initial position and drag downwards to set ego's direction. Click 2D Goal Pose Click on the desired parking spot and drag perpendicular to the lane. Depending on the parking spot, the planner may or may not succeed in finding a route.","title":"5. Setting Pose Estimate and Goal Pose"},{"location":"system-under-test/autoware-auto-json-example/","text":"Example JSON Configuration for an Autoware Auto Vehicle # Bridge Type top # ROS2 Published Topics top # Topic Sensor Name /lgsvl/state_report CAN Bus /gnss/fix GPS /lgsvl/gnss_odom GPS Odometry /imu/imu_raw IMU /lidar_front/points_raw LidarFront /lidar_rear/points_raw LidarRear /lgsvl/clock Simulation Clock Subscribed Topics top # Topic Sensor Name /lgsvl/vehicle_control_cmd Autoware Car Control /lgsvl/vehicle_state_cmd Autoware Auto Vehicle State Complete JSON Configuration top # [ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/lgsvl/state_report\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gnss/fix\", \"Frame\": \"gnss\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 30.0, \"Topic\": \"/lgsvl/gnss_odom\", \"Frame\": \"odom\", \"ChildFrame\": \"base_link\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"LidarFront\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/lidar_front/points_raw\", \"Frame\": \"lidar_front\" }, \"transform\": { \"x\": 0.022, \"y\": 1.49, \"z\": 1.498, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"LidarRear\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/lidar_rear/points_raw\", \"Frame\": \"lidar_rear\" }, \"transform\": { \"x\": 0.022, \"y\": 1.49, \"z\": 0.308, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LGSVLControlSensor\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/lgsvl/vehicle_control_cmd\" } }, { \"type\": \"VehicleStateSensor\", \"name\": \"Autoware Auto Vehicle State\", \"params\": { \"Topic\": \"/lgsvl/vehicle_state_cmd\" } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"ClockSensor\", \"name\": \"Simulation Clock\", \"params\": { \"Topic\": \"/lgsvl/clock\" } } ]","title":"Example JSON Configuration for an Autoware Auto Vehicle [](#top)"},{"location":"system-under-test/autoware-auto-json-example/#bridge-type","text":"ROS2","title":"Bridge Type"},{"location":"system-under-test/autoware-auto-json-example/#published-topics","text":"Topic Sensor Name /lgsvl/state_report CAN Bus /gnss/fix GPS /lgsvl/gnss_odom GPS Odometry /imu/imu_raw IMU /lidar_front/points_raw LidarFront /lidar_rear/points_raw LidarRear /lgsvl/clock Simulation Clock","title":"Published Topics"},{"location":"system-under-test/autoware-auto-json-example/#subscribed-topics","text":"Topic Sensor Name /lgsvl/vehicle_control_cmd Autoware Car Control /lgsvl/vehicle_state_cmd Autoware Auto Vehicle State","title":"Subscribed Topics"},{"location":"system-under-test/autoware-auto-json-example/#complete-json-configuration","text":"[ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/lgsvl/state_report\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gnss/fix\", \"Frame\": \"gnss\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 30.0, \"Topic\": \"/lgsvl/gnss_odom\", \"Frame\": \"odom\", \"ChildFrame\": \"base_link\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"LidarFront\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/lidar_front/points_raw\", \"Frame\": \"lidar_front\" }, \"transform\": { \"x\": 0.022, \"y\": 1.49, \"z\": 1.498, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"LidarRear\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/lidar_rear/points_raw\", \"Frame\": \"lidar_rear\" }, \"transform\": { \"x\": 0.022, \"y\": 1.49, \"z\": 0.308, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LGSVLControlSensor\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/lgsvl/vehicle_control_cmd\" } }, { \"type\": \"VehicleStateSensor\", \"name\": \"Autoware Auto Vehicle State\", \"params\": { \"Topic\": \"/lgsvl/vehicle_state_cmd\" } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"ClockSensor\", \"name\": \"Simulation Clock\", \"params\": { \"Topic\": \"/lgsvl/clock\" } } ]","title":"Complete JSON Configuration"},{"location":"system-under-test/autoware-instructions/","text":"Autoware.AI 1.14.0 with SVL Simulator # The software and source code in this repository are intended only for use with the SVL Simulator and should not be used in a real vehicle. Table of Contents General Setup Requirements Install Docker CE Install NVIDIA Container Toolkit Install SVL Simulator Install Autoware Launch Autoware Alongside SVL Simulator Driving by following vector map: Adding a Vehicle Adding an HD Map General top # This guide goes through how to run Autoware.AI with the SVL Simulator. In order to run Autoware with the SVL Simulator, it is easiest to pull the official Autoware Docker image (see the official guide, Case 1 for more details), but it is also possible to build Autoware from source . Autoware communicates with the SVL Simulator using the rosbridge_suite, which provides JSON interfacing with ROS publishers/subscribers. The official Autoware Docker containers have rosbridge_suite included. Setup top # Requirements top # Linux operating system NVIDIA graphics card Install Docker CE top # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps to run docker as a non-root user. Install NVIDIA Container Toolkit top # Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed, enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.59 Driver Version: 440.59 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available from the official documentation . Install SVL Simulator top # This guide outlines the steps required to setup Autoware.AI for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator . Install Autoware top # Make sure you have Git Large File Storage (LFS) installed before cloning the repository in the next step. If git lfs outputs git: 'lfs' is not a git command. , then you need to install it: Instructions for installation are here . Verify the installation: $ git lfs install Git LFS initialized. Create a directory called shared_dir in your home directory to hold HD maps and launch files for the simulator. The Autoware Docker container will mount this folder. mkdir ~/shared_dir cd ~/shared_dir git clone https://github.com/lgsvl/autoware-data.git If there wasn't a line beginning with Filtering content: output, then Git LFS hasn't been installed. Remove the autoware-data directory, install Git LFS with git lfs install , and then re-issue the git clone . Clone the docker repository from autoware.ai into a working directory: cd $WORKING_DIRECTORY git clone https://github.com/Autoware-AI/docker.git Launch Autoware Alongside SVL Simulator top # Run the Autoware 1.14.0 container and enter into it: cd $WORKING_DIRECTORY/docker/generic ./run.sh -t 1.14.0 If you get the usermod error as follows: usermod: user autoware is currently used by process 1 Check if the $UID is 1000 $ echo $UID If your $UID is 1000, you would not have usermod error. Otherwise, it's better to build container locally to avoid usermod error as follows: $ ./build.sh --version 1.14.0 $ ./run.sh -t local Once inside the container, install a missing ROS package: sudo apt update && sudo apt install ros-$ROS_DISTRO-image-transport-plugins -y If you need to check which $ROS_DISTRO you have installed run the following: ls /opt/ros/ Launch the runtime manager: roslaunch runtime_manager runtime_manager.launch A few terminals will open, as well as a GUI for the runtime manager. In the runtime manager, click on the 'Quick Start' tab and load the following launch files from ~/shared_dir/autoware-data/BorregasAve/ by clicking \"Ref\" to the right of each text box: my_map.launch my_sensing_simulator.launch my_localization.launch my_detection.launch my_mission_planning.launch my_motion_planning.launch Click \"Map\" to load the launch file pertaining to the HD maps. An \"Ok\" should appear to the right of the \"Ref\" button when successfully loaded. Then click \"Sensing\" which also launches rosbridge. Run the SVL Simulator Create a Simulation choosing BorregasAve map and Jaguar2015XE (Autoware) or another Autoware compatible vehicle. Select Autoware.AI under Autopilot selection Enter localhost:9090 for the Bridge Connection String. Run the created Simulation A vehicle should appear in Borregas Ave in Sunnyvale, CA. In the Autoware Runtime Manager, continue loading the other launch files - click \"Localization\" and wait for the time to display to the right of \"Ref\". Then click \"Rviz\" to launch Rviz - the vector map and location of the vehicle in the map should show. The vehicle may be mis-localized as the initial pose is important for NDT matching. To fix this, click \"2D Pose Estimate\" in Rviz, then click an approximate position for the vehicle on the map and drag in the direction it is facing before releasing the mouse button. This should allow NDT matching to find the vehicle pose (it may take a few tries). Note that the point cloud will not show up in rviz until ndt matching starts publishing a pose. An alternative would be to use GNSS for an initial pose or for localization. Open my_localization.launch file in map folder of autoware-data repository and uncomment nmea2tfpose part and comment ndt_matching instead. Driving by following vector map: # To drive following the HD map follow these steps: Start Mission Planning launch file in Autoware Runtime Manager. In rviz, mark a destination by clicking '2D Nav Goal' and clicking at the destination and dragging along the road direction. Make sure to only choose a route that looks valid along the lane centerlines that are marked with orange lines in rviz. If an invalid destination is selected nothing will change in rviz, and you will need to relaunch the Mission Planning launch file in the Quick Launch tab to try another destination. After choosing a valid destination the route will be highlighted in blue in rviz. Lastly, start Motion Planning launch file. then ego vehicle starts driving autonomously. Adding a Vehicle top # The default vehicles have the calibration files included in the LGSVL Autoware Data Github repository. Adding an HD Map top # The default maps have the Vector map files included in the LGSVL Autoware Data Github repository.","title":"ROS (Autoware.AI)"},{"location":"system-under-test/autoware-instructions/#general","text":"This guide goes through how to run Autoware.AI with the SVL Simulator. In order to run Autoware with the SVL Simulator, it is easiest to pull the official Autoware Docker image (see the official guide, Case 1 for more details), but it is also possible to build Autoware from source . Autoware communicates with the SVL Simulator using the rosbridge_suite, which provides JSON interfacing with ROS publishers/subscribers. The official Autoware Docker containers have rosbridge_suite included.","title":"General"},{"location":"system-under-test/autoware-instructions/#setup","text":"","title":"Setup"},{"location":"system-under-test/autoware-instructions/#requirements","text":"Linux operating system NVIDIA graphics card","title":"Requirements"},{"location":"system-under-test/autoware-instructions/#install-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps to run docker as a non-root user.","title":"Install Docker CE"},{"location":"system-under-test/autoware-instructions/#install-nvidia-container-toolkit","text":"Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed, enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.59 Driver Version: 440.59 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available from the official documentation .","title":"Install NVIDIA Container Toolkit"},{"location":"system-under-test/autoware-instructions/#install-lgsvl-simulator","text":"This guide outlines the steps required to setup Autoware.AI for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator .","title":"Install SVL Simulator"},{"location":"system-under-test/autoware-instructions/#install-autoware","text":"Make sure you have Git Large File Storage (LFS) installed before cloning the repository in the next step. If git lfs outputs git: 'lfs' is not a git command. , then you need to install it: Instructions for installation are here . Verify the installation: $ git lfs install Git LFS initialized. Create a directory called shared_dir in your home directory to hold HD maps and launch files for the simulator. The Autoware Docker container will mount this folder. mkdir ~/shared_dir cd ~/shared_dir git clone https://github.com/lgsvl/autoware-data.git If there wasn't a line beginning with Filtering content: output, then Git LFS hasn't been installed. Remove the autoware-data directory, install Git LFS with git lfs install , and then re-issue the git clone . Clone the docker repository from autoware.ai into a working directory: cd $WORKING_DIRECTORY git clone https://github.com/Autoware-AI/docker.git","title":"Install Autoware"},{"location":"system-under-test/autoware-instructions/#launch-autoware-alongside-lgsvl-simulator","text":"Run the Autoware 1.14.0 container and enter into it: cd $WORKING_DIRECTORY/docker/generic ./run.sh -t 1.14.0 If you get the usermod error as follows: usermod: user autoware is currently used by process 1 Check if the $UID is 1000 $ echo $UID If your $UID is 1000, you would not have usermod error. Otherwise, it's better to build container locally to avoid usermod error as follows: $ ./build.sh --version 1.14.0 $ ./run.sh -t local Once inside the container, install a missing ROS package: sudo apt update && sudo apt install ros-$ROS_DISTRO-image-transport-plugins -y If you need to check which $ROS_DISTRO you have installed run the following: ls /opt/ros/ Launch the runtime manager: roslaunch runtime_manager runtime_manager.launch A few terminals will open, as well as a GUI for the runtime manager. In the runtime manager, click on the 'Quick Start' tab and load the following launch files from ~/shared_dir/autoware-data/BorregasAve/ by clicking \"Ref\" to the right of each text box: my_map.launch my_sensing_simulator.launch my_localization.launch my_detection.launch my_mission_planning.launch my_motion_planning.launch Click \"Map\" to load the launch file pertaining to the HD maps. An \"Ok\" should appear to the right of the \"Ref\" button when successfully loaded. Then click \"Sensing\" which also launches rosbridge. Run the SVL Simulator Create a Simulation choosing BorregasAve map and Jaguar2015XE (Autoware) or another Autoware compatible vehicle. Select Autoware.AI under Autopilot selection Enter localhost:9090 for the Bridge Connection String. Run the created Simulation A vehicle should appear in Borregas Ave in Sunnyvale, CA. In the Autoware Runtime Manager, continue loading the other launch files - click \"Localization\" and wait for the time to display to the right of \"Ref\". Then click \"Rviz\" to launch Rviz - the vector map and location of the vehicle in the map should show. The vehicle may be mis-localized as the initial pose is important for NDT matching. To fix this, click \"2D Pose Estimate\" in Rviz, then click an approximate position for the vehicle on the map and drag in the direction it is facing before releasing the mouse button. This should allow NDT matching to find the vehicle pose (it may take a few tries). Note that the point cloud will not show up in rviz until ndt matching starts publishing a pose. An alternative would be to use GNSS for an initial pose or for localization. Open my_localization.launch file in map folder of autoware-data repository and uncomment nmea2tfpose part and comment ndt_matching instead.","title":"Launch Autoware Alongside SVL Simulator"},{"location":"system-under-test/autoware-instructions/#driving-by-following-vector-map","text":"To drive following the HD map follow these steps: Start Mission Planning launch file in Autoware Runtime Manager. In rviz, mark a destination by clicking '2D Nav Goal' and clicking at the destination and dragging along the road direction. Make sure to only choose a route that looks valid along the lane centerlines that are marked with orange lines in rviz. If an invalid destination is selected nothing will change in rviz, and you will need to relaunch the Mission Planning launch file in the Quick Launch tab to try another destination. After choosing a valid destination the route will be highlighted in blue in rviz. Lastly, start Motion Planning launch file. then ego vehicle starts driving autonomously.","title":"Driving by following vector map:"},{"location":"system-under-test/autoware-instructions/#adding-a-vehicle","text":"The default vehicles have the calibration files included in the LGSVL Autoware Data Github repository.","title":"Adding a Vehicle"},{"location":"system-under-test/autoware-instructions/#adding-an-hd-map","text":"The default maps have the Vector map files included in the LGSVL Autoware Data Github repository.","title":"Adding an HD Map"},{"location":"system-under-test/autoware-json-example/","text":"Example JSON Configuration for an Autoware Vehicle # Bridge Type top # ROS Published Topics top # Topic Sensor Name /nmea_sentence GPS /odom GPS Odometry /imu_raw IMU /points_raw Lidar /simulator/camera_node/image/compressed Main Camera Subscribed Topics top # Topic Sensor Name /vehicle_cmd Autoware Car Control Complete JSON Configuration top # [ { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/odom\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/camera_node/image/compressed\", \"Frame\": \"camera\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"AutowareAiControlSensor\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Example JSON Configuration for an Autoware Vehicle [](#top)"},{"location":"system-under-test/autoware-json-example/#bridge-type","text":"ROS","title":"Bridge Type"},{"location":"system-under-test/autoware-json-example/#published-topics","text":"Topic Sensor Name /nmea_sentence GPS /odom GPS Odometry /imu_raw IMU /points_raw Lidar /simulator/camera_node/image/compressed Main Camera","title":"Published Topics"},{"location":"system-under-test/autoware-json-example/#subscribed-topics","text":"Topic Sensor Name /vehicle_cmd Autoware Car Control","title":"Subscribed Topics"},{"location":"system-under-test/autoware-json-example/#complete-json-configuration","text":"[ { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/odom\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/camera_node/image/compressed\", \"Frame\": \"camera\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"AutowareAiControlSensor\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Complete JSON Configuration"},{"location":"system-under-test/lgsvl-msgs/","text":"lgsvl_msgs # lgsvl_msgs is a ROS / ROS2 hybrid package that provides AD stack agnostic message definitions for interfacing with the SVL Simulator. The package contains the following definitions: - Detection3DArray.msg # A list of 3D detections - Detection3D.msg # 3D detection including id, label, score, and 3D bounding box - BoundingBox3D.msg # A 3D bounding box definition - Detection2DArray.msg # A list of 2D detections - Detection2D.msg # 2D detection including id, label, score, and 2D bounding box - BoundingBox2D.msg # A 2D bounding box definition - SignalArray.msg # A list of traffic light detections - Signal.msg # 3D detection of a traffic light including id, label, score, and 3D bounding box - CanBusData.msg # Can bus data for an ego vehicle published by the simulator - VehicleControlData.msg # Vehicle control commands that the simulator subscribes to - VehicleStateData.msg # Description of the full state of an ego vehicle - Ultrasonic.msg # Minimum detected distance by an ultrasonic sensor - VehicleOdometry.msg # Odometry for an ego vehicle Installation # lgsvl_msgs is built by the ROS build farm. Binaries can be installed on Ubuntu as follows: sudo apt update sudo apt install ros-$ROS_DISTRO-lgsvl_msgs Note: It may take up to a week for the builds to become available after a release. Building from source # ROS # Create a catkin workspace for the package and clone the repository into the source folder: mkdir -p catkin_ws/src cd catkin_ws/src git clone https://github.com/lgsvl/lgsvl_msgs.git Build the package from workspace root: cd .. catkin_make Source in terminal where rosnodes who publish / subscribe to the messages are running: source devel/setup.bash ROS 2 # Clone repository and build with colcon: git clone https://github.com/lgsvl/lgsvl_msgs.git cd lgsvl_msgs colcon build Source in terminal where rosnodes who publish / subscribe to the messages are running: source install/setup.bash","title":"The lgsvl_msgs package"},{"location":"system-under-test/ros2-bridge/","text":"SVL Simulator ROS 2 Bridge # The SVL Simulator can publish and subscribe to ROS 2 messages by connecting to the ROS2 LGSVL Bridge . This custom, native ROS2 bridge yields higher performance than the previously used ros2-web-bridge . This document describes the installation process. Installing Dependencies top # The ROS2 LGSVL Bridge requires the following packages to be built: - ROS2 (rcutils and rcl) - colcon - boost Installing ROS 2 top # See the official ROS 2 installation guide to install ROS 2. The choice of the distribution will somewhat depend on the Ubuntu version installed. Ubuntu 18.04 supports Dashing Diademata ( dashing ) and Eloquent Elusor ( eloquent ), with dashing being the LTS distro. Ubuntu 20.04 only supports Foxy Fitzroy ( foxy ), which is also an LTS distro. The ROS 2 LGSVL Bridge supports Dashing Diademata and newer distributions. For Ubuntu versions older than 18.04, which would require older ROS 2 distributions, we suggest using ROS 2 docker images . Installing Colcon top # Colcon is a command line tool that facilitates building ROS packages. To install: sudo apt update sudo apt install python3-colcon-common-extensions Installing Boost top # To install boost run: sudo apt install libboost-all-dev Installing the ROS 2 LGSVL Bridge top # There are two ways of installing ros2-lgsvl-bridge: Using the Package Manager Building source code 1. Using the Package Manager top # source /opt/ros/(name\\of\\ros2\\distro)/setup.bash sudo apt update sudo apt install ros-$ROS_DISTRO-lgsvl-bridge 2. Building source code. top # Follow these steps to install the ROS2 LGSVL Bridge: Clone the ros2-lgsvl-bridge repository from github: git clone https://github.com/lgsvl/ros2-lgsvl-bridge.git Source the installed ROS2 distribution in the terminal window: source /opt/ros/(name\\of\\ros2\\distro)/setup.bash Switch to the correct devel branch for the installed ROS 2 distribution: cd ros2-lgsvl-bridge git checkout ${ROS_DISTRO}-devel Build using colcon: colcon build --cmake-args '-DCMAKE_BUILD_TYPE=Release' Running the ROS2 LGSVL Bridge top # If the Bridge is already built, to run ROS2 bridge: source (path\\to\\bridge\\repository)/install/setup.bash lgsvl_bridge","title":"Setting up ROS 2 bridge"},{"location":"system-under-test/ros2-bridge/#installing-dependencies","text":"The ROS2 LGSVL Bridge requires the following packages to be built: - ROS2 (rcutils and rcl) - colcon - boost","title":"Installing Dependencies"},{"location":"system-under-test/ros2-bridge/#installing-ros-2","text":"See the official ROS 2 installation guide to install ROS 2. The choice of the distribution will somewhat depend on the Ubuntu version installed. Ubuntu 18.04 supports Dashing Diademata ( dashing ) and Eloquent Elusor ( eloquent ), with dashing being the LTS distro. Ubuntu 20.04 only supports Foxy Fitzroy ( foxy ), which is also an LTS distro. The ROS 2 LGSVL Bridge supports Dashing Diademata and newer distributions. For Ubuntu versions older than 18.04, which would require older ROS 2 distributions, we suggest using ROS 2 docker images .","title":"Installing ROS 2"},{"location":"system-under-test/ros2-bridge/#installing-colcon","text":"Colcon is a command line tool that facilitates building ROS packages. To install: sudo apt update sudo apt install python3-colcon-common-extensions","title":"Installing Colcon"},{"location":"system-under-test/ros2-bridge/#installing-boost","text":"To install boost run: sudo apt install libboost-all-dev","title":"Installing Boost"},{"location":"system-under-test/ros2-bridge/#installing-the-ros-2-lgsvl-bridge","text":"There are two ways of installing ros2-lgsvl-bridge: Using the Package Manager Building source code","title":"Installing the ROS 2 LGSVL Bridge"},{"location":"system-under-test/ros2-bridge/#1.-using-the-package-manager","text":"source /opt/ros/(name\\of\\ros2\\distro)/setup.bash sudo apt update sudo apt install ros-$ROS_DISTRO-lgsvl-bridge","title":"1. Using the Package Manager"},{"location":"system-under-test/ros2-bridge/#2.-building-source-code.","text":"Follow these steps to install the ROS2 LGSVL Bridge: Clone the ros2-lgsvl-bridge repository from github: git clone https://github.com/lgsvl/ros2-lgsvl-bridge.git Source the installed ROS2 distribution in the terminal window: source /opt/ros/(name\\of\\ros2\\distro)/setup.bash Switch to the correct devel branch for the installed ROS 2 distribution: cd ros2-lgsvl-bridge git checkout ${ROS_DISTRO}-devel Build using colcon: colcon build --cmake-args '-DCMAKE_BUILD_TYPE=Release'","title":"2. Building source code."},{"location":"system-under-test/ros2-bridge/#running-the-ros2-lgsvl-bridge","text":"If the Bridge is already built, to run ROS2 bridge: source (path\\to\\bridge\\repository)/install/setup.bash lgsvl_bridge","title":"Running the ROS2 LGSVL Bridge"},{"location":"system-under-test/sensor-message-publishing/","text":"Sensor Message Publishing # Overview top # One of the main functionalities of SVL Simulator is its ability to work with third-party tools, which include autonomous driving systems. Simulator provides output from virtual sensors that can be received and parsed by tools like Apollo or Autoware.Auto. Third-party systems can then provide instructions back to Simulator, which will execute them on virtual vehicle. The whole communication aims to be indistinguishable from its real-world equivalent, which means that each message sent and received by Simulator follows the data format expected by third party tools. Since multiple bridge plugins are supported, all the sensor plugins have to be bridge-agnostic. Plugins can check if the bridge is connected and decide when and what to send, but do not know the bridge type, and therefore its expected format. This splits responsibility for publishing a message between two plugin types: Sensor prepares data in bridge-agnostic format and requests for it to be sent Bridge converts message to bridge-specific format and sends it This article covers subjects of creating and dispatching bridge messages from sensor plugins. For details about creating new sensor plugins, see sensor plugins page. If you're creating a bridge plugin and want to make sure that sensor data is properly handled, see sensor plugins page. Sensor data types top # Data types for sensors don't have to follow any particular guidelines, but next to the data that sensor produces, you should consider including fields that might be required by some bridge plugins. Often used examples include timestamp or sequential message number. You do not need to worry about how bridge plugin will consume this data - just make sure all the fields have public read accessors. If your sensor produces small amount of data that's easy to convert, you can decide to call the publishing delegate directly and perform the whole operation synchronously. If instead you opt to use asynchronous approach (see asynchronous publishing ), you have to be aware about including any reused, heap-allocated resources (e.g. single large buffer included in each message), since the order of updating and accessing data from multiple threads will not be defined. Probably the most common examples that would call for reused resources are large array buffers. Of course it's possible to allocate completely new array for each message, but this would add unnecessary work for garbage collector and could affect performance. You might consider using pool of pre-allocated buffers, and while it's certainly possible to do so, you would have to depend on callbacks to track which buffers are actively in use. Fortunately, there is a built-in, simpler way to handle this kinds of cases. If you want to reuse resources and use asynchronous publishing, your data type should be a class, provide parameterless constructor, and implement the IThreadCachedBridgeData interface. This interface defines two methods: - CopyToCache - your implementation should perform a full deep copy of current instance data into the container of the same type, provided through parameter. Provided target instance is part of a pooling system that will make sure this data will not be accessed again until conversion and publishing on the bridge side are complete. This means only one thread will ever access it at a time, and you don't have to worry about thread safety. - GetHash - your implementation should return value that groups instances into compatible sub-pools. Returning one value for all instances is valid if you don't need sub-pools. If some instances are not compatible (e.g. they use pre-allocated buffers with different sizes), this should return value calculated based on incompatible parameters. Implementing IThreadCachedBridgeData interface is enough to enable thread-side caching functionality and no further changes are required on sensor side, as long as you're using BridgeMessageDispatcher (see asynchronous publishing section for details). The instance provided by IThreadCachedBridgeData interface is persistent, pooled resource. Depending on previous usage, its fields may or may not be initialized - make sure to clear any optional fields. If you're using arrays, consider either storing valid data size in separate parameter (it would allow you to use larger arrays to store varying amounts of data without the need to reallocate them every single time) or using GetHash method to split instances into separate sub-pools based on array size. For an example of IThreadCachedBridgeData interface usage, see Simulator.Bridge.Data.PointCloudData type in Simulator repository. This example includes good practices described above. Asynchronous publishing top # When you expect the conversion process to take a long time, you might want to avoid occupying main thread of the application, and perform the process asynchronously. You might also consider using multiple threads to increase overall processing speed. Both of these cases, and more, are covered by the BridgeMessageDispatcher class. Its default instance is created for each simulation and is accessible through the class' static field Instance . Note that this instance only exists during simulation. Interacting with the message dispatcher is simplified to a single method, TryQueueTask . Its signature and detailed description can be found below. public bool TryQueueTask<T>( Publisher<T> publisher, T data, Action<bool> callback = null, object exclusiveToken = null) where T : class, new() Parameters: publisher - Publisher delegate for active bridge, created by calling AddPublisher<T>(Topic) on bridge instance. See bridge registration section for an example. data - Sensor data to convert and send. If type of the data implements IThreadCachedBridgeData interface, thread-exclusive cache will be used to store copy of the data. Original data will never be accessed by worker thread in such case. See sensor data types section for more details. callback (optional) - Delegate that will be called after conversion and publishing process was resolved. This will always be executed, but if delegate boolean parameter is set to false , the process has failed. See callback section for more information. exclusiveToken (optional) - Token used to enforce exclusivity for given task. Only one publishing process with the same token can be active or queued at a time - new requests with the same token will be dropped. See exclusive publishing section for more details. Returns: bool - True if message was enqueued and is expected to be converted and sent. False if it was immediately dropped for any reason (see dropping messages section for a list of possible causes). Multi-threading top # BridgeMessageDispatcher internally manages a number of background threads that execute conversion and publishing code defined in bridge plugins. By default, only a single background thread is running, but dispatcher will automatically spawn new ones if the queue is not expected to be unloaded in a single cycle. Maximum number of worker threads is defined by logical core count of your CPU. If one or more threads are idle for a while, thread count will be reduced. This means that threads will be scaled up and down dynamically, based on current bridge data throughput. It's possible for extreme situations to occur, in which thread count reached its limit and yet queue size keeps growing. This can happen if large messages requiring long processing are send very often. At this stage CPU is usually close to 100% utilization on all cores and not much can be done. In this situation, dispatcher will block main thread execution until one of worker threads is finished with its work. This will usually severely reduce framerate, so you should consider reducing bridge data throughput or using more powerful CPU at that stage. Warning is displayed when this situation occurs. Dropping messages top # There are situations in which calling TryQueueTask will not publish the data. BridgeMessageDispatcher considers message as published when publisher delegate for active bridge finishes execution without throwing any exception. Note that this doesn't always mean that message was properly received, or even sent. This is heavily dependent on bridge implementation. If you're implementing your own bridge, it's suggested to throw exception whenever something goes wrong with publishing process. BridgeMessageDispatcher class will catch the exception, allow sensor to react to it, and display it in console. The exception will not be thrown again to prevent worker thread from crashing. For standard causes of dropping messages (unrelated to bridge implementation), consult the list below. Most of them will occur when enqueuing attempt happens, in which case TryQueueTask will return false . If task was enqueued correctly, and problem occurred during delegate execution, TryQueueTask will return true, but the problem still can be reacted to through callback . Time is paused ( TryQueueTask returns false , callback executes with false ) If time in simulation is paused, every module should hold its execution. If sensor ignores time state and attempts to publish data during pause, message will be dropped. Exclusive token is already in use ( TryQueueTask returns false , callback executes with false ) Message with the same token is either queued or being processed. See exclusive publishing for details about token usage. Exception was thrown by publisher delegate ( TryQueueTask returns true , callback executes with false ) Reasons for exception occurring at this stage are dependent on bridge type. Possible causes include lack of compatible publisher, errors during conversion process or problems with connection. For more details check console (if you're running Simulator in Unity Editor) or application logs. Callback top # When attempting to enqueue bridge message, you can provide optional delegate ( callback ) with a single boolean parameter. This delegate will always be invoked, even if message was dropped immediately and never enqueued. If bridge properly executed publisher delegate, callback will be invoked with true passed as its parameter. Details depends on bridge implementation, but this usually means that message was properly converted and sent. If message was dropped for any reason (see dropping messages ), or if exception was thrown anywhere in publisher delegate code, callback will be invoked with false passed as its parameter. Some uses of the callback include cleaning up resources after message is published, reacting to expected failures, or waiting for very expensive bridge operations to finish before continuing with sensor work. Exclusive publishing top # There are use cases in which multi-threaded character of the publishing process, mixed with unknown processing times, can have undesired implications. If, for example, you enqueue two subsequent messages with timestamps, older message might, in theory, finish publishing later. If message order is critical and you want to enforce it, there are a few options: publish messages synchronously - will block main thread publish using your own, single background thread - requires synchronization between main thread (that can access time) and background thread use callbacks, start new publishing process when previous finishes provide exclusive token While all options are viable, providing exclusive token is probably the easiest solution if synchronous execution is undesired. The exclusive token can be an instance of any object. Its purpose is simple - only one publish request with individual token can be active at a time. If you call TryQueueTask and provide exclusive token, two things can happen: if another request using the same token is currently active, message is immediately dropped and TryQueueTask returns false if the token is not used by any of the active requests, message is processed as usual - if it's not dropped for different reason, message is enqueued, token becomes active and TryQueueTask returns true The token becomes active immediately when request using it becomes enqueued, and becomes inactive when publish delegate execution either succeeds or crashes. If you provide the exclusive token, messages using it will always be published in chronological order, although their frequency depends on conversion time and queue load. Please note that using the exclusive token will drop already prepared message. If the performance cost of preparing the message is significant, you should consider using callback instead to wait for previous message to finish publishing before preparing the next one. Frequency-based publishing top # Many sensors have predefined frequency at which they collect and publish data. If your sensor is supposed to work with a set frequency, the easiest way to achieve this is to use FrequencySensorBase . FrequencySensorBase is an abstract class that preserves all standard SensorBase class behavior, but provides functionality that is supposed to simplify, or even completely remove any need for time tracking in your sensor code. When you create sensor derived from FrequencySensorBase class, you will need provide implementation for its abstract member, UseFixedUpdate . It's boolean property with only a read accessor. Simply make it return false if you want to use standard update loop, or true if your use case depends on using Unity's FixedUpdate loop. For vast majority of cases, using standard update loop should be enough. In both cases sensor updates will happen with set frequency, but you shouldn't expect perfect intervals. Your code will be executed when either Update (with intervals dependent on framerate) or FixedUpdate (with fixed intervals, 10 ms default in SVL Simulator) is executed by Unity. In both cases, most of the updates in the loop are ignored, and the ones closest to sensor intervals are chosen. Your code will be executed at that point. Sensor frequency is defined through Frequency parameter. It's marked as sensor parameter, and therefore will be visible among sensor settings. See sensor plugins page for more information about sensor parameters. Frequency is based on simulation time, so sensor will handle pause or non-realtime mode properly. Last thing that has to be done is overriding SensorUpdate method. Code inside will be executed with set frequency, unless simulation is paused. If you need to use either Update or FixedUpdate method directly, you can override them too, but remember to call their base implementation before your code. For an example of FrequencySensorBase usage, see publishing with set frequency section. Asynchronous GPU readback # In some cases, sensors utilize GPU capabilities to process data. This doesn't only include rendering in camera-based sensors, but also cases where highly parallel computations need to be performed on large data sets. In both cases data is stored on GPU (as either textures or compute buffers), and then read back to process and publish on CPU. Reading back large buffers or textures can take significant time, and may result in latency spikes if main thread is blocked during this process. As usual with long operations, you might want to perform this process asynchronously and make sure that main thread is able to keep stable framerate. You can see longer processing times in publishing frames on the frame time chart below. Upper row shows case with synchronous GPU readback, lower row shows the same setup with asynchronous GPU readback. Notice that latency spikes are significantly reduced. Simulator provides an utility class ( GpuReadbackPool<TData, T> ) that can help with pooling and tracking asynchronous GPU readbacks. It will internally handle some of the problems related to asynchronous reads: - Multiple readbacks can be active at once - required resources will be pooled and tracked automatically - Requests will always be finished in the same order they were started - Timestamp of the readback start will be stored with the data - You don't need to track state of active readbacks, callback will be called for each upon completion To use this system, you have to create an instance of GpuReadbackPool<TData, T> for your sensor. It requires explicit initialization through Initialize method, which takes buffer size and delegate called on completion as parameters. If the buffer size ever changes (e.g. when output texture resolution for sensor is modified), you can change the buffer size through Resize method. TData type parameter is usually GpuReadbackData<T> , but can also be a type derived from it. T must be a struct compatible with NativeArray<T> . With this kind of setup, using asynchronous GPU readbacks is very simple - when operations on GPU are finished, you can simply call: ReadbackPool.StartReadback(GpuResource); When readback finishes, delegate declared in Initialize method will be called. Instance of GpuReadbackData<T> will be passed as a parameter, containing gpuData (native array containing data read back from the GPU) and captureTime (timestamp of the request start) fields. If you used your own type derived from GpuReadbackData<T> , your custom fields will be accessible too. StartReadback has multiple overloads that allow you to specify some of the readback parameters (like texture format, buffer offset etc.). It also returns instance of the GpuReadbackData<T> , so you can populate it with any data at the time of readback start. This is especially useful if you want to include any custom fields. Please note that GpuReadbackPool<TData, T> is a passive class and requires explicit updates from outside. You should call Process method whenever you want to trigger a check on pending readbacks. This is usually done from Update method of sensor class. For an example of GpuReadbackPool<TData, T> usage, see reading GPU data asynchronously section. Code examples top # Snippets presented here are intended to explain parts of the sensor code responsible for message publishing. Large parts of the sensor code are omitted - see sensor plugins page for more complete examples. Custom sensor data type top # This example shows data type that supports thread-side caching through IThreadCachedBridgeData interface (see sensor data types for details). array field in this use case will contain reference to persistent buffer used by sensor. The same buffer is passed for all messages. For the sake of this example, let's assume that buffer size is constant, but data size is not. dataSize field will contain size of valid data in the buffer. Check one of publishing examples below to see how data is prepared. public class FloatArray : IThreadCachedBridgeData<FloatArray> { // Array reference passed from sensor. Shared across multiple messages. public float[] array; // Specifies size of valid data in array. public int dataSize; // Timestamp that is required by receiving side. public double timestamp; // Method defined by IThreadCachedBridgeData interface. // This will be called internally by BridgeMessageDispatcher before enqueuing message. // Parameter target contains instance provided by pooling system. // Implementation must perform deep copy from current instance to target. public void CopyToCache(FloatArray target) { // If array in target instance was not initialized or is too small, allocate new one. if (target.array == null || target.array.Length < dataSize) target.array = new float[dataSize]; // Copy all valid data to array in target instance. // Data beyond dataSize (if present) has undefined value, so it's skipped. Array.Copy(array, target.array, dataSize); // Value types can be simply assigned in target instance. target.dataSize = dataSize; target.timestamp = timestamp; } // Method defined by IThreadCachedBridgeData interface. // This should return hash that groups instances into compatible sub-pools. public int GetHash() { // No sub-pools required - return the same value for all instances. return 0; } } Bridge registration top # During initialization process, each sensor must define its expected interactions with bridge. OnBridgeSetup is an abstract method defined in SensorBase class, that is invoked once for each sensor if bridge instance is present. Each sensor must implement it. You can use it to get reference to the bridge instance and register publisher (or subscriber) of your data type. Delegate returned by AddPublisher method is responsible for converting and publishing messages. Data type used for this example ( FloatArray ) is described in its own section . Note that bridge plugin must implement conversion between this type and its own, bridge-specific message format. You can find more information about bridge plugin implementation on the bridge plugins page. // Declare sensor name and types of data that it will publish over the bridge. [SensorType(\"Float Array Sensor\", new[] {typeof(FloatArray)})] public class FloatArraySensor : SensorBase { // Field used to store reference to the bridge instance. BridgeInstance Bridge; // Field used to store publisher delegate. Publisher<FloatArray> Publish; // OnSetupBridge will be called during initialization if any bridge is present. public override void OnBridgeSetup(BridgeInstance bridge) { // Store reference to bridge instance so we can track its state. Bridge = bridge; // Register publisher of FloatArray data type for the bridge. // Returned delegate can be used to convert and publish messages. Publish = bridge.AddPublisher<FloatArray>(Topic); } // Visualization and sensor update code is omitted in this example. [...] } Publishing using exclusive token top # This example shows how custom data can be published using exclusive token - it assumes that message order is critical and enforces it by dropping any messages until previously enqueued one is published. Since array used as buffer is reused every frame, data type in this example utilizes thread-side caching . Exact time that conversion and publishing will take is unknown, so there's a risk that this data might be overwritten before conversion finishes. Using data type that supports caching is invisible from sensor code (as shown below), but all cross-thread access issues - like the one mentioned - are mitigated. // Declare sensor name and types of data that it will publish over the bridge. [SensorType(\"Float Array Sensor\", new[] {typeof(FloatArray)})] public class FloatArraySensor : SensorBase { // Field used to store reference to the bridge instance. BridgeInstance Bridge; // Field used to store publisher delegate. Publisher<FloatArray> Publish; // Preallocated array used as a buffer. public float[] array; // Update is called every frame. private void Update() { // Theoretical method that would calculate output for this sensor. // This is executed even if bridge is not connected, as data might be used inside Simulator. // Data is put in array. Since data size is dynamic, and array is not, dataSize stores valid data count. PrepareSensorData(array, out var dataSize); // Check if bridge exists and is currently connected. if (Bridge is {Status: Status.Connected}) { // Fetch current time from simulation manager. var time = SimulatorManager.Instance.CurrentTime; // Create data object for new message. Note that persistent buffer is passed as reference. var data = new FloatArray {array = array, dataSize = dataSize, timestamp = time}; // Try enqueueing publish request. Instance of this sensor is used as exclusive token. // If simulation is paused or token is already in use, this message will be dropped. // Sensor doesn't care whether message dropped or not, so returned value is ignored. BridgeMessageDispatcher.Instance.TryQueueTask(Publish, data, this); } } // Initialization and visualization code is omitted in this example. [...] } Publishing with set frequency top # This example shows how FrequencySensorBase can be used to calculate and publish data with set interval. See frequency-based publishing section for details. Similar to previous example, used data type utilizes thread-side caching to prevent simultaneous access from multiple threads to array buffer. // Declare sensor name and types of data that it will publish over the bridge. // Sensor type is derived from FrequencySensorBase instead of usual SensorBase. [SensorType(\"Float Array Sensor\", new[] {typeof(FloatArray)})] public class FloatArraySensor : FrequencySensorBase { // Field used to store reference to the bridge instance. BridgeInstance Bridge; // Field used to store publisher delegate. Publisher<FloatArray> Publish; // Preallocated array used as a buffer. public float[] array; // FixedUpdate is not required - standard update loop will be used. protected override bool UseFixedUpdate => false; // SensorUpdate is executed with frequency defined by Frequency sensor parameter in base class. // Frequency is based on simulation time, so pause or non-realtime mode will be handled properly. private override void SensorUpdate() { // Theoretical method that would calculate output for this sensor. // This is executed even if bridge is not connected, as data might be used inside Simulator. // Data is put in array. Since data size is dynamic, and array is not, dataSize stores valid data count. PrepareSensorData(array, out var dataSize); // Check if bridge exists and is currently connected. if (Bridge is {Status: Status.Connected}) { // Fetch current time from simulation manager. var time = SimulatorManager.Instance.CurrentTime; // Create data object for new message. Note that persistent buffer is passed as reference. var data = new FloatArray {array = array, dataSize = dataSize, timestamp = time}; // Try enqueueing publish request. This will always succeed, as both requirements are met: // - time is not paused (SensorUpdate wouldn't be called if it was) // - optional exclusive token is not provided BridgeMessageDispatcher.Instance.TryQueueTask(Publish, data); } } // Initialization and visualization code is omitted in this example. [...] } Reading GPU data asynchronously top # This example shows how GpuReadbackPool<TData, T> can be used to perform GPU readbacks asynchronously. // Declare sensor name and types of data that it will publish over the bridge. [SensorType(\"Float Array Sensor\", new[] {typeof(FloatArray)})] public class FloatArraySensor : SensorBase { // Field used to store reference to the bridge instance. BridgeInstance Bridge; // Field used to store publisher delegate. Publisher<FloatArray> Publish; // Preallocated array used as a buffer. public float[] array; // Buffer used for storing data on GPU side. private ComputeBuffer computeBuffer; // Asynchronous GPU readback pool. private GpuReadbackPool<GpuReadbackData<Vector4>, Vector4> readbackPool; // Initialization method - prepare required resources. protected override void Initialize() { // Assume 100 floats will be used per data packet. array = new float[100]; computeBuffer = new ComputeBuffer(100, sizeof(float)); // Initialize GPU readback pool with 100 floats capacity. // OnReadbackComplete delegate will be called for each completed readback. readbackPool = new GpuReadbackPool<GpuReadbackData<float>, float>(); readbackPool.Initialize(100, OnReadbackComplete); } // Deinitialization method - free any currently used resources. protected override void Deinitialize() { computeBuffer.Release(); readbackPool.Dispose(); } // Update is called every frame. private void Update() { // Explicitly trigger update of all currently pending readbacks. readbackPool.Process(); // Theoretical method that would calculate output for this sensor. // This is executed even if bridge is not connected, as data might be used inside Simulator. // Data is calculated and kept on the GPU, in the compute buffer. PrepareSensorData(computeBuffer); // Request asynchronous read of the GPU resource. // Completion will trigger OnReadbackComplete declared during initialization. readbackPool.StartReadback(computeBuffer); } // Delegate called when readback is completed private void OnReadbackComplete(GpuReadbackData<float> data) { // Check if bridge exists and is currently connected. if (Bridge is {Status: Status.Connected}) { // data.gpuData contains data read back from the GPU. // Perform copy from NativeArray to managed array, since our message type doesn't support it. data.gpuData.CopyTo(array); // Publish data. See previous examples for details. // Note that time passed is capture time, not current time. var data = new FloatArray {array = array, dataSize = 100, timestamp = data.captureTime}; BridgeMessageDispatcher.Instance.TryQueueTask(Publish, data); } } // Bridge setup and visualization code is omitted in this example. [...] }","title":"Sensor message publishing"},{"location":"system-under-test/sensor-message-publishing/#overview","text":"One of the main functionalities of SVL Simulator is its ability to work with third-party tools, which include autonomous driving systems. Simulator provides output from virtual sensors that can be received and parsed by tools like Apollo or Autoware.Auto. Third-party systems can then provide instructions back to Simulator, which will execute them on virtual vehicle. The whole communication aims to be indistinguishable from its real-world equivalent, which means that each message sent and received by Simulator follows the data format expected by third party tools. Since multiple bridge plugins are supported, all the sensor plugins have to be bridge-agnostic. Plugins can check if the bridge is connected and decide when and what to send, but do not know the bridge type, and therefore its expected format. This splits responsibility for publishing a message between two plugin types: Sensor prepares data in bridge-agnostic format and requests for it to be sent Bridge converts message to bridge-specific format and sends it This article covers subjects of creating and dispatching bridge messages from sensor plugins. For details about creating new sensor plugins, see sensor plugins page. If you're creating a bridge plugin and want to make sure that sensor data is properly handled, see sensor plugins page.","title":"Overview"},{"location":"system-under-test/sensor-message-publishing/#sensor-data-types","text":"Data types for sensors don't have to follow any particular guidelines, but next to the data that sensor produces, you should consider including fields that might be required by some bridge plugins. Often used examples include timestamp or sequential message number. You do not need to worry about how bridge plugin will consume this data - just make sure all the fields have public read accessors. If your sensor produces small amount of data that's easy to convert, you can decide to call the publishing delegate directly and perform the whole operation synchronously. If instead you opt to use asynchronous approach (see asynchronous publishing ), you have to be aware about including any reused, heap-allocated resources (e.g. single large buffer included in each message), since the order of updating and accessing data from multiple threads will not be defined. Probably the most common examples that would call for reused resources are large array buffers. Of course it's possible to allocate completely new array for each message, but this would add unnecessary work for garbage collector and could affect performance. You might consider using pool of pre-allocated buffers, and while it's certainly possible to do so, you would have to depend on callbacks to track which buffers are actively in use. Fortunately, there is a built-in, simpler way to handle this kinds of cases. If you want to reuse resources and use asynchronous publishing, your data type should be a class, provide parameterless constructor, and implement the IThreadCachedBridgeData interface. This interface defines two methods: - CopyToCache - your implementation should perform a full deep copy of current instance data into the container of the same type, provided through parameter. Provided target instance is part of a pooling system that will make sure this data will not be accessed again until conversion and publishing on the bridge side are complete. This means only one thread will ever access it at a time, and you don't have to worry about thread safety. - GetHash - your implementation should return value that groups instances into compatible sub-pools. Returning one value for all instances is valid if you don't need sub-pools. If some instances are not compatible (e.g. they use pre-allocated buffers with different sizes), this should return value calculated based on incompatible parameters. Implementing IThreadCachedBridgeData interface is enough to enable thread-side caching functionality and no further changes are required on sensor side, as long as you're using BridgeMessageDispatcher (see asynchronous publishing section for details). The instance provided by IThreadCachedBridgeData interface is persistent, pooled resource. Depending on previous usage, its fields may or may not be initialized - make sure to clear any optional fields. If you're using arrays, consider either storing valid data size in separate parameter (it would allow you to use larger arrays to store varying amounts of data without the need to reallocate them every single time) or using GetHash method to split instances into separate sub-pools based on array size. For an example of IThreadCachedBridgeData interface usage, see Simulator.Bridge.Data.PointCloudData type in Simulator repository. This example includes good practices described above.","title":"Sensor data types"},{"location":"system-under-test/sensor-message-publishing/#asynchronous-publishing","text":"When you expect the conversion process to take a long time, you might want to avoid occupying main thread of the application, and perform the process asynchronously. You might also consider using multiple threads to increase overall processing speed. Both of these cases, and more, are covered by the BridgeMessageDispatcher class. Its default instance is created for each simulation and is accessible through the class' static field Instance . Note that this instance only exists during simulation. Interacting with the message dispatcher is simplified to a single method, TryQueueTask . Its signature and detailed description can be found below. public bool TryQueueTask<T>( Publisher<T> publisher, T data, Action<bool> callback = null, object exclusiveToken = null) where T : class, new() Parameters: publisher - Publisher delegate for active bridge, created by calling AddPublisher<T>(Topic) on bridge instance. See bridge registration section for an example. data - Sensor data to convert and send. If type of the data implements IThreadCachedBridgeData interface, thread-exclusive cache will be used to store copy of the data. Original data will never be accessed by worker thread in such case. See sensor data types section for more details. callback (optional) - Delegate that will be called after conversion and publishing process was resolved. This will always be executed, but if delegate boolean parameter is set to false , the process has failed. See callback section for more information. exclusiveToken (optional) - Token used to enforce exclusivity for given task. Only one publishing process with the same token can be active or queued at a time - new requests with the same token will be dropped. See exclusive publishing section for more details. Returns: bool - True if message was enqueued and is expected to be converted and sent. False if it was immediately dropped for any reason (see dropping messages section for a list of possible causes).","title":"Asynchronous publishing"},{"location":"system-under-test/sensor-message-publishing/#multi-threading","text":"BridgeMessageDispatcher internally manages a number of background threads that execute conversion and publishing code defined in bridge plugins. By default, only a single background thread is running, but dispatcher will automatically spawn new ones if the queue is not expected to be unloaded in a single cycle. Maximum number of worker threads is defined by logical core count of your CPU. If one or more threads are idle for a while, thread count will be reduced. This means that threads will be scaled up and down dynamically, based on current bridge data throughput. It's possible for extreme situations to occur, in which thread count reached its limit and yet queue size keeps growing. This can happen if large messages requiring long processing are send very often. At this stage CPU is usually close to 100% utilization on all cores and not much can be done. In this situation, dispatcher will block main thread execution until one of worker threads is finished with its work. This will usually severely reduce framerate, so you should consider reducing bridge data throughput or using more powerful CPU at that stage. Warning is displayed when this situation occurs.","title":"Multi-threading"},{"location":"system-under-test/sensor-message-publishing/#dropping-messages","text":"There are situations in which calling TryQueueTask will not publish the data. BridgeMessageDispatcher considers message as published when publisher delegate for active bridge finishes execution without throwing any exception. Note that this doesn't always mean that message was properly received, or even sent. This is heavily dependent on bridge implementation. If you're implementing your own bridge, it's suggested to throw exception whenever something goes wrong with publishing process. BridgeMessageDispatcher class will catch the exception, allow sensor to react to it, and display it in console. The exception will not be thrown again to prevent worker thread from crashing. For standard causes of dropping messages (unrelated to bridge implementation), consult the list below. Most of them will occur when enqueuing attempt happens, in which case TryQueueTask will return false . If task was enqueued correctly, and problem occurred during delegate execution, TryQueueTask will return true, but the problem still can be reacted to through callback . Time is paused ( TryQueueTask returns false , callback executes with false ) If time in simulation is paused, every module should hold its execution. If sensor ignores time state and attempts to publish data during pause, message will be dropped. Exclusive token is already in use ( TryQueueTask returns false , callback executes with false ) Message with the same token is either queued or being processed. See exclusive publishing for details about token usage. Exception was thrown by publisher delegate ( TryQueueTask returns true , callback executes with false ) Reasons for exception occurring at this stage are dependent on bridge type. Possible causes include lack of compatible publisher, errors during conversion process or problems with connection. For more details check console (if you're running Simulator in Unity Editor) or application logs.","title":"Dropping messages"},{"location":"system-under-test/sensor-message-publishing/#callback","text":"When attempting to enqueue bridge message, you can provide optional delegate ( callback ) with a single boolean parameter. This delegate will always be invoked, even if message was dropped immediately and never enqueued. If bridge properly executed publisher delegate, callback will be invoked with true passed as its parameter. Details depends on bridge implementation, but this usually means that message was properly converted and sent. If message was dropped for any reason (see dropping messages ), or if exception was thrown anywhere in publisher delegate code, callback will be invoked with false passed as its parameter. Some uses of the callback include cleaning up resources after message is published, reacting to expected failures, or waiting for very expensive bridge operations to finish before continuing with sensor work.","title":"Callback"},{"location":"system-under-test/sensor-message-publishing/#exclusive-publishing","text":"There are use cases in which multi-threaded character of the publishing process, mixed with unknown processing times, can have undesired implications. If, for example, you enqueue two subsequent messages with timestamps, older message might, in theory, finish publishing later. If message order is critical and you want to enforce it, there are a few options: publish messages synchronously - will block main thread publish using your own, single background thread - requires synchronization between main thread (that can access time) and background thread use callbacks, start new publishing process when previous finishes provide exclusive token While all options are viable, providing exclusive token is probably the easiest solution if synchronous execution is undesired. The exclusive token can be an instance of any object. Its purpose is simple - only one publish request with individual token can be active at a time. If you call TryQueueTask and provide exclusive token, two things can happen: if another request using the same token is currently active, message is immediately dropped and TryQueueTask returns false if the token is not used by any of the active requests, message is processed as usual - if it's not dropped for different reason, message is enqueued, token becomes active and TryQueueTask returns true The token becomes active immediately when request using it becomes enqueued, and becomes inactive when publish delegate execution either succeeds or crashes. If you provide the exclusive token, messages using it will always be published in chronological order, although their frequency depends on conversion time and queue load. Please note that using the exclusive token will drop already prepared message. If the performance cost of preparing the message is significant, you should consider using callback instead to wait for previous message to finish publishing before preparing the next one.","title":"Exclusive publishing"},{"location":"system-under-test/sensor-message-publishing/#frequency-based-publishing","text":"Many sensors have predefined frequency at which they collect and publish data. If your sensor is supposed to work with a set frequency, the easiest way to achieve this is to use FrequencySensorBase . FrequencySensorBase is an abstract class that preserves all standard SensorBase class behavior, but provides functionality that is supposed to simplify, or even completely remove any need for time tracking in your sensor code. When you create sensor derived from FrequencySensorBase class, you will need provide implementation for its abstract member, UseFixedUpdate . It's boolean property with only a read accessor. Simply make it return false if you want to use standard update loop, or true if your use case depends on using Unity's FixedUpdate loop. For vast majority of cases, using standard update loop should be enough. In both cases sensor updates will happen with set frequency, but you shouldn't expect perfect intervals. Your code will be executed when either Update (with intervals dependent on framerate) or FixedUpdate (with fixed intervals, 10 ms default in SVL Simulator) is executed by Unity. In both cases, most of the updates in the loop are ignored, and the ones closest to sensor intervals are chosen. Your code will be executed at that point. Sensor frequency is defined through Frequency parameter. It's marked as sensor parameter, and therefore will be visible among sensor settings. See sensor plugins page for more information about sensor parameters. Frequency is based on simulation time, so sensor will handle pause or non-realtime mode properly. Last thing that has to be done is overriding SensorUpdate method. Code inside will be executed with set frequency, unless simulation is paused. If you need to use either Update or FixedUpdate method directly, you can override them too, but remember to call their base implementation before your code. For an example of FrequencySensorBase usage, see publishing with set frequency section.","title":"Frequency-based publishing"},{"location":"system-under-test/sensor-message-publishing/#asynchronous-gpu-readback","text":"In some cases, sensors utilize GPU capabilities to process data. This doesn't only include rendering in camera-based sensors, but also cases where highly parallel computations need to be performed on large data sets. In both cases data is stored on GPU (as either textures or compute buffers), and then read back to process and publish on CPU. Reading back large buffers or textures can take significant time, and may result in latency spikes if main thread is blocked during this process. As usual with long operations, you might want to perform this process asynchronously and make sure that main thread is able to keep stable framerate. You can see longer processing times in publishing frames on the frame time chart below. Upper row shows case with synchronous GPU readback, lower row shows the same setup with asynchronous GPU readback. Notice that latency spikes are significantly reduced. Simulator provides an utility class ( GpuReadbackPool<TData, T> ) that can help with pooling and tracking asynchronous GPU readbacks. It will internally handle some of the problems related to asynchronous reads: - Multiple readbacks can be active at once - required resources will be pooled and tracked automatically - Requests will always be finished in the same order they were started - Timestamp of the readback start will be stored with the data - You don't need to track state of active readbacks, callback will be called for each upon completion To use this system, you have to create an instance of GpuReadbackPool<TData, T> for your sensor. It requires explicit initialization through Initialize method, which takes buffer size and delegate called on completion as parameters. If the buffer size ever changes (e.g. when output texture resolution for sensor is modified), you can change the buffer size through Resize method. TData type parameter is usually GpuReadbackData<T> , but can also be a type derived from it. T must be a struct compatible with NativeArray<T> . With this kind of setup, using asynchronous GPU readbacks is very simple - when operations on GPU are finished, you can simply call: ReadbackPool.StartReadback(GpuResource); When readback finishes, delegate declared in Initialize method will be called. Instance of GpuReadbackData<T> will be passed as a parameter, containing gpuData (native array containing data read back from the GPU) and captureTime (timestamp of the request start) fields. If you used your own type derived from GpuReadbackData<T> , your custom fields will be accessible too. StartReadback has multiple overloads that allow you to specify some of the readback parameters (like texture format, buffer offset etc.). It also returns instance of the GpuReadbackData<T> , so you can populate it with any data at the time of readback start. This is especially useful if you want to include any custom fields. Please note that GpuReadbackPool<TData, T> is a passive class and requires explicit updates from outside. You should call Process method whenever you want to trigger a check on pending readbacks. This is usually done from Update method of sensor class. For an example of GpuReadbackPool<TData, T> usage, see reading GPU data asynchronously section.","title":"Asynchronous GPU readback"},{"location":"system-under-test/sensor-message-publishing/#code-examples","text":"Snippets presented here are intended to explain parts of the sensor code responsible for message publishing. Large parts of the sensor code are omitted - see sensor plugins page for more complete examples.","title":"Code examples"},{"location":"system-under-test/sensor-message-publishing/#custom-sensor-data-type","text":"This example shows data type that supports thread-side caching through IThreadCachedBridgeData interface (see sensor data types for details). array field in this use case will contain reference to persistent buffer used by sensor. The same buffer is passed for all messages. For the sake of this example, let's assume that buffer size is constant, but data size is not. dataSize field will contain size of valid data in the buffer. Check one of publishing examples below to see how data is prepared. public class FloatArray : IThreadCachedBridgeData<FloatArray> { // Array reference passed from sensor. Shared across multiple messages. public float[] array; // Specifies size of valid data in array. public int dataSize; // Timestamp that is required by receiving side. public double timestamp; // Method defined by IThreadCachedBridgeData interface. // This will be called internally by BridgeMessageDispatcher before enqueuing message. // Parameter target contains instance provided by pooling system. // Implementation must perform deep copy from current instance to target. public void CopyToCache(FloatArray target) { // If array in target instance was not initialized or is too small, allocate new one. if (target.array == null || target.array.Length < dataSize) target.array = new float[dataSize]; // Copy all valid data to array in target instance. // Data beyond dataSize (if present) has undefined value, so it's skipped. Array.Copy(array, target.array, dataSize); // Value types can be simply assigned in target instance. target.dataSize = dataSize; target.timestamp = timestamp; } // Method defined by IThreadCachedBridgeData interface. // This should return hash that groups instances into compatible sub-pools. public int GetHash() { // No sub-pools required - return the same value for all instances. return 0; } }","title":"Custom sensor data type"},{"location":"system-under-test/sensor-message-publishing/#bridge-registration","text":"During initialization process, each sensor must define its expected interactions with bridge. OnBridgeSetup is an abstract method defined in SensorBase class, that is invoked once for each sensor if bridge instance is present. Each sensor must implement it. You can use it to get reference to the bridge instance and register publisher (or subscriber) of your data type. Delegate returned by AddPublisher method is responsible for converting and publishing messages. Data type used for this example ( FloatArray ) is described in its own section . Note that bridge plugin must implement conversion between this type and its own, bridge-specific message format. You can find more information about bridge plugin implementation on the bridge plugins page. // Declare sensor name and types of data that it will publish over the bridge. [SensorType(\"Float Array Sensor\", new[] {typeof(FloatArray)})] public class FloatArraySensor : SensorBase { // Field used to store reference to the bridge instance. BridgeInstance Bridge; // Field used to store publisher delegate. Publisher<FloatArray> Publish; // OnSetupBridge will be called during initialization if any bridge is present. public override void OnBridgeSetup(BridgeInstance bridge) { // Store reference to bridge instance so we can track its state. Bridge = bridge; // Register publisher of FloatArray data type for the bridge. // Returned delegate can be used to convert and publish messages. Publish = bridge.AddPublisher<FloatArray>(Topic); } // Visualization and sensor update code is omitted in this example. [...] }","title":"Bridge registration"},{"location":"system-under-test/sensor-message-publishing/#publishing-using-exclusive-token","text":"This example shows how custom data can be published using exclusive token - it assumes that message order is critical and enforces it by dropping any messages until previously enqueued one is published. Since array used as buffer is reused every frame, data type in this example utilizes thread-side caching . Exact time that conversion and publishing will take is unknown, so there's a risk that this data might be overwritten before conversion finishes. Using data type that supports caching is invisible from sensor code (as shown below), but all cross-thread access issues - like the one mentioned - are mitigated. // Declare sensor name and types of data that it will publish over the bridge. [SensorType(\"Float Array Sensor\", new[] {typeof(FloatArray)})] public class FloatArraySensor : SensorBase { // Field used to store reference to the bridge instance. BridgeInstance Bridge; // Field used to store publisher delegate. Publisher<FloatArray> Publish; // Preallocated array used as a buffer. public float[] array; // Update is called every frame. private void Update() { // Theoretical method that would calculate output for this sensor. // This is executed even if bridge is not connected, as data might be used inside Simulator. // Data is put in array. Since data size is dynamic, and array is not, dataSize stores valid data count. PrepareSensorData(array, out var dataSize); // Check if bridge exists and is currently connected. if (Bridge is {Status: Status.Connected}) { // Fetch current time from simulation manager. var time = SimulatorManager.Instance.CurrentTime; // Create data object for new message. Note that persistent buffer is passed as reference. var data = new FloatArray {array = array, dataSize = dataSize, timestamp = time}; // Try enqueueing publish request. Instance of this sensor is used as exclusive token. // If simulation is paused or token is already in use, this message will be dropped. // Sensor doesn't care whether message dropped or not, so returned value is ignored. BridgeMessageDispatcher.Instance.TryQueueTask(Publish, data, this); } } // Initialization and visualization code is omitted in this example. [...] }","title":"Publishing using exclusive token"},{"location":"system-under-test/sensor-message-publishing/#publishing-with-set-frequency","text":"This example shows how FrequencySensorBase can be used to calculate and publish data with set interval. See frequency-based publishing section for details. Similar to previous example, used data type utilizes thread-side caching to prevent simultaneous access from multiple threads to array buffer. // Declare sensor name and types of data that it will publish over the bridge. // Sensor type is derived from FrequencySensorBase instead of usual SensorBase. [SensorType(\"Float Array Sensor\", new[] {typeof(FloatArray)})] public class FloatArraySensor : FrequencySensorBase { // Field used to store reference to the bridge instance. BridgeInstance Bridge; // Field used to store publisher delegate. Publisher<FloatArray> Publish; // Preallocated array used as a buffer. public float[] array; // FixedUpdate is not required - standard update loop will be used. protected override bool UseFixedUpdate => false; // SensorUpdate is executed with frequency defined by Frequency sensor parameter in base class. // Frequency is based on simulation time, so pause or non-realtime mode will be handled properly. private override void SensorUpdate() { // Theoretical method that would calculate output for this sensor. // This is executed even if bridge is not connected, as data might be used inside Simulator. // Data is put in array. Since data size is dynamic, and array is not, dataSize stores valid data count. PrepareSensorData(array, out var dataSize); // Check if bridge exists and is currently connected. if (Bridge is {Status: Status.Connected}) { // Fetch current time from simulation manager. var time = SimulatorManager.Instance.CurrentTime; // Create data object for new message. Note that persistent buffer is passed as reference. var data = new FloatArray {array = array, dataSize = dataSize, timestamp = time}; // Try enqueueing publish request. This will always succeed, as both requirements are met: // - time is not paused (SensorUpdate wouldn't be called if it was) // - optional exclusive token is not provided BridgeMessageDispatcher.Instance.TryQueueTask(Publish, data); } } // Initialization and visualization code is omitted in this example. [...] }","title":"Publishing with set frequency"},{"location":"system-under-test/sensor-message-publishing/#reading-gpu-data-asynchronously","text":"This example shows how GpuReadbackPool<TData, T> can be used to perform GPU readbacks asynchronously. // Declare sensor name and types of data that it will publish over the bridge. [SensorType(\"Float Array Sensor\", new[] {typeof(FloatArray)})] public class FloatArraySensor : SensorBase { // Field used to store reference to the bridge instance. BridgeInstance Bridge; // Field used to store publisher delegate. Publisher<FloatArray> Publish; // Preallocated array used as a buffer. public float[] array; // Buffer used for storing data on GPU side. private ComputeBuffer computeBuffer; // Asynchronous GPU readback pool. private GpuReadbackPool<GpuReadbackData<Vector4>, Vector4> readbackPool; // Initialization method - prepare required resources. protected override void Initialize() { // Assume 100 floats will be used per data packet. array = new float[100]; computeBuffer = new ComputeBuffer(100, sizeof(float)); // Initialize GPU readback pool with 100 floats capacity. // OnReadbackComplete delegate will be called for each completed readback. readbackPool = new GpuReadbackPool<GpuReadbackData<float>, float>(); readbackPool.Initialize(100, OnReadbackComplete); } // Deinitialization method - free any currently used resources. protected override void Deinitialize() { computeBuffer.Release(); readbackPool.Dispose(); } // Update is called every frame. private void Update() { // Explicitly trigger update of all currently pending readbacks. readbackPool.Process(); // Theoretical method that would calculate output for this sensor. // This is executed even if bridge is not connected, as data might be used inside Simulator. // Data is calculated and kept on the GPU, in the compute buffer. PrepareSensorData(computeBuffer); // Request asynchronous read of the GPU resource. // Completion will trigger OnReadbackComplete declared during initialization. readbackPool.StartReadback(computeBuffer); } // Delegate called when readback is completed private void OnReadbackComplete(GpuReadbackData<float> data) { // Check if bridge exists and is currently connected. if (Bridge is {Status: Status.Connected}) { // data.gpuData contains data read back from the GPU. // Perform copy from NativeArray to managed array, since our message type doesn't support it. data.gpuData.CopyTo(array); // Publish data. See previous examples for details. // Note that time passed is capture time, not current time. var data = new FloatArray {array = array, dataSize = 100, timestamp = data.captureTime}; BridgeMessageDispatcher.Instance.TryQueueTask(Publish, data); } } // Bridge setup and visualization code is omitted in this example. [...] }","title":"Reading GPU data asynchronously"},{"location":"system-under-test/simulator-messages/","text":"Simulator Messages # This page details the messages available by default in the simulator. Each bridge type (ROS, ROS2, CyberRT) will have its own message types. Table of Contents ROS Message Types ROS 2 Message Types CyberRT Message Types ROS Message Types top # The simulator supports many of the common standard ROS messages. Additionally, the simulator supports custom ROS messages defined for Autoware AI as well as the simulator's template messages for Autonomous Driving which are included in lgsvl_msgs. Below is a list of these messages. Message Type Namespace Associated Sensor Default Topic Name Notes std_msgs/Header Ros - - Standard header used for ROS messages std_msgs/Time Ros - - Usually used for timestamps in std_msgs/Header rosgraph_msg/Clock Ros Clock Sensor /clock Required when /use_sim_time param is set to true std_msgs/ColorRGBA Ros - - - sensor_msgs/CompressedImage Ros Camera Sensor - JPEG encoded images - usually subscribed to by AD stacks for camera images sensor_msgs/Image Ros Camera Sensor - Uncompressed images - usually subscribed to by AD stacks for camera images sensor_msgs/Imu Ros Imu Sensor - Linear acceleration, angular velocity, and (optional) orientation as a quaternion sensor_msgs/Joy Ros - - Reports the state of a joystick / wheel axes and buttons sensor_msgs/LaserScan Ros - - Single beam LiDAR points reported as an array of range measurements sensor_msgs/PointCloud2 Ros LiDAR Sensor - Point cloud data as typically output from a 3D LiDAR's driver sensor_msgs/PointField Ros - - Included in sensor_msgs/PointCloud2 to provide type information on the point sensor_msgs/NavSatStatus Ros GPS Sensor - Status report for GNSS sensor_msgs/NavSatFix Ros GPS Sensor - GNSS position fix reported llh format nav_msgs/Odometry Ros GPS Odometry Sensor - An odometry message containing a pose and a twist message nmea_msgs/Sentence Ros GPS Sensor - GNSS fix represented as an NMEA sentence - used in Autoware AI geometry_msgs/Point Ros - - XYZ coordinates geometry_msgs/Pose Ros - - 3D position and orientation (quaternion) geometry_msgs/PoseWithCovariance Ros - - geometry_msgs/Pose with an additional covariance field geometry_msgs/Quaternion Ros - - - geometry_msgs/Twist Ros - - Linear and angular velocity geometry_msgs/TwistStamped Ros - lgsvl_msgs/DetectedRadarObject - Contains s geometry_msgs/Twist and a std_msgs/Header geometry_msgs/TwistWithCovariance Ros - - Contains a geometry_msgs/Twist with an additional covariance field geometry_msgs/Vector3 Ros - - 3D vector with x, y, z coordinates std_srvs/Empty Ros - - Empty service std_srvs/SetBool Ros - - Service for setting a boolean parameter std_srvs/Trigger Ros - - Service for triggering an event autoware_msgs/steer_cmd Autoware - - Steer command message autoware_msgs/accel_cmd Autoware - - Acceleration command message autoware_msgs/brake_cmd Autoware - - Brake command message autoware_msgs/lamp_cmd Autoware - - Message for controlling lights on vehicle autoware_msgs/ControlCommand Autoware - - Vehicle control command providing linear velocity, linear acceleration, and a steering angle autoware_msgs/VehicleCmd Autoware AutowareAI Control Sensor /vehicle_cmd Complete vehicle control command containing the other 'command' message - simulator subscribes to this autoware_msgs/DetectedObject Autoware - - Attributes of objects detected by perception autoware_msgs/DetectedObjectArray Autoware - - Array of autoware_msgs/DetectedObject lgsvl_msgs/BoundingBox2D Lgsvl - - 2D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/BoundingBox3D Lgsvl - - 3D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/Detection2D Lgsvl - - Ground truth detection data for obstacles in 2D lgsvl_msgs/Detection2DArray Lgsvl Ground Truth 2D Sensor - Array of ground truth values for 2D detections lgsvl_msgs/Detection3D Lgsvl - - Ground truth detection data for obstacles in 3D lgsvl_msgs/Detection3DArray Lgsvl Ground Truth 3D Sensor - Array of ground truth values for 3D detections lgsvl_msgs/Signal Lgsvl - - Ground truth traffic light information lgsvl_msgs/SignalArray Lgsvl Signal Sensor - Array of lgsvl_msgs/Signal for all traffic lights in view of ego lgsvl_msgs/CanBusData Lgsvl CanBus Sensor - Collection of information available through CAN bus and published by simulator lgsvl_msgs/VehicleControlData Lgsvl Vehicle Control Sensor - Vehicle control command simulator can subscribe to lgsvl_msgs/VehicleStateData Lgsvl Vehicle State Sensor - Feedback from vehicle control command published by simulator which can be used for implementing a feedback controller lgsvl_msgs/VehicleOdometry Lgsvl Vehicle Odometry Sensor - Velocity and front and rear wheel angle measurements lgsvl_msgs/DetectedRadarObject Lgsvl - - Representation of an obstacle detected by radar lgsvl_msgs/DetectedRadarObjectArray Lgsvl Radar Sensor - Array of lgsvl_msgs/DetectedRadarObject published by the simulator lgsvl_srvs/Int Lgsvl - - Service that sends an integer value lgsvl_srvs/String Lgsvl - - Service that sends a string ROS 2 Message Types top # There is a lot of overlap between the message types supported for ROS and ROS 2 with the main difference being Autoware AI. Message Type Namespace Associated Sensor Default Topic Name Notes std_msgs/Header Ros2 - - Standard header used for ROS 2 messages builtin_interfaces/Time Ros2 - - Usually used for timestamps in std_msgs/Header rosgraph_msg/Clock Ros2 Clock Sensor /clock Required when /use_sim_time param is set to true std_msgs/ColorRGBA Ros2 - - - sensor_msgs/CompressedImage Ros2 Camera Sensor - JPEG encoded images - usually subscribed to by AD stacks for camera images sensor_msgs/Image Ros2 Camera Sensor - Uncompressed images - usually subscribed to by AD stacks for camera images sensor_msgs/Imu Ros2 Imu Sensor - Linear acceleration, angular velocity, and (optional) orientation as a quaternion nav_msgs/Odometry Ros2 GPS Odometry Sensor - An odometry message containing a pose and a twist message sensor_msgs/LaserScan Ros2 - - Single beam LiDAR points reported as an array of range measurements sensor_msgs/PointCloud2 Ros2 LiDAR Sensor - Point cloud data as typically output from a 3D LiDAR's driver sensor_msgs/PointField Ros2 - - Included in sensor_msgs/PointCloud2 to provide type information on the point sensor_msgs/NavSatStatus Ros2 GPS Sensor - Status report for GNSS sensor_msgs/NavSatFix Ros2 GPS Sensor - GNSS position fix reported llh format nmea_msgs/Sentence Ros2 GPS Sensor - GNSS fix represented as an NMEA sentence geometry_msgs/Point Ros2 - - XYZ coordinates geometry_msgs/Pose Ros2 - - 3D position and orientation (quaternion) geometry_msgs/PoseWithCovariance Ros2 - - geometry_msgs/Pose with an additional covariance field geometry_msgs/Quaternion Ros2 - - - geometry_msgs/Twist Ros2 - - Linear and angular velocity geometry_msgs/TwistStamped Ros2 - lgsvl_msgs/DetectedRadarObject - Contains s geometry_msgs/Twist and a std_msgs/Header geometry_msgs/TwistWithCovariance Ros2 - - Contains a geometry_msgs/Twist with an additional covariance field geometry_msgs/Vector3 Ros2 - - 3D vector with x, y, z coordinates lgsvl_msgs/BoundingBox2D Lgsvl - - 2D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/BoundingBox3D Lgsvl - - 3D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/Detection2D Lgsvl - - Ground truth detection data for obstacles in 2D lgsvl_msgs/Detection2DArray Lgsvl Ground Truth 2D Sensor - Array of ground truth values for 2D detections lgsvl_msgs/Detection3D Lgsvl - - Ground truth detection data for obstacles in 3D lgsvl_msgs/Detection3DArray Lgsvl Ground Truth 3D Sensor - Array of ground truth values for 3D detections lgsvl_msgs/Signal Lgsvl - - Ground truth traffic light information lgsvl_msgs/SignalArray Lgsvl Signal Sensor - Array of lgsvl_msgs/Signal for all traffic lights in view of ego lgsvl_msgs/CanBusData Lgsvl CanBus Sensor - Collection of information available through CAN bus and published by simulator lgsvl_msgs/VehicleControlData Lgsvl Vehicle Control Sensor - Vehicle control command simulator can subscribe to lgsvl_msgs/VehicleStateData Lgsvl Vehicle State Sensor - Feedback from vehicle control command published by simulator which can be used for implementing a feedback controller lgsvl_msgs/VehicleOdometry Lgsvl Vehicle Odometry Sensor - Velocity and front and rear wheel angle measurements lgsvl_msgs/DetectedRadarObject Lgsvl - - Representation of an obstacle detected by radar lgsvl_msgs/DetectedRadarObjectArray Lgsvl Radar Sensor - Array of lgsvl_msgs/DetectedRadarObject published by the simulator CyberRT Message Types top # Latest versions of Apollo (>=3.5) use a middleware called CyberRT. The SVL Simulator provides a bridge for communication with Apollo using CyberRT messages. The supported messages are listed below: Message Type Namespace Associated Sensor Default Channel Name Notes apollo.canbus.Chassis Cyber CanBus Sensor /apollo/canbus/chassis CAN bus message published by the simulator apollo.control.ControlCommand Cyber CanBus Sensor /apollo/control Vehicle control message the simulator subscribes to apollo.localization.Gps Cyber GPS Odometry Sensor /apollo/sensor/gnss/odometry Vehicle world coordinates in UTM as well as orientation and velocity apollo.drivers.gnss.GnssBestPose Cyber GPS Sensor /apollo/sensor/gnss/best_pose World coordinates in LLH apollo.drivers.gnss.Imu Cyber IMU Sensor /apollo/sensor/gnss/imu Raw IMU sensor measurements apollo.drivers.gnss.CorrectedImu Cyber IMU Sensor /apollo/sensor/gnss/corrected_imu Corrected IMU measurements along with orientation apollo.drivers.gnss.InsStat Cyber GPS-INS Status /apollo/sensor/gnss/ins_stat INS Status apollo.drivers.ContiRadar Cyber Radar Sensor /apollo/sensor/conti_radar Continental radar messages apollo.drivers.PointCloud Cyber LiDAR Sensor /apollo/sensor/lidar128/compensator/PointCloud2 Compensated LiDAR pointcloud message apollo.drivers.Image Cyber Camera Sensor /apollo/sensor/camera/front_12mm/image images from RGB camera apollo.drivers.CompressedImage Cyber Camera Sensor /apollo/sensor/camera/front_12mm/image/compressed Compressed (jpg encoded) images from RGB camera apollo.perception.PerceptionObstacles Cyber Ground Truth 3D Sensor /apollo/perception/obstacles List of detected obstacles used in modular testing to replace the perception module apollo.perception.TrafficLightDetection Cyber Signal Sensor /apollo/perception/traffic_light List of detected traffic lights and their status used in modular testing to replace the perception module","title":"Simulator messages"},{"location":"system-under-test/simulator-messages/#ros-message-types","text":"The simulator supports many of the common standard ROS messages. Additionally, the simulator supports custom ROS messages defined for Autoware AI as well as the simulator's template messages for Autonomous Driving which are included in lgsvl_msgs. Below is a list of these messages. Message Type Namespace Associated Sensor Default Topic Name Notes std_msgs/Header Ros - - Standard header used for ROS messages std_msgs/Time Ros - - Usually used for timestamps in std_msgs/Header rosgraph_msg/Clock Ros Clock Sensor /clock Required when /use_sim_time param is set to true std_msgs/ColorRGBA Ros - - - sensor_msgs/CompressedImage Ros Camera Sensor - JPEG encoded images - usually subscribed to by AD stacks for camera images sensor_msgs/Image Ros Camera Sensor - Uncompressed images - usually subscribed to by AD stacks for camera images sensor_msgs/Imu Ros Imu Sensor - Linear acceleration, angular velocity, and (optional) orientation as a quaternion sensor_msgs/Joy Ros - - Reports the state of a joystick / wheel axes and buttons sensor_msgs/LaserScan Ros - - Single beam LiDAR points reported as an array of range measurements sensor_msgs/PointCloud2 Ros LiDAR Sensor - Point cloud data as typically output from a 3D LiDAR's driver sensor_msgs/PointField Ros - - Included in sensor_msgs/PointCloud2 to provide type information on the point sensor_msgs/NavSatStatus Ros GPS Sensor - Status report for GNSS sensor_msgs/NavSatFix Ros GPS Sensor - GNSS position fix reported llh format nav_msgs/Odometry Ros GPS Odometry Sensor - An odometry message containing a pose and a twist message nmea_msgs/Sentence Ros GPS Sensor - GNSS fix represented as an NMEA sentence - used in Autoware AI geometry_msgs/Point Ros - - XYZ coordinates geometry_msgs/Pose Ros - - 3D position and orientation (quaternion) geometry_msgs/PoseWithCovariance Ros - - geometry_msgs/Pose with an additional covariance field geometry_msgs/Quaternion Ros - - - geometry_msgs/Twist Ros - - Linear and angular velocity geometry_msgs/TwistStamped Ros - lgsvl_msgs/DetectedRadarObject - Contains s geometry_msgs/Twist and a std_msgs/Header geometry_msgs/TwistWithCovariance Ros - - Contains a geometry_msgs/Twist with an additional covariance field geometry_msgs/Vector3 Ros - - 3D vector with x, y, z coordinates std_srvs/Empty Ros - - Empty service std_srvs/SetBool Ros - - Service for setting a boolean parameter std_srvs/Trigger Ros - - Service for triggering an event autoware_msgs/steer_cmd Autoware - - Steer command message autoware_msgs/accel_cmd Autoware - - Acceleration command message autoware_msgs/brake_cmd Autoware - - Brake command message autoware_msgs/lamp_cmd Autoware - - Message for controlling lights on vehicle autoware_msgs/ControlCommand Autoware - - Vehicle control command providing linear velocity, linear acceleration, and a steering angle autoware_msgs/VehicleCmd Autoware AutowareAI Control Sensor /vehicle_cmd Complete vehicle control command containing the other 'command' message - simulator subscribes to this autoware_msgs/DetectedObject Autoware - - Attributes of objects detected by perception autoware_msgs/DetectedObjectArray Autoware - - Array of autoware_msgs/DetectedObject lgsvl_msgs/BoundingBox2D Lgsvl - - 2D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/BoundingBox3D Lgsvl - - 3D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/Detection2D Lgsvl - - Ground truth detection data for obstacles in 2D lgsvl_msgs/Detection2DArray Lgsvl Ground Truth 2D Sensor - Array of ground truth values for 2D detections lgsvl_msgs/Detection3D Lgsvl - - Ground truth detection data for obstacles in 3D lgsvl_msgs/Detection3DArray Lgsvl Ground Truth 3D Sensor - Array of ground truth values for 3D detections lgsvl_msgs/Signal Lgsvl - - Ground truth traffic light information lgsvl_msgs/SignalArray Lgsvl Signal Sensor - Array of lgsvl_msgs/Signal for all traffic lights in view of ego lgsvl_msgs/CanBusData Lgsvl CanBus Sensor - Collection of information available through CAN bus and published by simulator lgsvl_msgs/VehicleControlData Lgsvl Vehicle Control Sensor - Vehicle control command simulator can subscribe to lgsvl_msgs/VehicleStateData Lgsvl Vehicle State Sensor - Feedback from vehicle control command published by simulator which can be used for implementing a feedback controller lgsvl_msgs/VehicleOdometry Lgsvl Vehicle Odometry Sensor - Velocity and front and rear wheel angle measurements lgsvl_msgs/DetectedRadarObject Lgsvl - - Representation of an obstacle detected by radar lgsvl_msgs/DetectedRadarObjectArray Lgsvl Radar Sensor - Array of lgsvl_msgs/DetectedRadarObject published by the simulator lgsvl_srvs/Int Lgsvl - - Service that sends an integer value lgsvl_srvs/String Lgsvl - - Service that sends a string","title":"ROS Message Types"},{"location":"system-under-test/simulator-messages/#ros2-message-types","text":"There is a lot of overlap between the message types supported for ROS and ROS 2 with the main difference being Autoware AI. Message Type Namespace Associated Sensor Default Topic Name Notes std_msgs/Header Ros2 - - Standard header used for ROS 2 messages builtin_interfaces/Time Ros2 - - Usually used for timestamps in std_msgs/Header rosgraph_msg/Clock Ros2 Clock Sensor /clock Required when /use_sim_time param is set to true std_msgs/ColorRGBA Ros2 - - - sensor_msgs/CompressedImage Ros2 Camera Sensor - JPEG encoded images - usually subscribed to by AD stacks for camera images sensor_msgs/Image Ros2 Camera Sensor - Uncompressed images - usually subscribed to by AD stacks for camera images sensor_msgs/Imu Ros2 Imu Sensor - Linear acceleration, angular velocity, and (optional) orientation as a quaternion nav_msgs/Odometry Ros2 GPS Odometry Sensor - An odometry message containing a pose and a twist message sensor_msgs/LaserScan Ros2 - - Single beam LiDAR points reported as an array of range measurements sensor_msgs/PointCloud2 Ros2 LiDAR Sensor - Point cloud data as typically output from a 3D LiDAR's driver sensor_msgs/PointField Ros2 - - Included in sensor_msgs/PointCloud2 to provide type information on the point sensor_msgs/NavSatStatus Ros2 GPS Sensor - Status report for GNSS sensor_msgs/NavSatFix Ros2 GPS Sensor - GNSS position fix reported llh format nmea_msgs/Sentence Ros2 GPS Sensor - GNSS fix represented as an NMEA sentence geometry_msgs/Point Ros2 - - XYZ coordinates geometry_msgs/Pose Ros2 - - 3D position and orientation (quaternion) geometry_msgs/PoseWithCovariance Ros2 - - geometry_msgs/Pose with an additional covariance field geometry_msgs/Quaternion Ros2 - - - geometry_msgs/Twist Ros2 - - Linear and angular velocity geometry_msgs/TwistStamped Ros2 - lgsvl_msgs/DetectedRadarObject - Contains s geometry_msgs/Twist and a std_msgs/Header geometry_msgs/TwistWithCovariance Ros2 - - Contains a geometry_msgs/Twist with an additional covariance field geometry_msgs/Vector3 Ros2 - - 3D vector with x, y, z coordinates lgsvl_msgs/BoundingBox2D Lgsvl - - 2D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/BoundingBox3D Lgsvl - - 3D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/Detection2D Lgsvl - - Ground truth detection data for obstacles in 2D lgsvl_msgs/Detection2DArray Lgsvl Ground Truth 2D Sensor - Array of ground truth values for 2D detections lgsvl_msgs/Detection3D Lgsvl - - Ground truth detection data for obstacles in 3D lgsvl_msgs/Detection3DArray Lgsvl Ground Truth 3D Sensor - Array of ground truth values for 3D detections lgsvl_msgs/Signal Lgsvl - - Ground truth traffic light information lgsvl_msgs/SignalArray Lgsvl Signal Sensor - Array of lgsvl_msgs/Signal for all traffic lights in view of ego lgsvl_msgs/CanBusData Lgsvl CanBus Sensor - Collection of information available through CAN bus and published by simulator lgsvl_msgs/VehicleControlData Lgsvl Vehicle Control Sensor - Vehicle control command simulator can subscribe to lgsvl_msgs/VehicleStateData Lgsvl Vehicle State Sensor - Feedback from vehicle control command published by simulator which can be used for implementing a feedback controller lgsvl_msgs/VehicleOdometry Lgsvl Vehicle Odometry Sensor - Velocity and front and rear wheel angle measurements lgsvl_msgs/DetectedRadarObject Lgsvl - - Representation of an obstacle detected by radar lgsvl_msgs/DetectedRadarObjectArray Lgsvl Radar Sensor - Array of lgsvl_msgs/DetectedRadarObject published by the simulator","title":"ROS 2 Message Types"},{"location":"system-under-test/simulator-messages/#cyberrt-message-types","text":"Latest versions of Apollo (>=3.5) use a middleware called CyberRT. The SVL Simulator provides a bridge for communication with Apollo using CyberRT messages. The supported messages are listed below: Message Type Namespace Associated Sensor Default Channel Name Notes apollo.canbus.Chassis Cyber CanBus Sensor /apollo/canbus/chassis CAN bus message published by the simulator apollo.control.ControlCommand Cyber CanBus Sensor /apollo/control Vehicle control message the simulator subscribes to apollo.localization.Gps Cyber GPS Odometry Sensor /apollo/sensor/gnss/odometry Vehicle world coordinates in UTM as well as orientation and velocity apollo.drivers.gnss.GnssBestPose Cyber GPS Sensor /apollo/sensor/gnss/best_pose World coordinates in LLH apollo.drivers.gnss.Imu Cyber IMU Sensor /apollo/sensor/gnss/imu Raw IMU sensor measurements apollo.drivers.gnss.CorrectedImu Cyber IMU Sensor /apollo/sensor/gnss/corrected_imu Corrected IMU measurements along with orientation apollo.drivers.gnss.InsStat Cyber GPS-INS Status /apollo/sensor/gnss/ins_stat INS Status apollo.drivers.ContiRadar Cyber Radar Sensor /apollo/sensor/conti_radar Continental radar messages apollo.drivers.PointCloud Cyber LiDAR Sensor /apollo/sensor/lidar128/compensator/PointCloud2 Compensated LiDAR pointcloud message apollo.drivers.Image Cyber Camera Sensor /apollo/sensor/camera/front_12mm/image images from RGB camera apollo.drivers.CompressedImage Cyber Camera Sensor /apollo/sensor/camera/front_12mm/image/compressed Compressed (jpg encoded) images from RGB camera apollo.perception.PerceptionObstacles Cyber Ground Truth 3D Sensor /apollo/perception/obstacles List of detected obstacles used in modular testing to replace the perception module apollo.perception.TrafficLightDetection Cyber Signal Sensor /apollo/perception/traffic_light List of detected traffic lights and their status used in modular testing to replace the perception module","title":"CyberRT Message Types"},{"location":"system-under-test/sut-introduction/","text":"System Under Test Introduction # System Under Test refers to any module or system that you connect to SVL Simulator to test or verify. This can be a specific software module and its algorithms, an end-to-end autonomous system including target computing hardware, or a reference robotics software stack running on a desktop PC. By connecting the System Under Test to SVL Simulator, you can virtually simulate the real-world conditions and environmental input required by the systems you are developing. This includes the physics state of the virtual 3D environment, including all dynamic actors like traffic, and sensor data that interprets this environment. Interfacing with SVL Simulator # The simulator provides several default communication interface mechanisms that are common within robotics, including ROS and ROS2-based communication. You can see all of the supported message types for ROS, ROS2, and CyberRT by our default bridges here . If you use a custom or proprietary communication protocol and interface for your System Under Test, SVL Simulator bridge plugins allow you to build the proper interface to handle the specific format and protocol of your communication. Reference Systems Under Test # SVL Simulator itself does not contain any autonomous vehicle software or logic that runs on a self-driving vehicle or robot - this system (the System Under Test) is provided and tested by the user together with the simulator. We instead provide guides and tutorials on open source reference platforms that can be easily connected to SVL Simulator. This provides a jumping off point for additional development of particular algorithms, replacement of various modules, higher-level testing for applications like Future Mobility. See the links below for guides on the running a reference autonomous driving system: ROS-based AD system Autoware.Auto (ROS 2) Apollo (latest) Apollo 5.0","title":"Introduction"},{"location":"system-under-test/sut-introduction/#interfacing-with-svl-simulator","text":"The simulator provides several default communication interface mechanisms that are common within robotics, including ROS and ROS2-based communication. You can see all of the supported message types for ROS, ROS2, and CyberRT by our default bridges here . If you use a custom or proprietary communication protocol and interface for your System Under Test, SVL Simulator bridge plugins allow you to build the proper interface to handle the specific format and protocol of your communication.","title":"Interfacing with SVL Simulator"},{"location":"system-under-test/sut-introduction/#reference-systems-under-test","text":"SVL Simulator itself does not contain any autonomous vehicle software or logic that runs on a self-driving vehicle or robot - this system (the System Under Test) is provided and tested by the user together with the simulator. We instead provide guides and tutorials on open source reference platforms that can be easily connected to SVL Simulator. This provides a jumping off point for additional development of particular algorithms, replacement of various modules, higher-level testing for applications like Future Mobility. See the links below for guides on the running a reference autonomous driving system: ROS-based AD system Autoware.Auto (ROS 2) Apollo (latest) Apollo 5.0","title":"Reference Systems Under Test"},{"location":"third-party-integration/openai-gym/","text":"Reinforcement Learning with OpenAI Gym # OpenAI Gym is a toolkit for developing reinforcement learning algorithms. Gym provides a collection of test problems called environments which can be used to train an agent using a reinforcement learning. Each environment defines the reinforcement learnign problem the agent will try to solve. To facilitate developing reinforcement learning algorithms with the SVL Simulator , we have developed gym-lgsvl , a custom environment that using the openai gym interface. gym-lgsvl can be used with general reinforcement learning algorithms implementations that are compatible with openai gym. Developers can modify the environment to define the specific reinforcement learning problem they are trying to solve. Requirements top # Python >= 3.5 Pip lgsvl simulator API openai gym numpy opencv Setup top # Install SVL Simulator using this guide : Install Python API using this guide . Install openai gym, numpy, and opencv: pip install --user gym, numpy, opencv-python Clone the gym-lgsvl repository: git clone https://github.com/lgsvl/gym-lgsvl.git Install gym-lgsvl using pip: cd gym-lgsvl/ pip install --user -e . Getting Started top # The simulator must be running on the menu scene to be used with the gym-lgsvl . The scene can be loaded either in the Unity Editor or the simulator binary build (download latest release ). The binary build will have superior performance. The script named random_agent.py will launch a random agent which will sample acceleration and steering values for the ego vehicle for every step of the episode. Run the agent to test your setup: ./random_agent.py After a few seconds the BorregasAve scene should load and spawn the ego vehicle along with some NPC vehicles. The ego vehicle will drive randomly. gym_lgsvl can be used with RL libraries that support openai gym environments. Below is an example of training using the A2C implementation from baselines : python -m baselines.run --alg=a2c --env=gym_lgsvl:lgsvl-v0 --num_timesteps=1e5 Customizing the environment top # The specifics of the environment you will need will depend on the reinforcement learning problem you are trying to solve. By default, the gym-lgsvl environment has a simple setup intended to be a starting point for building more advanced problems. Training an agent with the default environment would be difficult without modiification. In the default configuration, the vehicle uses a single front facing camera as observation and uses continuous control parameters for driving the vehicle. For more advanced state representations, modifications will be needed. The entire environment is defined in lgsvl_env.py . CONFIG top # Some of the basic configuration are passed to the environment through CONFIG . action_space and observation_space definitions are required and are defined using gym.spaces . The default action_space is: \"action_space\" : spaces.Box( np.array([-1,-1]), np.array([+1,+1,]), dtype=np.float32, ), which defines a continuous action space defined as a 2-D array. The first element is the steering value and the second is braking/throttle (negative values are braking and positive are throttle). The observation space is defined as a single camera image from the front camera using the Box space from gym: \"observation_space\" : spaces.Box( low=0, high=255, shape=(297, 528, 3), dtype=np.uint8 ) # RGB image from front camera The shape tuple specifies the image size. The simulator API will always send 1920x1080 images. If any other size is used to define the observation space, the camera images will be resized to the specified size before being passed on as an observation. Reward calculation top # The environment also calculates a reward function based on the distance travelled by the ego vehicle and collisions. The reward is calculated in the _calculate_reward() function based on distance travelled. The collision penalty is added on at the end (if applicable) when the collision callback is invoked ( _on_collision ). The collision callback will also terminate the episode and start a new episode. Sensors top # By default the ego vehicle will only use the front facing camera. The ego vehicle is setup using _setup_ego() . Sensors are also defined here. To define more sensors, grab the sensor from the sensors list and invoke its specific methods to save data. To collect observations, _get_observations() is called. NPC Behavior top # The NPCs are defined in _setup_npc . The NPCs are spawned randomly around the ego vehicle spawn point and will follow the lanes and traffic rules to move around.","title":"OpenAI Gym"},{"location":"third-party-integration/openai-gym/#requirements","text":"Python >= 3.5 Pip lgsvl simulator API openai gym numpy opencv","title":"Requirements"},{"location":"third-party-integration/openai-gym/#setup","text":"Install SVL Simulator using this guide : Install Python API using this guide . Install openai gym, numpy, and opencv: pip install --user gym, numpy, opencv-python Clone the gym-lgsvl repository: git clone https://github.com/lgsvl/gym-lgsvl.git Install gym-lgsvl using pip: cd gym-lgsvl/ pip install --user -e .","title":"Setup"},{"location":"third-party-integration/openai-gym/#getting-started","text":"The simulator must be running on the menu scene to be used with the gym-lgsvl . The scene can be loaded either in the Unity Editor or the simulator binary build (download latest release ). The binary build will have superior performance. The script named random_agent.py will launch a random agent which will sample acceleration and steering values for the ego vehicle for every step of the episode. Run the agent to test your setup: ./random_agent.py After a few seconds the BorregasAve scene should load and spawn the ego vehicle along with some NPC vehicles. The ego vehicle will drive randomly. gym_lgsvl can be used with RL libraries that support openai gym environments. Below is an example of training using the A2C implementation from baselines : python -m baselines.run --alg=a2c --env=gym_lgsvl:lgsvl-v0 --num_timesteps=1e5","title":"Getting Started"},{"location":"third-party-integration/openai-gym/#customizing-the-environment","text":"The specifics of the environment you will need will depend on the reinforcement learning problem you are trying to solve. By default, the gym-lgsvl environment has a simple setup intended to be a starting point for building more advanced problems. Training an agent with the default environment would be difficult without modiification. In the default configuration, the vehicle uses a single front facing camera as observation and uses continuous control parameters for driving the vehicle. For more advanced state representations, modifications will be needed. The entire environment is defined in lgsvl_env.py .","title":"Customizing the environment"},{"location":"third-party-integration/openai-gym/#config","text":"Some of the basic configuration are passed to the environment through CONFIG . action_space and observation_space definitions are required and are defined using gym.spaces . The default action_space is: \"action_space\" : spaces.Box( np.array([-1,-1]), np.array([+1,+1,]), dtype=np.float32, ), which defines a continuous action space defined as a 2-D array. The first element is the steering value and the second is braking/throttle (negative values are braking and positive are throttle). The observation space is defined as a single camera image from the front camera using the Box space from gym: \"observation_space\" : spaces.Box( low=0, high=255, shape=(297, 528, 3), dtype=np.uint8 ) # RGB image from front camera The shape tuple specifies the image size. The simulator API will always send 1920x1080 images. If any other size is used to define the observation space, the camera images will be resized to the specified size before being passed on as an observation.","title":"CONFIG"},{"location":"third-party-integration/openai-gym/#reward-calculation","text":"The environment also calculates a reward function based on the distance travelled by the ego vehicle and collisions. The reward is calculated in the _calculate_reward() function based on distance travelled. The collision penalty is added on at the end (if applicable) when the collision callback is invoked ( _on_collision ). The collision callback will also terminate the episode and start a new episode.","title":"Reward calculation"},{"location":"third-party-integration/openai-gym/#sensors","text":"By default the ego vehicle will only use the front facing camera. The ego vehicle is setup using _setup_ego() . Sensors are also defined here. To define more sensors, grab the sensor from the sensors list and invoke its specific methods to save data. To collect observations, _get_observations() is called.","title":"Sensors"},{"location":"third-party-integration/openai-gym/#npc-behavior","text":"The NPCs are defined in _setup_npc . The NPCs are spawned randomly around the ego vehicle spawn point and will follow the lanes and traffic rules to move around.","title":"NPC Behavior"},{"location":"tutorials/create-ros2-ad-stack/","text":"How to create a ROS2-based AD stack with SVL Simulator # This documentation describes how to develop ROS2 nodes to receive sensor data from SVL Simulator and send control commands to drive a car. The Lane Following model is a ROS2 -based Autonomous Driving stack developed with SVL Simulator . In high-level overview, the model is composed of three modules: a sensor module, a perception module, and a control module. The sensor module receives raw sensor data such as camera images from the simulator and preprocess the data before feeding into the perception module. Then, the perception module takes in the preprocessed data, extracts lane information, and predicts steering wheel commands. Finally, the control module sends a predicted control command back to the simulator, which would drive a car autonomously. Table of Contents Requirements Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Creating a ROS2 Package setup.py package.xml Building a ROS2 Package Running ROS2 LGSVL Bridge Writing ROS2 Subscriber Node Subscribe to a single topic Subscribe to multiple topics simultaneously Writing ROS2 Publisher Node Publish command back to SVL Simulator Running ROS2 Node References Requirements top # Docker Python3 ROS2 TensorFlow, Keras SVL Simulator Setup top # Our AD stack implementation is based on ROS2 and uses rosbridge to communicate with SVL Simulator. To do that, we need to prepare Ubuntu machine with ROS2 installed. We provide a Docker image containing Ubuntu 18.04 and ROS2 installed so you can just pull the Docker image and start writing code right away. Installing Docker CE top # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing NVIDIA Docker top # Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling Docker Image top # docker pull lgsvl/lanefollowing:latest What's inside Docker Image # Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS1 melodic + rosbridge ROS2 dashing + rosbridge Jupyter Notebook Creating a ROS2 Package top # A ROS2 package is simply a directory and should contain files named package.xml and setup.py . Create folders as below and create setup.py and package.xml . Please note that the package name must match with the folder name of your ROS package. ros2_ws/ src/ lane_following/ setup.py package.xml setup.py # from setuptools import setup package_name = 'lane_following' setup( name=package_name, version='0.0.1', packages=[ 'train', ], py_modules=[ 'collect', 'drive', ], install_requires=['setuptools'], zip_safe=True, author='David Uhm', author_email='david.uhm@lge.com', maintainer='David Uhm', maintainer_email='david.uhm@lge.com', keywords=[ 'ROS', 'ROS2', 'deep learning', 'lane following', 'end to end', 'SVL Simulator', 'Autonomous Driving' ], classifiers=[ 'Intended Audience :: Developers', 'Programming Language :: Python', 'Topic :: Software Development', ], description='ROS2-based End-to-End Lane Following model', license='BSD', tests_require=['pytest'], entry_points={ 'console_scripts': [ 'collect = collect:main', 'drive = drive:main', ], }, ) package.xml # <?xml version=\"1.0\"?> <package format=\"2\"> <name>lane_following</name> <version>0.0.1</version> <description>ROS2-based End-to-End Lane Following model</description> <maintainer email=\"david.uhm@lge.com\">David Uhm</maintainer> <license>BSD</license> <exec_depend>rclpy</exec_depend> <exec_depend>std_msgs</exec_depend> <exec_depend>sensor_msgs</exec_depend> <test_depend>ament_copyright</test_depend> <test_depend>ament_flake8</test_depend> <test_depend>ament_pep257</test_depend> <test_depend>python3-pytest</test_depend> <export> <build_type>ament_python</build_type> </export> </package> Building a ROS2 Package top # Now, you can build your package as below: source /opt/ros/dashing/setup.bash cd ~/ros2_ws colcon build --symlink-install Running ROS2 LGSVL Bridge top # ROS2 LGSVL Bridge provides a JSON API to ROS functionality for non-ROS programs such as SVL Simulator. It is not provided as part of a default ROS2 Dashing installation, but has been installed separately for you in the Docker image already. You can run ROS2 LGSVL Bridge to connect your ROS node with SVL Simulator as below: source /path/to/ros2-lgsvl-bridge/install/setup.bash lgsvl_bridge Writing ROS2 Subscriber Node top # ROS nodes communicate with each other by passing messages. Messages are routed via a topic with publish/subscribe concepts. A node sends out a message by publishing it to a given topic. Then, a node that is interested in a certain kind of data will subscribe to the appropriate topic. In our cases, SVL Simulator publishes sensor data such as camera images or LiDAR point clouds via rosbridge, and then the Lane Following model subscribes to that topic to receive sensor data, preprocesses the data, feeds them into the pretrained model, and finally computes a control command based on perceived sensor data. Below is an example of how to subscribe to sensor data topics from a ROS node. You can subscribe to a single topic only or multiple topics simultaneously. Subscribe to a single topic top # import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage class Drive(Node): def __init__(self): super().__init__('drive') self.sub_image = self.create_subscription(CompressedImage, '/simulator/sensor/camera/center/compressed', self.callback) def callback(self, msg): self.get_logger().info('Subscribed: {}'.format(msg.data)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main() Subscribe to multiple topics simultaneously top # In order to subscribe to ROS messages of different types from multiple sources, we need to take the timestamps of those messages into account. ROS2 Message Filters is the ROS package that synchronizes incoming messages by the timestamps contained in their headers and outputs them in the form of a single callback. Install this package in your ROS2 workspace and build it. cd ~/ros2_ws/src git clone https://github.com/intel/ros2_message_filters.git cd .. colcon build --symlink-install Then, you can import it in your Python script by import message_filters and use it as below: import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage from geometry_msgs.msg import TwistStamped import message_filters class Collect(Node): def __init__(self): super().__init__('collect') sub_center_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/center/compressed') sub_left_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/left/compressed') sub_right_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/right/compressed') sub_control = message_filters.Subscriber(self, TwistStamped, '/simulator/control/command') ts = message_filters.ApproximateTimeSynchronizer([sub_center_camera, sub_left_camera, sub_right_camera, sub_control], 1, 0.1) ts.registerCallback(self.callback) def callback(self, center_camera, left_camera, right_camera, control): self.get_logger().info('Subscribed: {}'.format(control.twist.angular.x)) def main(args=None): rclpy.init(args=args) collect = Collect() rclpy.spin(collect) if __name__ == '__main__': main() Writing ROS2 Publisher Node top # The publisher sends data to a topic. When you create a publisher you have to tell ROS of which type the data will be. In order to drive a car autonomously, the Lane Following model publishes a predicted control command back to the simulator via rosbridge. Publish command back to SVL Simulator top # import rclpy from rclpy.node import Node from geometry_msgs.msg import TwistStamped class Drive(Node): def __init__(self): super().__init__('drive') self.control_pub = self.create_publisher(TwistStamped, '/lanefollowing/steering_cmd') self.timer_period = 0.02 # seconds self.timer = self.create_timer(self.timer_period, self.callback) self.steering = 0. def callback(self): message = TwistStamped() message.twist.angular.x = float(self.steering) self.control_pub.publish(message) self.get_logger().info('Publishing: {}'.format(message.twist.angular.x)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main() Running ROS2 Node top # Once you have setup the rosbridge connection to SVL Simulator, you can launch your ROS node as follows: source /opt/ros/dashing/setup.bash source /path/to/ros2_ws/install/local_setup.bash ros2 run {your_package} {your_node} References top # Lane Following Github Repository SVL Simulator ROS2 Documentation ROS2 Message Filters","title":"Creating a simple ROS2-based AD stack"},{"location":"tutorials/create-ros2-ad-stack/#requirements","text":"Docker Python3 ROS2 TensorFlow, Keras SVL Simulator","title":"Requirements"},{"location":"tutorials/create-ros2-ad-stack/#setup","text":"Our AD stack implementation is based on ROS2 and uses rosbridge to communicate with SVL Simulator. To do that, we need to prepare Ubuntu machine with ROS2 installed. We provide a Docker image containing Ubuntu 18.04 and ROS2 installed so you can just pull the Docker image and start writing code right away.","title":"Setup"},{"location":"tutorials/create-ros2-ad-stack/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"tutorials/create-ros2-ad-stack/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing NVIDIA Docker"},{"location":"tutorials/create-ros2-ad-stack/#pulling-docker-image","text":"docker pull lgsvl/lanefollowing:latest","title":"Pulling Docker Image"},{"location":"tutorials/create-ros2-ad-stack/#whats-inside-docker-image","text":"Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS1 melodic + rosbridge ROS2 dashing + rosbridge Jupyter Notebook","title":"What's inside Docker Image"},{"location":"tutorials/create-ros2-ad-stack/#creating-a-ros2-package","text":"A ROS2 package is simply a directory and should contain files named package.xml and setup.py . Create folders as below and create setup.py and package.xml . Please note that the package name must match with the folder name of your ROS package. ros2_ws/ src/ lane_following/ setup.py package.xml","title":"Creating a ROS2 Package"},{"location":"tutorials/create-ros2-ad-stack/#setuppy","text":"from setuptools import setup package_name = 'lane_following' setup( name=package_name, version='0.0.1', packages=[ 'train', ], py_modules=[ 'collect', 'drive', ], install_requires=['setuptools'], zip_safe=True, author='David Uhm', author_email='david.uhm@lge.com', maintainer='David Uhm', maintainer_email='david.uhm@lge.com', keywords=[ 'ROS', 'ROS2', 'deep learning', 'lane following', 'end to end', 'SVL Simulator', 'Autonomous Driving' ], classifiers=[ 'Intended Audience :: Developers', 'Programming Language :: Python', 'Topic :: Software Development', ], description='ROS2-based End-to-End Lane Following model', license='BSD', tests_require=['pytest'], entry_points={ 'console_scripts': [ 'collect = collect:main', 'drive = drive:main', ], }, )","title":"setup.py"},{"location":"tutorials/create-ros2-ad-stack/#packagexml","text":"<?xml version=\"1.0\"?> <package format=\"2\"> <name>lane_following</name> <version>0.0.1</version> <description>ROS2-based End-to-End Lane Following model</description> <maintainer email=\"david.uhm@lge.com\">David Uhm</maintainer> <license>BSD</license> <exec_depend>rclpy</exec_depend> <exec_depend>std_msgs</exec_depend> <exec_depend>sensor_msgs</exec_depend> <test_depend>ament_copyright</test_depend> <test_depend>ament_flake8</test_depend> <test_depend>ament_pep257</test_depend> <test_depend>python3-pytest</test_depend> <export> <build_type>ament_python</build_type> </export> </package>","title":"package.xml"},{"location":"tutorials/create-ros2-ad-stack/#building-a-ros2-package","text":"Now, you can build your package as below: source /opt/ros/dashing/setup.bash cd ~/ros2_ws colcon build --symlink-install","title":"Building a ROS2 Package"},{"location":"tutorials/create-ros2-ad-stack/#running-ros2-lgsvl-bridge","text":"ROS2 LGSVL Bridge provides a JSON API to ROS functionality for non-ROS programs such as SVL Simulator. It is not provided as part of a default ROS2 Dashing installation, but has been installed separately for you in the Docker image already. You can run ROS2 LGSVL Bridge to connect your ROS node with SVL Simulator as below: source /path/to/ros2-lgsvl-bridge/install/setup.bash lgsvl_bridge","title":"Running ROS2 LGSVL Bridge"},{"location":"tutorials/create-ros2-ad-stack/#writing-ros2-subscriber-node","text":"ROS nodes communicate with each other by passing messages. Messages are routed via a topic with publish/subscribe concepts. A node sends out a message by publishing it to a given topic. Then, a node that is interested in a certain kind of data will subscribe to the appropriate topic. In our cases, SVL Simulator publishes sensor data such as camera images or LiDAR point clouds via rosbridge, and then the Lane Following model subscribes to that topic to receive sensor data, preprocesses the data, feeds them into the pretrained model, and finally computes a control command based on perceived sensor data. Below is an example of how to subscribe to sensor data topics from a ROS node. You can subscribe to a single topic only or multiple topics simultaneously.","title":"Writing ROS2 Subscriber Node"},{"location":"tutorials/create-ros2-ad-stack/#subscribe-to-a-single-topic","text":"import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage class Drive(Node): def __init__(self): super().__init__('drive') self.sub_image = self.create_subscription(CompressedImage, '/simulator/sensor/camera/center/compressed', self.callback) def callback(self, msg): self.get_logger().info('Subscribed: {}'.format(msg.data)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main()","title":"Subscribe to a single topic"},{"location":"tutorials/create-ros2-ad-stack/#subscribe-to-multiple-topics-simultaneously","text":"In order to subscribe to ROS messages of different types from multiple sources, we need to take the timestamps of those messages into account. ROS2 Message Filters is the ROS package that synchronizes incoming messages by the timestamps contained in their headers and outputs them in the form of a single callback. Install this package in your ROS2 workspace and build it. cd ~/ros2_ws/src git clone https://github.com/intel/ros2_message_filters.git cd .. colcon build --symlink-install Then, you can import it in your Python script by import message_filters and use it as below: import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage from geometry_msgs.msg import TwistStamped import message_filters class Collect(Node): def __init__(self): super().__init__('collect') sub_center_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/center/compressed') sub_left_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/left/compressed') sub_right_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/right/compressed') sub_control = message_filters.Subscriber(self, TwistStamped, '/simulator/control/command') ts = message_filters.ApproximateTimeSynchronizer([sub_center_camera, sub_left_camera, sub_right_camera, sub_control], 1, 0.1) ts.registerCallback(self.callback) def callback(self, center_camera, left_camera, right_camera, control): self.get_logger().info('Subscribed: {}'.format(control.twist.angular.x)) def main(args=None): rclpy.init(args=args) collect = Collect() rclpy.spin(collect) if __name__ == '__main__': main()","title":"Subscribe to multiple topics simultaneously"},{"location":"tutorials/create-ros2-ad-stack/#writing-ros2-publisher-node","text":"The publisher sends data to a topic. When you create a publisher you have to tell ROS of which type the data will be. In order to drive a car autonomously, the Lane Following model publishes a predicted control command back to the simulator via rosbridge.","title":"Writing ROS2 Publisher Node"},{"location":"tutorials/create-ros2-ad-stack/#publish-command-back-to-svl-simulator","text":"import rclpy from rclpy.node import Node from geometry_msgs.msg import TwistStamped class Drive(Node): def __init__(self): super().__init__('drive') self.control_pub = self.create_publisher(TwistStamped, '/lanefollowing/steering_cmd') self.timer_period = 0.02 # seconds self.timer = self.create_timer(self.timer_period, self.callback) self.steering = 0. def callback(self): message = TwistStamped() message.twist.angular.x = float(self.steering) self.control_pub.publish(message) self.get_logger().info('Publishing: {}'.format(message.twist.angular.x)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main()","title":"Publish command back to SVL Simulator"},{"location":"tutorials/create-ros2-ad-stack/#running-ros2-node","text":"Once you have setup the rosbridge connection to SVL Simulator, you can launch your ROS node as follows: source /opt/ros/dashing/setup.bash source /path/to/ros2_ws/install/local_setup.bash ros2 run {your_package} {your_node}","title":"Running ROS2 Node"},{"location":"tutorials/create-ros2-ad-stack/#references","text":"Lane Following Github Repository SVL Simulator ROS2 Documentation ROS2 Message Filters","title":"References"},{"location":"tutorials/ground-truth-obstacles/","text":"Ground Truth Obstacles # You can use the SVL Simulator to view, publish, and compare ground truth obstacle information. The simulator allows visualization of 2D or 3D bounding boxes of vehicles and pedestrians, and publishes detailed information about the ground truth obstacles. Sensor Configuration for Ground Truth Sensors top # [ { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth Sensor\", \"params\": { \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"roll\": 0, \"yaw\": 0 } }, { \"type\": \"PerceptionSensor2D\", \"name\": \"2D Ground Truth Sensor\", \"params\": { \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"roll\": 0, \"yaw\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Control Sensor\" } ] View Ground Truth Obstacles in Simulator top # Ground truth obstacles for vehicles and pedestrians can be visualized in the simulator with both 3D bounding boxes and 2D bounding boxes. To view 3D Bounding boxes in the simulator: Launch SVL simulator and click the Open Browser button Create a sensor configuration with ROS bridge and the sensors listed above on your desired vehicle Create a simulation: Set Random Traffic mode Set your desired map (e.g., BorregasAve ) Set your desried vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration created above Enable Random Traffic and Random Pedestrians Run simulation Enable visualization for 3D Ground Truth Sensor inside the simulator You should see 3D bounding boxes highlighting NPC vehicles and pedestrians in the simulator main view. To view 2D bounding boxes: Follow the steps listed above Enable visualization for 2D Ground Truth Sensor inside the simulator You should see 2D bounding boxes highlighting NPC vehicles and pedestrians in the camera view. Bounding Box Colors top # Green: Vehicles Yellow: Pedestrians Subscribe to Ground Truth ROS Messages top # SVL Simulator also publishes custom ROS messages describing the ground truth data of non-ego vehicles. In order to subscribe to the ground truth messages, you will need the ROS package lgsvl_msgs . It contains custom ROS message types for 2D and 3D bounding boxes. You will also need to be running rosbridge. Install the lgsvl_msgs ROS Package # Clone lgsvl_msgs to your ROS workspace or msgs directory: $ git clone https://github.com/lgsvl/lgsvl_msgs {MY_ROS_WS} Build the ROS workspace: $ catkin_make ROS Topics and Message Types for Ground Truth Messages top # Ground Truth 2D messages Topic: /simulator/ground_truth/2d_detections Message type: lgsvl_msgs/Detection2DArray: Link Ground Truth 3D messages Topic: /simulator/ground_truth/3d_detections Message type: lgsvl_msgs/Detection3DArray: Link View Object Detections from Apollo in SVL Simulator top # If you are running Apollo with SVL Simulator, you can also visualize object detection outputs from Apollo in the simulator using the Apollo Perception Visualizer 3D Sensor . Make sure that the Apollo perception module is running and detecting obstacles in Dreamview as below: To view object detections from Apollo: Launch SVL simulator and click the Open Browser button Add the Apollo Perception Visualizer 3D Sensor into your Apollo sensor configuration with the CyberRT bridge Set sensor topic as /apollo/perception/obstacles Create a simulation: Set Random Traffic mode Set your desired map (e.g., BorregasAve ) Set your desried vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration created above Enable Random Traffic and Random Pedestrians Run simulation Enable visualization for the Apollo Perception Visualizer 3D Sensor inside the simulator You should see 3D bounding boxes highlighting Apollo LiDAR detections in the simulator main view.","title":"Viewing and subscribing to ground truth obstacles"},{"location":"tutorials/ground-truth-obstacles/#sensor-configuration-for-ground-truth-sensors","text":"[ { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth Sensor\", \"params\": { \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"roll\": 0, \"yaw\": 0 } }, { \"type\": \"PerceptionSensor2D\", \"name\": \"2D Ground Truth Sensor\", \"params\": { \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"roll\": 0, \"yaw\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Control Sensor\" } ]","title":"Sensor Configuration for Ground Truth Sensors"},{"location":"tutorials/ground-truth-obstacles/#view-ground-truth-obstacles-in-simulator","text":"Ground truth obstacles for vehicles and pedestrians can be visualized in the simulator with both 3D bounding boxes and 2D bounding boxes. To view 3D Bounding boxes in the simulator: Launch SVL simulator and click the Open Browser button Create a sensor configuration with ROS bridge and the sensors listed above on your desired vehicle Create a simulation: Set Random Traffic mode Set your desired map (e.g., BorregasAve ) Set your desried vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration created above Enable Random Traffic and Random Pedestrians Run simulation Enable visualization for 3D Ground Truth Sensor inside the simulator You should see 3D bounding boxes highlighting NPC vehicles and pedestrians in the simulator main view. To view 2D bounding boxes: Follow the steps listed above Enable visualization for 2D Ground Truth Sensor inside the simulator You should see 2D bounding boxes highlighting NPC vehicles and pedestrians in the camera view.","title":"View Ground Truth Obstacles in Simulator"},{"location":"tutorials/ground-truth-obstacles/#bounding-box-colors","text":"Green: Vehicles Yellow: Pedestrians","title":"Bounding Box Colors"},{"location":"tutorials/ground-truth-obstacles/#subscribe-to-ground-truth-ros-messages","text":"SVL Simulator also publishes custom ROS messages describing the ground truth data of non-ego vehicles. In order to subscribe to the ground truth messages, you will need the ROS package lgsvl_msgs . It contains custom ROS message types for 2D and 3D bounding boxes. You will also need to be running rosbridge.","title":"Subscribe to Ground Truth ROS Messages"},{"location":"tutorials/ground-truth-obstacles/#install-the-lgsvl_msgs-ros-package","text":"Clone lgsvl_msgs to your ROS workspace or msgs directory: $ git clone https://github.com/lgsvl/lgsvl_msgs {MY_ROS_WS} Build the ROS workspace: $ catkin_make","title":"Install the lgsvl_msgs ROS Package"},{"location":"tutorials/ground-truth-obstacles/#ros-topics-and-message-types-for-ground-truth-messages","text":"Ground Truth 2D messages Topic: /simulator/ground_truth/2d_detections Message type: lgsvl_msgs/Detection2DArray: Link Ground Truth 3D messages Topic: /simulator/ground_truth/3d_detections Message type: lgsvl_msgs/Detection3DArray: Link","title":"ROS Topics and Message Types for Ground Truth Messages"},{"location":"tutorials/ground-truth-obstacles/#view-object-detections-from-apollo-in-svl-simulator","text":"If you are running Apollo with SVL Simulator, you can also visualize object detection outputs from Apollo in the simulator using the Apollo Perception Visualizer 3D Sensor . Make sure that the Apollo perception module is running and detecting obstacles in Dreamview as below: To view object detections from Apollo: Launch SVL simulator and click the Open Browser button Add the Apollo Perception Visualizer 3D Sensor into your Apollo sensor configuration with the CyberRT bridge Set sensor topic as /apollo/perception/obstacles Create a simulation: Set Random Traffic mode Set your desired map (e.g., BorregasAve ) Set your desried vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration created above Enable Random Traffic and Random Pedestrians Run simulation Enable visualization for the Apollo Perception Visualizer 3D Sensor inside the simulator You should see 3D bounding boxes highlighting Apollo LiDAR detections in the simulator main view.","title":"View Object Detections from Apollo in SVL Simulator"},{"location":"tutorials/lane-following/","text":"ROS2 End-to-End Lane Following Model with SVL Simulator # This documentation describes applying a deep learning neural network for lane following in SVL Simulator . In this project, we use SVL Simulator for customizing sensors (one main camera and two side cameras) for a car, collect data for training, and deploying and testing a trained model. This project was inspired by NVIDIA's End-to-End Deep Learning Model for Self-Driving Cars Video top # Table of Contents top # Getting Started Prerequisites Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Features Training Details Network Architecture Hyperparameters Dataset How to Collect Data and Train Your Own Model with SVL Simulator Collect data from SVL Simulator Data preprocessing Train a model Drive with your trained model in SVL Simulator Future Works and Contributing References Getting Started top # First, clone this repository: git clone --recurse-submodules https://github.com/lgsvl/lanefollowing.git Next, pull the latest Docker image: docker pull lgsvl/lanefollowing:latest To build ROS2 packages: docker-compose up build_ros Now, launch the lane following model: docker-compose up drive (Optional) If you want visualizations, run drive_visual instead of drive: docker-compose up drive_visual That's it! Now, the lane following ROS2 node and the rosbridge should be up and running, waiting for SVL Simulator to connect. Prerequisites top # Docker CE NVIDIA Docker NVIDIA graphics card (required for training/inference with GPU) Setup top # Installing Docker CE top # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing NVIDIA Docker top # Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling Docker Image top # docker pull lgsvl/lanefollowing:latest What's inside Docker Image # Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS2 Dashing + rosbridge Jupyter Notebook Features top # Training mode: Manually drive the vehicle and collect data Autonomous Mode: The vehicle drives itself based on Lane Following model trained from the collected data ROS2-based Time synchronous data collection node deploying a trained model in a node Data preprocessing for training Data normalization Data augmentation Splitting data into training set and test set Writing/Reading data in HDF5 format Deep Learning model training: Train a model using Keras with TensorFlow backend Training Details top # Network Architecture top # The network has 559,419 parameters and consists of 9 layers, including 5 convolutional layers, 3 fully connected layers, and an output layer. Layer (type) Output Shape Param # lambda_1 (Lambda) (None, 70, 320, 3) 0 conv2d_1 (Conv2D) (None, 33, 158, 24) 1824 conv2d_2 (Conv2D) (None, 15, 77, 36) 21636 conv2d_3 (Conv2D) (None, 6, 37, 48) 43248 conv2d_4 (Conv2D) (None, 4, 35, 64) 27712 conv2d_5 (Conv2D) (None, 2, 33, 64) 36928 dropout_1 (Dropout) (None, 2, 33, 64) 0 flatten_1 (Flatten) (None, 4224) 0 dense_1 (Dense) (None, 100) 422500 dense_2 (Dense) (None, 50) 5050 dense_3 (Dense) (None, 10) 510 dense_4 (Dense) (None, 1) 11 Hyperparameters top # Learning rate: 1e-04 Learning rate decay: None Dropout rate: 0.5 Mini-batch size: 128 Epochs: 30 Optimization algorithm: Adam Loss function: Mean squared error Training/Test set ratio: 8:2 Dataset top # Number of training data: 48,624 labeled images Number of validation data: 12,156 labeled images Center Image top # Left Image top # Right Image top # Original Image top # Cropped Image top # Data Distribution top # How to Collect Data and Train Your Own Model with SVL Simulator top # Collect data from SVL Simulator top # To collect camera images as well as corresponding steering commands for training, we provide a ROS2 collect node which subscribes to three camera image topics and a control command topic, approximately synchronizes time stamps of those messages, and then saves them as csv and jpg files. The topic names and types are as below: - Center camera: /simulator/sensor/camera/center/compressed (sensor_msgs/CompressedImage) - Left camera: /simulator/sensor/camera/left/compressed (sensor_msgs/CompressedImage) - Right camera: /simulator/sensor/camera/right/compressed (sensor_msgs/CompressedImage) - Control command: /simulator/control/command (lgsvl_msgs/CanBusData) Complete sensor JSON configuration top # [ { \"type\": \"LaneFollowingSensor\", \"name\": \"Lane Following Sensor\", \"params\": { \"Topic\": \"/lanefollowing/steering_cmd\" } }, { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 15, \"Topic\": \"/simulator/control/command\" } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"ColorCameraSensor\", \"name\": \"Center Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/sensor/camera/center/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Left Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/sensor/camera/left/compressed\" }, \"transform\": { \"x\": -0.8, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Right Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/sensor/camera/right/compressed\" }, \"transform\": { \"x\": 0.8, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ] To drive a car and publish messages over rosbridge in training mode: - Launch SVL Simulator - Create a simulation in Random Traffic mode - Select your preferred map (e.g., San Francisco ) for training - Select your preferred vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration above - Make sure your vehicle has ROS2 bridge in the sensor setup - Run your simulation Finally, to launch rosbridge and collect ROS2 node in a terminal: docker-compose up collect The node will start collecting data as you drive the car around. You should be able to check log messages in the terminal where the collect node is running. The final data is saved in lanefollowing/ros2_ws/src/lane_following/train/data/ as csv and jpg files. Data preprocessing top # Before start training your model with the data you collected, data preprocessing is required. This task includes: - Resize image resolutions from 1920 x 1080 to 200 x 112 - Crop top portion of images as we are mostly interested in road part of an image - Data augmentation by adding artificial bias to side camera images or flipping images - Data normalization to help the model converge faster - Split data into training and testing dataset To run data preprocessing and obtain datasets for training: docker-compose up preprocess This will preprocess your data and write outputs in lanefollowing/ros2_ws/src/lane_following/train/data/hdf5/ into HDF5 format for better I/O performance for training. Train a model top # We use Keras with TensorFlow backend for training our model as an example. The hyperparameters such as learning rate, batch size, or number of epochs were chosen empirically. You can train a model as is but you are also welcome to modify the model architecture or any hyperparameters as you like in the code. To start training: docker-compose up train After training is done, your final trained model will be in lanefollowing/ros2_ws/src/lane_following/train/model/{current-date-and-time-in-utc}.h5 and your model is ready to drive autonomously. Drive with your trained model in SVL Simulator top # Now, it's time to deploy your trained model and test drive with it using SVL Simulator. You can replace your trained model with an existing one in lanefollowing/ros2_ws/src/lane_following/model/model.h5 as this is the path for deployment. To drive a car in autonomous mode: - Launch SVL Simulator - Create a simulation in Random Traffic mode - Select your preferred map (e.g., SingleLaneRoad ) for driving - Select your preferred vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration above - You are free to remove both side cameras from the sensor JSON as the model only uses the center camera in driving mode - Make sure your vehicle has ROS2 bridge in the sensor setup - Run your simulation To launch rosbridge and drive ROS2 node in a terminal: docker-compose up drive Or, if you want visualizations as well, run drive_visual instead: docker-compose up drive_visual Your car will start driving autonomously and try to mimic your driving behavior when training the model. Note that the model only controls steering inputs as you drive your vehicle forward. Future Works and Contributing top # Though the network can successfully drive and follow lanes on the bridge, there's still a lot of room for future improvements (i.e., biased to drive straight, afraid of shadows, few training data, and etc). - To improve model robustness collect more training data by driving in a wide variety of environments - Changing weather and lighting effects (rain, fog, road wetness, time of day) - Adding more road layouts and road textures - Adding more shadows on roads - Adding NPC cars around the ego vehicle - Predict the car throttle along with the steering angle - Take into accounts time series analysis using RNN (Recurrent Neural Network) References top # Lane Following Github Repository Lane Following Sensor SVL Simulator NVIDIA's End-to-End Deep Learning Model for Self-Driving Cars","title":"Deep learning lane following model"},{"location":"tutorials/lane-following/#video","text":"","title":"Video"},{"location":"tutorials/lane-following/#table-of-contents","text":"Getting Started Prerequisites Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Features Training Details Network Architecture Hyperparameters Dataset How to Collect Data and Train Your Own Model with SVL Simulator Collect data from SVL Simulator Data preprocessing Train a model Drive with your trained model in SVL Simulator Future Works and Contributing References","title":"Table of Contents"},{"location":"tutorials/lane-following/#getting-started","text":"First, clone this repository: git clone --recurse-submodules https://github.com/lgsvl/lanefollowing.git Next, pull the latest Docker image: docker pull lgsvl/lanefollowing:latest To build ROS2 packages: docker-compose up build_ros Now, launch the lane following model: docker-compose up drive (Optional) If you want visualizations, run drive_visual instead of drive: docker-compose up drive_visual That's it! Now, the lane following ROS2 node and the rosbridge should be up and running, waiting for SVL Simulator to connect.","title":"Getting Started"},{"location":"tutorials/lane-following/#prerequisites","text":"Docker CE NVIDIA Docker NVIDIA graphics card (required for training/inference with GPU)","title":"Prerequisites"},{"location":"tutorials/lane-following/#setup","text":"","title":"Setup"},{"location":"tutorials/lane-following/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"tutorials/lane-following/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing NVIDIA Docker"},{"location":"tutorials/lane-following/#pulling-docker-image","text":"docker pull lgsvl/lanefollowing:latest","title":"Pulling Docker Image"},{"location":"tutorials/lane-following/#whats-inside-docker-image","text":"Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS2 Dashing + rosbridge Jupyter Notebook","title":"What's inside Docker Image"},{"location":"tutorials/lane-following/#features","text":"Training mode: Manually drive the vehicle and collect data Autonomous Mode: The vehicle drives itself based on Lane Following model trained from the collected data ROS2-based Time synchronous data collection node deploying a trained model in a node Data preprocessing for training Data normalization Data augmentation Splitting data into training set and test set Writing/Reading data in HDF5 format Deep Learning model training: Train a model using Keras with TensorFlow backend","title":"Features"},{"location":"tutorials/lane-following/#training-details","text":"","title":"Training Details"},{"location":"tutorials/lane-following/#network-architecture","text":"The network has 559,419 parameters and consists of 9 layers, including 5 convolutional layers, 3 fully connected layers, and an output layer. Layer (type) Output Shape Param # lambda_1 (Lambda) (None, 70, 320, 3) 0 conv2d_1 (Conv2D) (None, 33, 158, 24) 1824 conv2d_2 (Conv2D) (None, 15, 77, 36) 21636 conv2d_3 (Conv2D) (None, 6, 37, 48) 43248 conv2d_4 (Conv2D) (None, 4, 35, 64) 27712 conv2d_5 (Conv2D) (None, 2, 33, 64) 36928 dropout_1 (Dropout) (None, 2, 33, 64) 0 flatten_1 (Flatten) (None, 4224) 0 dense_1 (Dense) (None, 100) 422500 dense_2 (Dense) (None, 50) 5050 dense_3 (Dense) (None, 10) 510 dense_4 (Dense) (None, 1) 11","title":"Network Architecture"},{"location":"tutorials/lane-following/#hyperparameters","text":"Learning rate: 1e-04 Learning rate decay: None Dropout rate: 0.5 Mini-batch size: 128 Epochs: 30 Optimization algorithm: Adam Loss function: Mean squared error Training/Test set ratio: 8:2","title":"Hyperparameters"},{"location":"tutorials/lane-following/#dataset","text":"Number of training data: 48,624 labeled images Number of validation data: 12,156 labeled images","title":"Dataset"},{"location":"tutorials/lane-following/#center-image","text":"","title":"Center Image"},{"location":"tutorials/lane-following/#left-image","text":"","title":"Left Image"},{"location":"tutorials/lane-following/#right-image","text":"","title":"Right Image"},{"location":"tutorials/lane-following/#original-image","text":"","title":"Original Image"},{"location":"tutorials/lane-following/#cropped-image","text":"","title":"Cropped Image"},{"location":"tutorials/lane-following/#data-distribution","text":"","title":"Data Distribution"},{"location":"tutorials/lane-following/#how-to-collect-data-and-train-your-own-model-with-svl-simulator","text":"","title":"How to Collect Data and Train Your Own Model with SVL Simulator"},{"location":"tutorials/lane-following/#collect-data-from-svl-simulator","text":"To collect camera images as well as corresponding steering commands for training, we provide a ROS2 collect node which subscribes to three camera image topics and a control command topic, approximately synchronizes time stamps of those messages, and then saves them as csv and jpg files. The topic names and types are as below: - Center camera: /simulator/sensor/camera/center/compressed (sensor_msgs/CompressedImage) - Left camera: /simulator/sensor/camera/left/compressed (sensor_msgs/CompressedImage) - Right camera: /simulator/sensor/camera/right/compressed (sensor_msgs/CompressedImage) - Control command: /simulator/control/command (lgsvl_msgs/CanBusData)","title":"Collect data from SVL Simulator"},{"location":"tutorials/lane-following/#complete-sensor-json-configuration","text":"[ { \"type\": \"LaneFollowingSensor\", \"name\": \"Lane Following Sensor\", \"params\": { \"Topic\": \"/lanefollowing/steering_cmd\" } }, { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 15, \"Topic\": \"/simulator/control/command\" } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"ColorCameraSensor\", \"name\": \"Center Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/sensor/camera/center/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Left Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/sensor/camera/left/compressed\" }, \"transform\": { \"x\": -0.8, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Right Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/sensor/camera/right/compressed\" }, \"transform\": { \"x\": 0.8, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ] To drive a car and publish messages over rosbridge in training mode: - Launch SVL Simulator - Create a simulation in Random Traffic mode - Select your preferred map (e.g., San Francisco ) for training - Select your preferred vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration above - Make sure your vehicle has ROS2 bridge in the sensor setup - Run your simulation Finally, to launch rosbridge and collect ROS2 node in a terminal: docker-compose up collect The node will start collecting data as you drive the car around. You should be able to check log messages in the terminal where the collect node is running. The final data is saved in lanefollowing/ros2_ws/src/lane_following/train/data/ as csv and jpg files.","title":"Complete sensor JSON configuration"},{"location":"tutorials/lane-following/#data-preprocessing","text":"Before start training your model with the data you collected, data preprocessing is required. This task includes: - Resize image resolutions from 1920 x 1080 to 200 x 112 - Crop top portion of images as we are mostly interested in road part of an image - Data augmentation by adding artificial bias to side camera images or flipping images - Data normalization to help the model converge faster - Split data into training and testing dataset To run data preprocessing and obtain datasets for training: docker-compose up preprocess This will preprocess your data and write outputs in lanefollowing/ros2_ws/src/lane_following/train/data/hdf5/ into HDF5 format for better I/O performance for training.","title":"Data preprocessing"},{"location":"tutorials/lane-following/#train-a-model","text":"We use Keras with TensorFlow backend for training our model as an example. The hyperparameters such as learning rate, batch size, or number of epochs were chosen empirically. You can train a model as is but you are also welcome to modify the model architecture or any hyperparameters as you like in the code. To start training: docker-compose up train After training is done, your final trained model will be in lanefollowing/ros2_ws/src/lane_following/train/model/{current-date-and-time-in-utc}.h5 and your model is ready to drive autonomously.","title":"Train a model"},{"location":"tutorials/lane-following/#drive-with-your-trained-model-in-svl-simulator","text":"Now, it's time to deploy your trained model and test drive with it using SVL Simulator. You can replace your trained model with an existing one in lanefollowing/ros2_ws/src/lane_following/model/model.h5 as this is the path for deployment. To drive a car in autonomous mode: - Launch SVL Simulator - Create a simulation in Random Traffic mode - Select your preferred map (e.g., SingleLaneRoad ) for driving - Select your preferred vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration above - You are free to remove both side cameras from the sensor JSON as the model only uses the center camera in driving mode - Make sure your vehicle has ROS2 bridge in the sensor setup - Run your simulation To launch rosbridge and drive ROS2 node in a terminal: docker-compose up drive Or, if you want visualizations as well, run drive_visual instead: docker-compose up drive_visual Your car will start driving autonomously and try to mimic your driving behavior when training the model. Note that the model only controls steering inputs as you drive your vehicle forward.","title":"Drive with your trained model in SVL Simulator"},{"location":"tutorials/lane-following/#future-works-and-contributing","text":"Though the network can successfully drive and follow lanes on the bridge, there's still a lot of room for future improvements (i.e., biased to drive straight, afraid of shadows, few training data, and etc). - To improve model robustness collect more training data by driving in a wide variety of environments - Changing weather and lighting effects (rain, fog, road wetness, time of day) - Adding more road layouts and road textures - Adding more shadows on roads - Adding NPC cars around the ego vehicle - Predict the car throttle along with the steering angle - Take into accounts time series analysis using RNN (Recurrent Neural Network)","title":"Future Works and Contributing"},{"location":"tutorials/lane-following/#references","text":"Lane Following Github Repository Lane Following Sensor SVL Simulator NVIDIA's End-to-End Deep Learning Model for Self-Driving Cars","title":"References"},{"location":"tutorials/local-automation-tutorial/","text":"Local Automation (CI/CD) Tutorial # Table of Contents Overview Prerequisites Example Simulations Getting Started Overview top # This documentation describes a tutorial for users to get started with local automation (continuous integration) for SVL Simulator with step-by-step instructions to follow. The example simulations refered in this tutorial use the ROS 2 Navigation stack (Nav2) to navigate and control a robot autonomously. For a guide to setting up and using the simulator alongside Nav2, please see the Robot simulation with ROS 2 Navigation Stack guide. Prerequisites top # SVL Simulator release 2021.3 or later Linux operating system (preferably Ubuntu 20.04 or later) Local Automation tool ROS 2 Navigation stack (Nav2) Example Simulations top # Local-Test: Robot Freemode (Nav2) Local-Test: Robot Cut-in Scenario (Nav2) Local-Test: Robot Sudden Braking (Nav2) Local-Test: Robot Traffic Cones (Nav2) Getting Started top # Launch SVL Simulator Make sure the above example simulations are configured correctly: Click on the Open Browser button to launch the simulator Web UI in a web browser In the Simulations section, locate the Local-Test: Robot Freemode (Nav2) simulation under the Available from Others tab and add it by clicking the Add button Select your cluster from the dropdown menu in the General tab Select the LGSeocho map in the Test case tab Select the LGCloi vehicle with the Navigation2 sensor configuration in the Test case tab Select the Other ROS 2 Autopilot with your bridge connection address (e.g., localhost:9090 ) in the Autopilot tab Click on the Publish button to finish the simulation setup in the Publish tab Repeat the above steps for the remaining simulations below: Local-Test: Robot Cut-in Scenario (Nav2) Local-Test: Robot Sudden Braking (Nav2) Local-Test: Robot Traffic Cones (Nav2) Note down the UUID of example simulation Set up the Local Automation tool following the Local Automation mode ) guide Bring up Nav2 stack locally following the Robot simulation with ROS 2 Navigation Stack guide Create a new file run.sh on the same directory where the wrapper script is downloaded Paste the below script with your ACCESS-TOKEN and a desired PATH-TO-LOGFILE : # Note that this script has been written for Bash export SVLSIM__ACCESS_TOKEN=\"<ACCESS-TOKEN>\" LOGFILE=\"<PATH-TO-LOGFILE>\" # UUIDs for example simulations declare -r SIMULATIONS=\" 6ac9ff51-ef76-46cf-836f-94ced3705937 0996920f-3be3-4c19-a529-0b08ac1ab679 5922903a-b766-4ebd-8cd3-c96a255d64a9 015be758-8f59-480f-be31-c8d79b1becd8 \" # Initialize a CLI session if result=$(sh svlsim.sh cli initialize 2> ${LOGFILE}); then : SUCCESS else echo \"ABORT. Failed to initialize a CLI session: ${result}\" exit 1 fi # Start simulations for simulation_id in ${SIMULATIONS}; do if result=$(sh svlsim.sh simulation start ${simulation_id} 2>> ${LOGFILE}); then echo \"[${simulation_id}] Starting...\" else echo \"[${simulation_id}] ERROR: ${result}\" continue fi while true; do sleep 2 if status=$(sh svlsim.sh simulation check ${simulation_id} 2>> ${LOGFILE}); then if [ ${status} != \"progressing\" ]; then echo \"[${simulation_id}] Done: ${status}\" break fi else echo \"[${simulation_id}] Abort: ${status}\" break fi done done Finally, run the script in a terminal: $ bash run.sh On the main window of the simulator, the example simulations should now run one after the other along with Nav2 stack without the need for clicking inside a UI.","title":"Local Automation (CI/CD) Tutorial"},{"location":"tutorials/local-automation-tutorial/#overview","text":"This documentation describes a tutorial for users to get started with local automation (continuous integration) for SVL Simulator with step-by-step instructions to follow. The example simulations refered in this tutorial use the ROS 2 Navigation stack (Nav2) to navigate and control a robot autonomously. For a guide to setting up and using the simulator alongside Nav2, please see the Robot simulation with ROS 2 Navigation Stack guide.","title":"Overview"},{"location":"tutorials/local-automation-tutorial/#prerequisites","text":"SVL Simulator release 2021.3 or later Linux operating system (preferably Ubuntu 20.04 or later) Local Automation tool ROS 2 Navigation stack (Nav2)","title":"Prerequisites"},{"location":"tutorials/local-automation-tutorial/#example-simulations","text":"Local-Test: Robot Freemode (Nav2) Local-Test: Robot Cut-in Scenario (Nav2) Local-Test: Robot Sudden Braking (Nav2) Local-Test: Robot Traffic Cones (Nav2)","title":"Example Simulations"},{"location":"tutorials/local-automation-tutorial/#getting-started","text":"Launch SVL Simulator Make sure the above example simulations are configured correctly: Click on the Open Browser button to launch the simulator Web UI in a web browser In the Simulations section, locate the Local-Test: Robot Freemode (Nav2) simulation under the Available from Others tab and add it by clicking the Add button Select your cluster from the dropdown menu in the General tab Select the LGSeocho map in the Test case tab Select the LGCloi vehicle with the Navigation2 sensor configuration in the Test case tab Select the Other ROS 2 Autopilot with your bridge connection address (e.g., localhost:9090 ) in the Autopilot tab Click on the Publish button to finish the simulation setup in the Publish tab Repeat the above steps for the remaining simulations below: Local-Test: Robot Cut-in Scenario (Nav2) Local-Test: Robot Sudden Braking (Nav2) Local-Test: Robot Traffic Cones (Nav2) Note down the UUID of example simulation Set up the Local Automation tool following the Local Automation mode ) guide Bring up Nav2 stack locally following the Robot simulation with ROS 2 Navigation Stack guide Create a new file run.sh on the same directory where the wrapper script is downloaded Paste the below script with your ACCESS-TOKEN and a desired PATH-TO-LOGFILE : # Note that this script has been written for Bash export SVLSIM__ACCESS_TOKEN=\"<ACCESS-TOKEN>\" LOGFILE=\"<PATH-TO-LOGFILE>\" # UUIDs for example simulations declare -r SIMULATIONS=\" 6ac9ff51-ef76-46cf-836f-94ced3705937 0996920f-3be3-4c19-a529-0b08ac1ab679 5922903a-b766-4ebd-8cd3-c96a255d64a9 015be758-8f59-480f-be31-c8d79b1becd8 \" # Initialize a CLI session if result=$(sh svlsim.sh cli initialize 2> ${LOGFILE}); then : SUCCESS else echo \"ABORT. Failed to initialize a CLI session: ${result}\" exit 1 fi # Start simulations for simulation_id in ${SIMULATIONS}; do if result=$(sh svlsim.sh simulation start ${simulation_id} 2>> ${LOGFILE}); then echo \"[${simulation_id}] Starting...\" else echo \"[${simulation_id}] ERROR: ${result}\" continue fi while true; do sleep 2 if status=$(sh svlsim.sh simulation check ${simulation_id} 2>> ${LOGFILE}); then if [ ${status} != \"progressing\" ]; then echo \"[${simulation_id}] Done: ${status}\" break fi else echo \"[${simulation_id}] Abort: ${status}\" break fi done done Finally, run the script in a terminal: $ bash run.sh On the main window of the simulator, the example simulations should now run one after the other along with Nav2 stack without the need for clicking inside a UI.","title":"Getting Started"},{"location":"tutorials/modular-testing/","text":"Modular Testing # Our 3D Ground Truth sensor and Signal sensor now publish ground truth perception data to Apollo stack via CyberRT bridge or ROS Apollo bridge. The modular testing feature is useful for testing planning module of Apollo stack based on the assumption that the perception output is 100% accurate without any errors. In other words, we can completely bypass Apollo's perception modules (i.e., object detection and traffic light detection) and use ground truth labels for perception published by our simulator instead. Video: Modular Testing Introduction # Video: How to run modular testing random traffic simulation from cloud-based web UI # Sensor Specifications top # 3D Ground Truth Sensor (more details) # 3D Ground Truth sensor replaces Apollo's object detection module. It detects every NPC including vehicles and pedestrians around the EGO vehicle within a distance, which can be specified using a sensor parameter MaxDistance , and publishes ground truth labels such as 3D bounding boxes for the detected objects. Major message fields: ID: integer ID for an object Timestamp: time in seconds since Unix epoch Velocity: object velocity vector in m/s Acceleration: object acceleration vector in m/s Width: object width in meters Length: object length in meters Height: object height in meters Type: vehicle or pedestrian Position: easting, northing, altitude in GPS position Theta: object heading in radians Tracking time: duration of detection in seconds Polygon point: corner points for an object Sensor parameters: Frequency: 10 MaxDistance: 100 Topic: /apollo/perception/obstacles Signal Sensor (more details) # Signal sensor replaces Apollo's traffic light detection module. It detects every signal that is connected to the current lane of the EGO vehicle within a distance, which can also be specified using a sensor parameter MaxDistance , and then publishes the signal IDs as well as the current signal color information. It is important to note that the signal ID coming from the map annotation in the simulator must match with the ID in the Apollo HD map you're currently using in Apollo, because Apollo uses this ID to locate the detected traffic light in the map. Major message fields: ID: traffic light string ID must match with the signal ID in Apollo HD map Timestamp: time in seconds since Unix epoch Color: red, yellow, green, or black Blink: is traffic light blinking Sensor parameters: Frequency: 10 MaxDistance: 100 Topic: /apollo/perception/traffic_light Sensor JSON top # The following sensor parameters can be added to a vehicle sensor configuration (and can replace the LiDAR and camera sensors which will improve simulation performance). Refer to Vehicles in My Library for more information on vehicle sensor configuration. { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/traffic_light\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } How to Run Apollo Modular Testing on GoMentumDTL Map top {: #how-to-run-apollo-modular-testing-on- gomentumdtl -map data-toc-label='How to Run Apollo Modular Testing on GoMentumDTL Map'} # Configure and run simulator Map Choose GoMentumDTL map Vehicle Choose Lincoln2017MKZ vehicle for Apollo Attach 3D Ground Truth sensor and Signal sensor with the sensor parameters above Optionally, you can remove Color Camera and LiDAR sensors from the sensor configuration as these sensors are not required for modular testing Run simulation with Random Traffic enabled Launch Apollo following these instructions Make sure that you're receiving ground truth messages via the perception topics on Apollo side Topic for object detection: /apollo/perception/obstacles Topic for traffic light detection: /apollo/perception/traffic_light Run cyber_monitor for CyberRT bridge or rostopic echo for ROS Apollo bridge inside Apollo container In Dreamview, enable Localization module and verify if you can visually see that: Your EGO vehicle is placed on the right position on the Apollo HD map for GoMentumDTL Bounding boxes for vehicles and pedestrians around the EGO vehicle (if any) including heading and velocity vectors Traffic light signals that are connected to the current lane of EGO vehicle are detected Enable Transform , Prediction , Planning , Routing , and Control Note that we are not running Perception and Traffic Light modules for modular testing Apollo should now be able to drive to your destination without running Perception modules","title":"Modular testing with the Apollo AD stack"},{"location":"tutorials/modular-testing/#video-modular-testing-introduction","text":"","title":"Video: Modular Testing Introduction"},{"location":"tutorials/modular-testing/#video-how-to-run-modular-testing-random-traffic-simulation-from-cloud-based-web-ui","text":"","title":"Video: How to run modular testing random traffic simulation from cloud-based web UI"},{"location":"tutorials/modular-testing/#sensor-specifications","text":"","title":"Sensor Specifications"},{"location":"tutorials/modular-testing/#3d-ground-truth-sensor-more-details","text":"3D Ground Truth sensor replaces Apollo's object detection module. It detects every NPC including vehicles and pedestrians around the EGO vehicle within a distance, which can be specified using a sensor parameter MaxDistance , and publishes ground truth labels such as 3D bounding boxes for the detected objects. Major message fields: ID: integer ID for an object Timestamp: time in seconds since Unix epoch Velocity: object velocity vector in m/s Acceleration: object acceleration vector in m/s Width: object width in meters Length: object length in meters Height: object height in meters Type: vehicle or pedestrian Position: easting, northing, altitude in GPS position Theta: object heading in radians Tracking time: duration of detection in seconds Polygon point: corner points for an object Sensor parameters: Frequency: 10 MaxDistance: 100 Topic: /apollo/perception/obstacles","title":"3D Ground Truth Sensor (more details)"},{"location":"tutorials/modular-testing/#signal-sensor-more-details","text":"Signal sensor replaces Apollo's traffic light detection module. It detects every signal that is connected to the current lane of the EGO vehicle within a distance, which can also be specified using a sensor parameter MaxDistance , and then publishes the signal IDs as well as the current signal color information. It is important to note that the signal ID coming from the map annotation in the simulator must match with the ID in the Apollo HD map you're currently using in Apollo, because Apollo uses this ID to locate the detected traffic light in the map. Major message fields: ID: traffic light string ID must match with the signal ID in Apollo HD map Timestamp: time in seconds since Unix epoch Color: red, yellow, green, or black Blink: is traffic light blinking Sensor parameters: Frequency: 10 MaxDistance: 100 Topic: /apollo/perception/traffic_light","title":"Signal Sensor (more details)"},{"location":"tutorials/modular-testing/#sensor-json","text":"The following sensor parameters can be added to a vehicle sensor configuration (and can replace the LiDAR and camera sensors which will improve simulation performance). Refer to Vehicles in My Library for more information on vehicle sensor configuration. { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/traffic_light\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Sensor JSON"},{"location":"tutorials/modular-testing/#how-to-run-apollo-modular-testing-on-gomentumdtl-map-top-how-to-run-apollo-modular-testing-on-gomentumdtl-map-data-toc-labelhow-to-run-apollo-modular-testing-on-gomentumdtl-map","text":"Configure and run simulator Map Choose GoMentumDTL map Vehicle Choose Lincoln2017MKZ vehicle for Apollo Attach 3D Ground Truth sensor and Signal sensor with the sensor parameters above Optionally, you can remove Color Camera and LiDAR sensors from the sensor configuration as these sensors are not required for modular testing Run simulation with Random Traffic enabled Launch Apollo following these instructions Make sure that you're receiving ground truth messages via the perception topics on Apollo side Topic for object detection: /apollo/perception/obstacles Topic for traffic light detection: /apollo/perception/traffic_light Run cyber_monitor for CyberRT bridge or rostopic echo for ROS Apollo bridge inside Apollo container In Dreamview, enable Localization module and verify if you can visually see that: Your EGO vehicle is placed on the right position on the Apollo HD map for GoMentumDTL Bounding boxes for vehicles and pedestrians around the EGO vehicle (if any) including heading and velocity vectors Traffic light signals that are connected to the current lane of EGO vehicle are detected Enable Transform , Prediction , Planning , Routing , and Control Note that we are not running Perception and Traffic Light modules for modular testing Apollo should now be able to drive to your destination without running Perception modules","title":"How to Run Apollo Modular Testing on GoMentumDTL Map top {: #how-to-run-apollo-modular-testing-on-gomentumdtl-map data-toc-label='How to Run Apollo Modular Testing on GoMentumDTL Map'}"},{"location":"tutorials/robotics-mapping/","text":"Mapping an environment in ROS 2 # Many robots operate in pre-mapped environments. In Nav2 the map of the environment is used both for localization and for generating a costmap for motion planning. This document demonstrates how to create a map of the environment using SLAM toolbox . Table of Contents Prerequisites Installing ROS 2 SVL robot startup Installing Nav2 Installing SLAM toolbox Setting up a simulation Mapping the environment Setting NavOrigin for your generated costmap How to place NavOrigin using the VSE tool Example usage in Python API script: Prerequisites top # SVL Simulator release 2021.2.2 or later Linux operating system (preferably Ubuntu 18.04 or later) For a guide to setting up and using the simulator see the Getting Started guide. For other robotics guides see Running a basic Robotics Simulation with ROS 2 and Running a ROS2 Robotics Simulation with Nav2 . Installing ROS 2 top # ROS2 can be installed by following the steps in the official installation guide . SVL Robot Startup top # Robots often have many 3D coordinate frames that change in position and orientation over time. tf2 is a transform library used to keep track of the relative transformation between these coordinate frames in ROS/ROS 2. The simulator does not directly broadcast tf transforms. Therefore, an external ROS2 node is needed for this. svl_robot_startup is a ROS2 package that publishes static transforms for the robot sensors and subscribes to the /odom topic published by the simulator to figure out and broadcast the transform between the odom and basefootprint frames. The SVL Simulator's ROS 2 bridge lgsvl_bridge is included as a dependency and added to the launch file to enable communication with the simulator. The navigation2 and nav2_bringup packages are also added as dependencies as they will be needed for running Nav2. Follow these steps to setup the package and install dependencies: source /opt/ros/foxy/setup.bash mkdir -p robot_ws/src cd robot_ws/src git clone https://github.com/lgsvl/svl_robot_bringup.git cd .. rosdep update rosdep install --from-path src -iy --rosdistro foxy colcon build --symlink-install Installing Nav2 # Nav2 be installed by rosdep in the previous step. If, for whatever reason, the installation was skipped and can be manually installed using the following commands. sudo apt update sudo apt install ros-foxy-navigation2 ros-foxy-nav2-bringup Installing SLAM toolbox # SLAM toolbox provides a set of open-source tools for 2D SLAM which will be used in this tutorial for mapping the environment. It can be built from source (follow instructions on GitHub) or installed using the following command: sudo apt install ros-foxy-slam-toolbox Setting up a Simulation top # Click on the Simulations tab and click Add New to create a new simulation. Enter a Simulation Name and select the Cluster . Click Next . Optional - Enable Interactive Mode to be able to start the simulation at the desired time by pressing a play button in the simulator screen. Select the Random Traffic Runtime Template. Select LGSeocho under Map (if it is not present you will need to go back to the store and add the map to your library). Select the LGCloi ego vehicle and the Nav2 Sensor Configuration. Click Next . Select Other ROS2 Autopilot under Autopilot and enter the Bridge Connection address (by default localhost:9090 ). Click Publish Mapping the environment # In a new terminal start the robot_tf_launch : source /opt/ros/foxy/setup.bash cd robot_ws source install/setup.bash ros2 launch svl_robot_bringup robot_tf_launch.py Start the simulation in the SVL Simulator Run the Simulator binary and click OPEN BROWSER In the web user interface, open the Simulations page, select the recently built simulation and click Run Simulation Launch navigation_launch fron Nav2: source /opt/ros/foxy/setup.bash ros2 launch nav2_bringup navigation_launch.py Launch slam toolbox: souce /opt/ros/foxy/setup.bash ros2 launch slam_toolbox online_async_launch.py Launch rviz: source /opt/ros/foxy/setup.bash ros2 launch nav2_bringup rviz_launch.py You can start mapping by either giving destinations using the Navigation2 Goal button in rviz or by manually driving the robot around using the keyboard. Once satisfied with the map it can be saved using: ros2 run nav2_map_server map_saver_cli -f ~/map Note on saving maps : Map saving tends to fail due to timeout. Attempting to save multiple times can result in a successful map save. See this issue on GitHub for more information. Setting NavOrigin for your generated costmap # In order to be able to use the python API functionalities for setting an initial pose or a destination of a robot along with your generated costmap, a NavOrigin game object should be placed in a scene such that its transform matches with the origin of the costmap you generated. Also, it should have the same offsets as the ones in your costmap yaml file. To find transform and offset values of your costmap origin: Transform of the origin: Coordinates of the lower-left pixel in the generated map image file. Offsets of the origin: The generated YAML file contains an offset for your origin. The offsets are given in 2D position with a third value indicating a rotation. How to place NavOrigin using the VSE tool # You can use the Visual Scenario Editor (VSE) tool of SVL Simulator for a placement of NavOrigin in a scene following the instructions below: Launch SVL Simulator Click on the Visual Editor button to start the VSE tool Select your map in the Map section (e.g., LGSeocho) Place NavOrigin on a map by clicking the ScenarioNavOrigin button in the Add section Move and rotate the NavOrigin so that its placement matches with your costmap origin Write down the x/y/z position and rotation values of the NavOrigin (e.g., in the attached image below, position is (53.4, 0, -57.6) and rotation is ((0, 0, 0))) Example usage in Python API script: # import lgsvl from lgsvl import Vector, Quaternion, Transform # Establish a connection to SVL Simulator sim = lgsvl.Simulator(lgsvl.wise.SimulatorSettings.simulator_host, lgsvl.wise.SimulatorSettings.simulator_port) # Load LGSeocho map scene = lgsvl.wise.DefaultAssets.map_lgseocho if sim.current_scene == scene: sim.reset() else: sim.load(scene) # Set up LGCloi robot in a scene state = lgsvl.AgentState() state.transform = sim.get_spawn()[0] robot = sim.add_agent(lgsvl.wise.DefaultAssets.ego_lgcloi_navigation2, lgsvl.AgentType.EGO, state) robot.connect_bridge(lgsvl.wise.SimulatorSettings.bridge_host, lgsvl.wise.SimulatorSettings.bridge_port) # Use x/y/z position and rotation values of NavOrigin transform = Transform(Vector(53.4, 0, -57.6), Vector(0, 0, 0)) # Use offsets from a costmap YAML file offset = Vector(-14.7, -24.6, 0) # Set NavOrigin in a scene sim.set_nav_origin(transform, offset) # Get NavOrigin from a scene and verify its values nav_origin = sim.get_nav_origin() print('NavOrigin transform:', nav_origin['transform']) print('NavOrigin offset:', nav_origin['offset']) # Map a destination point in ROS coordinates to Simulator coordinates dst_ros = ((7.118, -0.016, 0), (0, 0, 0.0004, 0.999)) dst_sim = sim.map_from_nav(Vector(*dst_ros[0]), Quaternion(*dst_ros[1])) # Set initial pose of robot robot.set_initial_pose() sim.run(5) # Set destination for Navigation2 stack robot.set_destination(dst_sim) sim.run()","title":"Mapping a simulation environment in ROS 2"},{"location":"tutorials/robotics-mapping/#prerequisites","text":"SVL Simulator release 2021.2.2 or later Linux operating system (preferably Ubuntu 18.04 or later) For a guide to setting up and using the simulator see the Getting Started guide. For other robotics guides see Running a basic Robotics Simulation with ROS 2 and Running a ROS2 Robotics Simulation with Nav2 .","title":"Prerequisites"},{"location":"tutorials/robotics-mapping/#installing-ros2","text":"ROS2 can be installed by following the steps in the official installation guide .","title":"Installing ROS 2"},{"location":"tutorials/robotics-mapping/#svl-robot-startup","text":"Robots often have many 3D coordinate frames that change in position and orientation over time. tf2 is a transform library used to keep track of the relative transformation between these coordinate frames in ROS/ROS 2. The simulator does not directly broadcast tf transforms. Therefore, an external ROS2 node is needed for this. svl_robot_startup is a ROS2 package that publishes static transforms for the robot sensors and subscribes to the /odom topic published by the simulator to figure out and broadcast the transform between the odom and basefootprint frames. The SVL Simulator's ROS 2 bridge lgsvl_bridge is included as a dependency and added to the launch file to enable communication with the simulator. The navigation2 and nav2_bringup packages are also added as dependencies as they will be needed for running Nav2. Follow these steps to setup the package and install dependencies: source /opt/ros/foxy/setup.bash mkdir -p robot_ws/src cd robot_ws/src git clone https://github.com/lgsvl/svl_robot_bringup.git cd .. rosdep update rosdep install --from-path src -iy --rosdistro foxy colcon build --symlink-install","title":"SVL robot startup"},{"location":"tutorials/robotics-mapping/#installing-nav2","text":"Nav2 be installed by rosdep in the previous step. If, for whatever reason, the installation was skipped and can be manually installed using the following commands. sudo apt update sudo apt install ros-foxy-navigation2 ros-foxy-nav2-bringup","title":"Installing Nav2"},{"location":"tutorials/robotics-mapping/#installing-slam-toolbox","text":"SLAM toolbox provides a set of open-source tools for 2D SLAM which will be used in this tutorial for mapping the environment. It can be built from source (follow instructions on GitHub) or installed using the following command: sudo apt install ros-foxy-slam-toolbox","title":"Installing SLAM toolbox"},{"location":"tutorials/robotics-mapping/#sim-setup","text":"Click on the Simulations tab and click Add New to create a new simulation. Enter a Simulation Name and select the Cluster . Click Next . Optional - Enable Interactive Mode to be able to start the simulation at the desired time by pressing a play button in the simulator screen. Select the Random Traffic Runtime Template. Select LGSeocho under Map (if it is not present you will need to go back to the store and add the map to your library). Select the LGCloi ego vehicle and the Nav2 Sensor Configuration. Click Next . Select Other ROS2 Autopilot under Autopilot and enter the Bridge Connection address (by default localhost:9090 ). Click Publish","title":"Setting up a simulation"},{"location":"tutorials/robotics-mapping/#mapping-the-environment","text":"In a new terminal start the robot_tf_launch : source /opt/ros/foxy/setup.bash cd robot_ws source install/setup.bash ros2 launch svl_robot_bringup robot_tf_launch.py Start the simulation in the SVL Simulator Run the Simulator binary and click OPEN BROWSER In the web user interface, open the Simulations page, select the recently built simulation and click Run Simulation Launch navigation_launch fron Nav2: source /opt/ros/foxy/setup.bash ros2 launch nav2_bringup navigation_launch.py Launch slam toolbox: souce /opt/ros/foxy/setup.bash ros2 launch slam_toolbox online_async_launch.py Launch rviz: source /opt/ros/foxy/setup.bash ros2 launch nav2_bringup rviz_launch.py You can start mapping by either giving destinations using the Navigation2 Goal button in rviz or by manually driving the robot around using the keyboard. Once satisfied with the map it can be saved using: ros2 run nav2_map_server map_saver_cli -f ~/map Note on saving maps : Map saving tends to fail due to timeout. Attempting to save multiple times can result in a successful map save. See this issue on GitHub for more information.","title":"Mapping the environment"},{"location":"tutorials/robotics-mapping/#setting-navorigin-for-your-generated-costmap","text":"In order to be able to use the python API functionalities for setting an initial pose or a destination of a robot along with your generated costmap, a NavOrigin game object should be placed in a scene such that its transform matches with the origin of the costmap you generated. Also, it should have the same offsets as the ones in your costmap yaml file. To find transform and offset values of your costmap origin: Transform of the origin: Coordinates of the lower-left pixel in the generated map image file. Offsets of the origin: The generated YAML file contains an offset for your origin. The offsets are given in 2D position with a third value indicating a rotation.","title":"Setting NavOrigin for your generated costmap"},{"location":"tutorials/robotics-mapping/#how-to-place-navorigin-using-the-vse-tool","text":"You can use the Visual Scenario Editor (VSE) tool of SVL Simulator for a placement of NavOrigin in a scene following the instructions below: Launch SVL Simulator Click on the Visual Editor button to start the VSE tool Select your map in the Map section (e.g., LGSeocho) Place NavOrigin on a map by clicking the ScenarioNavOrigin button in the Add section Move and rotate the NavOrigin so that its placement matches with your costmap origin Write down the x/y/z position and rotation values of the NavOrigin (e.g., in the attached image below, position is (53.4, 0, -57.6) and rotation is ((0, 0, 0)))","title":"How to place NavOrigin using the VSE tool"},{"location":"tutorials/robotics-mapping/#example-usage-in-python-api-script","text":"import lgsvl from lgsvl import Vector, Quaternion, Transform # Establish a connection to SVL Simulator sim = lgsvl.Simulator(lgsvl.wise.SimulatorSettings.simulator_host, lgsvl.wise.SimulatorSettings.simulator_port) # Load LGSeocho map scene = lgsvl.wise.DefaultAssets.map_lgseocho if sim.current_scene == scene: sim.reset() else: sim.load(scene) # Set up LGCloi robot in a scene state = lgsvl.AgentState() state.transform = sim.get_spawn()[0] robot = sim.add_agent(lgsvl.wise.DefaultAssets.ego_lgcloi_navigation2, lgsvl.AgentType.EGO, state) robot.connect_bridge(lgsvl.wise.SimulatorSettings.bridge_host, lgsvl.wise.SimulatorSettings.bridge_port) # Use x/y/z position and rotation values of NavOrigin transform = Transform(Vector(53.4, 0, -57.6), Vector(0, 0, 0)) # Use offsets from a costmap YAML file offset = Vector(-14.7, -24.6, 0) # Set NavOrigin in a scene sim.set_nav_origin(transform, offset) # Get NavOrigin from a scene and verify its values nav_origin = sim.get_nav_origin() print('NavOrigin transform:', nav_origin['transform']) print('NavOrigin offset:', nav_origin['offset']) # Map a destination point in ROS coordinates to Simulator coordinates dst_ros = ((7.118, -0.016, 0), (0, 0, 0.0004, 0.999)) dst_sim = sim.map_from_nav(Vector(*dst_ros[0]), Quaternion(*dst_ros[1])) # Set initial pose of robot robot.set_initial_pose() sim.run(5) # Set destination for Navigation2 stack robot.set_destination(dst_sim) sim.run()","title":"Example usage in Python API script:"},{"location":"tutorials/robotics-pub-sub/","text":"Running a basic Robotics Simulation with ROS 2 # SVL Simulator release 2021.2.2 or later allow robotics simulations. This document describes the process of setting up a simple robotics simulation that communicates through the ROS 2 bridge. Table of Contents Prerequisites Installing ROS 2 Installing lgsvl_bridge Setting up a Simulation CLoi robot ego vehicle Creating a sensor configuration Adding sensors Clock Sensor Differential Drive Control Sensor (Optional) Keyboard Control Sensor Other Sensor Setting up a simulation Running a simulation Prerequisites top # SVL Simulator release 2021.2.2 or later. Linux operating system (preferably Ubuntu 18.04 or later) For a guide to setting up and using the simulator see the Getting Started guide. Installing ROS 2 top # ROS2 can be installed by following the steps in the official installation guide . Installing lgsvl_bridge top # To communicate with the simulator the lgsvl_bridge ROS 2 package is needed. To install it, run: sudo apt update sudo apt install ros-$ROS_DISTRO-lgsvl-bridge The bridge can be launched by running lgsvl_bridge in a terminal where ROS2 is sourced. Setting up a Simulation top # To run a simulation, we must have an ego vehicle (CLOi robot) and an environment available on the web user interface as well as a valid sensor configuration. We will go through the steps of setting these up here. To access the web user interface, run the Simulator executable and click OPEN BROWSER . Note that you may need to link to cloud and build an account/login if you have not already done so. Robot (ego vehicle) top # To run a robotics simulation a robot ego vehicle is needed. With 2021.2.2 release the CLoi robot has been provided in the Asset Store under Vehicles . If it is not already added to your library click the red button with a + sign on it to add it. Click on the model to expand it. Under Sensor Configurations there will be several default sensor configurations listed. For example, the Navigation2 configuration, has the basic set of sensors needed for Nav2. Here we will create a new sensor configuration for basic functionality. Creating a sensor configuration # To create a new sensor configuration, click on the configuration icon to the right of the Sensor Configuration title (see below) and in the following page click Add New Configuration . You will be prompted to give the new configuration a name and choose a Bridge type. Choose ROS 2 from the drop-down (currently only ROS 2 bridge supports robotics). Adding sensors # The new configuration will only have a BaseLink connected to it which is the root object used as parent for all subsequent sensors. The position of each sensor will be with respect to BaseLink , whose position is pre-defined by the model designer (usually the center of the wheel accel). Subsequent sensors can be added by clicking the + button next to the parent object (here BaseLink). The most basic sensor configuration for robotics only needs a control to subscribe to control commands. The default control plugin for the LG CLoi robot is the Differential Drive Control plugin. It is highly recommended to add the Clock sensor as well. Clock Sensor top # Add the Clock sensor by clicking on the + button and searching for it. The location of the sensor is not important and the Frame parameter is unused. The topic name however must be configured to /clock which is the topic ROS 2 expects to see. Note To effectively use the Clock sensor the use_sim_time param must be set to true for all ROS 2 nodes that deal with time. Differential Drive Control Sensor top # The Differential Drive Control Sensor is mainly responsible for subscribing to control commands in the form of a geometry_msgs/Twist ROS 2 message and driving the robot. In addition, the sensor uses wheel rotations to calculate a dead reckoning type position estimate which is published through a nav_msgs/Odometry type message. To configure the sensor, populate the fields as seen below. This sensor requires the path to the links for the left and right wheels of the robot model. For the CLoi robot model these paths are LeftWheelLinkPath : link_MainBody/SuspensionLeft/link/wheel_left/link and RightWheelLinkPath : link_MainBody/SuspensionRight/link/wheel_right/link . These paths can be found by opening the robot model in Unity and looking at the hierarchy, see below. This sensor also has an internal PID controller used for controlling the individual wheels. The gain values can be set here, and are by default set to P_Gain : 1 , I_Gain : 0.05 , D_Gain: 0 . The OdometryTopic is /odom and the OdometryChildFrame is set to odom . The Frame name is set to base_footprint . The Topic name which refers to the Control Command topic is set to /cmd_vel . (Optional) Keyboard Control Sensor # Add the Keyboard Control sensor to enable driving the robot with the keyboard. No configuration is required for this sensor. Other Sensors top # The robot model can take any other sensor that supports ROS 2. This includes RGB and Depth Cameras, Lidar, GPS/GNSS, IMU, etc. To see a full list of sensors refer to the Sensor List Setting up a Simulation top # Click on the Simulations tab and click Add New to create a new simulation. Enter a Simulation Name and select the Cluster . Click Next . Optional - Enable Interactive Mode to be able to start the simulation at the desired time by pressing a play button in the simulator screen. Select the Random Traffic Runtime Template. Select LGSeocho (or any other map) under Map (if it is not present you will need to go back to the store and add the map to your library). Select the LGCloi ego vehicle and the newly created Sensor Configuration. Click Next . Select Other ROS2 Autopilot under Autopilot and enter the Bridge Connection address (by default localhost:9090 ). Click Publish Running a simulation top # After completing the setup outlined above follow these steps to run a simulation. Start the simulation in the SVL Simulator Run the Simulator binary and click OPEN BROWSER In the web user interface, open the Simulations page, select the recently built simulation and click Run Simulation In a new terminal start the lgsvl_bridge : source /opt/ros/foxy/setup.bash lgsvl_bridge Once launched you can switch to the simulator window and click the Play button at the bottom of the screen to start the interactive simulation. In a new terminal echo the /odom topic: source /opt/ros/foxy/setup.bash ros2 topic echo /odom You should now see the odometry data published on the screen. You can confirm that it is changing by driving the robot around using the keyboard. Hit Ctrl + C when done to stop echo. Give the robot a control command by running the command below. source /opt/ros/foxy/setup.bash ros2 topic pub /cmd_vel geometry_msgs/msg/Twist \"{linear: {x: 0.2}, angular: {z: 0.2}}\" -r 10 You should see the robot driving forward and turning to the left. Play around with the numbers to change its behavior. For a tutorial on how to autonomously drive the robot using Navigation2 see this document.","title":"Running a basic Robotics Simulation with ROS 2"},{"location":"tutorials/robotics-pub-sub/#prerequisites","text":"SVL Simulator release 2021.2.2 or later. Linux operating system (preferably Ubuntu 18.04 or later) For a guide to setting up and using the simulator see the Getting Started guide.","title":"Prerequisites"},{"location":"tutorials/robotics-pub-sub/#installing-ros2","text":"ROS2 can be installed by following the steps in the official installation guide .","title":"Installing ROS 2"},{"location":"tutorials/robotics-pub-sub/#installing-lgsvl-bridge","text":"To communicate with the simulator the lgsvl_bridge ROS 2 package is needed. To install it, run: sudo apt update sudo apt install ros-$ROS_DISTRO-lgsvl-bridge The bridge can be launched by running lgsvl_bridge in a terminal where ROS2 is sourced.","title":"Installing lgsvl_bridge"},{"location":"tutorials/robotics-pub-sub/#setup-simulation","text":"To run a simulation, we must have an ego vehicle (CLOi robot) and an environment available on the web user interface as well as a valid sensor configuration. We will go through the steps of setting these up here. To access the web user interface, run the Simulator executable and click OPEN BROWSER . Note that you may need to link to cloud and build an account/login if you have not already done so.","title":"Setting up a Simulation"},{"location":"tutorials/robotics-pub-sub/#cloi","text":"To run a robotics simulation a robot ego vehicle is needed. With 2021.2.2 release the CLoi robot has been provided in the Asset Store under Vehicles . If it is not already added to your library click the red button with a + sign on it to add it. Click on the model to expand it. Under Sensor Configurations there will be several default sensor configurations listed. For example, the Navigation2 configuration, has the basic set of sensors needed for Nav2. Here we will create a new sensor configuration for basic functionality.","title":"CLoi robot ego vehicle"},{"location":"tutorials/robotics-pub-sub/#creating-a-sensor-configuration","text":"To create a new sensor configuration, click on the configuration icon to the right of the Sensor Configuration title (see below) and in the following page click Add New Configuration . You will be prompted to give the new configuration a name and choose a Bridge type. Choose ROS 2 from the drop-down (currently only ROS 2 bridge supports robotics).","title":"Creating a sensor configuration"},{"location":"tutorials/robotics-pub-sub/#adding-sensors","text":"The new configuration will only have a BaseLink connected to it which is the root object used as parent for all subsequent sensors. The position of each sensor will be with respect to BaseLink , whose position is pre-defined by the model designer (usually the center of the wheel accel). Subsequent sensors can be added by clicking the + button next to the parent object (here BaseLink). The most basic sensor configuration for robotics only needs a control to subscribe to control commands. The default control plugin for the LG CLoi robot is the Differential Drive Control plugin. It is highly recommended to add the Clock sensor as well.","title":"Adding sensors"},{"location":"tutorials/robotics-pub-sub/#clock","text":"Add the Clock sensor by clicking on the + button and searching for it. The location of the sensor is not important and the Frame parameter is unused. The topic name however must be configured to /clock which is the topic ROS 2 expects to see. Note To effectively use the Clock sensor the use_sim_time param must be set to true for all ROS 2 nodes that deal with time.","title":"Clock Sensor"},{"location":"tutorials/robotics-pub-sub/#diff-drive","text":"The Differential Drive Control Sensor is mainly responsible for subscribing to control commands in the form of a geometry_msgs/Twist ROS 2 message and driving the robot. In addition, the sensor uses wheel rotations to calculate a dead reckoning type position estimate which is published through a nav_msgs/Odometry type message. To configure the sensor, populate the fields as seen below. This sensor requires the path to the links for the left and right wheels of the robot model. For the CLoi robot model these paths are LeftWheelLinkPath : link_MainBody/SuspensionLeft/link/wheel_left/link and RightWheelLinkPath : link_MainBody/SuspensionRight/link/wheel_right/link . These paths can be found by opening the robot model in Unity and looking at the hierarchy, see below. This sensor also has an internal PID controller used for controlling the individual wheels. The gain values can be set here, and are by default set to P_Gain : 1 , I_Gain : 0.05 , D_Gain: 0 . The OdometryTopic is /odom and the OdometryChildFrame is set to odom . The Frame name is set to base_footprint . The Topic name which refers to the Control Command topic is set to /cmd_vel .","title":"Differential Drive Control Sensor"},{"location":"tutorials/robotics-pub-sub/#optional-keyboard-control-sensor","text":"Add the Keyboard Control sensor to enable driving the robot with the keyboard. No configuration is required for this sensor.","title":"(Optional) Keyboard Control Sensor"},{"location":"tutorials/robotics-pub-sub/#other","text":"The robot model can take any other sensor that supports ROS 2. This includes RGB and Depth Cameras, Lidar, GPS/GNSS, IMU, etc. To see a full list of sensors refer to the Sensor List","title":"Other Sensor"},{"location":"tutorials/robotics-pub-sub/#sim-setup","text":"Click on the Simulations tab and click Add New to create a new simulation. Enter a Simulation Name and select the Cluster . Click Next . Optional - Enable Interactive Mode to be able to start the simulation at the desired time by pressing a play button in the simulator screen. Select the Random Traffic Runtime Template. Select LGSeocho (or any other map) under Map (if it is not present you will need to go back to the store and add the map to your library). Select the LGCloi ego vehicle and the newly created Sensor Configuration. Click Next . Select Other ROS2 Autopilot under Autopilot and enter the Bridge Connection address (by default localhost:9090 ). Click Publish","title":"Setting up a simulation"},{"location":"tutorials/robotics-pub-sub/#sim","text":"After completing the setup outlined above follow these steps to run a simulation. Start the simulation in the SVL Simulator Run the Simulator binary and click OPEN BROWSER In the web user interface, open the Simulations page, select the recently built simulation and click Run Simulation In a new terminal start the lgsvl_bridge : source /opt/ros/foxy/setup.bash lgsvl_bridge Once launched you can switch to the simulator window and click the Play button at the bottom of the screen to start the interactive simulation. In a new terminal echo the /odom topic: source /opt/ros/foxy/setup.bash ros2 topic echo /odom You should now see the odometry data published on the screen. You can confirm that it is changing by driving the robot around using the keyboard. Hit Ctrl + C when done to stop echo. Give the robot a control command by running the command below. source /opt/ros/foxy/setup.bash ros2 topic pub /cmd_vel geometry_msgs/msg/Twist \"{linear: {x: 0.2}, angular: {z: 0.2}}\" -r 10 You should see the robot driving forward and turning to the left. Play around with the numbers to change its behavior. For a tutorial on how to autonomously drive the robot using Navigation2 see this document.","title":"Running a simulation"},{"location":"tutorials/robotics-ros2/","text":"Running a ROS2 Robotics Simulation with Nav2 # The SVL Simulator robotics release adds support for robots as ego vehicles. Robots differ from ego vehicles in the way their motion is controlled. The robotics release of the simulator makes use of Unity's Articulation Body physics components to realistically model joints for robotic movements. This robotics release of the SVL Simulator also provides an ego vehicle, the LG CLOi robot, and an accompanying control plugin that defines the robot motion. Autonomous navigation of the robot in simulated environments is possible using the simulator with the ROS 2 Navigation stack (Nav2) . This document guides users through setting up and running a simulation alongside Nav2. Table of Contents Prerequisites Installing ROS 2 SVL robot startup Setting up a Simulation CLOi robot ego vehicle Clock Sensor LiDAR2D Sensor LiDAR Sensor Destination Sensor Differential Drive Control Sensor Setting up a simulation Running a simulation with Nav2 Prerequisites top # SVL Simulator release 2021.2.2 or later Linux operating system (preferably Ubuntu 18.04 or later) For a guide to setting up and using the simulator see the Getting Started guide. Installing ROS 2 top # ROS2 can be installed by following the steps in the official installation guide . SVL Robot Startup top # Robots often have many 3D coordinate frames that change in position and orientation over time. tf2 is a transform library used to keep track of the relative transformation between these coordinate frames in ROS/ROS 2. The simulator does not directly broadcast tf transforms. Therefore, an external ROS2 node is needed for this. svl_robot_startup is a ROS2 package that publishes static transforms for the robot sensors and subscribes to the /odom topic published by the simulator to figure out and broadcast the transform between the odom and basefootprint frames. In addition, the launch file in the package can optionally use pointcloud_to_laserscan to convert the single beam LiDAR's PointCloud2 message to a LaserScan message type using by passing an command line argument. This node is not launched by default. The SVL Simulator's ROS 2 bridge lgsvl_bridge is included as a dependency and added to the launch file to enable communication with the simulator. The navigation2 and nav2_bringup packages are also added as dependencies as they will be needed for running Nav2. Follow these steps to setup the package and install dependencies: source /opt/ros/foxy/setup.bash mkdir -p robot_ws/src cd robot_ws/src git clone https://github.com/lgsvl/svl_robot_bringup.git cd .. rosdep update rosdep install --from-path src -iy --rosdistro foxy colcon build --symlink-install Setting up a Simulation top # To run a simulation, we must have an ego vehicle (CLOi robot) and an environment available on the web user interface as well as a valid sensor configuration. We will go through the steps of setting these up here. To access the web user interface, run the Simulator executable and click OPEN BROWSER . Note that you may need to link to cloud and build an account/login if you have not already done so. CLOi robot ego vehicle top # The CLOi robot is available by default in the store and is located under Store > Vehicles in the web user interface. The ego vehicle includes the implementation of the vehicle dynamics for the robot. See dynamics documentation to read more about this. The steps here are only provided to give insight into how the sensor configuration is setup and how changes can be made. You may skip this section if you wish to quickly run a simulation. If it is not already added to your library click the red button with a + sign on it to add it. Click on the model to expand it and under Sensor Configurations choose the Navigation2 configuration. The Navigation2 sensor configuration is a basic sensor configuration for running the simulator with Nav2 that has the following sensors: We will go through the noteworthy details of each sensor below. Clock Sensor top # Click on the Clock Sensor to view its parameters. The only notable parameter here is the topic which must be set to /clock . ROS2 will use the time provided in this topic as reference time when the use_sim_time parameter is set to true . LiDAR2D Sensor top # Click on the LiDAR Sensor to view its details. The position of the sensor is set to {0.27, 0, 0.1125} . These are the coordinates of the sensor relative to base_link . The CenterAngle is set to 0 . The RotationFrequency is set to 10 and the Compensated parameter was marked as No . The Topic name is set to /scan and the Frame name is set to base_scan . LiDAR Sensor top # The LiDAR sensor published pointcloud messages which are not the type of message Nav2 looks for by default. It is, however, possible to convert the message to laserscan using the ROS 2 package pointcloud_to_laserscan . The Nav2_PointCloud sensor configuration uses this lidar instead of the LiDAR2D sensor and is configured to be used with the pointcloud_to_laserscan node. The position of the sensor is set to {0.27, 0, 0.1125} . These are the coordinates of the sensor relative to base_link . The LaserCount parameter is set to 1 which is the number of beams the LiDAR will have. To simulate a planar LiDAR we set this value to 1 and also set the CenterAngle parameter to 0 . The RotationFrequency was set to 10 (Hz) and the Compensated parameter was marked as No . The Topic name is set to /cloud_in which is the topic name the pointcloud_to_laserscan node expects. Frame name is set to base_scan which is the same frame that will be passed on to the LaserScan message which Nav2 expects. Destination Sensor top # The destination sensor will not be used in this tutorial. The sensor enables users to programmatically set the initial pose and desired destination of the robot through Python API. Differential Drive Control Sensor top # The Differential Drive Control Sensor is responsible for subscribing to control commands from Nav2 and driving the robot as well as publishing the Odometry message. Click on the sensor to view its details. This sensor translates a commanded velocity pose into motion for each of the two drive wheels on the robot. To do this the sensor requires the path to the links for the left and right wheels of the robot model. For the CLOi robot model these paths are LeftWheelLinkPath : link_MainBody/SuspensionLeft/link/wheel_left/link and RightWheelLinkPath : link_MainBody/SuspensionRight/link/wheel_right/link . To find the path for a different robot the user would need to open the robot in the Unity Editor and take note of the path between the main body of the robot and each of the drive wheels. For example, the image below shows the expanded hierarchy for the CLOi robot in the Unity Editor. The gain values for the PID controller can also be set here, and are by default set to P_Gain : 1 , I_Gain : 0.05 , D_Gain: 0 . The OdometryTopic is /odom and the OdometryChildFrame is set to odom . The Frame name is set to base_footprint . The Topic name which refers to the Control Command topic coming from Nav2 is set to /cmd_vel . Setting up a Simulation top # Click on the Simulations tab and click Add New to create a new simulation. Enter a Simulation Name and select the Cluster . Click Next . Optional - Enable Interactive Mode to be able to start the simulation at the desired time by pressing a play button in the simulator screen. Select the Random Traffic Runtime Template. Select LGSeocho under Map (if it is not present you will need to go back to the store and add the map to your library). Select the LGCloi ego vehicle and the Nav2 Sensor Configuration. Click Next . Select Other ROS2 Autopilot under Autopilot and enter the Bridge Connection address (by default localhost:9090 ). Click Publish Running a simulation with Nav2 top # After completing the setup outlined above follow these steps to run a simulation with Nav2. Start the simulation in the SVL Simulator Run the Simulator binary and click OPEN BROWSER In the web user interface, open the Simulations page, select the recently built simulation and click Run Simulation In a new terminal start the robot_tf_launch : source /opt/ros/foxy/setup.bash cd robot_ws source install/setup.bash ros2 launch svl_robot_bringup robot_tf_launch.py This launch file will start the lgsvl_bridge node, the pointcloud_to_laserscan node, the odom_tf node and a set of static tf nodes. Note: Users who wish to use the Nav2_PointCloud sensor configuration, should use the following command to launch the required nodes, including pointcloud_to_laserscan : bash ros2 launch svl_robot_bringup robot_tf_launch.py pointcloud:=true In a new terminal launch Nav2: source /opt/ros/foxy/setup.bash ros2 launch nav2_bringup bringup_launch.py map:=robot_ws/src/svl_robot_bringup/maps/seocho.yaml params:=robot_ws/src/svl_robot_bringup/params/nav2_params.yml In a new terminal launch rviz: source /opt/ros/foxy/setup.bash ros2 launch nav2_bringup rviz_launch.py rviz_config:=robot_ws/src/svl_robot_bringup/rviz/nav2_cloi.rviz In rviz you will need to set the initial pose using the 2D Pose Estimate button. You will need to mark the current position of the robot on the map in rviz. The default starting position in the Seocho map is shown below. Once the pose is set, the costmaps will be loaded and the particle cloud from AMCL will be displayed. In rviz give the robot a destination using the 2D Goal Pose button. The robot should now navigate to its destination.","title":"Robot simulation with ROS 2 Navigation Stack"},{"location":"tutorials/robotics-ros2/#prerequisites","text":"SVL Simulator release 2021.2.2 or later Linux operating system (preferably Ubuntu 18.04 or later) For a guide to setting up and using the simulator see the Getting Started guide.","title":"Prerequisites"},{"location":"tutorials/robotics-ros2/#installing-ros2","text":"ROS2 can be installed by following the steps in the official installation guide .","title":"Installing ROS 2"},{"location":"tutorials/robotics-ros2/#svl-robot-startup","text":"Robots often have many 3D coordinate frames that change in position and orientation over time. tf2 is a transform library used to keep track of the relative transformation between these coordinate frames in ROS/ROS 2. The simulator does not directly broadcast tf transforms. Therefore, an external ROS2 node is needed for this. svl_robot_startup is a ROS2 package that publishes static transforms for the robot sensors and subscribes to the /odom topic published by the simulator to figure out and broadcast the transform between the odom and basefootprint frames. In addition, the launch file in the package can optionally use pointcloud_to_laserscan to convert the single beam LiDAR's PointCloud2 message to a LaserScan message type using by passing an command line argument. This node is not launched by default. The SVL Simulator's ROS 2 bridge lgsvl_bridge is included as a dependency and added to the launch file to enable communication with the simulator. The navigation2 and nav2_bringup packages are also added as dependencies as they will be needed for running Nav2. Follow these steps to setup the package and install dependencies: source /opt/ros/foxy/setup.bash mkdir -p robot_ws/src cd robot_ws/src git clone https://github.com/lgsvl/svl_robot_bringup.git cd .. rosdep update rosdep install --from-path src -iy --rosdistro foxy colcon build --symlink-install","title":"SVL robot startup"},{"location":"tutorials/robotics-ros2/#setup-simulation","text":"To run a simulation, we must have an ego vehicle (CLOi robot) and an environment available on the web user interface as well as a valid sensor configuration. We will go through the steps of setting these up here. To access the web user interface, run the Simulator executable and click OPEN BROWSER . Note that you may need to link to cloud and build an account/login if you have not already done so.","title":"Setting up a Simulation"},{"location":"tutorials/robotics-ros2/#cloi","text":"The CLOi robot is available by default in the store and is located under Store > Vehicles in the web user interface. The ego vehicle includes the implementation of the vehicle dynamics for the robot. See dynamics documentation to read more about this. The steps here are only provided to give insight into how the sensor configuration is setup and how changes can be made. You may skip this section if you wish to quickly run a simulation. If it is not already added to your library click the red button with a + sign on it to add it. Click on the model to expand it and under Sensor Configurations choose the Navigation2 configuration. The Navigation2 sensor configuration is a basic sensor configuration for running the simulator with Nav2 that has the following sensors: We will go through the noteworthy details of each sensor below.","title":"CLOi robot ego vehicle"},{"location":"tutorials/robotics-ros2/#clock","text":"Click on the Clock Sensor to view its parameters. The only notable parameter here is the topic which must be set to /clock . ROS2 will use the time provided in this topic as reference time when the use_sim_time parameter is set to true .","title":"Clock Sensor"},{"location":"tutorials/robotics-ros2/#lidar2d","text":"Click on the LiDAR Sensor to view its details. The position of the sensor is set to {0.27, 0, 0.1125} . These are the coordinates of the sensor relative to base_link . The CenterAngle is set to 0 . The RotationFrequency is set to 10 and the Compensated parameter was marked as No . The Topic name is set to /scan and the Frame name is set to base_scan .","title":"LiDAR2D Sensor"},{"location":"tutorials/robotics-ros2/#lidar","text":"The LiDAR sensor published pointcloud messages which are not the type of message Nav2 looks for by default. It is, however, possible to convert the message to laserscan using the ROS 2 package pointcloud_to_laserscan . The Nav2_PointCloud sensor configuration uses this lidar instead of the LiDAR2D sensor and is configured to be used with the pointcloud_to_laserscan node. The position of the sensor is set to {0.27, 0, 0.1125} . These are the coordinates of the sensor relative to base_link . The LaserCount parameter is set to 1 which is the number of beams the LiDAR will have. To simulate a planar LiDAR we set this value to 1 and also set the CenterAngle parameter to 0 . The RotationFrequency was set to 10 (Hz) and the Compensated parameter was marked as No . The Topic name is set to /cloud_in which is the topic name the pointcloud_to_laserscan node expects. Frame name is set to base_scan which is the same frame that will be passed on to the LaserScan message which Nav2 expects.","title":"LiDAR Sensor"},{"location":"tutorials/robotics-ros2/#destination-sensor","text":"The destination sensor will not be used in this tutorial. The sensor enables users to programmatically set the initial pose and desired destination of the robot through Python API.","title":"Destination Sensor"},{"location":"tutorials/robotics-ros2/#diff-drive","text":"The Differential Drive Control Sensor is responsible for subscribing to control commands from Nav2 and driving the robot as well as publishing the Odometry message. Click on the sensor to view its details. This sensor translates a commanded velocity pose into motion for each of the two drive wheels on the robot. To do this the sensor requires the path to the links for the left and right wheels of the robot model. For the CLOi robot model these paths are LeftWheelLinkPath : link_MainBody/SuspensionLeft/link/wheel_left/link and RightWheelLinkPath : link_MainBody/SuspensionRight/link/wheel_right/link . To find the path for a different robot the user would need to open the robot in the Unity Editor and take note of the path between the main body of the robot and each of the drive wheels. For example, the image below shows the expanded hierarchy for the CLOi robot in the Unity Editor. The gain values for the PID controller can also be set here, and are by default set to P_Gain : 1 , I_Gain : 0.05 , D_Gain: 0 . The OdometryTopic is /odom and the OdometryChildFrame is set to odom . The Frame name is set to base_footprint . The Topic name which refers to the Control Command topic coming from Nav2 is set to /cmd_vel .","title":"Differential Drive Control Sensor"},{"location":"tutorials/robotics-ros2/#sim-setup","text":"Click on the Simulations tab and click Add New to create a new simulation. Enter a Simulation Name and select the Cluster . Click Next . Optional - Enable Interactive Mode to be able to start the simulation at the desired time by pressing a play button in the simulator screen. Select the Random Traffic Runtime Template. Select LGSeocho under Map (if it is not present you will need to go back to the store and add the map to your library). Select the LGCloi ego vehicle and the Nav2 Sensor Configuration. Click Next . Select Other ROS2 Autopilot under Autopilot and enter the Bridge Connection address (by default localhost:9090 ). Click Publish","title":"Setting up a simulation"},{"location":"tutorials/robotics-ros2/#nav2","text":"After completing the setup outlined above follow these steps to run a simulation with Nav2. Start the simulation in the SVL Simulator Run the Simulator binary and click OPEN BROWSER In the web user interface, open the Simulations page, select the recently built simulation and click Run Simulation In a new terminal start the robot_tf_launch : source /opt/ros/foxy/setup.bash cd robot_ws source install/setup.bash ros2 launch svl_robot_bringup robot_tf_launch.py This launch file will start the lgsvl_bridge node, the pointcloud_to_laserscan node, the odom_tf node and a set of static tf nodes. Note: Users who wish to use the Nav2_PointCloud sensor configuration, should use the following command to launch the required nodes, including pointcloud_to_laserscan : bash ros2 launch svl_robot_bringup robot_tf_launch.py pointcloud:=true In a new terminal launch Nav2: source /opt/ros/foxy/setup.bash ros2 launch nav2_bringup bringup_launch.py map:=robot_ws/src/svl_robot_bringup/maps/seocho.yaml params:=robot_ws/src/svl_robot_bringup/params/nav2_params.yml In a new terminal launch rviz: source /opt/ros/foxy/setup.bash ros2 launch nav2_bringup rviz_launch.py rviz_config:=robot_ws/src/svl_robot_bringup/rviz/nav2_cloi.rviz In rviz you will need to set the initial pose using the 2D Pose Estimate button. You will need to mark the current position of the robot on the map in rviz. The default starting position in the Seocho map is shown below. Once the pose is set, the costmaps will be loaded and the particle cloud from AMCL will be displayed. In rviz give the robot a destination using the 2D Goal Pose button. The robot should now navigate to its destination.","title":"Running a simulation with Nav2"},{"location":"tutorials/running-on-aws/","text":"How to Run SVL Simulator on AWS # Table of Contents Setup an AWS RoboMaker Development Environment Install the NVIDIA Drivers Install and Setup SVL Simulator Install an Autopilot Setup an AWS RoboMaker Development Environment top # Go to AWS RoboMaker in the AWS Console . Click on the \u201c3-lines\u201d in the upper left to make the navigation pane visible. Navigate to Development > Development environments . Create a Foxy development environment. Use the default instance type; it will be changed once the development environment has been created. Wait until the creation of the development environment has finished and the initial screen appears, then close the browser tab. Navigate to the EC2 service in the console. Select the instance created by RoboMaker and then select Stop instance from the Instance state dropdown menu. Once it has stopped, navigate to Actions > Instance Settings and select Change instance type . Choose g4dn.2xlarge for the new instance type. Navigate to Development > Development environments , select the development environment you created and press Open environment . Once the development environment has started, launch a Virtual Desktop . Open an XTerm window. Update the OS packages and install Terminal : $ sudo apt update $ sudo apt full-upgrade $ sudo apt install gnome-terminal Reboot, launch a Virtual Desktop , and open a Terminal window. Install the NVIDIA Drivers top # Launch Additional Drivers : $ sudo software-properties-gtk --open-tab=4 Select Using NVIDIA driver metapackage from nvidia-driver-470 and press Apply Changes . Reboot, launch a Virtual Desktop , and open a Terminal window. Confirm that the NVIDIA drivers have been installed correctly by running nvidia-smi . Its output should be similar to: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.86 Driver Version: 470.86 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:00:1E.0 Off | 0 | | N/A 62C P0 30W / 70W | 146MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 3656 C ...64-linux-gnu/dcv/dcvagent 143MiB | +-----------------------------------------------------------------------------+ Install and Setup SVL Simulator top # Download and install the latest version of SVL Simulator (2021.3): $ base=svlsimulator-linux64-2021.3 $ wget https://github.com/lgsvl/simulator/releases/download/2021.3/$base.zip $ unzip $base.zip $ cd $base Launch it: $ ./simulator & To finish the setup, follow the SVL Simulator installation procedure starting from Link to Cloud . Because Docker is already installed on the Foxy development environment, you can ignore the Installing Docker under Linux section. Install an Autopilot top # The AWS instance that the development environment is running on has enough CPU, GPU, and memory resources to support running an autopilot in addition to SVL Simulator. However, it does not have enough disk space in its root filesystem to install and build one. To add another volume to the instance: Create an empty volume . Select gp3 for the type and size 100 GiB. Attach it to the instance Format and automatically mount the volume . The advantage of keeping the autopilot build on a separate volume from the root volume of the instance is that it can be retained when the instance is terminated. Now you are ready to install, build, and run an autopilot. Browse to the list of autopilot tutorials to get started.","title":"Running SVL Simulator on AWS"},{"location":"tutorials/running-on-aws/#setup-an-aws-robomaker-development-environment","text":"Go to AWS RoboMaker in the AWS Console . Click on the \u201c3-lines\u201d in the upper left to make the navigation pane visible. Navigate to Development > Development environments . Create a Foxy development environment. Use the default instance type; it will be changed once the development environment has been created. Wait until the creation of the development environment has finished and the initial screen appears, then close the browser tab. Navigate to the EC2 service in the console. Select the instance created by RoboMaker and then select Stop instance from the Instance state dropdown menu. Once it has stopped, navigate to Actions > Instance Settings and select Change instance type . Choose g4dn.2xlarge for the new instance type. Navigate to Development > Development environments , select the development environment you created and press Open environment . Once the development environment has started, launch a Virtual Desktop . Open an XTerm window. Update the OS packages and install Terminal : $ sudo apt update $ sudo apt full-upgrade $ sudo apt install gnome-terminal Reboot, launch a Virtual Desktop , and open a Terminal window.","title":"Setup an AWS RoboMaker Development Environment"},{"location":"tutorials/running-on-aws/#install-the-nvidia-drivers","text":"Launch Additional Drivers : $ sudo software-properties-gtk --open-tab=4 Select Using NVIDIA driver metapackage from nvidia-driver-470 and press Apply Changes . Reboot, launch a Virtual Desktop , and open a Terminal window. Confirm that the NVIDIA drivers have been installed correctly by running nvidia-smi . Its output should be similar to: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.86 Driver Version: 470.86 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:00:1E.0 Off | 0 | | N/A 62C P0 30W / 70W | 146MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 3656 C ...64-linux-gnu/dcv/dcvagent 143MiB | +-----------------------------------------------------------------------------+","title":"Install the NVIDIA Drivers"},{"location":"tutorials/running-on-aws/#install-and-setup-svl-simulator","text":"Download and install the latest version of SVL Simulator (2021.3): $ base=svlsimulator-linux64-2021.3 $ wget https://github.com/lgsvl/simulator/releases/download/2021.3/$base.zip $ unzip $base.zip $ cd $base Launch it: $ ./simulator & To finish the setup, follow the SVL Simulator installation procedure starting from Link to Cloud . Because Docker is already installed on the Foxy development environment, you can ignore the Installing Docker under Linux section.","title":"Install and Setup SVL Simulator"},{"location":"tutorials/running-on-aws/#install-an-autopilot","text":"The AWS instance that the development environment is running on has enough CPU, GPU, and memory resources to support running an autopilot in addition to SVL Simulator. However, it does not have enough disk space in its root filesystem to install and build one. To add another volume to the instance: Create an empty volume . Select gp3 for the type and size 100 GiB. Attach it to the instance Format and automatically mount the volume . The advantage of keeping the autopilot build on a separate volume from the root volume of the instance is that it can be retained when the instance is terminated. Now you are ready to install, build, and run an autopilot. Browse to the list of autopilot tutorials to get started.","title":"Install an Autopilot"},{"location":"user-interface/bridge-connection-ui/","text":"Bridge Connection UI # When in a non-Headless Simulation, a list of published and subscribed topics can be found in the Simulator menu (plug icon). At the top of the menu is the selected vehicle. The bridge status can be: Disconnected , Connecting , or Connected The bridge address is the same that was entered as the Bridge Connection String when creating the Simulation. Each topic is then listed in the following format: PUB or SUB : indicates if the Simulator publishes or subscribes to messages on this topic Topic : is the topic that the messages are published/subscribed to Type : is the message type on this topic Count : is the total number of messages published/received when the bridge was connected","title":"Bridge connection UI"},{"location":"user-interface/config-and-cmd-line-params/","text":"Configuration File and Command Line Parameters # Configuration File top # The Simulator configuration file config.yml includes parameters shared between different users and allows administrators to setup deployment specific settings. The list of supported configuration parameters is below: Parameter Name Type Default Value Description api_hostname string localhost The IP address of the network interface (or the hostname assigned to it) on which Simulator should listen for Python API commands. Specify \"*\" to listen on all network interfaces. api_port integer 8181 Port number used by Python API to connect. cloud_url string https://wise.svlsimulator.com Address of the web user interface. cloud_proxy string - URL pointing to HTTP proxy server for connecting to the web user interface. data_path string OS dependent set by Unity Local database path headless bool false Whether or not simulator should work in headless mode only. read_only bool false Whether or not the user is allowed to change anything in the database. This mode is used to run Simulator in public demo mode. To use a custom config.yml file, include it in the same folder as Simulator.exe or modify the one in the root directory of the cloned project. The file is expected to be in YAML format. Example of a config.yml file: config.yml # headless: false read_only: false api_hostname: \"localhost\" api_port: 8181 cloud_url: \"https://wise.staging.lgsvlsimulator.com\" data_path: \"path-to-local-database\" Command Line Parameters top # Simulator accepts command line parameters during start up. These command line parameters override the values from the configuration file. The list of supported command line parameters is below: Parameter Name Argument Type Default Value Description --apihost string localhost The IP address of the network interface (or the hostname assigned to it) on which Simulator should listen for Python API commands. Specify \"*\" to listen on all network interfaces. --apiport integer 8181 Port number used by Python API to connect. --cloudurl string https://wise.svlsimulator.com Address of the web user interface. --data or -d string OS dependent set by Unity Local database path - - retryForever (none) - If present, Simulator attempts to connect to the web user interface indefinitely. --simid string auto gen To overwrite simid in database or set on first start.","title":"Configuration file and command line parameters"},{"location":"user-interface/config-and-cmd-line-params/#configuration-file","text":"The Simulator configuration file config.yml includes parameters shared between different users and allows administrators to setup deployment specific settings. The list of supported configuration parameters is below: Parameter Name Type Default Value Description api_hostname string localhost The IP address of the network interface (or the hostname assigned to it) on which Simulator should listen for Python API commands. Specify \"*\" to listen on all network interfaces. api_port integer 8181 Port number used by Python API to connect. cloud_url string https://wise.svlsimulator.com Address of the web user interface. cloud_proxy string - URL pointing to HTTP proxy server for connecting to the web user interface. data_path string OS dependent set by Unity Local database path headless bool false Whether or not simulator should work in headless mode only. read_only bool false Whether or not the user is allowed to change anything in the database. This mode is used to run Simulator in public demo mode. To use a custom config.yml file, include it in the same folder as Simulator.exe or modify the one in the root directory of the cloned project. The file is expected to be in YAML format. Example of a config.yml file:","title":"Configuration File"},{"location":"user-interface/config-and-cmd-line-params/#configyml","text":"headless: false read_only: false api_hostname: \"localhost\" api_port: 8181 cloud_url: \"https://wise.staging.lgsvlsimulator.com\" data_path: \"path-to-local-database\"","title":"config.yml"},{"location":"user-interface/config-and-cmd-line-params/#command-line-parameters","text":"Simulator accepts command line parameters during start up. These command line parameters override the values from the configuration file. The list of supported command line parameters is below: Parameter Name Argument Type Default Value Description --apihost string localhost The IP address of the network interface (or the hostname assigned to it) on which Simulator should listen for Python API commands. Specify \"*\" to listen on all network interfaces. --apiport integer 8181 Port number used by Python API to connect. --cloudurl string https://wise.svlsimulator.com Address of the web user interface. --data or -d string OS dependent set by Unity Local database path - - retryForever (none) - If present, Simulator attempts to connect to the web user interface indefinitely. --simid string auto gen To overwrite simid in database or set on first start.","title":"Command Line Parameters"},{"location":"user-interface/keyboard-shortcuts/","text":"Simulator Controls # Key Bindings top # Officially supported: # F1 - Help menu \u2190 \u2191 \u2193 \u2192 - Drive vehicle forward/brake, turn 1 - 0 - Agent select and follow cam Camera Controls top # Mouse Right-Click hold & drag - Look/Rotate ~ - Free Camera W S - Zoom A D - Strafe Q E - Up/Down For developer use: # N - Toggle Non-Player Character (NPC) vehicles P - Toggle Pedestrians End - Toggle ignition (must be on for vehicle to move) Page Up - Shift to forward drive gear Page Down - Shift to reverse gear F12 - Reset Active Vehicle location Miscellaneous top # H - Toggle headlights F - Toggle fog lights I - Toggle interior light Right Shift - Toggle parking brake (needs to be off for vehicle to move) M - Toggle hazard lights < - Toggle left blinker > - Toggle right blinker Logitech G920 Wheel Inputs top # Right Pedal RB - Accelerate Middle Pedal LB - Brake Rotate Wheel - Turn A - Cycle through Agents B - Toggle Non-Player Character (NPC) vehicles X - Toggle Sensor Visualizers Y - Cycle through Camera states (Follow camera, Cinematic camera, Free camera) Start - [Hamburger Menu/3 horizonal lines] Pause Simulation (Interactive Simulation Only) Select - [Overlapping Squares] Open Menu RSB - Cycle through Agent headlight states LSB - Toggle Reverse Center - [Xbox] Toggle Interior Light (if available)","title":"Keyboard shortcuts"},{"location":"user-interface/keyboard-shortcuts/#key-bindings","text":"","title":"Key Bindings"},{"location":"user-interface/keyboard-shortcuts/#officially-supported","text":"F1 - Help menu \u2190 \u2191 \u2193 \u2192 - Drive vehicle forward/brake, turn 1 - 0 - Agent select and follow cam","title":"Officially supported:"},{"location":"user-interface/keyboard-shortcuts/#camera-controls","text":"Mouse Right-Click hold & drag - Look/Rotate ~ - Free Camera W S - Zoom A D - Strafe Q E - Up/Down","title":"Camera Controls"},{"location":"user-interface/keyboard-shortcuts/#for-developer-use","text":"N - Toggle Non-Player Character (NPC) vehicles P - Toggle Pedestrians End - Toggle ignition (must be on for vehicle to move) Page Up - Shift to forward drive gear Page Down - Shift to reverse gear F12 - Reset Active Vehicle location","title":"For developer use:"},{"location":"user-interface/keyboard-shortcuts/#miscellaneous","text":"H - Toggle headlights F - Toggle fog lights I - Toggle interior light Right Shift - Toggle parking brake (needs to be off for vehicle to move) M - Toggle hazard lights < - Toggle left blinker > - Toggle right blinker","title":"Miscellaneous"},{"location":"user-interface/keyboard-shortcuts/#logitech-g920-wheel-inputs","text":"Right Pedal RB - Accelerate Middle Pedal LB - Brake Rotate Wheel - Turn A - Cycle through Agents B - Toggle Non-Player Character (NPC) vehicles X - Toggle Sensor Visualizers Y - Cycle through Camera states (Follow camera, Cinematic camera, Free camera) Start - [Hamburger Menu/3 horizonal lines] Pause Simulation (Interactive Simulation Only) Select - [Overlapping Squares] Open Menu RSB - Cycle through Agent headlight states LSB - Toggle Reverse Center - [Xbox] Toggle Interior Light (if available)","title":"Logitech G920 Wheel Inputs"},{"location":"user-interface/sensor-visualizers/","text":"Sensor Visualizers # When in a non-Headless Simulation, sensor visualizers can be toggled from the menu. To visualize a sensor, click the \"eye\" next to the sensor name. For each sensor, you can click the \"coordinate\" icon next to the sensor name to turn on the visualization of the sensor's transform. This includes the sensor and parent name and transform position/rotation. A white line shows the parent/child relationship. Sensors are identified by the name parameter from the JSON configuration. For full details on the possible JSON parameters see Sensor Parameters Not all sensors have visualizations available, only sensors who have will show their visualizations. Table of Contents Cameras Color Camera Depth Camera Segmentation Camera 2D Ground Truth LiDAR Radar 3D Ground Truth Lane-line Sensor Cameras top # When a camera is visualized, the image the sensor see is visualized in a window. This window can be resized by clicking-and-dragging the icon in the bottom right corner and can be made full-screen with the box icon in the top right corner. The window can be moved by clicking-and-dragging the top bar. To close the window, either click the X or click the \"eye\" button again. Color Camera top # Visualized Color camera shows the same things that are visible from the normal follow and free cameras, defined in the sensor configuration. Depth Camera top # Visualized Depth camera shows objects colored on a grayscale based on the distance between the camera and the object. Segmentation Camera top # Visualized Segmentation camera shows objects colored according to the object tag. Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F If a tag is included in \"InstanceSegmentationTags\", each instance of objects with that tag is colored differently, but all will have same hue. 2D Ground Truth top # Visualized 2D Ground Truth shows the same things as a color camera except pedestrians are enclosed in a yellow wire box and NPCs are enclosed in a green wire box. LiDAR top # Visualized LiDAR shows the point cloud that is detected. Radar top # Visualized Radar shows the radar cones and creates wireframe boxes enclosing NPCs in a green box, bicycles in a cyan box, and other EGOs in a magenta box. 3D Ground Truth top # Visualized 3D Ground Truth creates wireframe boxes enclosing pedestrians in a yellow box and NPCs in a green box. Lane-line Sensor top # Visualized Lane-line Sensor shows lines for current lane as an overlay for color image, with a perspective defined in JSON parameters.","title":"Sensor visualizers"},{"location":"user-interface/sensor-visualizers/#cameras","text":"When a camera is visualized, the image the sensor see is visualized in a window. This window can be resized by clicking-and-dragging the icon in the bottom right corner and can be made full-screen with the box icon in the top right corner. The window can be moved by clicking-and-dragging the top bar. To close the window, either click the X or click the \"eye\" button again.","title":"Cameras"},{"location":"user-interface/sensor-visualizers/#color-camera","text":"Visualized Color camera shows the same things that are visible from the normal follow and free cameras, defined in the sensor configuration.","title":"Color Camera"},{"location":"user-interface/sensor-visualizers/#depth-camera","text":"Visualized Depth camera shows objects colored on a grayscale based on the distance between the camera and the object.","title":"Depth Camera"},{"location":"user-interface/sensor-visualizers/#segmentation-camera","text":"Visualized Segmentation camera shows objects colored according to the object tag. Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F If a tag is included in \"InstanceSegmentationTags\", each instance of objects with that tag is colored differently, but all will have same hue.","title":"Segmentation Camera"},{"location":"user-interface/sensor-visualizers/#2d-ground-truth","text":"Visualized 2D Ground Truth shows the same things as a color camera except pedestrians are enclosed in a yellow wire box and NPCs are enclosed in a green wire box.","title":"2D Ground Truth"},{"location":"user-interface/sensor-visualizers/#lidar","text":"Visualized LiDAR shows the point cloud that is detected.","title":"LiDAR"},{"location":"user-interface/sensor-visualizers/#radar","text":"Visualized Radar shows the radar cones and creates wireframe boxes enclosing NPCs in a green box, bicycles in a cyan box, and other EGOs in a magenta box.","title":"Radar"},{"location":"user-interface/sensor-visualizers/#3d-ground-truth","text":"Visualized 3D Ground Truth creates wireframe boxes enclosing pedestrians in a yellow box and NPCs in a green box.","title":"3D Ground Truth"},{"location":"user-interface/sensor-visualizers/#lane-line-sensor","text":"Visualized Lane-line Sensor shows lines for current lane as an overlay for color image, with a perspective defined in JSON parameters.","title":"Lane-line Sensor"},{"location":"user-interface/simulation-menu/","text":"Simulation Menu # When in a non-Headless Simulation, a menu can be accessed by clicking on the \"hamburger\" menu icon in the bottom left. Simulation Menu Contents Toggle Menu Bar Pause/Play Button Stop Menu Sensor Menu Interactive Menu Bridge Menu Controls Menu Information Menu Simulation Time Camera Mode Vehicle Select Menu 1) Toggle Menu Bar top # This button opens or closes the simulator menu bar. 2) Pause/Play Button top # This button pauses or plays the simulation. 3) Stop Menu top # This menu is accessed from the Stop button. Confirm or cancel stopping the simulation. Confirmation exits simulator to the main screen. 4) Sensor Menu top # This menu is accessed from the \"eye\" button. It lists all sensors on the selected vehicle and allows for the sensors to be visualized, as well as their coordinate transform relative to the parent transform sensor or BaseLink transform. See Sensor Visualization for more details. 4a. Sensor Name 4b. Toggle buttons to visualize sensor transform or sensor data 4c. Full screen window or close window buttons 4d. Click and drag to manually resize window 4e. Example of sensor data visualization with GroundTruth3D sensor 4f. Example of sensor transform information 5) Interactive Menu top # This menu is only available in an Interactive Simulation and is accessed from the \"sliders\" button. It contains tools to change the environment of the simulation at run-time. 5a. Time of day slider with a toggle button for freezing time or incrementing time. 5b. Environment effect parameter sliders. 5c. Toggles for NPC vehicles or Pedestrians. 6) Bridge Menu top # This menu is accessed from the \"plug\" button. It lists information on the bridge status as well as all published and subscribed topics. See Bridge Topics for more details. 7) Controls Menu top # This menu is accessed from the \"controller\" button. It lists all the keyboard commands in the simulation. Keyboard control for vehicles requires the Keyboard sensor. See Keyboard Shortcuts for more details. 8) Information Menu top # This menu is accessed from the i button. It lists the build info as well as any errors, logs, or warnings that are created in the current simulation. To clear all of these messages, click the \"Trash can\" icon in the bottom right of the menu. 8a. Build info and console log data 8b. Trash button to delete console log data 8c. Frames per second display 9) Simulation Time top # This timer displays simulation time. {hh:mm:ss:ms} 10) Camera Mode top # The camera icon in the bottom right indicates if the camera mode is currently follow, cinematic or free-roam. Clicking the camera button cycles between the three camera modes. When switching back to the follow camera, the camera will automatically be positioned behind the active vehicle. Follow Mode: Camera remains centered on the selected vehicle. W and S zoom and Mouse RightClick rotates the view around the center of the car. If the view has not been manually rotated, the camera will stay behind the vehicle. Cinematic Mode: Camera rotates between static views and animated camera movement around the selected vehicle. Free-roam Mode: Camera can be moved freely around the map using camera controls. 11) Vehicle Select Menu top # The vehicle listed in the bottom right is the current active vehicle. This menu is affected by keyboard input 1-0. Selecting a different vehicle will change the view to the follow camera of the selected vehicle. The number preceding each vehicle corresponds to the number key to select the vehicle.","title":"Simulation menu"},{"location":"user-interface/simulation-menu/#toggle-menu-bar","text":"This button opens or closes the simulator menu bar.","title":"Toggle Menu Bar"},{"location":"user-interface/simulation-menu/#play-pause-button","text":"This button pauses or plays the simulation.","title":"Pause/Play Button"},{"location":"user-interface/simulation-menu/#stop-menu","text":"This menu is accessed from the Stop button. Confirm or cancel stopping the simulation. Confirmation exits simulator to the main screen.","title":"Stop Menu"},{"location":"user-interface/simulation-menu/#sensor-menu","text":"This menu is accessed from the \"eye\" button. It lists all sensors on the selected vehicle and allows for the sensors to be visualized, as well as their coordinate transform relative to the parent transform sensor or BaseLink transform. See Sensor Visualization for more details. 4a. Sensor Name 4b. Toggle buttons to visualize sensor transform or sensor data 4c. Full screen window or close window buttons 4d. Click and drag to manually resize window 4e. Example of sensor data visualization with GroundTruth3D sensor 4f. Example of sensor transform information","title":"Sensor Menu"},{"location":"user-interface/simulation-menu/#interactive-menu","text":"This menu is only available in an Interactive Simulation and is accessed from the \"sliders\" button. It contains tools to change the environment of the simulation at run-time. 5a. Time of day slider with a toggle button for freezing time or incrementing time. 5b. Environment effect parameter sliders. 5c. Toggles for NPC vehicles or Pedestrians.","title":"Interactive Menu"},{"location":"user-interface/simulation-menu/#bridge-menu","text":"This menu is accessed from the \"plug\" button. It lists information on the bridge status as well as all published and subscribed topics. See Bridge Topics for more details.","title":"Bridge Menu"},{"location":"user-interface/simulation-menu/#controls-menu","text":"This menu is accessed from the \"controller\" button. It lists all the keyboard commands in the simulation. Keyboard control for vehicles requires the Keyboard sensor. See Keyboard Shortcuts for more details.","title":"Controls Menu"},{"location":"user-interface/simulation-menu/#information-menu","text":"This menu is accessed from the i button. It lists the build info as well as any errors, logs, or warnings that are created in the current simulation. To clear all of these messages, click the \"Trash can\" icon in the bottom right of the menu. 8a. Build info and console log data 8b. Trash button to delete console log data 8c. Frames per second display","title":"Information Menu"},{"location":"user-interface/simulation-menu/#simulation-time","text":"This timer displays simulation time. {hh:mm:ss:ms}","title":"Simulation Time"},{"location":"user-interface/simulation-menu/#camera-mode","text":"The camera icon in the bottom right indicates if the camera mode is currently follow, cinematic or free-roam. Clicking the camera button cycles between the three camera modes. When switching back to the follow camera, the camera will automatically be positioned behind the active vehicle. Follow Mode: Camera remains centered on the selected vehicle. W and S zoom and Mouse RightClick rotates the view around the center of the car. If the view has not been manually rotated, the camera will stay behind the vehicle. Cinematic Mode: Camera rotates between static views and animated camera movement around the selected vehicle. Free-roam Mode: Camera can be moved freely around the map using camera controls.","title":"Camera Mode"},{"location":"user-interface/simulation-menu/#vehicle-select-menu","text":"The vehicle listed in the bottom right is the current active vehicle. This menu is affected by keyboard input 1-0. Selecting a different vehicle will change the view to the follow camera of the selected vehicle. The number preceding each vehicle corresponds to the number key to select the vehicle.","title":"Vehicle Select Menu"},{"location":"user-interface/simulator-main-menu/","text":"Simulator Main Menu # Simulator has multiple options on the main screen. Users can link to the cloud, run online and offline simulations and open the Visual Scenario Editor. Linking top # This is the menu when the user is running the application for the first time. Users must link their hardware to the cloud as a cluster. Link To Cloud button will open the web browser to the sign up page. After signing in, users choose the name for the hardware cluster they are currently using and the simulator will be linked. NOTE If you have not signed up, do so first, then re-open the simulator and click the Link To Cloud. Online Mode top # Online mode is available after linking to a cluster is complete. Open Browser will open the cloud user account page in the web browser. Visual Editor will open the Visual Scenario Editor to create user scenarios. Online will open a drop down menu. Go Offline will disconnect the user from the cloud and allow users to run previous run simulations without connecting. Unlink will disconnect the user from the cloud and unlink the cluster from the machine. Clear Cache will delete local saved database information. Loaded Assets displays all loaded assets with compiled code. Unity will not release these assets until the application or Editor is closed. Settings allows users to change Fullscreen, Resolution, Graphics Quality Preset or Custom options. Quit will close the application. Application information is displayed here including Simulator version, Unity Version and Cloud URL. Reference this for support. Offline Mode top # When you select offline mode, only previously saved and run simulations can be run without using the online web user interface. Make sure you have at least one simulation configuration set up on your account and have run it locally. Drop down menu for all currently cached simulations run on this machine. Play for selected offline simulation. Offline will open a drop down menu for returning to online mode.","title":"Simulator main menu"},{"location":"user-interface/simulator-main-menu/#linking","text":"This is the menu when the user is running the application for the first time. Users must link their hardware to the cloud as a cluster. Link To Cloud button will open the web browser to the sign up page. After signing in, users choose the name for the hardware cluster they are currently using and the simulator will be linked. NOTE If you have not signed up, do so first, then re-open the simulator and click the Link To Cloud.","title":"Linking"},{"location":"user-interface/simulator-main-menu/#online-mode","text":"Online mode is available after linking to a cluster is complete. Open Browser will open the cloud user account page in the web browser. Visual Editor will open the Visual Scenario Editor to create user scenarios. Online will open a drop down menu. Go Offline will disconnect the user from the cloud and allow users to run previous run simulations without connecting. Unlink will disconnect the user from the cloud and unlink the cluster from the machine. Clear Cache will delete local saved database information. Loaded Assets displays all loaded assets with compiled code. Unity will not release these assets until the application or Editor is closed. Settings allows users to change Fullscreen, Resolution, Graphics Quality Preset or Custom options. Quit will close the application. Application information is displayed here including Simulator version, Unity Version and Cloud URL. Reference this for support.","title":"Online Mode"},{"location":"user-interface/simulator-main-menu/#offline-mode","text":"When you select offline mode, only previously saved and run simulations can be run without using the online web user interface. Make sure you have at least one simulation configuration set up on your account and have run it locally. Drop down menu for all currently cached simulations run on this machine. Play for selected offline simulation. Offline will open a drop down menu for returning to online mode.","title":"Offline Mode"},{"location":"user-interface/web/clusters/","text":"Clusters # The Clusters page allows you to add a new cluster to your account and make it available for creating simulations and executing them. If you do not have at least one valid cluster created and linked to your account, you will not be able to create or run any simulations. A cluster is the physical computing setup used to run a single simulation. Clusters can either be \"local\", where they consist of one or more machines connected to the same LAN, or \"cloud\", where they are one or more machines in the cloud controlled by the \"SVL Web User Interface\". A simulation that you create must designate the specific cluster on which the SVL Simulator binary executable will run. For more details about the cluster and simulations refer to the Cluster Simulation Introduction . A tutorial video on local cluster creation can be found here . Adding a Local Cluster top # Open the SVL Simulator executable and click \"Link to Cloud\". If this button is not available, look in the top right to make sure the simulator is Online. If Offline, click the button to toggle. You may need to toggle off and on if a connection is not working. When the browser opens and you are taken to the SVL Simulator online interface, log in then navigate to \"Clusters\". Enter a name for your local cluster and click \"Create cluster\". Editing a Local Cluster top # To edit a local cluster, click the three-dotted menu at the top right corner of a cluster card, and then click \"Edit\". You can then rename your cluster, or if there are multiple instances attached to the cluster, delete an instance. Be sure to click \"Save\" after making any changes. Deleting a Local Cluster top # To delete a local cluster, click the three-dotted menu at the top right corner of a cluster card, and then click \"Delete\".","title":"Clusters"},{"location":"user-interface/web/clusters/#local-cluster","text":"Open the SVL Simulator executable and click \"Link to Cloud\". If this button is not available, look in the top right to make sure the simulator is Online. If Offline, click the button to toggle. You may need to toggle off and on if a connection is not working. When the browser opens and you are taken to the SVL Simulator online interface, log in then navigate to \"Clusters\". Enter a name for your local cluster and click \"Create cluster\".","title":"Local Cluster"},{"location":"user-interface/web/clusters/#editing-a-local-cluster","text":"To edit a local cluster, click the three-dotted menu at the top right corner of a cluster card, and then click \"Edit\". You can then rename your cluster, or if there are multiple instances attached to the cluster, delete an instance. Be sure to click \"Save\" after making any changes.","title":"Editing a Local Cluster"},{"location":"user-interface/web/clusters/#deleting-a-local-cluster","text":"To delete a local cluster, click the three-dotted menu at the top right corner of a cluster card, and then click \"Delete\".","title":"Deleting a Local Cluster"},{"location":"user-interface/web/library/","text":"Library # The Library page in the SVL Simulator user interface displays the set of maps and vehicles that you have added to your account, and which are available for use in simulations. A tutorial video on adding an asset from the Store, as well as uploading your own asset, can be found here . Introduction top # Library is the central place for all of the content and data associated with your SVL Simulator account. You can access your maps and vehicles from anywhere once you log in to your account, allowing you to save your content in the cloud, including your own map environments, plugins, vehicle models, and sensor configurations. Maps top # When you log in for the first time, your Library page will show some widely used default assets that are automatically made available to your account. You can click: \"All\" to see all maps that you have added to your Library. \"My Content\" lists maps for which you are the owner (meaning they were built and uploaded by you). \"My Shares\" lists maps that are owned by you and were to other users (using the Sharing feature). \"Added from Store\" lists maps from other users that you have added to your library from store. \"Shared with Me\" lists maps that were shared with you by other users. Adding a map top # You can add maps to your Library from either the Store page or by clicking the \"Add new\" button at the top right. To upload your own map: From the Library, click the \"Add new\" button. Drag and drop or click to upload an AssetBundle which you have built for the version of SVL Simulator you are running. Once the upload has finished and the AssetBundle has been processed, fill in the details about your new map, including Map Name, Description, License, Copyright, and Tags. Click \"Publish\" to publish the map to your personal library, allowing it to be used in your simulation configurations. You can click \"Keep as draft\" to save and publish later. Viewing a map top # Once you have successfully added a map to your Library, you will be able to see the map and its details when you select it. There are several additional actions possible on a map page: Edit: You can edit the map's detail information. Share: You can share your private maps with other users to allow them to use the map in their simulations. Download the associated HD map in various supported formats that were built into the asset. Previewing the HD Map top # Select the eye icon in the HD Maps header to open a dialog with an interactive version of the HD map in the browser. The preview includes the lane lines of the roads and can be zoomed or rotated to inspect different parts of the map. Editing a map top # Editing a map allows you to update the details about your new map, including Map Name, Description, License, Copyright, and Tags. If the map is your own, then you can also update the asset bundle of the map and also upload new asset bundles for supporting various 'SVL Simulator' versions. There are several additional actions possible when editing: Add to Store: You can make the map public so that other users may add it to their library. Unpublish: You can \"unpublish\" the map and return it to the \"draft\" status, disabling the ability for you to use it in a valid simulation. Delete Map: Delete the map entirely from your account and library. Once you do this, you will need to go back to where you obtained the map (or re-upload it) in order to add it to your library again. NOTE: Any changes to the map such as Removing from the Store, or Unpublishing it, or Deleting will not be retroactively applied. That is the changes will have no impact on the users who already have the map in their library. For example, if a map is already in a user's library, then that user can continue to use the map in the simulations even if the map has been removed from the store or has been deleted or unpublished by the owner. Creating a map top # Please see the Assets document for more information on creating and building custom maps and vehicles for SVL Simulator in Developer Mode. You can read the Map Annotation page for instructions on annotating, importing, and exporting HD maps. Vehicles top # The Vehicle tab under Library contains the list of vehicles available for you to use in your simulations. Adding a vehicle top # You can add vehicles to your Library from either the Store page or by clicking the \"Add new\" button at the top right. To upload your own vehicle: From your Library, click the \"Add new\" button. Drag and drop or click to upload an AssetBundle which you have built for the version of 'SVL Simulator' you are running. You can see how to create and build your own vehicle here . Once your vehicle has successfully uploaded, you can fill in its information. Click \"Next\" to configure the sensors for this vehicle. Each vehicle can have multiple configurations and must have at least one configuration to use the vehicle in a simulation. Click \"Create New Configuration\" to create a new configuration or \"Next\" to continue without a configuration. You can create a configuration later by editing the vehicle . A tutorial video on creating a sensor configuration for a vehicle can be found here . To create the configuration, enter a name for the configuration, select a Bridge , and click \"Create\". Once you have created a configuration, you may add sensors using the \"+\" icon next to the \"Root (Base Link)\". More information about sensor configuration is available in Editing Vehicles Click \"Publish\" to publish the vehicle to your personal Library, allowing it to be used in your simulation configurations. You can click \"Keep as draft\" to save and publish later. Viewing a Vehicle top # Once you have successfully added a vehicle to your Library page, you will be able to see the vehicle and its details when you select it. On a vehicle page, there are several additional actions you can take: Share: You can share your private vehicles with other users to allow them to use the vehicle in their simulations. Edit: Change the vehicle's detailed information including Name, Description, License, Copyright, and Tags. Sensor Configurations : Add, Modify, Copy, or Delete a sensor configuration for the vehicle. Editing a Vehicle top # Editing a vehicle allows you to update the details about your new vehicle, including Name, Description, License, Copyright, and Tags. If the vehicle is your own, then you can also update the asset bundle of the vehicle and also upload new asset bundles for supporting various 'SVL Simulator' versions. There are several additional actions possible when editing: Add to Store: You can make the vehicle public so that other users may add it to their library. Unpublish: You can \"unpublish\" the vehicle and return it to the \"draft\" status, disabling the ability for you to use it in a valid simulation. Delete Vehicle: Delete the vehicle entirely from your account and library. Once you do this, you will need to go back to where you obtained the vehicle (or re-upload it) in order to add it to your library again. NOTE: Any changes to the vehicle such as Removing from the Store, or Unpublishing it, or Deleting will not be retroactively applied. That is the changes will have no impact on the users who already have the vehicle in their library. For example, if a vehicle is already in a user's library, then that user can continue to use the vehicle in the simulations even if the vehicle has been removed from the store or has been deleted or unpublished by the owner. Managing Sensor Configurations top # Each vehicle can have multiple sensor configurations. When you use a vehicle in a simulation, you will have an opportunity to select the sensor configuration to use in that simulation. The currently available sensor configurations are listed on the vehicle page. If a vehicle was added from the Store or if a vehicle is shared with you, you will also see the configurations created by the owner of the vehicle. To create or modify a new sensor configuration when viewing a vehicle , click the icon to the right of \"Sensor Configurations\". When editing a vehicle , click the \"Sensor Configurations\" tab at the top of the view. A tutorial video on creating a sensor configuration for a vehicle can be found here . The Sensor Configurations tab provides several actions: Create: You can create a new configuration using the \"Create New Configuration\" button. View / Edit: You can view any configuration or edit a configuration you own by selecting the configuration or the Edit icon for a configuration. Copy: You can create a copy of an existing configuration using the Copy icon. If you want to modify a configuration from a different users, you must first create your own copy of that configuration and then modify the copy. Delete: You can delete any configuration that you own using the Delete icon which will permanently remove it entirely from the vehicle. You will be prompted to confirm the deletion before continuing. Modifying a Sensor Configuration top # The sensor configuration editor allows you to visually update the position and properties of a sensor used in this configuration. Each sensor has a parent which is either the \"Root (Base Link)\" or is another sensor. To add new sensors, click the \"+\" icon next to the parent and select the sensors you wish to add. Each sensor can be customized by selecting the senor from the tree view and modifying the values in the form that loads below. The \"Sensor Type\" field is read-only and represents the plugin name. The \"Name\" field must be unique for this sensor configuration and helps to identify the sensor in the tree view and in the simulation. The transformation fields, \"X\", \"Y\", \"Z\", \"Pitch\", \"Yaw\", and \"Roll\" configure the position of the sensor relative to its parent position and orientation. Many sensors have additional configuration properties that can be customized. These fields are displayed below the transformation fields. Note: You do not need to \"Save\" after editing a sensor but must click \"Save\" to commit all of the changes you have made. Interacting with the Visual Editor top # The visual editor is integrated with the parameter editor and allows you to visually place sensors around the vehicle. It includes several tools for manipulating a sensor: Selection tool selects the active sensor. Move tool moves the sensor along the X, Y, or Z axis. Rotate tool rotates the sensor around the X, Y, or Z axis. Vehicle Transparency tool toggles the transparency of the vehicle model to help when placing sensors in or below the vehicle. Orientation tool illustrates the current orientation of the vehicle. Clicking one of the axis cones will snap the camera to look down that axis. The editor also includes buttons to Undo and Redo any changes you've made to the sensor configuration. These actions will undo any sensor configuration changes include parameter values are not restricted to changes made in the visual editor. Migrating Legacy Sensor Configurations top # If you have an older legacy sensor configuration that uses the type instead of the plugin field to identify the plugins, you can select the \"<>\" icon in the Preview header to paste this configuration into the JSON Editor. Clicking \"Save\" will validate and save the configuration and return an updated version compatible with the current schema. NOTE : Most of the sensor type names have changed for the 2021.1 release. You can find the latest type names from the sensors documentation. Working with Sensor Configurations created by other users top # When a vehicle is shared with you or when a vehicle is added to the library, you also gain access to the sensor configurations created by the owner of the vehicle. Sometimes, these sensor configurations may contain plugins that you do not have in your library. When that occurs, you will see warning messages with available actions for each type of problem. If a plugin is available in the store but not in your Library, you can add it by clicking \"Add to Library\" next to the warning. If a plugin is owned by another user but not in the Store, you can request access to it from the owner by clicking \"Request Access\" next to the warning. If a plugin has been deleted by the owner but still exists in the sensor configuration, it must be removed by the owner before you can use this sensor configuration in a simulation. Bridge Types # No bridge : This is the default. This is used when there is no need to connect to an AD Stack. This selection does not require any additional information while setting up the Simulation. ROS : This bridge allows connecting to ROS1 based AV stacks. (like Autoware). The ROS1 Bridge requires an IP address and port number while setting up the Simulation. ROSApollo : Deprecated. ROS2 : This bridge allows connecting to ROS2 based AV stacks. ROS2 Bridge requires an IP address and port number while setting up the Simulation. CyberRT : This bridge allows connections to Apollo 5.0. CyberRT Bridge. CyberRT requires an IP address and port number while setting up the Simulation. Example sensor configuration JSON # You can find example sensor configurations for several Autopilot systems on the following pages: Apollo 5.0 JSON Autoware.AI JSON Autoware.Auto JSON Below is a shortened version of the JSON configuration on the Jaguar2015XE (Autoware) default vehicle. It uses a ROS bridge type. The JSON includes: a GPS sensor in the center of the vehicle that publishes data on the \"/nmea_sentence\" topic a LiDAR sensor 2.312m above the center of the vehicle that publishes data on the \"/points_raw\" topic a Keyboard Control input which allows the keyboard input to control the car a Vehicle Control input which subscribes to the Autoware AD Stack control commands [ { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Keyboard Control\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ] Plugins top # When you log in for the first time, your Library page will contain the default Bridge plugins. You can click: \"All\" to see all plugins that you have added to your Library. \"Owned by me\" lists plugins for which you are the owner (meaning that these were built and uploaded by you). \"Shared by me\" lists plugins for which you are the owner were shared to other users by you. \"Shared with me\" lists plugins that were shared to you by other users. Adding a plugin top # You can add plugins to your Library from either the Store page or by clicking the \"Add new\" button at the top right. To upload your own plugin: From the Library, click the \"Add new\" button. Drag and drop, or click to upload an AssetBundle which you have built for the version of SVL Simulator you are running. Once the upload has finished and the AssetBundle has been processed, fill in the details about your new plugin, including Plugin Name, Description, License, Copyright, and Tags. Click \"Publish\" to publish the plugin to your personal library, allowing it to be used in your simulations. You can click \"Keep as draft\" to save and publish later. Viewing a plugin top # Once you have successfully added a plugin to your Library page, you will be able to see the plugin and its details. There are several additional actions possible on a plugin page: Edit: You can edit the plugin's detail information. Share: You can share your private plugins with other users to allow them to use the plugin in their simulations. Editing a plugin top # Editing a plugin allows you to update the details about your new plugin, including Plugin Name, Description, License, Copyright, and Tags. If the plugin is your own, then you can also update the asset bundle of the plugin and also upload new asset bundles for supporting various 'SVL Simulator' versions. There are several additional actions possible when editing: Add to Store: You can make the plugin public so that other users may add it to their library. Unpublish: You can \"unpublish\" the plugin and return it to \"draft\" status, disabling the ability for you to use it in a valid simulation. Delete Plugin: Delete the plugin entirely from your account and library. Once you do this, you will need to go back to where you obtained the plugin (or re-upload it) in order to add it to your library again. NOTE: Any changes to the plugin such as Removing from the Store, or Unpublishing it, or Deleting will not be retroactively applied. That is the changes will have no impact on the users who already have the plugin in their library. For example, if a plugin is already in a user's library, then that user can continue to use the plugin in sensor configurations and use them in simulations even if the plugin has been removed from the store or has been deleted or unpublished by the owner.","title":"Library"},{"location":"user-interface/web/library/#introduction","text":"Library is the central place for all of the content and data associated with your SVL Simulator account. You can access your maps and vehicles from anywhere once you log in to your account, allowing you to save your content in the cloud, including your own map environments, plugins, vehicle models, and sensor configurations.","title":"Introduction"},{"location":"user-interface/web/library/#maps","text":"When you log in for the first time, your Library page will show some widely used default assets that are automatically made available to your account. You can click: \"All\" to see all maps that you have added to your Library. \"My Content\" lists maps for which you are the owner (meaning they were built and uploaded by you). \"My Shares\" lists maps that are owned by you and were to other users (using the Sharing feature). \"Added from Store\" lists maps from other users that you have added to your library from store. \"Shared with Me\" lists maps that were shared with you by other users.","title":"Maps"},{"location":"user-interface/web/library/#adding-a-map","text":"You can add maps to your Library from either the Store page or by clicking the \"Add new\" button at the top right. To upload your own map: From the Library, click the \"Add new\" button. Drag and drop or click to upload an AssetBundle which you have built for the version of SVL Simulator you are running. Once the upload has finished and the AssetBundle has been processed, fill in the details about your new map, including Map Name, Description, License, Copyright, and Tags. Click \"Publish\" to publish the map to your personal library, allowing it to be used in your simulation configurations. You can click \"Keep as draft\" to save and publish later.","title":"Adding a map"},{"location":"user-interface/web/library/#viewing-a-map","text":"Once you have successfully added a map to your Library, you will be able to see the map and its details when you select it. There are several additional actions possible on a map page: Edit: You can edit the map's detail information. Share: You can share your private maps with other users to allow them to use the map in their simulations. Download the associated HD map in various supported formats that were built into the asset.","title":"Viewing a map"},{"location":"user-interface/web/library/#previewing-the-hd-map","text":"Select the eye icon in the HD Maps header to open a dialog with an interactive version of the HD map in the browser. The preview includes the lane lines of the roads and can be zoomed or rotated to inspect different parts of the map.","title":"Previewing the HD map"},{"location":"user-interface/web/library/#editing-a-map","text":"Editing a map allows you to update the details about your new map, including Map Name, Description, License, Copyright, and Tags. If the map is your own, then you can also update the asset bundle of the map and also upload new asset bundles for supporting various 'SVL Simulator' versions. There are several additional actions possible when editing: Add to Store: You can make the map public so that other users may add it to their library. Unpublish: You can \"unpublish\" the map and return it to the \"draft\" status, disabling the ability for you to use it in a valid simulation. Delete Map: Delete the map entirely from your account and library. Once you do this, you will need to go back to where you obtained the map (or re-upload it) in order to add it to your library again. NOTE: Any changes to the map such as Removing from the Store, or Unpublishing it, or Deleting will not be retroactively applied. That is the changes will have no impact on the users who already have the map in their library. For example, if a map is already in a user's library, then that user can continue to use the map in the simulations even if the map has been removed from the store or has been deleted or unpublished by the owner.","title":"Editing a map"},{"location":"user-interface/web/library/#creating-a-map","text":"Please see the Assets document for more information on creating and building custom maps and vehicles for SVL Simulator in Developer Mode. You can read the Map Annotation page for instructions on annotating, importing, and exporting HD maps.","title":"Creating a map"},{"location":"user-interface/web/library/#vehicles","text":"The Vehicle tab under Library contains the list of vehicles available for you to use in your simulations.","title":"Vehicles"},{"location":"user-interface/web/library/#adding-a-vehicle","text":"You can add vehicles to your Library from either the Store page or by clicking the \"Add new\" button at the top right. To upload your own vehicle: From your Library, click the \"Add new\" button. Drag and drop or click to upload an AssetBundle which you have built for the version of 'SVL Simulator' you are running. You can see how to create and build your own vehicle here . Once your vehicle has successfully uploaded, you can fill in its information. Click \"Next\" to configure the sensors for this vehicle. Each vehicle can have multiple configurations and must have at least one configuration to use the vehicle in a simulation. Click \"Create New Configuration\" to create a new configuration or \"Next\" to continue without a configuration. You can create a configuration later by editing the vehicle . A tutorial video on creating a sensor configuration for a vehicle can be found here . To create the configuration, enter a name for the configuration, select a Bridge , and click \"Create\". Once you have created a configuration, you may add sensors using the \"+\" icon next to the \"Root (Base Link)\". More information about sensor configuration is available in Editing Vehicles Click \"Publish\" to publish the vehicle to your personal Library, allowing it to be used in your simulation configurations. You can click \"Keep as draft\" to save and publish later.","title":"Adding a vehicle"},{"location":"user-interface/web/library/#viewing-a-vehicle","text":"Once you have successfully added a vehicle to your Library page, you will be able to see the vehicle and its details when you select it. On a vehicle page, there are several additional actions you can take: Share: You can share your private vehicles with other users to allow them to use the vehicle in their simulations. Edit: Change the vehicle's detailed information including Name, Description, License, Copyright, and Tags. Sensor Configurations : Add, Modify, Copy, or Delete a sensor configuration for the vehicle.","title":"Viewing a Vehicle"},{"location":"user-interface/web/library/#editing-a-vehicle","text":"Editing a vehicle allows you to update the details about your new vehicle, including Name, Description, License, Copyright, and Tags. If the vehicle is your own, then you can also update the asset bundle of the vehicle and also upload new asset bundles for supporting various 'SVL Simulator' versions. There are several additional actions possible when editing: Add to Store: You can make the vehicle public so that other users may add it to their library. Unpublish: You can \"unpublish\" the vehicle and return it to the \"draft\" status, disabling the ability for you to use it in a valid simulation. Delete Vehicle: Delete the vehicle entirely from your account and library. Once you do this, you will need to go back to where you obtained the vehicle (or re-upload it) in order to add it to your library again. NOTE: Any changes to the vehicle such as Removing from the Store, or Unpublishing it, or Deleting will not be retroactively applied. That is the changes will have no impact on the users who already have the vehicle in their library. For example, if a vehicle is already in a user's library, then that user can continue to use the vehicle in the simulations even if the vehicle has been removed from the store or has been deleted or unpublished by the owner.","title":"Editing a Vehicle"},{"location":"user-interface/web/library/#managing-sensor-configurations","text":"Each vehicle can have multiple sensor configurations. When you use a vehicle in a simulation, you will have an opportunity to select the sensor configuration to use in that simulation. The currently available sensor configurations are listed on the vehicle page. If a vehicle was added from the Store or if a vehicle is shared with you, you will also see the configurations created by the owner of the vehicle. To create or modify a new sensor configuration when viewing a vehicle , click the icon to the right of \"Sensor Configurations\". When editing a vehicle , click the \"Sensor Configurations\" tab at the top of the view. A tutorial video on creating a sensor configuration for a vehicle can be found here . The Sensor Configurations tab provides several actions: Create: You can create a new configuration using the \"Create New Configuration\" button. View / Edit: You can view any configuration or edit a configuration you own by selecting the configuration or the Edit icon for a configuration. Copy: You can create a copy of an existing configuration using the Copy icon. If you want to modify a configuration from a different users, you must first create your own copy of that configuration and then modify the copy. Delete: You can delete any configuration that you own using the Delete icon which will permanently remove it entirely from the vehicle. You will be prompted to confirm the deletion before continuing.","title":"Managing Sensor Configurations"},{"location":"user-interface/web/library/#modifying-a-sensor-configuration","text":"The sensor configuration editor allows you to visually update the position and properties of a sensor used in this configuration. Each sensor has a parent which is either the \"Root (Base Link)\" or is another sensor. To add new sensors, click the \"+\" icon next to the parent and select the sensors you wish to add. Each sensor can be customized by selecting the senor from the tree view and modifying the values in the form that loads below. The \"Sensor Type\" field is read-only and represents the plugin name. The \"Name\" field must be unique for this sensor configuration and helps to identify the sensor in the tree view and in the simulation. The transformation fields, \"X\", \"Y\", \"Z\", \"Pitch\", \"Yaw\", and \"Roll\" configure the position of the sensor relative to its parent position and orientation. Many sensors have additional configuration properties that can be customized. These fields are displayed below the transformation fields. Note: You do not need to \"Save\" after editing a sensor but must click \"Save\" to commit all of the changes you have made.","title":"Modifying a Sensor Configuration"},{"location":"user-interface/web/library/#interacting-with-the-visual-editor","text":"The visual editor is integrated with the parameter editor and allows you to visually place sensors around the vehicle. It includes several tools for manipulating a sensor: Selection tool selects the active sensor. Move tool moves the sensor along the X, Y, or Z axis. Rotate tool rotates the sensor around the X, Y, or Z axis. Vehicle Transparency tool toggles the transparency of the vehicle model to help when placing sensors in or below the vehicle. Orientation tool illustrates the current orientation of the vehicle. Clicking one of the axis cones will snap the camera to look down that axis. The editor also includes buttons to Undo and Redo any changes you've made to the sensor configuration. These actions will undo any sensor configuration changes include parameter values are not restricted to changes made in the visual editor.","title":"Interacting with the Visual Editor"},{"location":"user-interface/web/library/#migrating-legacy-sensor-configurations","text":"If you have an older legacy sensor configuration that uses the type instead of the plugin field to identify the plugins, you can select the \"<>\" icon in the Preview header to paste this configuration into the JSON Editor. Clicking \"Save\" will validate and save the configuration and return an updated version compatible with the current schema. NOTE : Most of the sensor type names have changed for the 2021.1 release. You can find the latest type names from the sensors documentation.","title":"Migrating Legacy Sensor Configurations"},{"location":"user-interface/web/library/#working-with-sensor-configurations-created-by-other-users","text":"When a vehicle is shared with you or when a vehicle is added to the library, you also gain access to the sensor configurations created by the owner of the vehicle. Sometimes, these sensor configurations may contain plugins that you do not have in your library. When that occurs, you will see warning messages with available actions for each type of problem. If a plugin is available in the store but not in your Library, you can add it by clicking \"Add to Library\" next to the warning. If a plugin is owned by another user but not in the Store, you can request access to it from the owner by clicking \"Request Access\" next to the warning. If a plugin has been deleted by the owner but still exists in the sensor configuration, it must be removed by the owner before you can use this sensor configuration in a simulation.","title":"Working with Sensor Configurations created by other users"},{"location":"user-interface/web/library/#bridge-types","text":"No bridge : This is the default. This is used when there is no need to connect to an AD Stack. This selection does not require any additional information while setting up the Simulation. ROS : This bridge allows connecting to ROS1 based AV stacks. (like Autoware). The ROS1 Bridge requires an IP address and port number while setting up the Simulation. ROSApollo : Deprecated. ROS2 : This bridge allows connecting to ROS2 based AV stacks. ROS2 Bridge requires an IP address and port number while setting up the Simulation. CyberRT : This bridge allows connections to Apollo 5.0. CyberRT Bridge. CyberRT requires an IP address and port number while setting up the Simulation.","title":"Bridge Types"},{"location":"user-interface/web/library/#example-json","text":"You can find example sensor configurations for several Autopilot systems on the following pages: Apollo 5.0 JSON Autoware.AI JSON Autoware.Auto JSON Below is a shortened version of the JSON configuration on the Jaguar2015XE (Autoware) default vehicle. It uses a ROS bridge type. The JSON includes: a GPS sensor in the center of the vehicle that publishes data on the \"/nmea_sentence\" topic a LiDAR sensor 2.312m above the center of the vehicle that publishes data on the \"/points_raw\" topic a Keyboard Control input which allows the keyboard input to control the car a Vehicle Control input which subscribes to the Autoware AD Stack control commands [ { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Keyboard Control\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Example sensor configuration JSON"},{"location":"user-interface/web/library/#plugins","text":"When you log in for the first time, your Library page will contain the default Bridge plugins. You can click: \"All\" to see all plugins that you have added to your Library. \"Owned by me\" lists plugins for which you are the owner (meaning that these were built and uploaded by you). \"Shared by me\" lists plugins for which you are the owner were shared to other users by you. \"Shared with me\" lists plugins that were shared to you by other users.","title":"Plugins"},{"location":"user-interface/web/library/#adding-a-plugin","text":"You can add plugins to your Library from either the Store page or by clicking the \"Add new\" button at the top right. To upload your own plugin: From the Library, click the \"Add new\" button. Drag and drop, or click to upload an AssetBundle which you have built for the version of SVL Simulator you are running. Once the upload has finished and the AssetBundle has been processed, fill in the details about your new plugin, including Plugin Name, Description, License, Copyright, and Tags. Click \"Publish\" to publish the plugin to your personal library, allowing it to be used in your simulations. You can click \"Keep as draft\" to save and publish later.","title":"Adding a plugin"},{"location":"user-interface/web/library/#viewing-a-plugin","text":"Once you have successfully added a plugin to your Library page, you will be able to see the plugin and its details. There are several additional actions possible on a plugin page: Edit: You can edit the plugin's detail information. Share: You can share your private plugins with other users to allow them to use the plugin in their simulations.","title":"Viewing a plugin"},{"location":"user-interface/web/library/#editing-a-plugin","text":"Editing a plugin allows you to update the details about your new plugin, including Plugin Name, Description, License, Copyright, and Tags. If the plugin is your own, then you can also update the asset bundle of the plugin and also upload new asset bundles for supporting various 'SVL Simulator' versions. There are several additional actions possible when editing: Add to Store: You can make the plugin public so that other users may add it to their library. Unpublish: You can \"unpublish\" the plugin and return it to \"draft\" status, disabling the ability for you to use it in a valid simulation. Delete Plugin: Delete the plugin entirely from your account and library. Once you do this, you will need to go back to where you obtained the plugin (or re-upload it) in order to add it to your library again. NOTE: Any changes to the plugin such as Removing from the Store, or Unpublishing it, or Deleting will not be retroactively applied. That is the changes will have no impact on the users who already have the plugin in their library. For example, if a plugin is already in a user's library, then that user can continue to use the plugin in sensor configurations and use them in simulations even if the plugin has been removed from the store or has been deleted or unpublished by the owner.","title":"Editing a plugin"},{"location":"user-interface/web/simulations/","text":"Simulations # The Simulations page contains the list of test case simulations you can execute. It serves as the central place from which you can create new simulations, as well as start and stop your simulations. A tutorial video on simulations creation and management can be found here . Creating a new simulation top # Please refer to the Running Simulator document for detailed instructions on creating a new simulation. The following pages denote the pieces of a new simulation configuration. General top # Simulation Name - Enter a name for your simulation Description (optional) - Enter a description for your simulation Tags (optional) - Specify tags for your simulation, which can be used to search through or filter simulations later Select Cluster - Choose the simulation cluster which will actually run the simulator executable and execute the simulation. If you select a cloud cluster, the following three toggles can not be changed. Create test report - Enable to generate a test report after execution. Headless mode - Enable to run the simulator without rendering visually in the executable Interactive mode - Enable to run the simulator interactively, with the ability to visualize sensors, pause, etc. within the executable Test case top # Select a runtime template: Random Traffic: A random simulation where starting conditions are defined here in the user interface Visual scenario editor: Run a scenario created by the Visual Scenario Editor Python API: Directly input the Python API test case script to automatically execute from the user interface. If you have selected a local cluster, you will still need to bring up your Autopilot separately. API Only: The simulator is simply started in API Only mode, and you are expected to use a runner to execute test cases (Python API, Visual Scenario Editor) Curently, only the Python API runtime template is available if you have selected a cloud cluster. Runtime Template: Random Traffic # Map: Select the map for the simulation Vehicle: Select one or more ego vehicles Simulation Date: Set the date within simulation Time of Day: Set the time of day Rain: [0-1] set how much rain should fall Fog: [0-1] set how thick fog should be Wetness: [0-1] set how wet the road surfaces should be Cloudiness: [0-1] set how much cloud cover there should be Random Traffic: Enable for non-ego traffic vehicles Random Pedestrians: Enable for pedestrian agents Random Bicyclists: Enable for bicyclists Use Pre-defined Seed: Enable then input an integer to deterministically produce a random simulation that can be reproduced Find the example guide here . Runtime Template: Visual scenario editor # Scenario: Upload the JSON file created by the Visual Scenario Editor. Runtime Template: Python API # Python Script: Input the Python API test case script that you would like to automatically execute when starting the simulation from the web user interface. Note that this is only supported for clusters running the Linux version of SVL Simulator. Map: Select the map for the simulation. This must match the name of map passed to sim.load() . Vehicle: Select one or more ego vehicles. These must match any hardcoded vehicle names passed to sim.add_agent() . NOTE: If you have selected a cloud cluster, only a single vehicle can be selected. Please see here for additional information on Python API mode. Runtime Template: API Only # For API Only simulations , you do not need to set any starting parameters, as the runner and test case you are expected to execute should determine the starting configuration. This could be from Visual Scenario Editor, or a Python API test case script. See the below documents for additional information on running the various types of test cases using the API Only runtime template: Python API Scripts Autopilot top # Local cluster top # Autopilot: A list of compatible Autopilot systems are highlighted to choose from, based on the ego vehicle and the cluster type selected on the Test case pane. Bridge IP: the IP address and port number that your bridge is running at, e.g. localhost:9090 or 127.0.0.1:9000 . Publish top # Finally, click \"Publish\" to publish the new simulation to your library and to be able to execute it. You can now run your simulation. Make sure the cluster you specified for the simulation is online, then click \"Run Simulation\".","title":"Simulations"},{"location":"user-interface/web/simulations/#creating-a-new-simulation","text":"Please refer to the Running Simulator document for detailed instructions on creating a new simulation. The following pages denote the pieces of a new simulation configuration.","title":"Creating a new simulation"},{"location":"user-interface/web/simulations/#general","text":"Simulation Name - Enter a name for your simulation Description (optional) - Enter a description for your simulation Tags (optional) - Specify tags for your simulation, which can be used to search through or filter simulations later Select Cluster - Choose the simulation cluster which will actually run the simulator executable and execute the simulation. If you select a cloud cluster, the following three toggles can not be changed. Create test report - Enable to generate a test report after execution. Headless mode - Enable to run the simulator without rendering visually in the executable Interactive mode - Enable to run the simulator interactively, with the ability to visualize sensors, pause, etc. within the executable","title":"General"},{"location":"user-interface/web/simulations/#test-case","text":"Select a runtime template: Random Traffic: A random simulation where starting conditions are defined here in the user interface Visual scenario editor: Run a scenario created by the Visual Scenario Editor Python API: Directly input the Python API test case script to automatically execute from the user interface. If you have selected a local cluster, you will still need to bring up your Autopilot separately. API Only: The simulator is simply started in API Only mode, and you are expected to use a runner to execute test cases (Python API, Visual Scenario Editor) Curently, only the Python API runtime template is available if you have selected a cloud cluster.","title":"Test case"},{"location":"user-interface/web/simulations/#random-traffic","text":"Map: Select the map for the simulation Vehicle: Select one or more ego vehicles Simulation Date: Set the date within simulation Time of Day: Set the time of day Rain: [0-1] set how much rain should fall Fog: [0-1] set how thick fog should be Wetness: [0-1] set how wet the road surfaces should be Cloudiness: [0-1] set how much cloud cover there should be Random Traffic: Enable for non-ego traffic vehicles Random Pedestrians: Enable for pedestrian agents Random Bicyclists: Enable for bicyclists Use Pre-defined Seed: Enable then input an integer to deterministically produce a random simulation that can be reproduced Find the example guide here .","title":"Runtime Template: Random Traffic"},{"location":"user-interface/web/simulations/#vse","text":"Scenario: Upload the JSON file created by the Visual Scenario Editor.","title":"Runtime Template: Visual scenario editor"},{"location":"user-interface/web/simulations/#python-api","text":"Python Script: Input the Python API test case script that you would like to automatically execute when starting the simulation from the web user interface. Note that this is only supported for clusters running the Linux version of SVL Simulator. Map: Select the map for the simulation. This must match the name of map passed to sim.load() . Vehicle: Select one or more ego vehicles. These must match any hardcoded vehicle names passed to sim.add_agent() . NOTE: If you have selected a cloud cluster, only a single vehicle can be selected. Please see here for additional information on Python API mode.","title":"Runtime Template: Python API"},{"location":"user-interface/web/simulations/#api-only","text":"For API Only simulations , you do not need to set any starting parameters, as the runner and test case you are expected to execute should determine the starting configuration. This could be from Visual Scenario Editor, or a Python API test case script. See the below documents for additional information on running the various types of test cases using the API Only runtime template: Python API Scripts","title":"Runtime Template: API Only"},{"location":"user-interface/web/simulations/#autopilot","text":"","title":"Autopilot"},{"location":"user-interface/web/simulations/#local-cluster","text":"Autopilot: A list of compatible Autopilot systems are highlighted to choose from, based on the ego vehicle and the cluster type selected on the Test case pane. Bridge IP: the IP address and port number that your bridge is running at, e.g. localhost:9090 or 127.0.0.1:9000 .","title":"Local cluster"},{"location":"user-interface/web/simulations/#publish","text":"Finally, click \"Publish\" to publish the new simulation to your library and to be able to execute it. You can now run your simulation. Make sure the cluster you specified for the simulation is online, then click \"Run Simulation\".","title":"Publish"},{"location":"user-interface/web/store/","text":"Store # This document describes the Store page in the SVL Simulator user interface. The online user interface of the SVL Simulator enables you to create, configure, and execute simulation test cases from within your logged in SVL Simulator account in the browser. This includes the ability to browse through, and choose from, a default set of maps, vehicles (with sensor configurations), and plugins. These can be viewed in the Store page. A tutorial video on uploading an asset and adding it to the Store can be found here . For a complete walkthrough from installation to creating and running your first simulation, see the Running SVL Simulator document. Introduction top # When you first log in to your SVL Simulator account, you will see the Store page which includes views for Maps, Vehicles, and Plugins. Each section contains the full list of available selections that you can add to your own account, which can then be used in your simulations. You can always navigate back to this page by clicking \"Store\" in the navigation menu on the left. The Store section contains all maps, vehicles, and plugins that can currently be used in the 'SVL Simulator'. These assets have been published to the Store by their owners to be used by anybody, in accordance to the License, by adding these assets to their library and then including them in their simulations. For your own custom maps, vehicles, plugins, or simulation configurations, you can add or upload them directly to your account from the \"Library\" page . Each type of asset (Map, Vehicle, and Plugin) can be browsed or searched using the tabs at the top of the view: All assets include those assets that can be used by any user in their simulations in accordance with the License for that asset. Trending assets include the most popular assets in the store and are sorted by their popularity. Shared with Me includes assets that have been shared with you by other users. If you have previously removed a shared asset from your library, you can add it again from this list.","title":"Store"},{"location":"user-interface/web/store/#introduction","text":"When you first log in to your SVL Simulator account, you will see the Store page which includes views for Maps, Vehicles, and Plugins. Each section contains the full list of available selections that you can add to your own account, which can then be used in your simulations. You can always navigate back to this page by clicking \"Store\" in the navigation menu on the left. The Store section contains all maps, vehicles, and plugins that can currently be used in the 'SVL Simulator'. These assets have been published to the Store by their owners to be used by anybody, in accordance to the License, by adding these assets to their library and then including them in their simulations. For your own custom maps, vehicles, plugins, or simulation configurations, you can add or upload them directly to your account from the \"Library\" page . Each type of asset (Map, Vehicle, and Plugin) can be browsed or searched using the tabs at the top of the view: All assets include those assets that can be used by any user in their simulations in accordance with the License for that asset. Trending assets include the most popular assets in the store and are sorted by their popularity. Shared with Me includes assets that have been shared with you by other users. If you have previously removed a shared asset from your library, you can add it again from this list.","title":"Introduction"},{"location":"user-interface/web/test-results-visualization/","text":"Visualizing Test Results top # The visualization tab allows you to playback a simulation and see the current state of each sensor over time. The visualization tab is only available in test reports of completed cloud simulations . For local simulations, you can use an interactive visualizer tool on the autonomous driving system side such as RViz for ROS or cyber_visualizer for Apollo. To navigate to the visualizer from Test Results, Select View for a test result. If there are multiple vehicles, select a vehicle. Select the Visualization tab. Note: Only Apollo-based recordings are currently supported by cloud simulations and the visualizer. The visualization section breaks down into the following features: Viewer Sensor Menu Playback Control Widgets Callbacks Viewer top # The primary feature of the Visualization Tab is the Viewer which creates a rendering of available sensor data to aid inspecting the results of a simulation. LiDAR is rendered as dots ranging from white to red illustrating the altitude relative to the ground. Detected obstacles are rendered enclosed in a blue box with labels above them. Trajectories are rendered as a wide line at ground level indicating the planned route for the vehicle. Each sensor can be enabled or disabled using the Sensor Menu to aid in focusing on specific results. Full Screen Mode top # The Viewer can switch to full screen mode using the full screen button to gain more space to view the visualization and widgets. Pressing the Escape key or clicking the full screen button a second time will exit full screen. Camera Angles top # Perspective - The camera is positioned above and behind the vehicle. Pan, tilt, zoom, and rotate are all available. Driver - The camera is fixed at the front of the vehicle. Only rotate is available. Top down - The camera is fixed above the vehicle looking down. Pan, zoom, and rotate are available. Pan / Tilt / Zoom / Rotate top # The camera position can be further refined by panning, tilting, zooming, or rotating if supported by the current camera angle. To pan or tilt , click and drag within the viewer. To zoom , hold Control (Command on Mac) and scroll with the scroll wheel within the viewer. To rotate , hold Control (Command on Mac), click, and drag within the viewer. To recenter the viewer after panning or rotating, click the recenter button. Note that zoom and tilt values are maintained when re-centering. Sensor Menu top # The Sensor menu contains a list of all available sensors for the selected vehicle in this simulation. They are arranged in a tree based on the full name of the sensor from the simulation. Branches of the tree can be collapsed or expanded using the triangle icon. If the sensor can be visualized in the viewer, selecting the sensor will toggle it on and off in the viewer. When the sensor is visible, the eye icon will be green. To view the details of the sensor at the current time, click the message icon to display the Message Widget. When the widget is visible, the message icon will be green. If the sensor cannot be visualized in the view, selecting the sensor will toggle a widget for that sensor. When the widget is visible, the widget icon will be green. Hiding the sensor menu top # The Sensor menu can be hidden using the close button to reveal more of the viewer. When hidden, the menu is collapsed to a menu icon. To expand the menu, click the menu icon. Playback Control top # The playback control manages the playback of the simulation results. The control consists of the following components: A slider component which can be selected or dragged to move to a specific time of the simulation. Tick labels above the slider which indicate the time relative to the start of the simulation. The play/pause button which toggles automatic playback of the simulation at the current time. The simulation time which displays the current time according to the simulation's time. The lookahead slider allows selection of the amount of time, in seconds, to display lookahead predictions if supported by the simulation. Widgets top # Each sensor can be further visualized with floating widgets displayed on top of the viewer . By default, all widgets are hidden but may be shown by selecting the eye icon from the sensor menu . Once displayed, each widget may be moved, resized, and closed. To move a widget , click and drag anywhere where the cursor changes to the move icon. To resize a widget , click and drag from the lower right corner of the widget. To close a widget , click the close button in the upper right corner of the widget. To view a different sensor in an open widget, click the sensor name in the title bar which will open a dropdown menu. This menu will only include sensors that can be displayed in the current widget. Typing part of the desired sensor's name will filter the dropdown to only those that match the entered text. Camera Widget top # The camera widget displays the image captured by a camera sensor at the current playback time. Meter Widget top # The meter widget includes a gauge component that displays a numeric value within a predefined range. The range and unit of the value are pre-configured by the simulator. This widget is used for data streams like the current velocity or acceleration. Note: The meter widget does not support resizing. Message Widget top # The message widget displays the data for the selected sensor at the current playback time. The data is presented as a tree which can be expanded or collapsed using the triangle icon. The entire tree can be expanded or collapsed using the expand button in the title bar. The data, in JSON format, may also be copied to the clipboard using the clipboard icon in the title bar. Note: Because the content of the widget is selectable, you must click and drag the border around the message in order to move the widget. Callbacks top # Callback events are categorized as one-time events during a test case that either affect the ego vehicle, or that the ego vehicle causes in the environment. The following types of events are supported: Collision involving ego (EgoCollision) Speed limit violation (SpeedViolation) Sudden braking (SuddenBrake) Sudden steer (SuddenSteer) Low simulation performance (LowFPS) The occurrence of at least one of these events will cause a simulation to be evaluated as \"Failed\". For each callback event, the returned information is reported, including the simulation time at which the event occurred. Selecting the eye icon will cause the visualization to seek to the time of the event in order to view the state of each sensor at that time. EgoCollision top # A collision involving an ego vehicle. Field Description Units OtherType Type of agent with which ego collided NPC|Pedestrian|Obstacle EgoVelocity Velocity vector of ego at time at collision time x, y, z in m/s OtherVelocity Velocity vector of agent/object at collision time km/hr EgoCollisionTotal Number of ego collisions in the event SpeedViolation top # A speed limit violation by an ego vehicle, based on the lane speed limit in the map annotations. Metric Description Units Duration Duration of speed limit violation event time Max speed Maximum speed of ego vehicle during event. m/s SpeedLimit The ego lane's annotated speed limit of the HD map m/s EgoStuck top # An ego stuck event by the ego vehicle, in which the ego travels a threshold distance and then stops moving for a threshold time SuddenBrake top # A sudden braking event by the ego vehicle, in which it exceeds a threshold deceleration value. SuddenSteer top # A sudden steering event by the ego vehicle, in which absolute magnitude of steering angle exceeds a threshold value. LowFPS top # A low simulation performance to indicate that results may not be accurate due to sufficiently low framerate during simulation on the cluster.","title":"Visualizing Test Results [[top]] {: #how-to-visualize-test-results data-toc-label='Visualizing Test Results'}"},{"location":"user-interface/web/test-results-visualization/#viewer","text":"The primary feature of the Visualization Tab is the Viewer which creates a rendering of available sensor data to aid inspecting the results of a simulation. LiDAR is rendered as dots ranging from white to red illustrating the altitude relative to the ground. Detected obstacles are rendered enclosed in a blue box with labels above them. Trajectories are rendered as a wide line at ground level indicating the planned route for the vehicle. Each sensor can be enabled or disabled using the Sensor Menu to aid in focusing on specific results.","title":"Visualization: Viewer"},{"location":"user-interface/web/test-results-visualization/#viewer-full-screen-mode","text":"The Viewer can switch to full screen mode using the full screen button to gain more space to view the visualization and widgets. Pressing the Escape key or clicking the full screen button a second time will exit full screen.","title":"Full Screen Mode top"},{"location":"user-interface/web/test-results-visualization/#camera-angles-top","text":"Perspective - The camera is positioned above and behind the vehicle. Pan, tilt, zoom, and rotate are all available. Driver - The camera is fixed at the front of the vehicle. Only rotate is available. Top down - The camera is fixed above the vehicle looking down. Pan, zoom, and rotate are available.","title":"Camera Angles top"},{"location":"user-interface/web/test-results-visualization/#pan-tilt-zoom-rotate-top","text":"The camera position can be further refined by panning, tilting, zooming, or rotating if supported by the current camera angle. To pan or tilt , click and drag within the viewer. To zoom , hold Control (Command on Mac) and scroll with the scroll wheel within the viewer. To rotate , hold Control (Command on Mac), click, and drag within the viewer. To recenter the viewer after panning or rotating, click the recenter button. Note that zoom and tilt values are maintained when re-centering.","title":"Pan / Tilt / Zoom / Rotate top"},{"location":"user-interface/web/test-results-visualization/#sensor-menu","text":"The Sensor menu contains a list of all available sensors for the selected vehicle in this simulation. They are arranged in a tree based on the full name of the sensor from the simulation. Branches of the tree can be collapsed or expanded using the triangle icon. If the sensor can be visualized in the viewer, selecting the sensor will toggle it on and off in the viewer. When the sensor is visible, the eye icon will be green. To view the details of the sensor at the current time, click the message icon to display the Message Widget. When the widget is visible, the message icon will be green. If the sensor cannot be visualized in the view, selecting the sensor will toggle a widget for that sensor. When the widget is visible, the widget icon will be green.","title":"Visualization: Sensor Menu"},{"location":"user-interface/web/test-results-visualization/#hiding-the-sensor-menu-top","text":"The Sensor menu can be hidden using the close button to reveal more of the viewer. When hidden, the menu is collapsed to a menu icon. To expand the menu, click the menu icon.","title":"Hiding the sensor menu top"},{"location":"user-interface/web/test-results-visualization/#playback-control","text":"The playback control manages the playback of the simulation results. The control consists of the following components: A slider component which can be selected or dragged to move to a specific time of the simulation. Tick labels above the slider which indicate the time relative to the start of the simulation. The play/pause button which toggles automatic playback of the simulation at the current time. The simulation time which displays the current time according to the simulation's time. The lookahead slider allows selection of the amount of time, in seconds, to display lookahead predictions if supported by the simulation.","title":"Visualization: Playback Control"},{"location":"user-interface/web/test-results-visualization/#widgets","text":"Each sensor can be further visualized with floating widgets displayed on top of the viewer . By default, all widgets are hidden but may be shown by selecting the eye icon from the sensor menu . Once displayed, each widget may be moved, resized, and closed. To move a widget , click and drag anywhere where the cursor changes to the move icon. To resize a widget , click and drag from the lower right corner of the widget. To close a widget , click the close button in the upper right corner of the widget. To view a different sensor in an open widget, click the sensor name in the title bar which will open a dropdown menu. This menu will only include sensors that can be displayed in the current widget. Typing part of the desired sensor's name will filter the dropdown to only those that match the entered text.","title":"Visualization: Widgets"},{"location":"user-interface/web/test-results-visualization/#camera-widget-top","text":"The camera widget displays the image captured by a camera sensor at the current playback time.","title":"Camera Widget top"},{"location":"user-interface/web/test-results-visualization/#meter-widget-top","text":"The meter widget includes a gauge component that displays a numeric value within a predefined range. The range and unit of the value are pre-configured by the simulator. This widget is used for data streams like the current velocity or acceleration. Note: The meter widget does not support resizing.","title":"Meter Widget top"},{"location":"user-interface/web/test-results-visualization/#message-widget-top","text":"The message widget displays the data for the selected sensor at the current playback time. The data is presented as a tree which can be expanded or collapsed using the triangle icon. The entire tree can be expanded or collapsed using the expand button in the title bar. The data, in JSON format, may also be copied to the clipboard using the clipboard icon in the title bar. Note: Because the content of the widget is selectable, you must click and drag the border around the message in order to move the widget.","title":"Message Widget top"},{"location":"user-interface/web/test-results-visualization/#callbacks","text":"Callback events are categorized as one-time events during a test case that either affect the ego vehicle, or that the ego vehicle causes in the environment. The following types of events are supported: Collision involving ego (EgoCollision) Speed limit violation (SpeedViolation) Sudden braking (SuddenBrake) Sudden steer (SuddenSteer) Low simulation performance (LowFPS) The occurrence of at least one of these events will cause a simulation to be evaluated as \"Failed\". For each callback event, the returned information is reported, including the simulation time at which the event occurred. Selecting the eye icon will cause the visualization to seek to the time of the event in order to view the state of each sensor at that time.","title":"Visualization: Callbacks"},{"location":"user-interface/web/test-results-visualization/#egocollision","text":"A collision involving an ego vehicle. Field Description Units OtherType Type of agent with which ego collided NPC|Pedestrian|Obstacle EgoVelocity Velocity vector of ego at time at collision time x, y, z in m/s OtherVelocity Velocity vector of agent/object at collision time km/hr EgoCollisionTotal Number of ego collisions in the event","title":"EgoCollision"},{"location":"user-interface/web/test-results-visualization/#speedviolation","text":"A speed limit violation by an ego vehicle, based on the lane speed limit in the map annotations. Metric Description Units Duration Duration of speed limit violation event time Max speed Maximum speed of ego vehicle during event. m/s SpeedLimit The ego lane's annotated speed limit of the HD map m/s","title":"SpeedViolation"},{"location":"user-interface/web/test-results-visualization/#egostuck","text":"An ego stuck event by the ego vehicle, in which the ego travels a threshold distance and then stops moving for a threshold time","title":"EgoStuck"},{"location":"user-interface/web/test-results-visualization/#suddenbrake","text":"A sudden braking event by the ego vehicle, in which it exceeds a threshold deceleration value.","title":"SuddenBrake"},{"location":"user-interface/web/test-results-visualization/#suddensteer","text":"A sudden steering event by the ego vehicle, in which absolute magnitude of steering angle exceeds a threshold value.","title":"SuddenSteer"},{"location":"user-interface/web/test-results-visualization/#lowfps","text":"A low simulation performance to indicate that results may not be accurate due to sufficiently low framerate during simulation on the cluster.","title":"LowFPS"},{"location":"user-interface/web/test-results/","text":"Test Results # The Test Results contains the full collection of test results generated from running test cases. You can view individual test reports of executed test cases. Test reports contain detailed information about the test case, including the success/failure result, important statistics for each ego vehicle, interesting events that happened, and a video of the ego vehicle's drive during the test case. Test Results top # On the left-hand side of the web user interface, clicking \"Test Results\" brings up the Test Results page. This page contains a table of all executed test cases which enabled test report generation. For each test case, you can see when the report was created, the name of the test case for the report, an evaluated test case result, and a button to view the detailed report. In order to create and view a test case report, make sure to enable the \"Create Test Case report\" radio button when creating a simulation . Test Cases must have test report generation enabled when they are executed in order to be shown in Test Results. For video recording, the Video Recording sensor must also be included. Please see the sensor configurations page for the JSON. Each test result will show the evaluated result, with either \"Success\", \"Failed\", or \"In Progress\". You can view each test report for additional details or messages about the reason for the evaluation. Viewing a Test Report top # To view a detailed test report, click the \"View\" button on the right for a given test result. The top bar indicates the Test Report name, the associated simulation, and buttons to expand the starting simulation configuration details, and delete the report. The \"Iterations\" section displays all iterations for this test report. For test cases run with a Python API script, one test case can contain many iterations, each with its own result, statistics, and events. The highlighted box indicates the selected iteration. Simulation evaluation top # The test report indicates the evaluation status, or result, of the executed simulation. The below table specifies the conditions and criteria for a simulation result. In the future, additional criteria as well as user-customizable criteria will be supported. Result Cause/Criteria SUCCESS The test case did not result in FAILED or ERROR FAILED The test case involved at least one failure event callback ERROR The test case resulted in an error and failed to execute properly The \"Start Time\" indicates the cluster's system date and time when the test case was started/stopped. For test cases with a distributed cluster, the master node's system time is used. The \"Simulation time duration\" indicates the total duration of time of the test case in simulation , excluding paused time. Note that this can be different than the \"Real time duration\", which measures how much total time passed in the real world during execution. In the vehicle section, for each ego vehicle in the test case, you can view the vehicle bridge type, bridge IP and port location. If you included the Video Recording sensor in at least one ego vehicle, you can see the file location where the recorded video of the simulation has been stored. The following section, Sensors, exists for every ego vehicle involved in the test case. In test cases involving multiple ego vehicles, you will be able to see sensor statistics and callback event information for each ego vehicle. Sensors: Statistics top # The Callbacks section gives notable information about the ego vehicle during the test case run. Metric Description Units Distance travelled Total distance traveled, as measured by wheel odometry km Average speed The average speed of the ego vehicle km/hr Max speed The maximum speed of the ego vehicle km/hr Min speed The minimum speed of the ego vehicle km/hr Max longitudinal acceleration Maximum instantaneous acceleration along the vehicle's longitudinal axis m/s 2 Max lateral acceleration Maximum instantaneous acceleration along the vehicle's lateral (perpendicular to longitudinal) axis m/s 2 Max longitudinal jerk Maximum instantaneous jerk along the vehicle's longitudinal axis m/s 3 Max lateral jerk Maximum instantaneous jerk along the vehicle's lateral (perpendicular to longitudinal) axis m/s 3 Callbacks top # Callback events are categorized as one-time events during a test case that either affect the ego vehicle, or that the ego vehicle causes in the environment. The following types of events are supported: Collision involving ego (EgoCollision) Speed limit violation (SpeedViolation) Sudden braking (SuddenBrake) Sudden steer (SuddenSteer) Low simulation performance (LowFPS) The occurrence of at least one of these events will cause a simulation to be evaluated as \"Failed\". For each callback event, the returned information is reported, including the simulation time at which the event occurred. Selecting the eye icon will cause the visualization to seek to the time of the event in order to view the state of each sensor at that time. EgoCollision top # A collision involving an ego vehicle. Field Description Units OtherType Type of agent with which ego collided NPC|Pedestrian|Obstacle EgoVelocity Velocity vector of ego at time at collision time x, y, z in m/s OtherVelocity Velocity vector of agent/object at collision time km/hr EgoCollisionTotal Number of ego collisions in the event SpeedViolation top # A speed limit violation by an ego vehicle, based on the lane speed limit in the map annotations. Metric Description Units Duration Duration of speed limit violation event time Max speed Maximum speed of ego vehicle during event. m/s SpeedLimit The ego lane's annotated speed limit of the HD map m/s EgoStuck top # An ego stuck event by the ego vehicle, in which the ego travels a threshold distance and then stops moving for a threshold time SuddenBrake top # A sudden braking event by the ego vehicle, in which it exceeds a threshold deceleration value. SuddenSteer top # A sudden steering event by the ego vehicle, in which absolute magnitude of steering angle exceeds a threshold value. LowFPS top # A low simulation performance to indicate that results may not be accurate due to sufficiently low framerate during simulation on the cluster.","title":"Test Results"},{"location":"user-interface/web/test-results/#test-results","text":"On the left-hand side of the web user interface, clicking \"Test Results\" brings up the Test Results page. This page contains a table of all executed test cases which enabled test report generation. For each test case, you can see when the report was created, the name of the test case for the report, an evaluated test case result, and a button to view the detailed report. In order to create and view a test case report, make sure to enable the \"Create Test Case report\" radio button when creating a simulation . Test Cases must have test report generation enabled when they are executed in order to be shown in Test Results. For video recording, the Video Recording sensor must also be included. Please see the sensor configurations page for the JSON. Each test result will show the evaluated result, with either \"Success\", \"Failed\", or \"In Progress\". You can view each test report for additional details or messages about the reason for the evaluation.","title":"Test Results"},{"location":"user-interface/web/test-results/#viewing-a-test-report","text":"To view a detailed test report, click the \"View\" button on the right for a given test result. The top bar indicates the Test Report name, the associated simulation, and buttons to expand the starting simulation configuration details, and delete the report. The \"Iterations\" section displays all iterations for this test report. For test cases run with a Python API script, one test case can contain many iterations, each with its own result, statistics, and events. The highlighted box indicates the selected iteration.","title":"Viewing a Test Report"},{"location":"user-interface/web/test-results/#simulation-evaluation","text":"The test report indicates the evaluation status, or result, of the executed simulation. The below table specifies the conditions and criteria for a simulation result. In the future, additional criteria as well as user-customizable criteria will be supported. Result Cause/Criteria SUCCESS The test case did not result in FAILED or ERROR FAILED The test case involved at least one failure event callback ERROR The test case resulted in an error and failed to execute properly The \"Start Time\" indicates the cluster's system date and time when the test case was started/stopped. For test cases with a distributed cluster, the master node's system time is used. The \"Simulation time duration\" indicates the total duration of time of the test case in simulation , excluding paused time. Note that this can be different than the \"Real time duration\", which measures how much total time passed in the real world during execution. In the vehicle section, for each ego vehicle in the test case, you can view the vehicle bridge type, bridge IP and port location. If you included the Video Recording sensor in at least one ego vehicle, you can see the file location where the recorded video of the simulation has been stored. The following section, Sensors, exists for every ego vehicle involved in the test case. In test cases involving multiple ego vehicles, you will be able to see sensor statistics and callback event information for each ego vehicle.","title":"Simulation evaluation"},{"location":"user-interface/web/test-results/#sensors-statistics","text":"The Callbacks section gives notable information about the ego vehicle during the test case run. Metric Description Units Distance travelled Total distance traveled, as measured by wheel odometry km Average speed The average speed of the ego vehicle km/hr Max speed The maximum speed of the ego vehicle km/hr Min speed The minimum speed of the ego vehicle km/hr Max longitudinal acceleration Maximum instantaneous acceleration along the vehicle's longitudinal axis m/s 2 Max lateral acceleration Maximum instantaneous acceleration along the vehicle's lateral (perpendicular to longitudinal) axis m/s 2 Max longitudinal jerk Maximum instantaneous jerk along the vehicle's longitudinal axis m/s 3 Max lateral jerk Maximum instantaneous jerk along the vehicle's lateral (perpendicular to longitudinal) axis m/s 3","title":"Sensors: Statistics"},{"location":"user-interface/web/test-results/#callbacks","text":"Callback events are categorized as one-time events during a test case that either affect the ego vehicle, or that the ego vehicle causes in the environment. The following types of events are supported: Collision involving ego (EgoCollision) Speed limit violation (SpeedViolation) Sudden braking (SuddenBrake) Sudden steer (SuddenSteer) Low simulation performance (LowFPS) The occurrence of at least one of these events will cause a simulation to be evaluated as \"Failed\". For each callback event, the returned information is reported, including the simulation time at which the event occurred. Selecting the eye icon will cause the visualization to seek to the time of the event in order to view the state of each sensor at that time.","title":"Visualization: Callbacks"},{"location":"user-interface/web/test-results/#egocollision","text":"A collision involving an ego vehicle. Field Description Units OtherType Type of agent with which ego collided NPC|Pedestrian|Obstacle EgoVelocity Velocity vector of ego at time at collision time x, y, z in m/s OtherVelocity Velocity vector of agent/object at collision time km/hr EgoCollisionTotal Number of ego collisions in the event","title":"EgoCollision"},{"location":"user-interface/web/test-results/#speedviolation","text":"A speed limit violation by an ego vehicle, based on the lane speed limit in the map annotations. Metric Description Units Duration Duration of speed limit violation event time Max speed Maximum speed of ego vehicle during event. m/s SpeedLimit The ego lane's annotated speed limit of the HD map m/s","title":"SpeedViolation"},{"location":"user-interface/web/test-results/#egostuck","text":"An ego stuck event by the ego vehicle, in which the ego travels a threshold distance and then stops moving for a threshold time","title":"EgoStuck"},{"location":"user-interface/web/test-results/#suddenbrake","text":"A sudden braking event by the ego vehicle, in which it exceeds a threshold deceleration value.","title":"SuddenBrake"},{"location":"user-interface/web/test-results/#suddensteer","text":"A sudden steering event by the ego vehicle, in which absolute magnitude of steering angle exceeds a threshold value.","title":"SuddenSteer"},{"location":"user-interface/web/test-results/#lowfps","text":"A low simulation performance to indicate that results may not be accurate due to sufficiently low framerate during simulation on the cluster.","title":"LowFPS"},{"location":"visual-scenario-editor/about-vse/","text":"Visual Scenario Editor # Visual Scenario Editor (VSE) is a tool for visual creating and editing scenarios for the SVL Simulator. Each scenario represents a test case. VSE focuses on quick scenario creation using visual controls while supporting custom scriptable plugins to extend the tools panels. Video: How to create and run VSE scenario top # How To Run top # Visual Scenario Editor is integrated into the SVL Simulator executable. To enter the VSE, the Simulator has to be logged in, linked to the cloud, and cannot be running any simulation. The Visual Editor button is available when the Simulator is properly connected to the cloud. The Visual scenario editor runtime template Simulation can run a scenario created using VSE. Creating A Scenario top # Refer to the Using VSE page to learn how to create a scenario in the VSE.","title":"About VSE"},{"location":"visual-scenario-editor/about-vse/#vse-video","text":"","title":"Video: How to create and run VSE scenario"},{"location":"visual-scenario-editor/about-vse/#how-to-run","text":"Visual Scenario Editor is integrated into the SVL Simulator executable. To enter the VSE, the Simulator has to be logged in, linked to the cloud, and cannot be running any simulation. The Visual Editor button is available when the Simulator is properly connected to the cloud. The Visual scenario editor runtime template Simulation can run a scenario created using VSE.","title":"How To Run"},{"location":"visual-scenario-editor/about-vse/#creating-a-scenario","text":"Refer to the Using VSE page to learn how to create a scenario in the VSE.","title":"Creating A Scenario"},{"location":"visual-scenario-editor/using-vse/","text":"Using the VSE # VSE allows placing agents on the map, changing parameters of those agents, and setting waypoints for their movement. Each waypoint can affect the agent with different trigger effects like waiting for a fixed time at the waypoint. Camera Management top # The animations above present in order: camera movement, zooming in and out, and camera rotation. VSE uses a drag and drop system to move around the map. Press the middle mouse button (scroll wheel) down somewhere on the map and move the mouse while holding the middle button down, release the button to stop moving the map. To zoom in and zoom out, use the mouse wheel scroll. If Free Camera mode is selected in options, the camera can be rotated with the drag and drop while holding the mouse right-click. Additionally, the camera can be moved with the keyboard \"WSAD\" keys. Quick Edit Panel top # Selecting a map element (for example agent) with the left mouse button shows the quick edit panel above the selected element. Remove deletes the selected element from the scenario. Move allows to reposition a selected element with a drag and drop system. Use the right mouse button to cancel the reposition. Rotate allows rotating selected elements with drag and drop system, drag button to the left or right to rotate the selected object. Use the right mouse button to revert the rotation. Resize allows scaling selected elements with drag and drop system, drag button to the left (shrinking), or to the right (enlarging). Use the right mouse button to revert the resizing. Future updates will include a plugins system for custom quick edit buttons. Undo top # VSE includes an undo feature that allows reverting the last changes. Use the \"ctrl\"+\"z\" on your keyboard to revert the most recent operation. Undo manager registers multiple records on the stack, using undo applies the last operation record, and removes it from the stack. This way, the undo can be used multiple times to revert many most recent actions. Confirmation Popup top # Operations like resetting scenarios with unsaved changes require additional confirmation. VSE shows a popup with the operation description that requires user confirmation before it is performed. Log Popup top # VSE displays important information on an additional log panel. This panel shows and hides automatically, it lasts longer the longer message is. If multiple messages occur, they are displayed one by one in the log panel before hiding it. File Selector top # The file selector is a simple built-in UI dialog for selecting the destination file on the hard-drive. It allows to change manually the directory path, go one directory up, enter internal directories and select a file with a required extension. When saving a scenario, it can override an existing file, or it can be saved to a new file with a selected name. VSE Inspector # VSE Inspector provides more options for using the VSE. Refer to the VSE Inspector page to learn about inspector features.","title":"Using VSE"},{"location":"visual-scenario-editor/using-vse/#camera-management","text":"The animations above present in order: camera movement, zooming in and out, and camera rotation. VSE uses a drag and drop system to move around the map. Press the middle mouse button (scroll wheel) down somewhere on the map and move the mouse while holding the middle button down, release the button to stop moving the map. To zoom in and zoom out, use the mouse wheel scroll. If Free Camera mode is selected in options, the camera can be rotated with the drag and drop while holding the mouse right-click. Additionally, the camera can be moved with the keyboard \"WSAD\" keys.","title":"Camera Management"},{"location":"visual-scenario-editor/using-vse/#quick-edit-panel","text":"Selecting a map element (for example agent) with the left mouse button shows the quick edit panel above the selected element. Remove deletes the selected element from the scenario. Move allows to reposition a selected element with a drag and drop system. Use the right mouse button to cancel the reposition. Rotate allows rotating selected elements with drag and drop system, drag button to the left or right to rotate the selected object. Use the right mouse button to revert the rotation. Resize allows scaling selected elements with drag and drop system, drag button to the left (shrinking), or to the right (enlarging). Use the right mouse button to revert the resizing. Future updates will include a plugins system for custom quick edit buttons.","title":"Quick Edit Panel"},{"location":"visual-scenario-editor/using-vse/#undo","text":"VSE includes an undo feature that allows reverting the last changes. Use the \"ctrl\"+\"z\" on your keyboard to revert the most recent operation. Undo manager registers multiple records on the stack, using undo applies the last operation record, and removes it from the stack. This way, the undo can be used multiple times to revert many most recent actions.","title":"Undo"},{"location":"visual-scenario-editor/using-vse/#confirmation-popup","text":"Operations like resetting scenarios with unsaved changes require additional confirmation. VSE shows a popup with the operation description that requires user confirmation before it is performed.","title":"Confirmation Popup"},{"location":"visual-scenario-editor/using-vse/#log-popup","text":"VSE displays important information on an additional log panel. This panel shows and hides automatically, it lasts longer the longer message is. If multiple messages occur, they are displayed one by one in the log panel before hiding it.","title":"Log Popup"},{"location":"visual-scenario-editor/using-vse/#file-selector","text":"The file selector is a simple built-in UI dialog for selecting the destination file on the hard-drive. It allows to change manually the directory path, go one directory up, enter internal directories and select a file with a required extension. When saving a scenario, it can override an existing file, or it can be saved to a new file with a selected name.","title":"File Selector"},{"location":"visual-scenario-editor/using-vse/#vse-inspector","text":"VSE Inspector provides more options for using the VSE. Refer to the VSE Inspector page to learn about inspector features.","title":"VSE Inspector"},{"location":"visual-scenario-editor/vse-editors/","text":"VSE Editors # Visual Scenario Editor provides multiple different editors that allow editing all the Simulator agents (Egos, NPCs, Pedestrians), controllable with all the parameters. VSE provides a system to create and add custom editors either on the map and for extending the Edit Panel of the VSE Inspector . Refer to Custom Parameter Edit Panels to know more about extending the VSE. Usage of some built-in editors is described below. Editing Waypoints Path top # NPC agents with selected NPCWaypointBehaviour , and all the pedestrians use waypoints' paths to describe how agents will move in a simulation. The destination point playback path uses the same waypoints path to set up the egos' movement in the playback mode. Users can set as many waypoints in the path as required, and parametrize each of them. Start editing the waypoints path with the Add Waypoints button, it will add a waypoint to the end of the existing path. Use the right mouse button to cancel adding waypoints, and the left mouse button to add a waypoint in the current position. Multiple waypoints can be added this way until the operation is canceled. An agent is rotated towards the first waypoint. Created waypoints can be edited like other scenario elements, when selected they can be removed, moved or the user can edit their parameters. After selecting a waypoint and clicking Add next , more waypoints can be added right after the selected one. VSE can change the loop parameter, an agent will continue moving from the beginning when the last waypoint is reached when the loop is selected. Waypoints support either a linear movement with a constant speed and a uniformly accelerated movement. An agent will constantly move with the set speed when the acceleration parameter is set to 0 . If acceleration is greater than zero, the speed parameter determines the maximum speed that the agent can reach when acceleration. If the current speed is greater than the set speed , acceleration becomes the deceleration parameter and the agent tries to slow down to the determined speed. If the acceleration value is not big enough, the set speed may not be reached during the movement. The animation above compares movement without acceleration on the left and with acceleration set on the right. VSE supports two different waypoints path: Linear and BezierSpline . An agent with a selected Linear path will drive straight from waypoint to waypoint, when the BezpierSpline path type is selected, Simulator uses the Bezier Spline algorithm to make the movement smooth. The animation above presents paths with the same waypoints, but different path types. The vehicle on the left uses the Linear path and the vehicle on the right uses the BezierSpline path. Editing Destination Point top # While editing an ego agent a destination point can be activated for this agent. Toggle the destination point value in the edit panel while editing an ego agent to activate or deactivate the destination point. The camera will focus on the destination point after pressing the camera button. Destination point allows adding waypoints' path for testing the created scenario in the playback mode. Show Playback Path views the path and allows editing it. Add Playback Waypoint adds another waypoint and the dropdown allows changing the path type. An ego vehicle will follow the path in the playback mode as other agents with waypoints behaviour do. Destination point's playback path will not affect the final simulation, it is used only in the VSE playback mode. Editing Behaviour top # NPCs supports various behaviour scripts that will control the vehicle. VSE provides a dropdown with all available NPC behaviours, selected behaviour is saved in the scenario. Some behaviours can be parameterized, for example, NPCLaneFollowBehaviour got the isLaneChange toggle parameter and the maxSpeed value. Only NPCs with the NPCWaypointBehaviour supports editing the waypoints. Editing Color top # VSE includes the color picker panel that allows selecting any color from the RGB and HSV pallet. NPCs support selecting a custom color of the vehicle. To change the vehicle color click the current color button and select the new color in the opened color picker. The changed color will be applied immediately, but the undo record is registered after closing the color picker panel. Editing Triggers top # Editing a waypoint allows adding different trigger effectors to this point. All the effectors added to one waypoint are executed in parallel. Some effectors require additional settings and parameters. Each effector can use a custom edit panel. Triggers can be copy and pasted. Use the copy icon (button on the left of the Trigger title) to copy the effectors, then use the paste icon (button on the right of the Trigger title) to clone the effectors to another waypoint. To add a new trigger effector, select it from the dropdown list and click the add button on the right side. A new effector will be added and available for addition under the dropdown. The simulator provides the following trigger effectors with the VSE: Time To Collision This effector calculates if an NPC can cause a collision with any ego, calculating current direction, velocity, and acceleration. The NPC will wait to proceed to the next waypoint such that a collision with the ego vehicle would occur. If no possible collision is found NPC does not wait. Wait For Distance Adding this effector makes NPC wait at the waypoint until any ego vehicle is closer than the max distance (in meters) set in this trigger effector. If no ego vehicle will get close enough, the NPC can wait infinitely. Wait Time The agent will wait for the fixed time (in seconds) at the waypoint before continuing movement towards the next waypoint. Waiting Point A waiting point makes the agent wait until any ego vehicle enters the activation zone. Activation zone can be moved like other scenario elements, by dragging the Move button. If it is required, the radius (in meters) of the activation zone can be changed in the Edit panel of the waypoint or by dragging the Resize button to the left (shrinking) or right (enlarging). Control Trigger This trigger effector will apply the edited policy to all marked controllables when the trigger is invoked. Click the Mark Controllables button and then click on the controllables on the map to mark them with this effector, right-click to cancel the marking process. Only one type of controllable can be marked by a Control Trigger, but multiple Control Triggers are allowed. See the Editing Controllables section to learn more about editing policies. Editing Controllables # After selecting controllables in the VSE, the editing panel allows changing their default policy. A policy is a list of control actions that will be applied when the controllable is initialized, each policy entry represents a single control action. VSE fills the action dropdown with all the allowed actions for the edited controllable. The value can be edited either by an input field, the decimal input field for wait and trigger actions, dropdown for state actions, and no value edition for the loop action. Each policy entry can be removed with the button on the right of the policy row. A whole policy can be copied and pasted between controllables of the same type. The copy button (on the left from the Add Policy Entry button) copies the policy to the clipboard so it can be pasted to another controllable of the same type (button on the right from the Add Policy Entry button). Custom Parameter Edit Panels # Panels that will be used in the Edit Panel have to extend the ParameterEditPanel class and their prefab have to be linked to the Edit Panel prefab in the Assets ( Assets/Prefabs/ScenarioEditor/InspectorContent/EditPanel ). In the future VSE will support loading external parameter edit panels from AssetBundles. VSE will instantiate all linked panels prefabs and will call their initialize and deinitialize methods. Panel can be visible after setting its gameobject to active. Most of the parameter edit panels depend on the selected element and become visible only if selected element fullfil their requirements. For example destination point edit panel becomes visible if selected element ( ScenarioManager.Instance.SelectedOtherElement ) is a ScenarioAgent and has an AgentDestinationPoint extension. VSE Extensions page describes more options for extending the visual scenario editor.","title":"VSE Editors"},{"location":"visual-scenario-editor/vse-editors/#editing-waypoints-path","text":"NPC agents with selected NPCWaypointBehaviour , and all the pedestrians use waypoints' paths to describe how agents will move in a simulation. The destination point playback path uses the same waypoints path to set up the egos' movement in the playback mode. Users can set as many waypoints in the path as required, and parametrize each of them. Start editing the waypoints path with the Add Waypoints button, it will add a waypoint to the end of the existing path. Use the right mouse button to cancel adding waypoints, and the left mouse button to add a waypoint in the current position. Multiple waypoints can be added this way until the operation is canceled. An agent is rotated towards the first waypoint. Created waypoints can be edited like other scenario elements, when selected they can be removed, moved or the user can edit their parameters. After selecting a waypoint and clicking Add next , more waypoints can be added right after the selected one. VSE can change the loop parameter, an agent will continue moving from the beginning when the last waypoint is reached when the loop is selected. Waypoints support either a linear movement with a constant speed and a uniformly accelerated movement. An agent will constantly move with the set speed when the acceleration parameter is set to 0 . If acceleration is greater than zero, the speed parameter determines the maximum speed that the agent can reach when acceleration. If the current speed is greater than the set speed , acceleration becomes the deceleration parameter and the agent tries to slow down to the determined speed. If the acceleration value is not big enough, the set speed may not be reached during the movement. The animation above compares movement without acceleration on the left and with acceleration set on the right. VSE supports two different waypoints path: Linear and BezierSpline . An agent with a selected Linear path will drive straight from waypoint to waypoint, when the BezpierSpline path type is selected, Simulator uses the Bezier Spline algorithm to make the movement smooth. The animation above presents paths with the same waypoints, but different path types. The vehicle on the left uses the Linear path and the vehicle on the right uses the BezierSpline path.","title":"Editing Waypoints Path"},{"location":"visual-scenario-editor/vse-editors/#editing-destination-point","text":"While editing an ego agent a destination point can be activated for this agent. Toggle the destination point value in the edit panel while editing an ego agent to activate or deactivate the destination point. The camera will focus on the destination point after pressing the camera button. Destination point allows adding waypoints' path for testing the created scenario in the playback mode. Show Playback Path views the path and allows editing it. Add Playback Waypoint adds another waypoint and the dropdown allows changing the path type. An ego vehicle will follow the path in the playback mode as other agents with waypoints behaviour do. Destination point's playback path will not affect the final simulation, it is used only in the VSE playback mode.","title":"Editing Destination Point"},{"location":"visual-scenario-editor/vse-editors/#editing-behaviour","text":"NPCs supports various behaviour scripts that will control the vehicle. VSE provides a dropdown with all available NPC behaviours, selected behaviour is saved in the scenario. Some behaviours can be parameterized, for example, NPCLaneFollowBehaviour got the isLaneChange toggle parameter and the maxSpeed value. Only NPCs with the NPCWaypointBehaviour supports editing the waypoints.","title":"Editing Behaviour"},{"location":"visual-scenario-editor/vse-editors/#editing-color","text":"VSE includes the color picker panel that allows selecting any color from the RGB and HSV pallet. NPCs support selecting a custom color of the vehicle. To change the vehicle color click the current color button and select the new color in the opened color picker. The changed color will be applied immediately, but the undo record is registered after closing the color picker panel.","title":"Editing Color"},{"location":"visual-scenario-editor/vse-editors/#editing-triggers","text":"Editing a waypoint allows adding different trigger effectors to this point. All the effectors added to one waypoint are executed in parallel. Some effectors require additional settings and parameters. Each effector can use a custom edit panel. Triggers can be copy and pasted. Use the copy icon (button on the left of the Trigger title) to copy the effectors, then use the paste icon (button on the right of the Trigger title) to clone the effectors to another waypoint. To add a new trigger effector, select it from the dropdown list and click the add button on the right side. A new effector will be added and available for addition under the dropdown. The simulator provides the following trigger effectors with the VSE: Time To Collision This effector calculates if an NPC can cause a collision with any ego, calculating current direction, velocity, and acceleration. The NPC will wait to proceed to the next waypoint such that a collision with the ego vehicle would occur. If no possible collision is found NPC does not wait. Wait For Distance Adding this effector makes NPC wait at the waypoint until any ego vehicle is closer than the max distance (in meters) set in this trigger effector. If no ego vehicle will get close enough, the NPC can wait infinitely. Wait Time The agent will wait for the fixed time (in seconds) at the waypoint before continuing movement towards the next waypoint. Waiting Point A waiting point makes the agent wait until any ego vehicle enters the activation zone. Activation zone can be moved like other scenario elements, by dragging the Move button. If it is required, the radius (in meters) of the activation zone can be changed in the Edit panel of the waypoint or by dragging the Resize button to the left (shrinking) or right (enlarging). Control Trigger This trigger effector will apply the edited policy to all marked controllables when the trigger is invoked. Click the Mark Controllables button and then click on the controllables on the map to mark them with this effector, right-click to cancel the marking process. Only one type of controllable can be marked by a Control Trigger, but multiple Control Triggers are allowed. See the Editing Controllables section to learn more about editing policies.","title":"Editing Triggers"},{"location":"visual-scenario-editor/vse-editors/#editing-controllables","text":"After selecting controllables in the VSE, the editing panel allows changing their default policy. A policy is a list of control actions that will be applied when the controllable is initialized, each policy entry represents a single control action. VSE fills the action dropdown with all the allowed actions for the edited controllable. The value can be edited either by an input field, the decimal input field for wait and trigger actions, dropdown for state actions, and no value edition for the loop action. Each policy entry can be removed with the button on the right of the policy row. A whole policy can be copied and pasted between controllables of the same type. The copy button (on the left from the Add Policy Entry button) copies the policy to the clipboard so it can be pasted to another controllable of the same type (button on the right from the Add Policy Entry button).","title":"Editing Controllables"},{"location":"visual-scenario-editor/vse-editors/#custom-parameter-edit-panels","text":"Panels that will be used in the Edit Panel have to extend the ParameterEditPanel class and their prefab have to be linked to the Edit Panel prefab in the Assets ( Assets/Prefabs/ScenarioEditor/InspectorContent/EditPanel ). In the future VSE will support loading external parameter edit panels from AssetBundles. VSE will instantiate all linked panels prefabs and will call their initialize and deinitialize methods. Panel can be visible after setting its gameobject to active. Most of the parameter edit panels depend on the selected element and become visible only if selected element fullfil their requirements. For example destination point edit panel becomes visible if selected element ( ScenarioManager.Instance.SelectedOtherElement ) is a ScenarioAgent and has an AgentDestinationPoint extension. VSE Extensions page describes more options for extending the visual scenario editor.","title":"Custom Parameter Edit Panels"},{"location":"visual-scenario-editor/vse-extensions/","text":"VSE Extensions # VSE system provides multiple options to extend its functionality. VSE allows adding tools to simplify work and new scenario elements to the VSE. Custom Scenario Editor Extension top # The scenario manager finds all classes implementing the IScenarioEditorExtension in the loaded Assembly and creates an instance of each implementation. If the implementing class extends the MonoBehaviour, the manager adds a new game object with the class component to the VSE. Otherwise, the manager uses a default constructor to create an extension instance. A custom extension instance will be created and initialized asynchronously by a scenario manager. Built-in extensions (like InputManager ) initialize before the custom extensions. The order of custom extensions cannot be set what may lead to race conditions if custom extensions' initialization methods require each other. Custom scenario editor extension allows to add a separate class for managing logic, for example, VSE uses ScenarioAgentsManager and ScenarioControllablesManager that manage different scenario elements during their lifetime. Extensions' initializations occur before the VSE starts, it can load and cache required resources. Serialized Extension top # VSE scenarios are serialized and deserialized by all the scenario editor extensions that implement ISerializedExtension . Each serialized extension gains access to the scenario data sequentially and shares the same scenario data object. Serialization and deserialization methods return the true result if the operation succeeds, and false otherwise so VSE cancels the operation. A serialized extension may edit the existing data. But as the order of custom extensions is not fixed, it may lead to a race condition. Warning: currently, VSE Runner does not support passing custom scenario data to the Simulator, so the saved data may be used only inside the VSE (for example, VSE camera orientation). Custom Scenario Element Source top # Default VSE scenario element sources are ego agents source, NPC agents source, pedestrian agents source, and controllables source. Inspector Add Panel instantiates an UI section for each ScenarioElementSource game object found inside the Scenario Manager. If a custom source is needed, the best way is to instantiate a ScenarioElementSource implementation during a Custom Scenario Editor Extension's initialization and parent source's transform to extension (if it overrides MonoBehaviour) or Scenario Manager. Add panel will prepare buttons for each source variant, and it will call the OnVariantSelected method on the left mouse button down. By default, it instantiated a new element and drags it. Built-in Extensions top # VSE provides some additional features that in the future may be available as an external plugin. Right now, they are built-in and available out of the box in the VSE. Scenario Nav Origin top # Scenario Nav Origin is a separate scenario element with custom properties. Only one Scenario Nav Origin may be available in the scenario. If a loaded map contains a Nav Origin, the extension will create a Scenario Nav Origin using the map's Nav Origin data. A Scenario Nav Origin created from the map data cannot be removed from the scenario. Those behaviors require a custom source implementation. There is a nav elements section in the add panel to handle the dedicated behavior. A Custom Scenario Element Source allows dragging a new Scenario Nav Origin if there are no objects of this type in the scenario. If there is an existing Scenario Nav Origin, instead of adding a new instance, VSE selects the existing one and centers the camera on it. Nav extension is a Serialized Extension implementation and adds the Nav Origin data to the serialized scenario. A Scenario Nav Origin can be moved around the map like other scenario elements and has a dedicated parameter edit panel after selection.","title":"VSE Extensions"},{"location":"visual-scenario-editor/vse-extensions/#custom-scenario-editor-extension","text":"The scenario manager finds all classes implementing the IScenarioEditorExtension in the loaded Assembly and creates an instance of each implementation. If the implementing class extends the MonoBehaviour, the manager adds a new game object with the class component to the VSE. Otherwise, the manager uses a default constructor to create an extension instance. A custom extension instance will be created and initialized asynchronously by a scenario manager. Built-in extensions (like InputManager ) initialize before the custom extensions. The order of custom extensions cannot be set what may lead to race conditions if custom extensions' initialization methods require each other. Custom scenario editor extension allows to add a separate class for managing logic, for example, VSE uses ScenarioAgentsManager and ScenarioControllablesManager that manage different scenario elements during their lifetime. Extensions' initializations occur before the VSE starts, it can load and cache required resources.","title":"Custom Scenario Editor Extension"},{"location":"visual-scenario-editor/vse-extensions/#serialized-extension","text":"VSE scenarios are serialized and deserialized by all the scenario editor extensions that implement ISerializedExtension . Each serialized extension gains access to the scenario data sequentially and shares the same scenario data object. Serialization and deserialization methods return the true result if the operation succeeds, and false otherwise so VSE cancels the operation. A serialized extension may edit the existing data. But as the order of custom extensions is not fixed, it may lead to a race condition. Warning: currently, VSE Runner does not support passing custom scenario data to the Simulator, so the saved data may be used only inside the VSE (for example, VSE camera orientation).","title":"Serialized Extension"},{"location":"visual-scenario-editor/vse-extensions/#custom-scenario-element-source","text":"Default VSE scenario element sources are ego agents source, NPC agents source, pedestrian agents source, and controllables source. Inspector Add Panel instantiates an UI section for each ScenarioElementSource game object found inside the Scenario Manager. If a custom source is needed, the best way is to instantiate a ScenarioElementSource implementation during a Custom Scenario Editor Extension's initialization and parent source's transform to extension (if it overrides MonoBehaviour) or Scenario Manager. Add panel will prepare buttons for each source variant, and it will call the OnVariantSelected method on the left mouse button down. By default, it instantiated a new element and drags it.","title":"Custom Scenario Element Source"},{"location":"visual-scenario-editor/vse-extensions/#built-in-extensions","text":"VSE provides some additional features that in the future may be available as an external plugin. Right now, they are built-in and available out of the box in the VSE.","title":"Built-in Extensions"},{"location":"visual-scenario-editor/vse-extensions/#scenario-nav-origin","text":"Scenario Nav Origin is a separate scenario element with custom properties. Only one Scenario Nav Origin may be available in the scenario. If a loaded map contains a Nav Origin, the extension will create a Scenario Nav Origin using the map's Nav Origin data. A Scenario Nav Origin created from the map data cannot be removed from the scenario. Those behaviors require a custom source implementation. There is a nav elements section in the add panel to handle the dedicated behavior. A Custom Scenario Element Source allows dragging a new Scenario Nav Origin if there are no objects of this type in the scenario. If there is an existing Scenario Nav Origin, instead of adding a new instance, VSE selects the existing one and centers the camera on it. Nav extension is a Serialized Extension implementation and adds the Nav Origin data to the serialized scenario. A Scenario Nav Origin can be moved around the map like other scenario elements and has a dedicated parameter edit panel after selection.","title":"Scenario Nav Origin"},{"location":"visual-scenario-editor/vse-inspector/","text":"VSE Inspector # Inspector allows selecting different panels for editing the Scenario. Inspector can be hidden and shown using the button attached to the inspector from the left side. Currently, there are the following panels available to select: File contains basic controls for the VSE like saving, loading, resetting scenario, exiting VSE, and changing the VSE options. Add allows adding new elements to the map. Edit allows editing selected map elements, for example selecting variant of an agent or editing waypoint parameters. Map views available maps in the library and allows changing map in VSE, changing map resets the scenario. Playback imitates the scenario simulation, for example, the agents' movement along with the set waypoints. Future updates will include a plugins system for adding custom inspector panels. File Panel top # The file panel in the VSE inspector allows: Load a scenario from a previously saved JSON file; loading a scenario will first clear (reset) the current scenario and can load a different map Save the current scenario to a JSON file Reset Scenario clears every added element Exit Editor closes the VSE and returns to the main Simulator window The options section in the file panel allows to adjust VSE usage: Camera Mode - toggles current camera mode, switches between Top-down camera , Leaned 45\u00b0 camera , and Free camera . Only Free camera allows rotating the camera with the mouse. Snap - toggling this option enables or disables snapping agents to the map lanes. Invert X - toggling this option inverts the rotation direction while rotating the camera with a horizontal mouse movement in the Free camera mode. Invert Y - toggling this option inverts the rotation direction while rotating the camera with a vertical mouse movement in the Free camera mode. Height occluder scrollbar - toggles the scrollbar that controls the height occluder, which disables visibility of game objects above the set level. This scrollbar is available on the left side of the screen if it is enabled. Add Panel top # Placing a new agent into the scenario requires dragging it from the agents' panel. Press the left mouse button on the agent type you want to add and drag it on the map; release the button to place the agent on the map. Different agent types can be added in the same way. If you want to cancel adding an agent, press the right mouse button while dragging an agent. If dragging finishes over the UI, it will cancel drag as well. By default, agents will be snapped to the map lanes; this option can be toggled in the File panel. VSE lists all the ego vehicles added to the cloud library that are available for used Simulator version. Ego vehicles' names that require downloading contain two cloud icons, pressing their button for the first time invokes the downloading process. The start and stop of the download process are confirmed in the log panel. VSE loads all the NPCs, pedestrians, and controllables that are configured for the Simulator. Each scenario element source (Ego vehicles, NPCs, pedestrians, controllables) are available in the separate panels with all available variants. If there are many variants, the list is divided into multiple pages. The description panel can be viewed for each scenario element when the pointer hovers over the button for a short time. Each element type displays different pieces of information. Edit Panel top # Parameters edit panel allows changing more data of the selected map element. After selecting an element, the parameters edit panel will fill with components possible to edit. For example, after selecting an NPC agent it is possible to select its variant, behaviour, color, and add waypoints for this agent. Selecting a waypoint allows you to set the speed and wait time for this waypoint. Note that only downloaded variants from your SVL Simulator account's \"My Library\" are available in the dropdown. Refer to the Vehicles section of My Library for more information on how to add vehicles to your library in the Web UI. Map Panel top # Map panel views every map available in your SVL Simulator account's \"My Library\" and allows switching the current map in the VSE. Changing the map always resets the scenario. Refer to the Maps section of My Library for more information on how to add maps to your library in the Web UI. Playback Panel top # The playback panel imitates some of the simulation behaviors to visualize the final scenario effect. Currently, only agents with waypoints path imitate their movement. Playback mode supports the trigger effectors. Extending the TriggerEffectorPlayback class allows overriding trigger effect implementation that will be called only in the VSE playback mode. Some triggers like TimeToCollision require a real-time simulation. To imitate the triggers correctly, VSE records the scenario in real-time when VSE enters the playback mode. The simulator requires an AD stack to control an ego vehicle. VSE does not support it, but it provides a simple waypoints' following system for Ego vehicles playback. Refer to the Editing Destination Point to read more about setting a playback path for ego vehicles. Following controls can affect the playback mode after recording a scenario: Play starts the playback. Pause freezes the playback at the current time. Stop stops the playback and resets time to the beginning. Timeline allows flexible time adjustment of the playback. Playback speed changes the time scale of the playback (available playback speeds: 1/4x, 1/2x, 1x, 2x, 4x). Editing Scenario Elements top # Users can parametrize placed scenario elements in the Edit Panel . After selecting a scenario element, different editors will be available. Refer to the VSE Editors page to learn more about scenario element editors.","title":"VSE Inspector"},{"location":"visual-scenario-editor/vse-inspector/#file-panel","text":"The file panel in the VSE inspector allows: Load a scenario from a previously saved JSON file; loading a scenario will first clear (reset) the current scenario and can load a different map Save the current scenario to a JSON file Reset Scenario clears every added element Exit Editor closes the VSE and returns to the main Simulator window The options section in the file panel allows to adjust VSE usage: Camera Mode - toggles current camera mode, switches between Top-down camera , Leaned 45\u00b0 camera , and Free camera . Only Free camera allows rotating the camera with the mouse. Snap - toggling this option enables or disables snapping agents to the map lanes. Invert X - toggling this option inverts the rotation direction while rotating the camera with a horizontal mouse movement in the Free camera mode. Invert Y - toggling this option inverts the rotation direction while rotating the camera with a vertical mouse movement in the Free camera mode. Height occluder scrollbar - toggles the scrollbar that controls the height occluder, which disables visibility of game objects above the set level. This scrollbar is available on the left side of the screen if it is enabled.","title":"File Panel"},{"location":"visual-scenario-editor/vse-inspector/#add-panel","text":"Placing a new agent into the scenario requires dragging it from the agents' panel. Press the left mouse button on the agent type you want to add and drag it on the map; release the button to place the agent on the map. Different agent types can be added in the same way. If you want to cancel adding an agent, press the right mouse button while dragging an agent. If dragging finishes over the UI, it will cancel drag as well. By default, agents will be snapped to the map lanes; this option can be toggled in the File panel. VSE lists all the ego vehicles added to the cloud library that are available for used Simulator version. Ego vehicles' names that require downloading contain two cloud icons, pressing their button for the first time invokes the downloading process. The start and stop of the download process are confirmed in the log panel. VSE loads all the NPCs, pedestrians, and controllables that are configured for the Simulator. Each scenario element source (Ego vehicles, NPCs, pedestrians, controllables) are available in the separate panels with all available variants. If there are many variants, the list is divided into multiple pages. The description panel can be viewed for each scenario element when the pointer hovers over the button for a short time. Each element type displays different pieces of information.","title":"Add Panel"},{"location":"visual-scenario-editor/vse-inspector/#edit-panel","text":"Parameters edit panel allows changing more data of the selected map element. After selecting an element, the parameters edit panel will fill with components possible to edit. For example, after selecting an NPC agent it is possible to select its variant, behaviour, color, and add waypoints for this agent. Selecting a waypoint allows you to set the speed and wait time for this waypoint. Note that only downloaded variants from your SVL Simulator account's \"My Library\" are available in the dropdown. Refer to the Vehicles section of My Library for more information on how to add vehicles to your library in the Web UI.","title":"Edit Panel"},{"location":"visual-scenario-editor/vse-inspector/#map-panel","text":"Map panel views every map available in your SVL Simulator account's \"My Library\" and allows switching the current map in the VSE. Changing the map always resets the scenario. Refer to the Maps section of My Library for more information on how to add maps to your library in the Web UI.","title":"Map Panel"},{"location":"visual-scenario-editor/vse-inspector/#playback-panel","text":"The playback panel imitates some of the simulation behaviors to visualize the final scenario effect. Currently, only agents with waypoints path imitate their movement. Playback mode supports the trigger effectors. Extending the TriggerEffectorPlayback class allows overriding trigger effect implementation that will be called only in the VSE playback mode. Some triggers like TimeToCollision require a real-time simulation. To imitate the triggers correctly, VSE records the scenario in real-time when VSE enters the playback mode. The simulator requires an AD stack to control an ego vehicle. VSE does not support it, but it provides a simple waypoints' following system for Ego vehicles playback. Refer to the Editing Destination Point to read more about setting a playback path for ego vehicles. Following controls can affect the playback mode after recording a scenario: Play starts the playback. Pause freezes the playback at the current time. Stop stops the playback and resets time to the beginning. Timeline allows flexible time adjustment of the playback. Playback speed changes the time scale of the playback (available playback speeds: 1/4x, 1/2x, 1x, 2x, 4x).","title":"Playback Panel"},{"location":"visual-scenario-editor/vse-inspector/#editing-scenario-elements","text":"Users can parametrize placed scenario elements in the Edit Panel . After selecting a scenario element, different editors will be available. Refer to the VSE Editors page to learn more about scenario element editors.","title":"Editing Scenario Elements"}]}