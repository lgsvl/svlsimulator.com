{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Explore our guides and examples for using SVL Simulator (formerly \"LGSVL Simulator\"). Get started here . Visit our website here: https://www.svlsimulator.com Table of Contents # Release Notes # Release features Release notes Contents of this release Limitations and license notice Previous releases Installation Guide # System requirements Installation procedure Building from source Running with WSL2 Getting Started # Introduction Conventions Run a simulation Tutorials # Modular testing with the Apollo AD stack Deep learning lane following model Creating a simple ROS2-based AD stack Viewing and subscribing to ground truth obstacles User Interface # Web User Interface Store Library Clusters Simulations Test Results Simulation User Interface Simulator main menu Simulation menu Sensor visualizers Bridge connection UI Configuration file and command line parameters Keyboard shortcuts System Under Test # Introduction Messages Simulator messages The lgsvl_msgs package ROS (Autoware.AI) ROS 2 Autoware.Auto Setting up ROS 2 bridge Apollo Latest Apollo Apollo 5.0 Running Simulations # Runtime templates Running simulator Offline mode Developer mode Creating Scenarios # Random traffic Visual scenario editor Python API Python API # Python API guide Python API quickstart guide Dreamview API Simulation Content # Sharing assets Building content Maps Creating a new map Map annotation Road network generation Adding destinations to a map Point cloud import Point cloud rendering Lane-line detector Vehicles Creating a new ego vehicle Vehicle dynamics Sensors List of sensors Lane-line sensor Plugins # Sensor plugins Bridge plugins Controllable plugins NPC plugins Pedestrian plugins Digital Twin # Digital Twin Lite example map Distributed Simulation # Introduction Running distributed simulation Third Party Integration # OpenAI Gym Support # Troubleshooting Unity help Frequently asked questions Contributing","title":"Home"},{"location":"#table-of-contents","text":"","title":"Table of Contents"},{"location":"#release-notes","text":"Release features Release notes Contents of this release Limitations and license notice Previous releases","title":"Release Notes"},{"location":"#installation-guide","text":"System requirements Installation procedure Building from source Running with WSL2","title":"Installation Guide"},{"location":"#getting-started","text":"Introduction Conventions Run a simulation","title":"Getting Started"},{"location":"#tutorials","text":"Modular testing with the Apollo AD stack Deep learning lane following model Creating a simple ROS2-based AD stack Viewing and subscribing to ground truth obstacles","title":"Tutorials"},{"location":"#user-interface","text":"Web User Interface Store Library Clusters Simulations Test Results Simulation User Interface Simulator main menu Simulation menu Sensor visualizers Bridge connection UI Configuration file and command line parameters Keyboard shortcuts","title":"User Interface"},{"location":"#system-under-test","text":"Introduction Messages Simulator messages The lgsvl_msgs package ROS (Autoware.AI) ROS 2 Autoware.Auto Setting up ROS 2 bridge Apollo Latest Apollo Apollo 5.0","title":"System Under Test"},{"location":"#running-simulations","text":"Runtime templates Running simulator Offline mode Developer mode","title":"Running Simulations"},{"location":"#creating-scenarios","text":"Random traffic Visual scenario editor Python API","title":"Creating Scenarios"},{"location":"#python-api","text":"Python API guide Python API quickstart guide Dreamview API","title":"Python API"},{"location":"#simulation-content","text":"Sharing assets Building content Maps Creating a new map Map annotation Road network generation Adding destinations to a map Point cloud import Point cloud rendering Lane-line detector Vehicles Creating a new ego vehicle Vehicle dynamics Sensors List of sensors Lane-line sensor","title":"Simulation Content"},{"location":"#plugins","text":"Sensor plugins Bridge plugins Controllable plugins NPC plugins Pedestrian plugins","title":"Plugins"},{"location":"#digital-twin","text":"Digital Twin Lite example map","title":"Digital Twin"},{"location":"#distributed-simulation","text":"Introduction Running distributed simulation","title":"Distributed Simulation"},{"location":"#third-party-integration","text":"OpenAI Gym","title":"Third Party Integration"},{"location":"#support","text":"Troubleshooting Unity help Frequently asked questions Contributing","title":"Support"},{"location":"archive/","text":"Documentation Archives You can access the documentation for previous releases of SVL Simulator here. 2021.1.1 2021.1 2020.06 2020.05 2020.03 2020.01 2019.12 and older: Please refer to the Docs directory in the simulator repository on Github for the relevant release version.","title":"Previous releases"},{"location":"creating-scenarios/api-example-descriptions/","text":"Python API Use Case Examples The SVL Simulator team has created sample Python scripts that use the SVL Simulator Python API to test specific scenarios or perform certain tasks. These example scripts can be found on our Github here . Please contact us if you would like to contribute examples that you are using, or submit a pull request . Scenarios top # We have created basic sample scenarios using the Python API. See here for a step-by-step guide on how to run one of these scenarios. Several are based on the sample Test Cases from NHTSA . The below scenarios assume that the simulator can be connected to an instance of Apollo 5.0. See the guide for getting connected with Apollo 5.0 here . The Apollo modules that need to be started are shown below (localization, perception, planning, prediction, routing, traffic light, transform, control): It is recommended to start Apollo and the modules before running a scenario. Apollo's destination can be set after Localization and Routing have been started. Vehicle Following top # Scripts: Perform Vehicle Following This scenario simulates the EGO vehicle approaching a slower NPC from behind. The EGO is expected to accelerate up to the speed limit and catch up to the NPC. For this scenario, the destination is the end of the lane. Encroaching Oncoming Vehicle top # Scripts: Detect and Respond to Encroaching Oncoming Vehicle This scenario simulates the EGO vehicle approaching an oncoming NPC that is half in the EGO's lane making a collision imminent. The EGO is expected to avoid a collision. Here the NPC uses the waypoint system to define its path. With waypoints, the NPC ignores other traffic and does not attempt to avoid collisions. For this scenario, the destinaion is the end of the lane. The same destination can be used as the Vehicle Following scripts. Other Uses top # Collecting data in KITTI format top # Script: kitti_parser.py This script shows an example of collecting data in the KITTI format. This data can be used to train for detecting vehicles in images. This script spawns the ego vehicle in a random position in the San Francisco map. Then a number of NPC vehicles are randomly spawned in front of the ego vehicle. Camera and ground truth data is saved in the KITTI format. This data can be used to train for detecting vehicles in images. For more information on KITTI please see: http://www.cvlibs.net/datasets/kitti/index.php The data format is defined in a README file downloadable from: https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_object.zip Automated Driving System Test Cases top # The United States National Highway Traffic Safety Administration released a report describing a framework for establishing sample preliminary tests. The report is available online: A Framework for Automated Driving System Testable Cases and Scenarios We created several of the described tests available here: NHTSA-sample-tests These tests run the ADS at different speeds. To accomplish this with Apollo, the speed limit in the HD map of the appropriate lanes needs to be adjusted and the planning configuration should also be changed to limit Apollo's top speed. The ADS destination is described in the report. For our implementation of the Perform Lane Change tests, the same destination as the above Overtaker and Traffic Jam scenarios is used.","title":"Python API Use Case Examples [](#top)"},{"location":"creating-scenarios/api-example-descriptions/#scenarios","text":"We have created basic sample scenarios using the Python API. See here for a step-by-step guide on how to run one of these scenarios. Several are based on the sample Test Cases from NHTSA . The below scenarios assume that the simulator can be connected to an instance of Apollo 5.0. See the guide for getting connected with Apollo 5.0 here . The Apollo modules that need to be started are shown below (localization, perception, planning, prediction, routing, traffic light, transform, control): It is recommended to start Apollo and the modules before running a scenario. Apollo's destination can be set after Localization and Routing have been started.","title":"Scenarios"},{"location":"creating-scenarios/api-example-descriptions/#vehicle-following","text":"Scripts: Perform Vehicle Following This scenario simulates the EGO vehicle approaching a slower NPC from behind. The EGO is expected to accelerate up to the speed limit and catch up to the NPC. For this scenario, the destination is the end of the lane.","title":"Vehicle Following"},{"location":"creating-scenarios/api-example-descriptions/#encroaching-oncoming-vehicle","text":"Scripts: Detect and Respond to Encroaching Oncoming Vehicle This scenario simulates the EGO vehicle approaching an oncoming NPC that is half in the EGO's lane making a collision imminent. The EGO is expected to avoid a collision. Here the NPC uses the waypoint system to define its path. With waypoints, the NPC ignores other traffic and does not attempt to avoid collisions. For this scenario, the destinaion is the end of the lane. The same destination can be used as the Vehicle Following scripts.","title":"Encroaching Oncoming Vehicle"},{"location":"creating-scenarios/api-example-descriptions/#other-uses","text":"","title":"Other Uses"},{"location":"creating-scenarios/api-example-descriptions/#collecting-data-in-kitti-format","text":"Script: kitti_parser.py This script shows an example of collecting data in the KITTI format. This data can be used to train for detecting vehicles in images. This script spawns the ego vehicle in a random position in the San Francisco map. Then a number of NPC vehicles are randomly spawned in front of the ego vehicle. Camera and ground truth data is saved in the KITTI format. This data can be used to train for detecting vehicles in images. For more information on KITTI please see: http://www.cvlibs.net/datasets/kitti/index.php The data format is defined in a README file downloadable from: https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_object.zip","title":"Collecting data in KITTI format"},{"location":"creating-scenarios/api-example-descriptions/#automated-driving-system-test-cases","text":"The United States National Highway Traffic Safety Administration released a report describing a framework for establishing sample preliminary tests. The report is available online: A Framework for Automated Driving System Testable Cases and Scenarios We created several of the described tests available here: NHTSA-sample-tests These tests run the ADS at different speeds. To accomplish this with Apollo, the speed limit in the HD map of the appropriate lanes needs to be adjusted and the planning configuration should also be changed to limit Apollo's top speed. The ADS destination is described in the report. For our implementation of the Perform Lane Change tests, the same destination as the above Overtaker and Traffic Jam scenarios is used.","title":"Automated Driving System Test Cases"},{"location":"creating-scenarios/python-scenarios/","text":"Creating Python API Scenarios","title":"Creating Python API Scenarios"},{"location":"creating-scenarios/visual-scenario-editor/","text":"Visual Scenario Editor Visual Scenario Editor (VSE) is a tool for visual creating and editing scenarios for the SVL Simulator. Each scenario represents a test case. VSE focuses on quick scenario creation using visual controls while supporting custom scriptable plugins to extend the tools panels. Video: How to create and run VSE scenario top # How To Run top # Visual Scenario Editor is integrated into the SVL Simulator executable. To enter the VSE, the Simulator has to be logged in, linked to the cloud, and cannot be running any simulation. The Visual Editor button is available when the Simulator is properly connected to the cloud. Creating A Scenario top # VSE allows placing agents on the map, changing parameters of those agents, and setting waypoints for their movement. Each waypoint can affect the agent with different trigger effects like waiting for a fixed time at the waypoint. Camera Management top # The animations above present in order: camera movement, zooming in and out, and camera rotation. VSE uses a drag and drop system to move around the map. Press the middle mouse button (scroll wheel) down somewhere on the map and move the mouse while holding the middle button down, release the button to stop moving the map. To zoom in and zoom out, use the mouse wheel scroll. If Free Camera mode is selected in options, the camera can be rotated with the drag and drop while holding the mouse right-click. Additionally, the camera can be moved with the keyboard \"WSAD\" keys. Confirmation Popup top # Operations like resetting scenarios with unsaved changes require additional confirmation. VSE shows a popup with the operation description that requires user confirmation before it is performed. Log Popup top # VSE displays important information on an additional log panel. This panel shows and hides automatically, it lasts longer the longer message is. If multiple messages occur, they are displayed one by one in the log panel before hiding it. Inspector top # Inspector allows selecting different panels for editing the Scenario. Inspector can be hidden and shown using the button attached to the inspector from the left side. Currently, there are the following panels available to select: File contains basic controls for the VSE like saving, loading, resetting scenario, exiting VSE, and changing the VSE options. Add allows adding new elements to the map. Edit allows editing selected map elements, for example selecting variant of an agent or editing waypoint parameters. Map views available maps in the library and allows changing map in VSE, changing map resets the scenario. Playback imitates the scenario simulation, for example, the agents' movement along with the set waypoints. Future updates will include a plugins system for adding custom inspector panels. File Panel top # The file panel in the VSE inspector allows: Load a scenario from a previously saved JSON file; loading a scenario will first clear (reset) the current scenario and can load a different map Save the current scenario to a JSON file Reset Scenario clears every added element Exit Editor closes the VSE and returns to the main Simulator window The options section in the file panel allows to adjust VSE usage: Camera Mode - toggles current camera mode, switches between Top-down camera , Leaned 45\u00b0 camera , and Free camera . Only Free camera allows rotating the camera with the mouse. Snap - toggling this option enables or disables snapping agents to the map lanes. Invert X - toggling this option inverts the rotation direction while rotating the camera with a horizontal mouse movement in the Free camera mode. Invert Y - toggling this option inverts the rotation direction while rotating the camera with a vertical mouse movement in the Free camera mode. Add Panel top # Placing a new agent into the scenario requires dragging it from the agents' panel. Press the left mouse button on the agent type you want to add and drag it on the map; release the button to place the agent on the map. Different agent types can be added in the same way. If you want to cancel adding an agent, press the right mouse button while dragging an agent. If dragging finishes over the UI, it will cancel drag as well. By default, agents will be snapped to the map lanes; this option can be toggled in the File panel. VSE lists all the ego vehicles added to the cloud library. Ego vehicles' names that require downloading contain two cloud icons, pressing their button for the first time invokes the downloading process. The start and stop of the download process are confirmed in the log panel. VSE loads all the NPCs, pedestrians, and controllables that are configured for the Simulator. Each scenario element source (Ego vehicles, NPCs, pedestrians, controllables) are available in the separate panels with all available variants. If there are many variants, the list is divided into multiple pages. The description panel can be viewed for each scenario element when the pointer hovers over the button for a short time. Each element type displays different pieces of information. Edit Panel top # Parameters edit panel allows changing more data of the selected map element. After selecting an element, the parameters edit panel will fill with components possible to edit. For example, after selecting an NPC agent it is possible to select its variant, behaviour, color, and add waypoints for this agent. Selecting a waypoint allows you to set the speed and wait time for this waypoint. Note that the available variants for EGO type will match the vehicles available in your SVL Simulator account's \"My Library\". Refer to the Vehicles section of My Library for more information on how to add vehicles to your library in the Web UI. Editing Destination Point top # While editing an ego agent a destination point can be activated for this agent. Toggle the destination point value in the edit panel while editing an ego agent to activate or deactivate the destination point. The camera will focus on the destination point after pressing the camera button. Editing Behaviour top # NPCs supports various behaviour scripts that will control the vehicle. VSE provides a dropdown with all available NPC behaviours, selected behaviour is saved in the scenario. Some behaviours can be parameterized, for example, NPCLaneFollowBehaviour got the isLaneChange toggle parameter and the maxSpeed value. Only NPCs with the NPCWaypointBehaviour supports editing the waypoints. Editing Color top # VSE includes the color picker panel that allows selecting any color from the RGB and HSV pallet. NPCs support selecting a custom color of the vehicle. To change the vehicle color click the current color button and select the new color in the opened color picker. The changed color will be applied immediately, but the undo record is registered after closing the color picker panel. Editing Triggers top # Editing a waypoint allows adding different trigger effectors to this point. All the effectors added to one waypoint are executed in parallel. Some effectors require additional settings and parameters. Each effector can use a custom edit panel. Triggers can be copy and pasted. Use the copy icon (button on the left of the Trigger title) to copy the effectors, then use the paste icon (button on the right of the Trigger title) to clone the effectors to another waypoint. To add a new trigger effector, select it from the dropdown list and click the add button on the right side. A new effector will be added and available for addition under the dropdown. The simulator provides the following trigger effectors with the VSE: Time To Collision This effector calculates if an NPC can cause a collision with any ego, calculating current direction, velocity, and acceleration. The NPC will wait to proceed to the next waypoint such that a collision with the ego vehicle would occur. If no possible collision is found NPC does not wait. Wait For Distance Adding this effector makes NPC wait at the waypoint until any ego vehicle is closer than the max distance (in meters) set in this trigger effector. If no ego vehicle will get close enough, the NPC can wait infinitely. Wait Time The agent will wait for the fixed time (in seconds) at the waypoint before continuing movement towards the next waypoint. Waiting Point A waiting point makes the agent wait until any ego vehicle enters the activation zone. Activation zone can be moved like other scenario elements, by dragging the Move button. If it is required, the radius (in meters) of the activation zone can be changed in the Edit panel of the waypoint or by dragging the Resize button to the left (shrinking) or right (enlarging). Control Trigger This trigger effector will apply the edited policy to all marked controllables when the trigger is invoked. Click the Mark Controllables button and then click on the controllables on the map to mark them with this effector, right-click to cancel the marking process. Only one type of controllable can be marked by a Control Trigger, but multiple Control Triggers are allowed. See the Editing Controllables section to learn more about editing policies. Editing Controllables # After selecting controllables in the VSE, the editing panel allows changing their default policy. A policy is a list of control actions that will be applied when the controllable is initialized, each policy entry represents a single control action. VSE fills the action dropdown with all the allowed actions for the edited controllable. The value can be edited either by an input field, the decimal input field for wait and trigger actions, dropdown for state actions, and no value edition for the loop action. Each policy entry can be removed with the button on the right of the policy row. A whole policy can be copied and pasted between controllables of the same type. The copy button (on the left from the Add Policy Entry button) copies the policy to the clipboard so it can be pasted to another controllable of the same type (button on the right from the Add Policy Entry button). Map Panel top # Map panel views every map available in your SVL Simulator account's \"My Library\" and allows switching the current map in the VSE. Changing the map always resets the scenario. Refer to the Maps section of My Library for more information on how to add maps to your library in the Web UI. Quick Edit Panel top # Selecting a map element (for example agent) with the left mouse button shows the quick edit panel above the selected element. Remove deletes the selected element from the scenario. Move allows to reposition a selected element with a drag and drop system. Use the right mouse button to cancel the reposition. Rotate allows rotating selected elements with drag and drop system, drag button to the left or right to rotate the selected object. Use the right mouse button to revert the rotation. Resize allows scaling selected elements with drag and drop system, drag button to the left (shrinking), or to the right (enlarging). Use the right mouse button to revert the resizing. Future updates will include a plugins system for custom quick edit buttons. Playback Panel top # The playback panel imitates some of the simulation behaviours to visualize the final scenario effect. Currently, only agents with waypoints imitate their movement. Playback can be controlled with the following controls: Play starts the playback. Pause freezes the playback at the current time. Stop stops the playback and resets time to the beginning. Timeline allows flexible time adjustment of the playback. Playback speed changes the time scale of the playback (available playback speeds: 1/4x, 1/2x, 1x, 2x, 4x). File Selector top # The file selector is a simple built-in UI dialog for selecting the destination file on the hard-drive. It allows to change manually the directory path, go one directory up, enter internal directories and select a file with a required extension. When saving a scenario, it can override an existing file, or it can be saved to a new file with a selected name. Undo top # VSE includes an undo feature that allows reverting the last changes. Use the \"ctrl\"+\"z\" on your keyboard to revert the most recent operation. Undo manager registers multiple records on the stack, using undo applies the last operation record, and removes it from the stack. This way, the undo can be used multiple times to revert many most recent actions. Scenario Example top # The Visual scenario editor runtime template Simulation can run a created Scenario in the VSE.","title":"Visual scenario editor"},{"location":"creating-scenarios/visual-scenario-editor/#vse-video","text":"","title":"Video: How to create and run VSE scenario"},{"location":"creating-scenarios/visual-scenario-editor/#how-to-run","text":"Visual Scenario Editor is integrated into the SVL Simulator executable. To enter the VSE, the Simulator has to be logged in, linked to the cloud, and cannot be running any simulation. The Visual Editor button is available when the Simulator is properly connected to the cloud.","title":"How To Run"},{"location":"creating-scenarios/visual-scenario-editor/#creating-a-scenario","text":"VSE allows placing agents on the map, changing parameters of those agents, and setting waypoints for their movement. Each waypoint can affect the agent with different trigger effects like waiting for a fixed time at the waypoint.","title":"Creating A Scenario"},{"location":"creating-scenarios/visual-scenario-editor/#camera-management","text":"The animations above present in order: camera movement, zooming in and out, and camera rotation. VSE uses a drag and drop system to move around the map. Press the middle mouse button (scroll wheel) down somewhere on the map and move the mouse while holding the middle button down, release the button to stop moving the map. To zoom in and zoom out, use the mouse wheel scroll. If Free Camera mode is selected in options, the camera can be rotated with the drag and drop while holding the mouse right-click. Additionally, the camera can be moved with the keyboard \"WSAD\" keys.","title":"Camera Management"},{"location":"creating-scenarios/visual-scenario-editor/#confirmation-popup","text":"Operations like resetting scenarios with unsaved changes require additional confirmation. VSE shows a popup with the operation description that requires user confirmation before it is performed.","title":"Confirmation Popup"},{"location":"creating-scenarios/visual-scenario-editor/#log-popup","text":"VSE displays important information on an additional log panel. This panel shows and hides automatically, it lasts longer the longer message is. If multiple messages occur, they are displayed one by one in the log panel before hiding it.","title":"Log Popup"},{"location":"creating-scenarios/visual-scenario-editor/#inspector","text":"Inspector allows selecting different panels for editing the Scenario. Inspector can be hidden and shown using the button attached to the inspector from the left side. Currently, there are the following panels available to select: File contains basic controls for the VSE like saving, loading, resetting scenario, exiting VSE, and changing the VSE options. Add allows adding new elements to the map. Edit allows editing selected map elements, for example selecting variant of an agent or editing waypoint parameters. Map views available maps in the library and allows changing map in VSE, changing map resets the scenario. Playback imitates the scenario simulation, for example, the agents' movement along with the set waypoints. Future updates will include a plugins system for adding custom inspector panels.","title":"Inspector"},{"location":"creating-scenarios/visual-scenario-editor/#file-panel","text":"The file panel in the VSE inspector allows: Load a scenario from a previously saved JSON file; loading a scenario will first clear (reset) the current scenario and can load a different map Save the current scenario to a JSON file Reset Scenario clears every added element Exit Editor closes the VSE and returns to the main Simulator window The options section in the file panel allows to adjust VSE usage: Camera Mode - toggles current camera mode, switches between Top-down camera , Leaned 45\u00b0 camera , and Free camera . Only Free camera allows rotating the camera with the mouse. Snap - toggling this option enables or disables snapping agents to the map lanes. Invert X - toggling this option inverts the rotation direction while rotating the camera with a horizontal mouse movement in the Free camera mode. Invert Y - toggling this option inverts the rotation direction while rotating the camera with a vertical mouse movement in the Free camera mode.","title":"File Panel"},{"location":"creating-scenarios/visual-scenario-editor/#add-panel","text":"Placing a new agent into the scenario requires dragging it from the agents' panel. Press the left mouse button on the agent type you want to add and drag it on the map; release the button to place the agent on the map. Different agent types can be added in the same way. If you want to cancel adding an agent, press the right mouse button while dragging an agent. If dragging finishes over the UI, it will cancel drag as well. By default, agents will be snapped to the map lanes; this option can be toggled in the File panel. VSE lists all the ego vehicles added to the cloud library. Ego vehicles' names that require downloading contain two cloud icons, pressing their button for the first time invokes the downloading process. The start and stop of the download process are confirmed in the log panel. VSE loads all the NPCs, pedestrians, and controllables that are configured for the Simulator. Each scenario element source (Ego vehicles, NPCs, pedestrians, controllables) are available in the separate panels with all available variants. If there are many variants, the list is divided into multiple pages. The description panel can be viewed for each scenario element when the pointer hovers over the button for a short time. Each element type displays different pieces of information.","title":"Add Panel"},{"location":"creating-scenarios/visual-scenario-editor/#edit-panel","text":"Parameters edit panel allows changing more data of the selected map element. After selecting an element, the parameters edit panel will fill with components possible to edit. For example, after selecting an NPC agent it is possible to select its variant, behaviour, color, and add waypoints for this agent. Selecting a waypoint allows you to set the speed and wait time for this waypoint. Note that the available variants for EGO type will match the vehicles available in your SVL Simulator account's \"My Library\". Refer to the Vehicles section of My Library for more information on how to add vehicles to your library in the Web UI.","title":"Edit Panel"},{"location":"creating-scenarios/visual-scenario-editor/#editing-destination-point","text":"While editing an ego agent a destination point can be activated for this agent. Toggle the destination point value in the edit panel while editing an ego agent to activate or deactivate the destination point. The camera will focus on the destination point after pressing the camera button.","title":"Editing Destination Point"},{"location":"creating-scenarios/visual-scenario-editor/#editing-behaviour","text":"NPCs supports various behaviour scripts that will control the vehicle. VSE provides a dropdown with all available NPC behaviours, selected behaviour is saved in the scenario. Some behaviours can be parameterized, for example, NPCLaneFollowBehaviour got the isLaneChange toggle parameter and the maxSpeed value. Only NPCs with the NPCWaypointBehaviour supports editing the waypoints.","title":"Editing Behaviour"},{"location":"creating-scenarios/visual-scenario-editor/#editing-color","text":"VSE includes the color picker panel that allows selecting any color from the RGB and HSV pallet. NPCs support selecting a custom color of the vehicle. To change the vehicle color click the current color button and select the new color in the opened color picker. The changed color will be applied immediately, but the undo record is registered after closing the color picker panel.","title":"Editing Color"},{"location":"creating-scenarios/visual-scenario-editor/#editing-triggers","text":"Editing a waypoint allows adding different trigger effectors to this point. All the effectors added to one waypoint are executed in parallel. Some effectors require additional settings and parameters. Each effector can use a custom edit panel. Triggers can be copy and pasted. Use the copy icon (button on the left of the Trigger title) to copy the effectors, then use the paste icon (button on the right of the Trigger title) to clone the effectors to another waypoint. To add a new trigger effector, select it from the dropdown list and click the add button on the right side. A new effector will be added and available for addition under the dropdown. The simulator provides the following trigger effectors with the VSE: Time To Collision This effector calculates if an NPC can cause a collision with any ego, calculating current direction, velocity, and acceleration. The NPC will wait to proceed to the next waypoint such that a collision with the ego vehicle would occur. If no possible collision is found NPC does not wait. Wait For Distance Adding this effector makes NPC wait at the waypoint until any ego vehicle is closer than the max distance (in meters) set in this trigger effector. If no ego vehicle will get close enough, the NPC can wait infinitely. Wait Time The agent will wait for the fixed time (in seconds) at the waypoint before continuing movement towards the next waypoint. Waiting Point A waiting point makes the agent wait until any ego vehicle enters the activation zone. Activation zone can be moved like other scenario elements, by dragging the Move button. If it is required, the radius (in meters) of the activation zone can be changed in the Edit panel of the waypoint or by dragging the Resize button to the left (shrinking) or right (enlarging). Control Trigger This trigger effector will apply the edited policy to all marked controllables when the trigger is invoked. Click the Mark Controllables button and then click on the controllables on the map to mark them with this effector, right-click to cancel the marking process. Only one type of controllable can be marked by a Control Trigger, but multiple Control Triggers are allowed. See the Editing Controllables section to learn more about editing policies.","title":"Editing Triggers"},{"location":"creating-scenarios/visual-scenario-editor/#editing-controllables","text":"After selecting controllables in the VSE, the editing panel allows changing their default policy. A policy is a list of control actions that will be applied when the controllable is initialized, each policy entry represents a single control action. VSE fills the action dropdown with all the allowed actions for the edited controllable. The value can be edited either by an input field, the decimal input field for wait and trigger actions, dropdown for state actions, and no value edition for the loop action. Each policy entry can be removed with the button on the right of the policy row. A whole policy can be copied and pasted between controllables of the same type. The copy button (on the left from the Add Policy Entry button) copies the policy to the clipboard so it can be pasted to another controllable of the same type (button on the right from the Add Policy Entry button).","title":"Editing Controllables"},{"location":"creating-scenarios/visual-scenario-editor/#map-panel","text":"Map panel views every map available in your SVL Simulator account's \"My Library\" and allows switching the current map in the VSE. Changing the map always resets the scenario. Refer to the Maps section of My Library for more information on how to add maps to your library in the Web UI.","title":"Map Panel"},{"location":"creating-scenarios/visual-scenario-editor/#quick-edit-panel","text":"Selecting a map element (for example agent) with the left mouse button shows the quick edit panel above the selected element. Remove deletes the selected element from the scenario. Move allows to reposition a selected element with a drag and drop system. Use the right mouse button to cancel the reposition. Rotate allows rotating selected elements with drag and drop system, drag button to the left or right to rotate the selected object. Use the right mouse button to revert the rotation. Resize allows scaling selected elements with drag and drop system, drag button to the left (shrinking), or to the right (enlarging). Use the right mouse button to revert the resizing. Future updates will include a plugins system for custom quick edit buttons.","title":"Quick Edit Panel"},{"location":"creating-scenarios/visual-scenario-editor/#playback-panel","text":"The playback panel imitates some of the simulation behaviours to visualize the final scenario effect. Currently, only agents with waypoints imitate their movement. Playback can be controlled with the following controls: Play starts the playback. Pause freezes the playback at the current time. Stop stops the playback and resets time to the beginning. Timeline allows flexible time adjustment of the playback. Playback speed changes the time scale of the playback (available playback speeds: 1/4x, 1/2x, 1x, 2x, 4x).","title":"Playback Panel"},{"location":"creating-scenarios/visual-scenario-editor/#file-selector","text":"The file selector is a simple built-in UI dialog for selecting the destination file on the hard-drive. It allows to change manually the directory path, go one directory up, enter internal directories and select a file with a required extension. When saving a scenario, it can override an existing file, or it can be saved to a new file with a selected name.","title":"File Selector"},{"location":"creating-scenarios/visual-scenario-editor/#undo","text":"VSE includes an undo feature that allows reverting the last changes. Use the \"ctrl\"+\"z\" on your keyboard to revert the most recent operation. Undo manager registers multiple records on the stack, using undo applies the last operation record, and removes it from the stack. This way, the undo can be used multiple times to revert many most recent actions.","title":"Undo"},{"location":"creating-scenarios/visual-scenario-editor/#scenario-example","text":"The Visual scenario editor runtime template Simulation can run a created Scenario in the VSE.","title":"Scenario Example"},{"location":"digital-twin/gomentum-dtl/","text":"Example map: GoMentum Station Digital Twin Lite We are providing the GoMentum Station Digital Twin Lite (GoMentum DTL) map as an example of a point cloud based environment. The GoMentum Station area is a testing ground dedicated for autonomous vehicles located in Concord, California . We drove our data collection car and collected sensor data in this area for generating the GoMentum DTL map. Using the point cloud importer and point cloud renderer , the Simulator can dynamically load and unload the point clouds based on the location of the ego vehicles. Thus, there is no limitation on the size of the map with improved performance of simulations. Figure: Preview of GoMentum Station Digital Twin Lite map Prerequisites top # SVL Simulator GoMentum DTL (Digital Twin Lite) map assetbundle Apollo 5.0 Instructions for how to run with Apollo 5.0 top # Follow these steps to run Apollo on GoMentum DTL: Launch SVL Simulator Click on the Open Browser button to launch the simulator Web UI in a web browser In the Simulations section, locate Local-Random: GoMentum DTL (Apollo) simulation under the Available from Others tab and add it by clicking the Add button Select your cluster from the cluster dropdown menu in the General tab Select the Apollo 5.0 as your sensor configuration for the Lincoln2017MKZ vehicle in the Test case tab Select the Apollo 5.0 as your autopilot and enter your bridge IP address and port number in the Autopilot tab Click the Publish button to finish the simulation setup in the Publish tab (Optional) GoMentum DTL map and Lincoln2017MKZ vehicle should be automatically added to the Library . If not, please follow the instructions for Adding a map and Adding a vehicle to add them manually. In the Vehicles section under the Library , locate Lincoln2017MKZ vehicle and click it to go into a sensor configuration page Select the Apollo 5.0 sensor configuration If you see a notification for any missing plugins, click the Add to Library button and make sure that you have all the sensor plugins listed in the configuration added to the Plugins section under the Library Locate Local-Random: GoMentum DTL (Apollo) simulation in the Simulations tab and click the Run Simulation button at the bottom to start simulation Local-Random: GoMentum DTL (Apollo) simulation should now be up and running in the main window of the simulator Finally, launch Apollo 5.0 alongside SVL Simulator Figure: Apollo dreamview and SVL Simulator detecting NPCs on the road with sensor visualizations enabled Figure: Apollo dreamview and SVL Simulator detecting an NPC in an intersection from top-down view Figure: Rviz showing point clouds published to ROS from SVL Simulator Known issues top # Overall, Apollo will be able to drive around most areas of GoMentum DTL with no issues, but there are some known issues in GoMentum DTL as this is an early access version. Here is a list of known issues for the current release that will be fixed soon in future releases. Dynamic objects top # Some other cars were captured while collecting sensor data from the field and remain as dummy noises on some roads. Later, we will have a Dynamic object removal feature in the Digital Twin Lite pipeline which removes dynamic objects such as cars or pedestrians from point clouds during the data processing. Figure: Streaks of blur on the road due to a moving car captured in ROSBAG Baked-in shadows top # Colorized point clouds have shadows baked in already (e.g., under trees) because shadows were captured in camera images during a data collection phase. In the future release, we\u2019ll remove the baked-in shadows from images and use simulated shadows instead based on the sun position in the simulator. Figure: Baked-in shadows from trees and buildings on the road","title":"Digital Twin Lite example map"},{"location":"digital-twin/gomentum-dtl/#prerequisites","text":"SVL Simulator GoMentum DTL (Digital Twin Lite) map assetbundle Apollo 5.0","title":"Prerequisites"},{"location":"digital-twin/gomentum-dtl/#instructions-for-how-to-run-with-apollo-5.0","text":"Follow these steps to run Apollo on GoMentum DTL: Launch SVL Simulator Click on the Open Browser button to launch the simulator Web UI in a web browser In the Simulations section, locate Local-Random: GoMentum DTL (Apollo) simulation under the Available from Others tab and add it by clicking the Add button Select your cluster from the cluster dropdown menu in the General tab Select the Apollo 5.0 as your sensor configuration for the Lincoln2017MKZ vehicle in the Test case tab Select the Apollo 5.0 as your autopilot and enter your bridge IP address and port number in the Autopilot tab Click the Publish button to finish the simulation setup in the Publish tab (Optional) GoMentum DTL map and Lincoln2017MKZ vehicle should be automatically added to the Library . If not, please follow the instructions for Adding a map and Adding a vehicle to add them manually. In the Vehicles section under the Library , locate Lincoln2017MKZ vehicle and click it to go into a sensor configuration page Select the Apollo 5.0 sensor configuration If you see a notification for any missing plugins, click the Add to Library button and make sure that you have all the sensor plugins listed in the configuration added to the Plugins section under the Library Locate Local-Random: GoMentum DTL (Apollo) simulation in the Simulations tab and click the Run Simulation button at the bottom to start simulation Local-Random: GoMentum DTL (Apollo) simulation should now be up and running in the main window of the simulator Finally, launch Apollo 5.0 alongside SVL Simulator Figure: Apollo dreamview and SVL Simulator detecting NPCs on the road with sensor visualizations enabled Figure: Apollo dreamview and SVL Simulator detecting an NPC in an intersection from top-down view Figure: Rviz showing point clouds published to ROS from SVL Simulator","title":"Instructions for how to run with Apollo 5.0"},{"location":"digital-twin/gomentum-dtl/#known-issues","text":"Overall, Apollo will be able to drive around most areas of GoMentum DTL with no issues, but there are some known issues in GoMentum DTL as this is an early access version. Here is a list of known issues for the current release that will be fixed soon in future releases.","title":"Known issues"},{"location":"digital-twin/gomentum-dtl/#dynamic-objects","text":"Some other cars were captured while collecting sensor data from the field and remain as dummy noises on some roads. Later, we will have a Dynamic object removal feature in the Digital Twin Lite pipeline which removes dynamic objects such as cars or pedestrians from point clouds during the data processing. Figure: Streaks of blur on the road due to a moving car captured in ROSBAG","title":"Dynamic objects"},{"location":"digital-twin/gomentum-dtl/#baked-in-shadows","text":"Colorized point clouds have shadows baked in already (e.g., under trees) because shadows were captured in camera images during a data collection phase. In the future release, we\u2019ll remove the baked-in shadows from images and use simulated shadows instead based on the sun position in the simulator. Figure: Baked-in shadows from trees and buildings on the road","title":"Baked-in shadows"},{"location":"distributed-simulation/custom-distributed-class/","text":"Custom Distributed Class Distributed system supports sending distributed messages from any class implementing IMessageSender to the objects of classes IMessageReceiver . A custom class can implement both interfaces, address key property will be shared. Address Key top # Key property of the IMessageSender and IMessageReceiver is an address for the distributed messages. This key must be globally unique and deterministic, on every machine in every Simulation run the key of an object has to return the same value. Registration top # Every object that is going to send or receive distributed messages has to register itself in the MessagesManager . In the Simulator MessagesManager instance is available in the: Loader.Instance.Network.MessagesManager , this property has a null reference if the simulation does not use a cluster. Whenever an object is ready to send or receive distributed messages it has to register itself with the RegisterObject method of the MessagesManager and when an object will no longer send or receiver messages it has to unregister itself with UnregisterObject method of the MessagesManager . Messages received for the unregistered address key will be stored and passed to proper objects after registration. Message Sender top # IMessageSender implementation requires: UnicastMessage , a basic implementation invokes UnicastMessage method of an MessagesManager instance; BroadcastMessage , a basic implementation invokes BroadcastMessage method of an MessagesManager instance; UnicastInitialMessages , requires sending every data required for the object initialization using UnicastMessage method. Message Receiver top # IMessageReceiver implementation requires: ReceiveMessage , parses every incomming data after object is registered.","title":"Custom Distributed Class [](#top)"},{"location":"distributed-simulation/custom-distributed-class/#address-key","text":"Key property of the IMessageSender and IMessageReceiver is an address for the distributed messages. This key must be globally unique and deterministic, on every machine in every Simulation run the key of an object has to return the same value.","title":"Address Key"},{"location":"distributed-simulation/custom-distributed-class/#registration","text":"Every object that is going to send or receive distributed messages has to register itself in the MessagesManager . In the Simulator MessagesManager instance is available in the: Loader.Instance.Network.MessagesManager , this property has a null reference if the simulation does not use a cluster. Whenever an object is ready to send or receive distributed messages it has to register itself with the RegisterObject method of the MessagesManager and when an object will no longer send or receiver messages it has to unregister itself with UnregisterObject method of the MessagesManager . Messages received for the unregistered address key will be stored and passed to proper objects after registration.","title":"Registration"},{"location":"distributed-simulation/custom-distributed-class/#message-sender","text":"IMessageSender implementation requires: UnicastMessage , a basic implementation invokes UnicastMessage method of an MessagesManager instance; BroadcastMessage , a basic implementation invokes BroadcastMessage method of an MessagesManager instance; UnicastInitialMessages , requires sending every data required for the object initialization using UnicastMessage method.","title":"Message Sender"},{"location":"distributed-simulation/custom-distributed-class/#message-receiver","text":"IMessageReceiver implementation requires: ReceiveMessage , parses every incomming data after object is registered.","title":"Message Receiver"},{"location":"distributed-simulation/distributed-components/","text":"Distributed Components DistributedComponent is an abstract class with the implementation of sending snapshots from the authoritative object to all connected peers. For example DistributedTransform synchronize transforms states in the clients' simulations to the corresponding transform state on the master basing on the position, rotation and scale sent in the snapshots. DistributedTransform component added to GameObject in the simulation will synchronize the transform, note that every DistributedComponent requires a DistributedObject added to the same GameObject or any parent GameObject. Custom Distributed Component top # Extending the DistributedComponent component requires the following implementations: ComponentKey property, if multiple components of this type are allowed in a single GameObject it has to be a unique key otherwise, it can be for example class name. GetSnapshot method which returns snapshot data inside a ByteStack object (refer to Distributed Messages for more information about ByteStack ). ApplySnapshot method which parses and applies the snapshot data from the message content to the object. Note that the ApplySnapshot method has to pop data in reverse order than GetSnapshot is pushing data. Distributed Components With Deltas top # A snapshot includes data required to recreate the same state on the client. Sending the whole snapshot with a whole object's state when only a single element changes will contain too much redundant data. DistributedComponentWithDeltas extends the basic implementation with two methods: SendDelta which sends the message with passed delta data inside a ByteStack object. ApplyDelta abstract method which parses and applies the delta data from the message content to the object. Note that the SendDelta method has to pop data in reverse order than ApplyDelta is pushing data. Distributed Transform top # DistributedTransform sends local position, local rotation and local scale of a transform component in the snapshots from the master to the clients. Only a single DistributedTransform component can be attached to a GameObject. Snapshots are send up to 60 times per second only if any element of the snapshot changes. The snapshots limit can be changed on the master by changing the SnapshotsPerSecondLimit property value Distributed Rigidbody top # DistributedRigidbody sends the position and rotation of the rigidbody in the same GameObject from the master to the clients. With the default setting snapshots are just applied to the rigidbodies. It is possible to change SimulationType to ExtrapolateVelocities , with this setting DistributedRigidbody extrapolates received velocity and angular velocity. Applied position and rotation includes the corrections calculated from the extrapolated velocities. Snapshots are send up to 60 times per second only if rigidbody is not in sleeping mode. The snapshots limit can be changed on the master by changing the SnapshotsPerSecondLimit property value","title":"Distributed Components [](#top)"},{"location":"distributed-simulation/distributed-components/#custom-distributed-component","text":"Extending the DistributedComponent component requires the following implementations: ComponentKey property, if multiple components of this type are allowed in a single GameObject it has to be a unique key otherwise, it can be for example class name. GetSnapshot method which returns snapshot data inside a ByteStack object (refer to Distributed Messages for more information about ByteStack ). ApplySnapshot method which parses and applies the snapshot data from the message content to the object. Note that the ApplySnapshot method has to pop data in reverse order than GetSnapshot is pushing data.","title":"Custom Distributed Component"},{"location":"distributed-simulation/distributed-components/#distributed-components-with-deltas","text":"A snapshot includes data required to recreate the same state on the client. Sending the whole snapshot with a whole object's state when only a single element changes will contain too much redundant data. DistributedComponentWithDeltas extends the basic implementation with two methods: SendDelta which sends the message with passed delta data inside a ByteStack object. ApplyDelta abstract method which parses and applies the delta data from the message content to the object. Note that the SendDelta method has to pop data in reverse order than ApplyDelta is pushing data.","title":"Distributed Components With Deltas"},{"location":"distributed-simulation/distributed-components/#distributed-transform","text":"DistributedTransform sends local position, local rotation and local scale of a transform component in the snapshots from the master to the clients. Only a single DistributedTransform component can be attached to a GameObject. Snapshots are send up to 60 times per second only if any element of the snapshot changes. The snapshots limit can be changed on the master by changing the SnapshotsPerSecondLimit property value","title":"Distributed Transform"},{"location":"distributed-simulation/distributed-components/#distributed-rigidbody","text":"DistributedRigidbody sends the position and rotation of the rigidbody in the same GameObject from the master to the clients. With the default setting snapshots are just applied to the rigidbodies. It is possible to change SimulationType to ExtrapolateVelocities , with this setting DistributedRigidbody extrapolates received velocity and angular velocity. Applied position and rotation includes the corrections calculated from the extrapolated velocities. Snapshots are send up to 60 times per second only if rigidbody is not in sleeping mode. The snapshots limit can be changed on the master by changing the SnapshotsPerSecondLimit property value","title":"Distributed Rigidbody"},{"location":"distributed-simulation/distributed-messages/","text":"Distributed Messages DistributedMessage is a class that is used to exchange data between peers in the distributed system. Every message contains the Content where the actual data is stored and meta-data like address key, message type, and timestamp. Address Key top # AddressKey determines which object will receive the message on other simulations. DistributedComponent uses own key as an address key so the message will be received by corresponding components on other machines. AddressKey has to be set when sending a new message. Content top # Content contains the actual data of the message in the BytesStack object. Note that poping data required reverse order than pushing. Message Type top # DistributedMessageType determines how the message will be handled in the UDP protocol. This type has to be set when sending a new message. Available distributed message types: ReliableUnordered packets won't be dropped, won't be duplicated, can arrive without order. Sequenced packets can be dropped, won't be duplicated, will arrive in order. ReliableOrdered packets won't be dropped, won't be duplicated, will arrive in order. ReliableSequenced packets can be dropped (except the last one), won't be duplicated, will arrive in order. Unreliable packets can be dropped, can be duplicated, can arrive without order. ReliableOrdered messages are the most reliable, but may add significant delays, this type is best for the initialization messages like add new NPC command. Unreliable packets have the lowest delay between sending and handling a message but are not reliable. Snapshots are sent as unreliable , an incoming snapshot will override the data and delayed packets will not be used. Timestamp top # The distributed system adds UTC timestamp to every message right before sending it, there is no need to set timestamp value when sending a message. A timestamp of the received message is set before passing the message to the addressed object. This timestamp is already corrected on the client by the value of connection latency and shows the approximate UTC DateTime when the message has been sent by the master. Bytes Stack top # BytesStack is the message's content where the data is stored. Every data has to be represented as a set of bytes and has to be pushed to the stack. BytesStack class supports byte , int , uint , long , float , double , bool and string values. Operations that can be executed on those values are: Push - adds the value on the top of the stack; Fetch - reads the data from the top of the stack and does not remove it from the stack; Pop - reads the data from the top of the stack and removed it from the stack, note that Pop calls have to be called in reverse order than Push . Byte Compression top # ByteCompression class adds extension methods to the BytesStack which add different values to the stack with limited bytes count: CompressFloatToInt and DecompressFloatFromInt ; PushEnum and PopEnum ; PushCompressedColor and PopDecompressedColor ; PushCompressedVector3 and PushCompressedVector3 ; PushUncompressedVector3 and PopUncompressedVector3 ; PushCompressedPosition and PopDecompressedPosition - position bounds are limited to the map bounds; PushCompressedRotation and PopDecompressedRotation .","title":"Distributed Messages [](#top)"},{"location":"distributed-simulation/distributed-messages/#address-key","text":"AddressKey determines which object will receive the message on other simulations. DistributedComponent uses own key as an address key so the message will be received by corresponding components on other machines. AddressKey has to be set when sending a new message.","title":"Address Key"},{"location":"distributed-simulation/distributed-messages/#content","text":"Content contains the actual data of the message in the BytesStack object. Note that poping data required reverse order than pushing.","title":"Content"},{"location":"distributed-simulation/distributed-messages/#message-type","text":"DistributedMessageType determines how the message will be handled in the UDP protocol. This type has to be set when sending a new message. Available distributed message types: ReliableUnordered packets won't be dropped, won't be duplicated, can arrive without order. Sequenced packets can be dropped, won't be duplicated, will arrive in order. ReliableOrdered packets won't be dropped, won't be duplicated, will arrive in order. ReliableSequenced packets can be dropped (except the last one), won't be duplicated, will arrive in order. Unreliable packets can be dropped, can be duplicated, can arrive without order. ReliableOrdered messages are the most reliable, but may add significant delays, this type is best for the initialization messages like add new NPC command. Unreliable packets have the lowest delay between sending and handling a message but are not reliable. Snapshots are sent as unreliable , an incoming snapshot will override the data and delayed packets will not be used.","title":"Message Type"},{"location":"distributed-simulation/distributed-messages/#timestamp","text":"The distributed system adds UTC timestamp to every message right before sending it, there is no need to set timestamp value when sending a message. A timestamp of the received message is set before passing the message to the addressed object. This timestamp is already corrected on the client by the value of connection latency and shows the approximate UTC DateTime when the message has been sent by the master.","title":"Timestamp"},{"location":"distributed-simulation/distributed-messages/#bytes-stack","text":"BytesStack is the message's content where the data is stored. Every data has to be represented as a set of bytes and has to be pushed to the stack. BytesStack class supports byte , int , uint , long , float , double , bool and string values. Operations that can be executed on those values are: Push - adds the value on the top of the stack; Fetch - reads the data from the top of the stack and does not remove it from the stack; Pop - reads the data from the top of the stack and removed it from the stack, note that Pop calls have to be called in reverse order than Push .","title":"Bytes Stack"},{"location":"distributed-simulation/distributed-messages/#byte-compression","text":"ByteCompression class adds extension methods to the BytesStack which add different values to the stack with limited bytes count: CompressFloatToInt and DecompressFloatFromInt ; PushEnum and PopEnum ; PushCompressedColor and PopDecompressedColor ; PushCompressedVector3 and PushCompressedVector3 ; PushUncompressedVector3 and PopUncompressedVector3 ; PushCompressedPosition and PopDecompressedPosition - position bounds are limited to the map bounds; PushCompressedRotation and PopDecompressedRotation .","title":"Byte Compression"},{"location":"distributed-simulation/distributed-objects/","text":"Distributed Objects DistributedObject component synchronizes the GameObject state on the cluster simulation clients. DistributedObject can limit broadcasts sent by the DistributedComponents added to the children GameObjects and by default changes the part of components' keys. Custom Key top # Every object synchronized in the cluster simulation required a unique key that is the same on every machine and cannot change after initialization. DistributedObject uses own path in the hierarchy as the unique key as a default implementation, but the synchronization will fail if there are two GameObjects with the same name in the same hierarchy path. The key of a DistributedObject can be changed by any component attached to the same GameObject which implements the IGloballyUniquelyIdentified interface. When registering a DistributedObject search if there is an IGloballyUniquelyIdentified implementation attached to the GameObject if an implementation is available GUID is used as the key prefix. If the GUID is null or empty registration waits until GUID changes to a not empty value. Advanced top # DistributedObject implementation provides features that require additional scripting. This section describe advanced use cases that can be achieved with the DistributedObject . Selective Distribution top # Some objects should not be distributed to all the connected clients. It is possible to limit the distribution by setting the Selective Distribution to True and adding endpoints to the list (using AddEndPointToSelectiveDistribution method). With Selective Distribution enabled only endpoints on the list will receive updates from the DistributedObject and all the DistributedComponents in children. Selective distribution is prepared for the use-case when more logic has to be handled by the client. For example, if a client should count the waypoints of selected NPC those waypoints have to be synchronized between that client and master. In this case, pathfinding will be calculated on the client, the master will perform the simulation updates and other clients will not be informed about those changes. Authoritative Object top # Available distributed components in the Simulator got different logic for the master and client. All the objects on the master are authoritative and send their state to the clients, where nonauthoritative objects apply the changes. This behavior can be changed by setting the custom value to the protected property IsAuthoritative , but it has to be done in the overridden Initialize method before calling the base method. In the case from the previous paragraph Selective Distribution client would perform the pathfinding and set the waypoints positions. The client sets the position of those GameObjects and has to send them to the master. To reverse the behavior where a master is sending the position of a DistributedRigidbody to the client, IsAuthoritative property has to be reversed ( false value on the master and true on the client). This way client will update the master with the Rigidbody changes.","title":"Distributed Objects [](#top)"},{"location":"distributed-simulation/distributed-objects/#custom-key","text":"Every object synchronized in the cluster simulation required a unique key that is the same on every machine and cannot change after initialization. DistributedObject uses own path in the hierarchy as the unique key as a default implementation, but the synchronization will fail if there are two GameObjects with the same name in the same hierarchy path. The key of a DistributedObject can be changed by any component attached to the same GameObject which implements the IGloballyUniquelyIdentified interface. When registering a DistributedObject search if there is an IGloballyUniquelyIdentified implementation attached to the GameObject if an implementation is available GUID is used as the key prefix. If the GUID is null or empty registration waits until GUID changes to a not empty value.","title":"Custom Key"},{"location":"distributed-simulation/distributed-objects/#advanced","text":"DistributedObject implementation provides features that require additional scripting. This section describe advanced use cases that can be achieved with the DistributedObject .","title":"Advanced"},{"location":"distributed-simulation/distributed-objects/#selective-distribution","text":"Some objects should not be distributed to all the connected clients. It is possible to limit the distribution by setting the Selective Distribution to True and adding endpoints to the list (using AddEndPointToSelectiveDistribution method). With Selective Distribution enabled only endpoints on the list will receive updates from the DistributedObject and all the DistributedComponents in children. Selective distribution is prepared for the use-case when more logic has to be handled by the client. For example, if a client should count the waypoints of selected NPC those waypoints have to be synchronized between that client and master. In this case, pathfinding will be calculated on the client, the master will perform the simulation updates and other clients will not be informed about those changes.","title":"Selective Distribution"},{"location":"distributed-simulation/distributed-objects/#authoritative-object","text":"Available distributed components in the Simulator got different logic for the master and client. All the objects on the master are authoritative and send their state to the clients, where nonauthoritative objects apply the changes. This behavior can be changed by setting the custom value to the protected property IsAuthoritative , but it has to be done in the overridden Initialize method before calling the base method. In the case from the previous paragraph Selective Distribution client would perform the pathfinding and set the waypoints positions. The client sets the position of those GameObjects and has to send them to the master. To reverse the behavior where a master is sending the position of a DistributedRigidbody to the client, IsAuthoritative property has to be reversed ( false value on the master and true on the client). This way client will update the master with the Rigidbody changes.","title":"Authoritative Object"},{"location":"distributed-simulation/distributed-python-api/","text":"Distributed Python API Cluster Simulation performs changes only on the master simulation, clients' simulations apply the changes received from the master and don't require to react on every API command. Only selected commands are distributed to the clients, for example, AddAgent , LoadScene and Reset commands. Command Setup top # If command should be distributed to the clients it has to implement the IDistributedObject interface. Master simulation can modify the arguments that will be sent to the clients inside the Execute methods. Asynchronous Commands top # Some commands, like AddAgent and LoadScene , requires more time to execute as they may download required Asset Bundles from the server. In this case master simulation has to wait for all the cluster machines to execute the command. Simulator uses ILockingCommand interface to solve this use case. Commands that implement this interface locks the Python API process until command's Executed action is invoked on every machine.","title":"Distributed Python API [](#top)"},{"location":"distributed-simulation/distributed-python-api/#command-setup","text":"If command should be distributed to the clients it has to implement the IDistributedObject interface. Master simulation can modify the arguments that will be sent to the clients inside the Execute methods.","title":"Command Setup"},{"location":"distributed-simulation/distributed-python-api/#asynchronous-commands","text":"Some commands, like AddAgent and LoadScene , requires more time to execute as they may download required Asset Bundles from the server. In this case master simulation has to wait for all the cluster machines to execute the command. Simulator uses ILockingCommand interface to solve this use case. Commands that implement this interface locks the Python API process until command's Executed action is invoked on every machine.","title":"Asynchronous Commands"},{"location":"distributed-simulation/distributed-simulation-introduction/","text":"Introduction to Distributed Simulation What is Distributed Simulation? top # Distributed simulation enables a cluster to run with multiple computing instances (main, clients). This enables you to run simulations that may require a large number of sensors, or with multiple ego vehicles, which may normally require a large amount of computing resources. Distributed simulation solves the inherent limitations of trying to run real-time simulations on one single-GPU instance by synchronizing across multiple machines (each with a GPU). What is a cluster? top # A Cluster is a single unit of simulation. It includes one or more machines that will perform a simulation. There is always one main machine in every cluster; the main machine performs the simulation updates like physics and synchronizes the simulation state with every client. See Clusters for details on editing clusters in the SVL Simulator web user interface. Simulation Synchronization top # For the proper functioning of the sensors distributed to the clients, it is required to synchronize the whole simulation environment. When the simulation changes on main, the main sends update messages to every client. Simulator by default synchronizes every Rigidbody in ego vehicles, NPCs, pedestrians and controllable objects instantiated by Simulator Manager. Main receives and distributes vehicle actions like setting lights state or wipers state, and distributes controllables control actions so every client can clone the simulation state. Components Synchronization top # Every component that has to be synchronized between cluster machines requires messages sender on the main and messages receiver on the clients. Taking a vehicle with Rigidbody as an example, Simulator adds DistributedObject component to the vehicle GameObject to synchronize enable and disable calls and DistributedRigidbody components which send required data to mock the state of Rigidbody on clients. By default, it is required that each DistributedObject has a unique path in the hierarchy - objects on the same hierarchy level require unique GameObject names. See Distributed Objects for details on distributing simulation objects. See Distributed Components for details on distributing components data. Sensors Distribution top # Main machine delegates sensors to clients, every sensor is simulated only on one machine. Some sensors like controls are simulated only on the main machine, and some sensors like camera sensors will always be delegated to the clients. See Sensors Distribution for details on distributing sensors. Custom Implementations top # Distributed simulation can be extended with custom implementations. See Distributed Python API for details on distributing simulation objects. See Custom Distributed Class for details on implementing a custom class that will distribute messages. See Distributed Messages for details on messages sent and received in the components.","title":"Introduction"},{"location":"distributed-simulation/distributed-simulation-introduction/#what-is-distributed-simulation","text":"Distributed simulation enables a cluster to run with multiple computing instances (main, clients). This enables you to run simulations that may require a large number of sensors, or with multiple ego vehicles, which may normally require a large amount of computing resources. Distributed simulation solves the inherent limitations of trying to run real-time simulations on one single-GPU instance by synchronizing across multiple machines (each with a GPU).","title":"What is Distributed Simulation"},{"location":"distributed-simulation/distributed-simulation-introduction/#what-is-a_cluster","text":"A Cluster is a single unit of simulation. It includes one or more machines that will perform a simulation. There is always one main machine in every cluster; the main machine performs the simulation updates like physics and synchronizes the simulation state with every client. See Clusters for details on editing clusters in the SVL Simulator web user interface.","title":"What is a cluster"},{"location":"distributed-simulation/distributed-simulation-introduction/#simulation-synchronization","text":"For the proper functioning of the sensors distributed to the clients, it is required to synchronize the whole simulation environment. When the simulation changes on main, the main sends update messages to every client. Simulator by default synchronizes every Rigidbody in ego vehicles, NPCs, pedestrians and controllable objects instantiated by Simulator Manager. Main receives and distributes vehicle actions like setting lights state or wipers state, and distributes controllables control actions so every client can clone the simulation state.","title":"Simulation Synchronization"},{"location":"distributed-simulation/distributed-simulation-introduction/#components-synchronization","text":"Every component that has to be synchronized between cluster machines requires messages sender on the main and messages receiver on the clients. Taking a vehicle with Rigidbody as an example, Simulator adds DistributedObject component to the vehicle GameObject to synchronize enable and disable calls and DistributedRigidbody components which send required data to mock the state of Rigidbody on clients. By default, it is required that each DistributedObject has a unique path in the hierarchy - objects on the same hierarchy level require unique GameObject names. See Distributed Objects for details on distributing simulation objects. See Distributed Components for details on distributing components data.","title":"Components Synchronization"},{"location":"distributed-simulation/distributed-simulation-introduction/#sensors-distribution","text":"Main machine delegates sensors to clients, every sensor is simulated only on one machine. Some sensors like controls are simulated only on the main machine, and some sensors like camera sensors will always be delegated to the clients. See Sensors Distribution for details on distributing sensors.","title":"Sensor Distribution"},{"location":"distributed-simulation/distributed-simulation-introduction/#custom-implementations","text":"Distributed simulation can be extended with custom implementations. See Distributed Python API for details on distributing simulation objects. See Custom Distributed Class for details on implementing a custom class that will distribute messages. See Distributed Messages for details on messages sent and received in the components.","title":"Custom Implementation"},{"location":"distributed-simulation/running-distributed-simulation/","text":"Running a Distributed Simulation When distributed simulation starts working, a simulation on each machine will start, and you can check the list of sensors allocated to the machine in Sensor Visualizers . Requirements top # Distributed simulation needs multiple computers connected to the same router. The operating system of each computer can be either Linux or Windows 10. In this document, we'll use: Two Linux PCs (Master, Client PC) Apollo 5 on Master PC Instructions top # Install Apollo 5 top # If you already installed and built Apollo 5, jump to launching Apollo. docker pull lgsvl/apollo-5.0 git clone --recurse-submodules https://github.com/lgsvl/apollo-5.0.git ~/apollo-5.0 Building Apollo and bridge top # cd ~/apollo-5.0 ./docker/scripts/dev_start.sh ./docker/scripts/dev_into.sh ./apollo.sh build_gpu Launching Apollo 5 top # bootstrap.sh bridge.sh You can check the network address of your bridge. This network address should be used in bridge setup in the web user interface. Launching SVL Simulator and Setting Cluster top # In Master Simulator, Run Simulator. Click LINK TO CLOUD. In the Create new cluster tab, give cluster name into New Cluster and click Create cluster. In Client Simulator, Run Simulator. Click LINK TO CLOUD. In the Add to Existing cluster tab, choose cluster name that master simulator made and click Create cluster. After this, Clusters should show one cluster which has two computer name together. If this setup doesn't work, you should remove your pre-existed cluster. Make sure that you now have a cluster which has each machine listed together, like the following: Adding a Map top # If you already added a map you want to use, jump to Adding a vehicle . In the left tab, under Store, click on Maps and enter a map's name. Here we're gonna use BorregasAve map. Click on + icon. You can check the message in the left bottom like the following: \"BorregasAve\" has been successfully added to your library. Adding a Vehicle top # If you already added a vehicle you want to use, jump to Creating simulations . In the left tab, under Store, click on Vehicles and enter a vehicle's name. Here we're gonna use Lincoln2017MKZ. Click on + icon. You can check the message in the left bottom like the following: \"Lincoln2017MKZ\" has been successfully added to your library. Adding sensors top # In the left tab, click Vehicles under Library. Choose Lincoln2017MKZ. Click Apollo 5.0 under Sensor Configurations. If you see exclamation mark, click Add to Library button. Creating Simulations in Web User Interface top # Below is how to make a new simulation with a distributed simulation cluster setup. In the left side, click Simulations. Click Add New. Give name in the Simulation Name. Select cluster name in the Select Cluster. Choose other setups like test report, headless mode, interactive mode as you wish and click next. Select Random Traffic in Runtime Template. Choose map and vehicle essentially and others optionally and click next. Choose Apollo 5.0 in the Autopilot and enter exact your IP address plus port 9090 where Apollo 5 is running and click next. Click Publish. Running Simulation top # Click Run Simulation. Make sure the simulation is started on both the master and client machines. Checking Bridge Status top # Click a power cord shape icon in the bottom and make sure that master and client simulator have connected to bridge in the left tab. Visualizing Sensors top # Click the an eye shape icon and make sure master and client simulators have own sensor lists automatically allocated by distributed simulation setup. Make sure that master simulator has light-computational sensors like CAN bus, IMU, etc. and the client has heavy-computational sensors like LiDARs, Cameras.","title":"Running distributed simulation"},{"location":"distributed-simulation/running-distributed-simulation/#instructions","text":"","title":"Instructions"},{"location":"distributed-simulation/running-distributed-simulation/#install-apollo-5","text":"If you already installed and built Apollo 5, jump to launching Apollo. docker pull lgsvl/apollo-5.0 git clone --recurse-submodules https://github.com/lgsvl/apollo-5.0.git ~/apollo-5.0","title":"Install Apollo 5 top"},{"location":"distributed-simulation/running-distributed-simulation/#building-apollo-and-bridge","text":"cd ~/apollo-5.0 ./docker/scripts/dev_start.sh ./docker/scripts/dev_into.sh ./apollo.sh build_gpu","title":"Building Apollo and bridge top"},{"location":"distributed-simulation/running-distributed-simulation/#launching-apollo-5","text":"bootstrap.sh bridge.sh You can check the network address of your bridge. This network address should be used in bridge setup in the web user interface.","title":"Launching Apollo 5 top"},{"location":"distributed-simulation/running-distributed-simulation/#launching-svl-simulator-and-setting-cluster","text":"In Master Simulator, Run Simulator. Click LINK TO CLOUD. In the Create new cluster tab, give cluster name into New Cluster and click Create cluster. In Client Simulator, Run Simulator. Click LINK TO CLOUD. In the Add to Existing cluster tab, choose cluster name that master simulator made and click Create cluster. After this, Clusters should show one cluster which has two computer name together. If this setup doesn't work, you should remove your pre-existed cluster. Make sure that you now have a cluster which has each machine listed together, like the following:","title":"Launching SVL Simulator and Setting Cluster top"},{"location":"distributed-simulation/running-distributed-simulation/#adding-a-map","text":"If you already added a map you want to use, jump to Adding a vehicle . In the left tab, under Store, click on Maps and enter a map's name. Here we're gonna use BorregasAve map. Click on + icon. You can check the message in the left bottom like the following: \"BorregasAve\" has been successfully added to your library.","title":"Adding a Map top"},{"location":"distributed-simulation/running-distributed-simulation/#adding-a-vehicle","text":"If you already added a vehicle you want to use, jump to Creating simulations . In the left tab, under Store, click on Vehicles and enter a vehicle's name. Here we're gonna use Lincoln2017MKZ. Click on + icon. You can check the message in the left bottom like the following: \"Lincoln2017MKZ\" has been successfully added to your library.","title":"Adding  a Vehicle top"},{"location":"distributed-simulation/running-distributed-simulation/#adding-sensors","text":"In the left tab, click Vehicles under Library. Choose Lincoln2017MKZ. Click Apollo 5.0 under Sensor Configurations. If you see exclamation mark, click Add to Library button.","title":"Adding sensors top"},{"location":"distributed-simulation/running-distributed-simulation/#creating-simulations-in-web-user-interface","text":"Below is how to make a new simulation with a distributed simulation cluster setup. In the left side, click Simulations. Click Add New. Give name in the Simulation Name. Select cluster name in the Select Cluster. Choose other setups like test report, headless mode, interactive mode as you wish and click next. Select Random Traffic in Runtime Template. Choose map and vehicle essentially and others optionally and click next. Choose Apollo 5.0 in the Autopilot and enter exact your IP address plus port 9090 where Apollo 5 is running and click next. Click Publish.","title":"Creating Simulations in Web User Interface top"},{"location":"distributed-simulation/running-distributed-simulation/#running-simulation","text":"Click Run Simulation. Make sure the simulation is started on both the master and client machines.","title":"Running Simulation top"},{"location":"distributed-simulation/running-distributed-simulation/#checking-bridge-status","text":"Click a power cord shape icon in the bottom and make sure that master and client simulator have connected to bridge in the left tab.","title":"Checking Bridge Status top"},{"location":"distributed-simulation/running-distributed-simulation/#visualizing-sensors","text":"Click the an eye shape icon and make sure master and client simulators have own sensor lists automatically allocated by distributed simulation setup. Make sure that master simulator has light-computational sensors like CAN bus, IMU, etc. and the client has heavy-computational sensors like LiDARs, Cameras.","title":"Visualizing Sensors top"},{"location":"distributed-simulation/sensors-distribution/","text":"Sensors Distribution Distribution sensors between different machines are the advantage of the cluster simulation. Distributed simulation synchronizes sensors' transforms between every cluster machine. One sensor can be simulated only on one machine, but one machine can still simulate multiple sensors. Sensor Setup top # Sensors distributed to the clients will not be simulated on the main, those sensors will not affect the simulation (for example manual control sensor has to be simulated on the main) and API requests callbacks will be delayed. Due to these restrictions distribution is disabled by default and enabling it requires additional setup. Every sensor which can be distributed to clients has to override the MainOnly value of the DistributionType property with MainOrClient or ClientOnly value. Sensors Load Balancing top # The current load balancing algorithm divides sensors into groups by their DistributionType type. Main distributes sensors by counting overhead sum and assigning each next sensor to the least overloaded machine available for the selected distribution type. Sensors can override the PerformanceLoad property to determine their load. The value 0.0 means that the sensor will not impact the performance at all, and the value 1.0 means that that the distributed simulation will try to simulate this sensor on a dedicated machine if possible, default value is 0.5 . Example sensors configurations are listed below: 1.0 PerformanceLoad for sensors like LiDAR , which parses the camera images and performs complex maths operations, should be classified under ClientOnly DistributionType type. 0.2 PerformanceLoad for sensors like RadarSensor , which performs complex maths operations, should be classified under MainOrClient DistributionType type. 0.05 PerformanceLoad for sensors like GpsSensor , which performs simple maths operations, should be classified under MainOrClient DistributionType type. Sensors like KeyboardControlSensor , which control objects in a simulation, have to be classified under MainOnly DistributionType type. As the main machine requires more resources it starts the algorithm with 0.15 overhead.","title":"Sensors Distribution [](#top)"},{"location":"distributed-simulation/sensors-distribution/#sensor-setup","text":"Sensors distributed to the clients will not be simulated on the main, those sensors will not affect the simulation (for example manual control sensor has to be simulated on the main) and API requests callbacks will be delayed. Due to these restrictions distribution is disabled by default and enabling it requires additional setup. Every sensor which can be distributed to clients has to override the MainOnly value of the DistributionType property with MainOrClient or ClientOnly value.","title":"Sensor Setup"},{"location":"distributed-simulation/sensors-distribution/#sensors-load-balancing","text":"The current load balancing algorithm divides sensors into groups by their DistributionType type. Main distributes sensors by counting overhead sum and assigning each next sensor to the least overloaded machine available for the selected distribution type. Sensors can override the PerformanceLoad property to determine their load. The value 0.0 means that the sensor will not impact the performance at all, and the value 1.0 means that that the distributed simulation will try to simulate this sensor on a dedicated machine if possible, default value is 0.5 . Example sensors configurations are listed below: 1.0 PerformanceLoad for sensors like LiDAR , which parses the camera images and performs complex maths operations, should be classified under ClientOnly DistributionType type. 0.2 PerformanceLoad for sensors like RadarSensor , which performs complex maths operations, should be classified under MainOrClient DistributionType type. 0.05 PerformanceLoad for sensors like GpsSensor , which performs simple maths operations, should be classified under MainOrClient DistributionType type. Sensors like KeyboardControlSensor , which control objects in a simulation, have to be classified under MainOnly DistributionType type. As the main machine requires more resources it starts the algorithm with 0.15 overhead.","title":"Sensors Load Balancing"},{"location":"getting-started/conventions/","text":"Conventions Table of Contents SVL Simulator Coordinate Systems Converting Between Coordinate Systems Map-Origin Aligning Map Origin to real world coordinates Map-Orientation-in-Unity Vehicle-Root-Position Setting up BaseLink Transform-Sensor Customizing-Sensor-Configurations SVL Simulator Coordinate Systems top # Inside the simulator positions and rotations are represented by Transforms in Unity. Unity uses a left-handed ZXY coordinate system for transforms with the Y-axis being the vertical axis. Each simulation object such as vehicles, sensors, maps, traffic lights, etc. will have a transform associated with it. Understanding the transforms is especially important for defining custom sensor configurations for ego vehicles. Converting between coordinate systems top # The right-handed XYZ coordinate system is more commonly used in Robotics and Autonomous Vehicle applications and users may require to convert coordinate systems for some use-cases when using the SVL Simulator. The tables below summarize conversions for 3D position vectors: ZXY (Unity) XYZ (RH) Z X X -Y Y Z XYZ (RH) ZXY (Unity) X Z Y -X Z Y The tables below summarize conversions for 3D rotation vectors: ZXY (Unity) XYZ (RH) Z -X X Y Y -Z XYZ (RH) ZXY (Unity) X -Z Y X Z -Y The tables below summarize conversions for quaternions: ZXY (Unity) XYZ (RH) Z -X X Y Y -Z W W XYZ (RH) ZXY (Unity) X Y Y -Z Z -X W W Map Origin top # Each simulator map includes a gameObject called MapOrigin . The Map Origin is the reference point of the map within Unity and has coordinate values of (0, 0, 0). The real-world position of the Map Origin is needed to output world coordinates using GNSS sensors. These values are stored within the gameObject as Easting and Northing values expressed in the UTM coordinate system as expressed below: Aligning Map Origin to real world coordinates # The position, rotation, and scale of the MapOrigin object are also involved in the transform between the virtual environment and real world map coordinates. Therefore, if the map is not perfect, instead of rotating and scaling the map, you can appropriately set values of the Transform component. In order make this process easier and achieve a good mapping of the virtual world to the real world, you can use Reference Points. Just add a Reference Point by clicking on the button in MapOrigin , then move the Point to a characteristic place and enter the coordinates of this point in the real world. Add at least 2 such points, then click Update Map Origin using Reference Points in MapOrigin . The values will be set to minimize mean squared error. You can see the error by selecting Reference Points. The green sphere shows GPS coordinates of the selected point (Latitude,Longitude). Map Orientation in Unity top # To ensure proper heading values in GNSS related topics, the map must be oriented in such a way the the Z-axis of the unity world coordinate is pointing East. Otherwise all orientation values will have an offset. This can be a bit unintuitive, since by default Unity will have the Z-axis pointing upward on the screen which can lead to users assuming that it aligns to the geographic North. The images below illustrate the map of GoMentum Station with the correct orientation in Unity compared to a satellie image of the area from Google Maps: As evident above, upon the first glance it appears that the GoMentum Station map in Unity is oriented incorrectly; however, if the user were to rotate the view-point so that the Z-axis would be pointing to the right-side of the screen (where the geographic East normally points), the two maps would align as seen in the image below: To switch Unity Editor scene view to this orientation, select Simulator -> Editor Tools. Then select Scene type and in Scene Tools select RotateSceneView . Press Run to have the scene view change to this orientation. This will also change the camera type to Isometric for easier alignment. Vehicle Root Position (BaseLink) top # The vehicle root is the reference point of the ego vehicle model defined in a GameObject called BaseLink under the ego vehicle. BaseLink is intended to be placed at the center of the rear axle, however, if the GameObject does not exist it will be created when the simulation starts and will be placed at the pivot of the ego vehicle model. All sensor positions entered into the sensor configuration in the web UI are defined in the coordinate frame attached to BaseLink . Setting up BaseLink top # Create a new GameObject named BaseLink and click 'Add Component' to add a script called BaseLink . Move position to the center of the rear axle. Add BaseLink.cs and link it to public variable in VehicleSMI.cs Transform Sensor top # Sometimes it can be useful to create a coordinate frame at some point on the ego vehicle to use as a reference for sensor positions. The Transform Sensor provides this functionality. The Transform Sensor is an \"empty\" sensor, meaning that it has no sensor functionality and only has a coordinate frame and can be used as a parent for other sensors in the Simulator sensor configuration. Customizing sensor configurations top # Customized sensor configurations are created in the SVL Simulator Web UI by providing JSON formatted descriptions (covered here ).","title":"Conventions"},{"location":"getting-started/conventions/#svl-simulator-coordinate-systems","text":"Inside the simulator positions and rotations are represented by Transforms in Unity. Unity uses a left-handed ZXY coordinate system for transforms with the Y-axis being the vertical axis. Each simulation object such as vehicles, sensors, maps, traffic lights, etc. will have a transform associated with it. Understanding the transforms is especially important for defining custom sensor configurations for ego vehicles.","title":"SVL Simulator Coordinate Systems"},{"location":"getting-started/conventions/#converting-between-coordinate-systems","text":"The right-handed XYZ coordinate system is more commonly used in Robotics and Autonomous Vehicle applications and users may require to convert coordinate systems for some use-cases when using the SVL Simulator. The tables below summarize conversions for 3D position vectors: ZXY (Unity) XYZ (RH) Z X X -Y Y Z XYZ (RH) ZXY (Unity) X Z Y -X Z Y The tables below summarize conversions for 3D rotation vectors: ZXY (Unity) XYZ (RH) Z -X X Y Y -Z XYZ (RH) ZXY (Unity) X -Z Y X Z -Y The tables below summarize conversions for quaternions: ZXY (Unity) XYZ (RH) Z -X X Y Y -Z W W XYZ (RH) ZXY (Unity) X Y Y -Z Z -X W W","title":"Converting Between Coordinate Systems"},{"location":"getting-started/conventions/#map-origin","text":"Each simulator map includes a gameObject called MapOrigin . The Map Origin is the reference point of the map within Unity and has coordinate values of (0, 0, 0). The real-world position of the Map Origin is needed to output world coordinates using GNSS sensors. These values are stored within the gameObject as Easting and Northing values expressed in the UTM coordinate system as expressed below:","title":"Map-Origin"},{"location":"getting-started/conventions/#aligning-map-origin-to-real-world-coordinates","text":"The position, rotation, and scale of the MapOrigin object are also involved in the transform between the virtual environment and real world map coordinates. Therefore, if the map is not perfect, instead of rotating and scaling the map, you can appropriately set values of the Transform component. In order make this process easier and achieve a good mapping of the virtual world to the real world, you can use Reference Points. Just add a Reference Point by clicking on the button in MapOrigin , then move the Point to a characteristic place and enter the coordinates of this point in the real world. Add at least 2 such points, then click Update Map Origin using Reference Points in MapOrigin . The values will be set to minimize mean squared error. You can see the error by selecting Reference Points. The green sphere shows GPS coordinates of the selected point (Latitude,Longitude).","title":"Aligning Map Origin to real world coordinates"},{"location":"getting-started/conventions/#map-orientation-in-unity","text":"To ensure proper heading values in GNSS related topics, the map must be oriented in such a way the the Z-axis of the unity world coordinate is pointing East. Otherwise all orientation values will have an offset. This can be a bit unintuitive, since by default Unity will have the Z-axis pointing upward on the screen which can lead to users assuming that it aligns to the geographic North. The images below illustrate the map of GoMentum Station with the correct orientation in Unity compared to a satellie image of the area from Google Maps: As evident above, upon the first glance it appears that the GoMentum Station map in Unity is oriented incorrectly; however, if the user were to rotate the view-point so that the Z-axis would be pointing to the right-side of the screen (where the geographic East normally points), the two maps would align as seen in the image below: To switch Unity Editor scene view to this orientation, select Simulator -> Editor Tools. Then select Scene type and in Scene Tools select RotateSceneView . Press Run to have the scene view change to this orientation. This will also change the camera type to Isometric for easier alignment.","title":"Map-Orientation-in-Unity"},{"location":"getting-started/conventions/#vehicle-root-position","text":"The vehicle root is the reference point of the ego vehicle model defined in a GameObject called BaseLink under the ego vehicle. BaseLink is intended to be placed at the center of the rear axle, however, if the GameObject does not exist it will be created when the simulation starts and will be placed at the pivot of the ego vehicle model. All sensor positions entered into the sensor configuration in the web UI are defined in the coordinate frame attached to BaseLink .","title":"Vehicle-Root-Position"},{"location":"getting-started/conventions/#setting-up-baselink","text":"Create a new GameObject named BaseLink and click 'Add Component' to add a script called BaseLink . Move position to the center of the rear axle. Add BaseLink.cs and link it to public variable in VehicleSMI.cs","title":"Setting up BaseLink"},{"location":"getting-started/conventions/#transform-sensor","text":"Sometimes it can be useful to create a coordinate frame at some point on the ego vehicle to use as a reference for sensor positions. The Transform Sensor provides this functionality. The Transform Sensor is an \"empty\" sensor, meaning that it has no sensor functionality and only has a coordinate frame and can be used as a parent for other sensors in the Simulator sensor configuration.","title":"Transform-Sensor"},{"location":"getting-started/conventions/#customizing-sensor-configurations","text":"Customized sensor configurations are created in the SVL Simulator Web UI by providing JSON formatted descriptions (covered here ).","title":"Customizing-Sensor-Configurations"},{"location":"getting-started/getting-started/","text":"SVL Simulator: An Autonomous Vehicle Simulator Website | Documentation | Download Table of Contents Introduction Getting Started Downloading and starting simulator Running the simulator Guide to simulator functionality Next Steps Autonomous software tutorials Python API Building and running from source Contact Citation Introduction top # LG Electronics America R&D Center has developed a Unity-based multi-robot simulator for autonomous vehicle developers. We provide an out-of-the-box solution which can meet the needs of developers wishing to focus on testing their autonomous vehicle algorithms. The SVL Simulator provides integration with the open source AD system platforms Apollo , developed by Baidu and Autoware.AI and Autoware.Auto developed by the Autoware Foundation . Getting Started top # Downloading and starting simulator top # To get started with the simulator we suggest using the release binaries, then: Review the System Requirements . Follow our Installing the SVL Simulator guide to download, install, and start the simulator. You should now have registered for an account, received the \"Complete registration\" email, clicked the \"Verify email\" link, and entered a name for your local machine (cluster). Running the simulator top # At this point you're probably eager to see SVL Simulator in action. You can check out the various maps, vehicles, and plugins available in the Store , and can of course create your own Simulations, but for now we will get started with a pre-configured simulation: Click on \"Simulations\" on the left side, and then click \"Available from Others\" in the middle of the page. This will display a list of pre-configured simulations that are available. Locate the Simulation titled \"Local-Random: CubeTown (Manual Drive)\". You can scroll down through the list of available simulations until you find it, or you can type \"Manual\" into the \"Search\" field to more quickly find it. Once you locate it, click the red \"+\" icon to add this simulation to your account and customize the simulation settings. Most of the settings for this simulation are pre-configured but you will need to specify where to run this simulation. In the General settings, click the \"Select Cluster\" field to select the local cluster you created earlier when you linked your local simulator to your cloud account. Then click \"Next\" to review the \"Test case\" settings. In the \"Test case\" settings, click the \"Select Sensor Configuration\" field and then select \"Keyboard Control\" from the list that appears. Then (scroll down if necessary and) click \"Next\" to review the \"Autopilot\" settings. In the \"Autopilot\" settings, there is nothing to set since we are going to use keyboard control to control the vehicle. Click \"Next\" to display the \"Publish\" view. In the \"Publish\" view, click \"Publish\" to publish (save) this simulation to your library. Note that your simulations are private and visible only to you unless you explicitly decide to share it with others. You can now run your simulation. Make sure the local simulator (cluster) you specified for the simulation is online, then click \"Run Simulation\" and switch back to the SVL Simulator window. If this is the first time running a new simulation, the simulator will download all required assets such as the map, the vehicle, and any specified plugins. These downloaded assets will be locally cached for future (or offline) use, after which you should see (in the SVL Simulator window) a red ego vehicle on the CubeTown map. Press the triangular play/pause button button (on the left side of the simulator menu which you'll find at the bottom edge of the simulator window) to start (un-pause) the simulation. Explore the simulator and then check out the following section for more helpful information on controlling the simulator: Drive the ego vehicle by pressing the up-arrow key to accelerate forward, left and right arrows to steer, and down-arrow to brake. Press \"F12\" to reset the ego vehicle to the default position. Press \"N\" to toggle NPC vehicles off and on, or \"P\" to toggle pedestrians. Check out the different camera modes . Use the right mouse button (click, hold, and drag) to look and rotate the camera view around the scene. When finished, return to your browser window and click \"Stop Simulation\" in the online user interface to end the simulation. Guide to simulator functionality top # Look through the simulation menu to learn about the various on-screen options for controlling the simulator. Discover other keyboard shortcuts for manually controlling the ego vehicle and the entire simulation. Change the weather by adding rain or clouds or adjust the time of day using the interactive menu . Explore the various sensor visualizer and bridge connection options available in the simulator (available when using a vehicle which is configured with sensors and a bridge). Learn about options available on the simulator main screen including the Visual Scenario Editor in online mode and how to run offline simulations in offline mode. Next Steps top # Autonomous software tutorials top # SVL Simulator supports several open source autonomous software platforms (as well as proprietary ones through the use of custom bridge plug-ins). Check out the following tutorials to learn how to use Apollo or Autoware with SVL Simulator: Use Apollo (Latest) with SVL Simulator. Use Apollo 5.0 with SVL Simulator. Use Autoware.AI with SVL Simulator. Use Autoware.Auto with SVL Simulator. Note: To run a simulation with Apollo or Autoware you will need to either create a new simulation or add a pre-configured one. To create and configure a new simulation, check out the detailed walk-through instructions in Running SVL Simulator . To customize and add a pre-configured simulation, return to \"Simulations: Available from Others\" in the online user interface (review Step 1, above ) and search for \"Apollo\" or \"Autoware\" or \"Local-Random\". To customize one of these pre-configured simulations, refer to the Simulations documentation. Python API top # Learn how to control and script simulations at runtime using the Python API . Note: Check out the pre-configured \"API Only\" simulation for use with Python API mode. Building and running from source top # Building the simulator from source is only recommended for developers who wish to customize the simulator, build plugins, or make new assets. Check out our build instructions to build the simulator from source. Contact top # Please feel free to provide feedback or ask questions by creating a Github issue . For inquiries about collaboration, get in touch at contact@svlsimulator.com . Citation top # For citation please use the following: @ARTICLE{2020arXiv200503778R, author = {{Rong}, Guodong and {Shin}, Byung Hyun and {Tabatabaee}, Hadi and {Lu}, Qiang and {Lemke}, Steve and {Mo{\\v{z}}eiko}, M{\\={a}}rti{\\c{n}}{\\v{s}} and {Boise}, Eric and {Uhm}, Geehoon and {Gerow}, Mark and {Mehta}, Shalin and {Agafonov}, Eugene and {Kim}, Tae Hyung and {Sterner}, Eric and {Ushiroda}, Keunhae and {Reyes}, Michael and {Zelenkovsky}, Dmitry and {Kim}, Seonman}, title = \"{SVL Simulator: A High Fidelity Simulator for Autonomous Driving}\", journal = {arXiv e-prints}, keywords = {Computer Science - Robotics, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control}, year = 2020, month = may, eid = {arXiv:2005.03778}, pages = {arXiv:2005.03778}, archivePrefix = {arXiv}, eprint = {2005.03778}, primaryClass = {cs.RO} }","title":"Run a simulation"},{"location":"getting-started/getting-started/#introduction","text":"LG Electronics America R&D Center has developed a Unity-based multi-robot simulator for autonomous vehicle developers. We provide an out-of-the-box solution which can meet the needs of developers wishing to focus on testing their autonomous vehicle algorithms. The SVL Simulator provides integration with the open source AD system platforms Apollo , developed by Baidu and Autoware.AI and Autoware.Auto developed by the Autoware Foundation .","title":"Introduction"},{"location":"getting-started/getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/getting-started/#downloading-and-starting-simulator","text":"To get started with the simulator we suggest using the release binaries, then: Review the System Requirements . Follow our Installing the SVL Simulator guide to download, install, and start the simulator. You should now have registered for an account, received the \"Complete registration\" email, clicked the \"Verify email\" link, and entered a name for your local machine (cluster).","title":"Downloading and starting simulator"},{"location":"getting-started/getting-started/#running-the-simulator","text":"At this point you're probably eager to see SVL Simulator in action. You can check out the various maps, vehicles, and plugins available in the Store , and can of course create your own Simulations, but for now we will get started with a pre-configured simulation: Click on \"Simulations\" on the left side, and then click \"Available from Others\" in the middle of the page. This will display a list of pre-configured simulations that are available. Locate the Simulation titled \"Local-Random: CubeTown (Manual Drive)\". You can scroll down through the list of available simulations until you find it, or you can type \"Manual\" into the \"Search\" field to more quickly find it. Once you locate it, click the red \"+\" icon to add this simulation to your account and customize the simulation settings. Most of the settings for this simulation are pre-configured but you will need to specify where to run this simulation. In the General settings, click the \"Select Cluster\" field to select the local cluster you created earlier when you linked your local simulator to your cloud account. Then click \"Next\" to review the \"Test case\" settings. In the \"Test case\" settings, click the \"Select Sensor Configuration\" field and then select \"Keyboard Control\" from the list that appears. Then (scroll down if necessary and) click \"Next\" to review the \"Autopilot\" settings. In the \"Autopilot\" settings, there is nothing to set since we are going to use keyboard control to control the vehicle. Click \"Next\" to display the \"Publish\" view. In the \"Publish\" view, click \"Publish\" to publish (save) this simulation to your library. Note that your simulations are private and visible only to you unless you explicitly decide to share it with others. You can now run your simulation. Make sure the local simulator (cluster) you specified for the simulation is online, then click \"Run Simulation\" and switch back to the SVL Simulator window. If this is the first time running a new simulation, the simulator will download all required assets such as the map, the vehicle, and any specified plugins. These downloaded assets will be locally cached for future (or offline) use, after which you should see (in the SVL Simulator window) a red ego vehicle on the CubeTown map. Press the triangular play/pause button button (on the left side of the simulator menu which you'll find at the bottom edge of the simulator window) to start (un-pause) the simulation. Explore the simulator and then check out the following section for more helpful information on controlling the simulator: Drive the ego vehicle by pressing the up-arrow key to accelerate forward, left and right arrows to steer, and down-arrow to brake. Press \"F12\" to reset the ego vehicle to the default position. Press \"N\" to toggle NPC vehicles off and on, or \"P\" to toggle pedestrians. Check out the different camera modes . Use the right mouse button (click, hold, and drag) to look and rotate the camera view around the scene. When finished, return to your browser window and click \"Stop Simulation\" in the online user interface to end the simulation.","title":"Running the simulator"},{"location":"getting-started/getting-started/#guide-to-simulator-functionality","text":"Look through the simulation menu to learn about the various on-screen options for controlling the simulator. Discover other keyboard shortcuts for manually controlling the ego vehicle and the entire simulation. Change the weather by adding rain or clouds or adjust the time of day using the interactive menu . Explore the various sensor visualizer and bridge connection options available in the simulator (available when using a vehicle which is configured with sensors and a bridge). Learn about options available on the simulator main screen including the Visual Scenario Editor in online mode and how to run offline simulations in offline mode.","title":"Guide to simulator functionality"},{"location":"getting-started/getting-started/#next-steps","text":"","title":"Next Steps"},{"location":"getting-started/getting-started/#autonomous-software-tutorials","text":"SVL Simulator supports several open source autonomous software platforms (as well as proprietary ones through the use of custom bridge plug-ins). Check out the following tutorials to learn how to use Apollo or Autoware with SVL Simulator: Use Apollo (Latest) with SVL Simulator. Use Apollo 5.0 with SVL Simulator. Use Autoware.AI with SVL Simulator. Use Autoware.Auto with SVL Simulator. Note: To run a simulation with Apollo or Autoware you will need to either create a new simulation or add a pre-configured one. To create and configure a new simulation, check out the detailed walk-through instructions in Running SVL Simulator . To customize and add a pre-configured simulation, return to \"Simulations: Available from Others\" in the online user interface (review Step 1, above ) and search for \"Apollo\" or \"Autoware\" or \"Local-Random\". To customize one of these pre-configured simulations, refer to the Simulations documentation.","title":"Autonomous software tutorials"},{"location":"getting-started/getting-started/#python-api","text":"Learn how to control and script simulations at runtime using the Python API . Note: Check out the pre-configured \"API Only\" simulation for use with Python API mode.","title":"Python API"},{"location":"getting-started/getting-started/#building-from-source","text":"Building the simulator from source is only recommended for developers who wish to customize the simulator, build plugins, or make new assets. Check out our build instructions to build the simulator from source.","title":"Building and running from source"},{"location":"getting-started/getting-started/#contact","text":"Please feel free to provide feedback or ask questions by creating a Github issue . For inquiries about collaboration, get in touch at contact@svlsimulator.com .","title":"Contact"},{"location":"getting-started/getting-started/#citation","text":"For citation please use the following: @ARTICLE{2020arXiv200503778R, author = {{Rong}, Guodong and {Shin}, Byung Hyun and {Tabatabaee}, Hadi and {Lu}, Qiang and {Lemke}, Steve and {Mo{\\v{z}}eiko}, M{\\={a}}rti{\\c{n}}{\\v{s}} and {Boise}, Eric and {Uhm}, Geehoon and {Gerow}, Mark and {Mehta}, Shalin and {Agafonov}, Eugene and {Kim}, Tae Hyung and {Sterner}, Eric and {Ushiroda}, Keunhae and {Reyes}, Michael and {Zelenkovsky}, Dmitry and {Kim}, Seonman}, title = \"{SVL Simulator: A High Fidelity Simulator for Autonomous Driving}\", journal = {arXiv e-prints}, keywords = {Computer Science - Robotics, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control}, year = 2020, month = may, eid = {arXiv:2005.03778}, pages = {arXiv:2005.03778}, archivePrefix = {arXiv}, eprint = {2005.03778}, primaryClass = {cs.RO} }","title":"Citation"},{"location":"getting-started/introduction/","text":"Introduction What is SVL Simulator? # SVL Simulator is a simulation platform used for autonomous vehicle and robotic system development. As the development of advanced vehicles, transportation systems, and autonomous agents becomes increasingly complex, the testing needs and the infrastructure required to support them become critical to deploying safe systems on public roads and environments. SVL Simulator consists of the simulation software, software tools, the ecosystem of content and plugins that enable tailored use cases, and the cloud environment which enables simulation and scenario testing at scale. By simulating a virtual environment, one or more ego vehicles or autonomous systems and their sensors, and traffic and other dynamic objects, the simulation software provides a seamless and customizable interface with a user's System Under Test. This allows the developer to debug, perform modular testing, and perform integration testing. Who is SVL Simulator for? # SVL Simulator is built for engineers who build, test, and verify autonomous systems. We invite anyone involved in building future autonomous systems to try using our freely available software. What is SVL Simulator used for? # SVL Simulator enables you to perform integration testing, modular algorithm testing, and system verification. It encompasses the testing infrastructure and workflow required for safe autonomous vehicle or robot development and deployment. These are some of the specific applications developed using SVL Simulator: L4/L5 autonomous vehicle systems L2/L3 ADAS/AD systems Warehouse robotics Outdoor mobile robotics Future Mobility services Autonomous racing Sensor/sensor systems development and marketing Automotive and autonomous system security Synthetic data generation Real-time embedded systems for automotive What can you do with SVL Simulator? # Here are some of the specific use case examples for using SVL Simulator: Create and run scenarios involving complex traffic scenarios based on real-world data Debug and improve your localization module by testing in new Digital Twin environments Test planning module in isolation with virtual ground truth detections Test your autonomous vehicle stack end-to-end in real time (software-in-the-loop) Discover and prevent performance bottlenecks with hardware-in-the-loop simulation Keep track of development progress with test case reports for every simulation Parameterize and automate the execution of thousands of scenarios to discover high-value and interesting edge case scenarios Key Value-Add # Using SVL Simulator for simulation means you will have realistic, real-time, end-to-end simulation. We enable developers to connect their real System Under Test and rigorously test all parts of their system in simulation quickly, easily, and comprehensively. Key Features # This documentation website takes you through all of the features in SVL Simulator, but below are some of the key features: End to end simulation Real-time, high-performance simulation Multi-vehicle simulation Photorealistic environment simulation Extensibility and customizability Variety of scenario creation tools Procedural road network generation and environment creation tools HD map import, export, and annotation tools","title":"Introduction"},{"location":"getting-started/introduction/#what-is-svl-simulator","text":"SVL Simulator is a simulation platform used for autonomous vehicle and robotic system development. As the development of advanced vehicles, transportation systems, and autonomous agents becomes increasingly complex, the testing needs and the infrastructure required to support them become critical to deploying safe systems on public roads and environments. SVL Simulator consists of the simulation software, software tools, the ecosystem of content and plugins that enable tailored use cases, and the cloud environment which enables simulation and scenario testing at scale. By simulating a virtual environment, one or more ego vehicles or autonomous systems and their sensors, and traffic and other dynamic objects, the simulation software provides a seamless and customizable interface with a user's System Under Test. This allows the developer to debug, perform modular testing, and perform integration testing.","title":"What is SVL Simulator?"},{"location":"getting-started/introduction/#who-is-svl-simulator-for","text":"SVL Simulator is built for engineers who build, test, and verify autonomous systems. We invite anyone involved in building future autonomous systems to try using our freely available software.","title":"Who is SVL Simulator for?"},{"location":"getting-started/introduction/#what-is-svl-simulator-used-for","text":"SVL Simulator enables you to perform integration testing, modular algorithm testing, and system verification. It encompasses the testing infrastructure and workflow required for safe autonomous vehicle or robot development and deployment. These are some of the specific applications developed using SVL Simulator: L4/L5 autonomous vehicle systems L2/L3 ADAS/AD systems Warehouse robotics Outdoor mobile robotics Future Mobility services Autonomous racing Sensor/sensor systems development and marketing Automotive and autonomous system security Synthetic data generation Real-time embedded systems for automotive","title":"What is SVL Simulator used for?"},{"location":"getting-started/introduction/#what-can-you-do-with-svl-simulator","text":"Here are some of the specific use case examples for using SVL Simulator: Create and run scenarios involving complex traffic scenarios based on real-world data Debug and improve your localization module by testing in new Digital Twin environments Test planning module in isolation with virtual ground truth detections Test your autonomous vehicle stack end-to-end in real time (software-in-the-loop) Discover and prevent performance bottlenecks with hardware-in-the-loop simulation Keep track of development progress with test case reports for every simulation Parameterize and automate the execution of thousands of scenarios to discover high-value and interesting edge case scenarios","title":"What can you do with SVL Simulator?"},{"location":"getting-started/introduction/#key-value-add","text":"Using SVL Simulator for simulation means you will have realistic, real-time, end-to-end simulation. We enable developers to connect their real System Under Test and rigorously test all parts of their system in simulation quickly, easily, and comprehensively.","title":"Key Value-Add"},{"location":"getting-started/introduction/#key-features","text":"This documentation website takes you through all of the features in SVL Simulator, but below are some of the key features: End to end simulation Real-time, high-performance simulation Multi-vehicle simulation Photorealistic environment simulation Extensibility and customizability Variety of scenario creation tools Procedural road network generation and environment creation tools HD map import, export, and annotation tools","title":"Key Features"},{"location":"installation-guide/build-instructions/","text":"Simulator Build Instructions This document provides instructions on building the simulator and assets from source using the Unity Editor in developer mode . Note: When a terminal is mentioned, it refers to cmd.exe on Windows and Terminal on Ubuntu. Table of Contents Installing the Unity Editor Installing Git LFS Building a standalone executable Testing the simulator build Building assets Using custom assets with the simulator binary Testing custom assets with the Unity editor Installing the Unity Editor top # Download and Install Unity Hub Ubuntu: You may need to allow the downloaded AppImage to be run as an executable Right-click the AppImage Select Properties Go to the Permissions tab Check Allow executing file as program Alternatively, in the terminal run sudo chmod +x UnityHub.AppImage Download and Install Unity 2020.3.3f1 from the Unity Download Archive : Click the Unity Hub button to have Unity Hub start the installation process If the installation process fails to start on Ubuntu: Right click the Unity Hub button and select Copy Link Address In a terminal, type <PATH_TO_UNITY_HUB> <COPIED_LINK> The copied link will be in the form unityhub://Unity-VERSION/XXXXXX (e.g. unityhub://2020.3.3f1/76626098c1c4 ) Thus, if the Unity Hub application is in the current directory, type ./UnityHub.AppImage unityhub://2020.3.3f1/76626098c1c4 Unity Hub will open and guide you through the installation of Unity Editor IMPORTANT include the Windows Build Support (Mono) for both Windows and Linux when installing Unity (Optional) include support for Visual Studio for easier debugging Verify installation Under the Installs tab of Unity Hub there should be the expected version shown. In the bottom-left corner of the version, there should be an icon of the other OS (e.g. on a Linux computer, the Windows logo will be shown) Installing Git LFS top # Make sure you have git-lfs installed before cloning the Simulator repository . Instructions for installation are here Verify installation In a terminal enter git lfs install > Git LFS initialized. should print out Building a standalone executable top # Clone simulator project from GitHub (open-source) release branch. Open a terminal and navigate to where you want the Simulator to be downloaded to If you want the Simulator in your Documents folder, use cd in the terminal so that the input for the terminal is similar to /Documents$ Open-source user: git clone https://github.com/lgsvl/simulator.git Verify download Above clone will create a Simulator folder Open a File Explorer and navigate to where the Simulator folder is Navigate to Simulator/Assets/Materials/EnvironmentMaterials/ There should be a EnvironmentDamageAlbedo.png in this folder Open the image, it should look like the image below If the image cannot be opened, Git LFS was not installed before cloning the repository Install Git LFS following step 4 In a terminal, navigate to the Simulator folder so that the terminal is similar to /Simulator$ git lfs pull Check the image again Note: Please checkout the \"release-*\" branches or release tags for stable (ready features) and \"master\" branch for unstable (preview of work in progress). Run Unity Hub In the Projects tab, click Add and select the Simulator folder that was created by git clone in Step 5 In the Projects tab, verify that the Simulator is using Unity Version 2020.3.3f1 from the dropdown Double-click the name of the project to launch Unity Editor Note: On Ubuntu 18.04, create an empty sample project before adding an existing project as mentioned in step 1. Open the Simulator menu from the top toolbar and select Build... to open the build window (shown below with a red outline) Select the target OS for the build using the Executable Platform dropdown menu Verify Build Simulator is checked for the Simulator to be built Select a folder that the simulator will be built in (Optional) Check Development Build to create a Development Build that includes debug symbols and enables the Profiler Click Build Once the build process is complete a simulator executable will be available at the specified location. Testing the simulator build top # Follow these steps for a quick test of the simulator build. For a more complete guide please see the tutorial on running the simulator . (Ubuntu) Install the Vulkan userspace library sudo apt-get install libvulkan1 Double-click the Simulator.exe that was built Click Open Browser , and login to the Web UI (you may need to sign up) You may be prompted to create a cluster for local simulations In the Store choose any maps and click Add to My Library ex. the BorregasAve map Back in the Store choose one of the default vehicles such as the Lincoln2017Mkz and click Add to My Library In the Simulations tab, Add New simulation with the added map and the newly cloned vehicle: Enter Simulation Name and Select Cluster then click Next . Select the Random Traffic runtime template, choose the map from your library and select the Lincoln2017MKZ vehicle with the Keyboard Control sensor configuration then click Next . Click Next on the Autopilot page and click Publish to create the simulation. Press the Run simulation button The Unity window should now show a vehicle in the built environment Building assets top # The simulator build tools allow developers to build assets as well as standalone simulator binaries. Assets refer to maps, vehicles, sensors, controllables, npcs, pedestrians and bridges. Asset bundles are built without needing to rebuild the entire simulator. NOTE: AssetBundles built with one version of the simulator are NOT guaranteed to work with other versions of the simulator. Asset bundles will need to be re-created. NOTE: NPCs and Pedestrians must be built locally for custom binaries. They must be added to AssetBundles folder to be loaded by the custom simulator binary. Follow these steps to build assets: Place the source code for the asset the correct directory under Simulator/Assets/External for it to be discovered by the build tool. Based on the category the asset falls under it should be placed inside on of these subdirectories: Environments Vehicles NPCs Pedestrians Sensors Controllables Bridges For example, to build the CubeTown map we would clone the repository into the Environments subdirectory. Press Ctrl + R to refresh assets (only needed if auto-refresh is disabled in the editor). The asset should now be detected and displayed in the build tool. In the case of this example, CubeTown will show up under Environments . Check the box next to any asset you wish to build and click the Build button at the bottom of the tool. Once the asset is built it will be available under the Simulator/AssetBundles in a subdirectory named after the category of the asset that was built. For this example, CubeTown will be at Simulator/AssetBundles/Environments/environment_CubeTown . Using custom assets with the simulator binary top # To use built assets with the simulator binary they must first be uploaded to the cloud. For more information on this see the Library guide. Testing custom assets with the Unity editor top # Custom assets can be tested with the Unity Editor by using the Developer Settings panel accessible from the Simulator menu located in the top toolbar. Only local assets are supported and JSON must be manually added so Developer Settings can look up sensor data. Users can toggle Developer Debug Mode in Simulator drop down menu to load local asset classes at runtime to be able to access break point functionality. Be sure to toggle off when finished debugging.","title":"Building from source"},{"location":"installation-guide/build-instructions/#installing-unity-editor","text":"Download and Install Unity Hub Ubuntu: You may need to allow the downloaded AppImage to be run as an executable Right-click the AppImage Select Properties Go to the Permissions tab Check Allow executing file as program Alternatively, in the terminal run sudo chmod +x UnityHub.AppImage Download and Install Unity 2020.3.3f1 from the Unity Download Archive : Click the Unity Hub button to have Unity Hub start the installation process If the installation process fails to start on Ubuntu: Right click the Unity Hub button and select Copy Link Address In a terminal, type <PATH_TO_UNITY_HUB> <COPIED_LINK> The copied link will be in the form unityhub://Unity-VERSION/XXXXXX (e.g. unityhub://2020.3.3f1/76626098c1c4 ) Thus, if the Unity Hub application is in the current directory, type ./UnityHub.AppImage unityhub://2020.3.3f1/76626098c1c4 Unity Hub will open and guide you through the installation of Unity Editor IMPORTANT include the Windows Build Support (Mono) for both Windows and Linux when installing Unity (Optional) include support for Visual Studio for easier debugging Verify installation Under the Installs tab of Unity Hub there should be the expected version shown. In the bottom-left corner of the version, there should be an icon of the other OS (e.g. on a Linux computer, the Windows logo will be shown)","title":"Installing the Unity Editor"},{"location":"installation-guide/build-instructions/#installing-git-lfs","text":"Make sure you have git-lfs installed before cloning the Simulator repository . Instructions for installation are here Verify installation In a terminal enter git lfs install > Git LFS initialized. should print out","title":"Installing Git LFS"},{"location":"installation-guide/build-instructions/#build-stand-alone","text":"Clone simulator project from GitHub (open-source) release branch. Open a terminal and navigate to where you want the Simulator to be downloaded to If you want the Simulator in your Documents folder, use cd in the terminal so that the input for the terminal is similar to /Documents$ Open-source user: git clone https://github.com/lgsvl/simulator.git Verify download Above clone will create a Simulator folder Open a File Explorer and navigate to where the Simulator folder is Navigate to Simulator/Assets/Materials/EnvironmentMaterials/ There should be a EnvironmentDamageAlbedo.png in this folder Open the image, it should look like the image below If the image cannot be opened, Git LFS was not installed before cloning the repository Install Git LFS following step 4 In a terminal, navigate to the Simulator folder so that the terminal is similar to /Simulator$ git lfs pull Check the image again Note: Please checkout the \"release-*\" branches or release tags for stable (ready features) and \"master\" branch for unstable (preview of work in progress). Run Unity Hub In the Projects tab, click Add and select the Simulator folder that was created by git clone in Step 5 In the Projects tab, verify that the Simulator is using Unity Version 2020.3.3f1 from the dropdown Double-click the name of the project to launch Unity Editor Note: On Ubuntu 18.04, create an empty sample project before adding an existing project as mentioned in step 1. Open the Simulator menu from the top toolbar and select Build... to open the build window (shown below with a red outline) Select the target OS for the build using the Executable Platform dropdown menu Verify Build Simulator is checked for the Simulator to be built Select a folder that the simulator will be built in (Optional) Check Development Build to create a Development Build that includes debug symbols and enables the Profiler Click Build Once the build process is complete a simulator executable will be available at the specified location.","title":"Building a standalone executable"},{"location":"installation-guide/build-instructions/#test-simulator","text":"Follow these steps for a quick test of the simulator build. For a more complete guide please see the tutorial on running the simulator . (Ubuntu) Install the Vulkan userspace library sudo apt-get install libvulkan1 Double-click the Simulator.exe that was built Click Open Browser , and login to the Web UI (you may need to sign up) You may be prompted to create a cluster for local simulations In the Store choose any maps and click Add to My Library ex. the BorregasAve map Back in the Store choose one of the default vehicles such as the Lincoln2017Mkz and click Add to My Library In the Simulations tab, Add New simulation with the added map and the newly cloned vehicle: Enter Simulation Name and Select Cluster then click Next . Select the Random Traffic runtime template, choose the map from your library and select the Lincoln2017MKZ vehicle with the Keyboard Control sensor configuration then click Next . Click Next on the Autopilot page and click Publish to create the simulation. Press the Run simulation button The Unity window should now show a vehicle in the built environment","title":"Testing the simulator build"},{"location":"installation-guide/build-instructions/#building-assets","text":"The simulator build tools allow developers to build assets as well as standalone simulator binaries. Assets refer to maps, vehicles, sensors, controllables, npcs, pedestrians and bridges. Asset bundles are built without needing to rebuild the entire simulator. NOTE: AssetBundles built with one version of the simulator are NOT guaranteed to work with other versions of the simulator. Asset bundles will need to be re-created. NOTE: NPCs and Pedestrians must be built locally for custom binaries. They must be added to AssetBundles folder to be loaded by the custom simulator binary. Follow these steps to build assets: Place the source code for the asset the correct directory under Simulator/Assets/External for it to be discovered by the build tool. Based on the category the asset falls under it should be placed inside on of these subdirectories: Environments Vehicles NPCs Pedestrians Sensors Controllables Bridges For example, to build the CubeTown map we would clone the repository into the Environments subdirectory. Press Ctrl + R to refresh assets (only needed if auto-refresh is disabled in the editor). The asset should now be detected and displayed in the build tool. In the case of this example, CubeTown will show up under Environments . Check the box next to any asset you wish to build and click the Build button at the bottom of the tool. Once the asset is built it will be available under the Simulator/AssetBundles in a subdirectory named after the category of the asset that was built. For this example, CubeTown will be at Simulator/AssetBundles/Environments/environment_CubeTown .","title":"Building assets"},{"location":"installation-guide/build-instructions/#using-custom-assets-with-the-simulator-binary","text":"To use built assets with the simulator binary they must first be uploaded to the cloud. For more information on this see the Library guide.","title":"Using custom assets with the simulator binary"},{"location":"installation-guide/build-instructions/#testing-custom-assets-with-the-unity-editor","text":"Custom assets can be tested with the Unity Editor by using the Developer Settings panel accessible from the Simulator menu located in the top toolbar. Only local assets are supported and JSON must be manually added so Developer Settings can look up sensor data. Users can toggle Developer Debug Mode in Simulator drop down menu to load local asset classes at runtime to be able to access break point functionality. Be sure to toggle off when finished debugging.","title":"Testing custom assets with the Unity editor"},{"location":"installation-guide/installing-simulator/","text":"Installing the SVL Simulator Table of Contents Graphics Drivers Windows Linux Other Linux Requirements Vulkan Docker Installing the simulator Link to Cloud Graphics Drivers top # Windows top # Download driver manually from NVIDIA's website and install. Linux top # You can check if the NVIDIA drivers are already installed, and their version by running in terminal: $ nvidia-smi The output should be similar to: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.57 Driver Version: 450.57 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:01:00.0 On | N/A | | 0% 36C P8 11W / 280W | 202MiB / 11169MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1942 G /usr/lib/xorg/Xorg 114MiB | | 0 N/A N/A 2292 G /usr/bin/gnome-shell 84MiB | +-----------------------------------------------------------------------------+ If you do not have the latest NVIDIA drivers installed, then install the drivers for your system. For example: Run ubuntu-drivers devices to check the highest supported driver version for your system: == /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 == modalias : pci:v000010DEd00001B06sv00001462sd00003607bc03sc00i00 vendor : NVIDIA Corporation model : GP102 [GeForce GTX 1080 Ti] driver : nvidia-driver-435 - distro non-free driver : nvidia-driver-390 - distro non-free driver : nvidia-driver-450 - third-party free recommended driver : nvidia-driver-440 - distro non-free driver : xserver-xorg-video-nouveau - distro free builtin ... Install the appropriate driver: $ sudo apt install nvidia-driver-450 Other Linux Requirements top # Vulkan top # Make sure the Vulkan userspace library is installed: $ sudo apt install libvulkan1 NOTE: If you also have mesa-vulkan-drivers installed (e.g. for Intel motherboard video) they can prevent the simulator from being able to access your NVIDIA GPU. If the simulator crashes when starting, try removing the mesa vulkan drivers: $ sudo apt remove mesa-vulkan-drivers Docker top # Docker v19.03 or later must be installed on Linux in order to run local simulations that use the Python API or VSE runtime templates . Follow the Docker installation instructions to install Docker. Installing the simulator top # The Simulator release can be downloaded as a prebuilt binary from the simulator distribution . Download and extract the simulator binary zip file for your OS to a desired location. For Linux: svlsimulator-linux64-{release-version}.zip For Windows: svlsimulator-windows64-{release-version}.zip Enter the svlsimulator-{operating-system}64-{version-number} folder and run the executable application named simulator ( simulator.exe on Windows) by double-clicking it. Link to Cloud top # When the SVL Simulator program launches, it will connect to the SVL Simulator Cloud. Click Open Browser to open the simulator user interface in a web browser. If the Link to Cloud button is not available, check in the upper right corner to see if the simulator is Online or Offline . If offline, click the Offline button to re-connect the simulator to the cloud, then click Link to Cloud . You should arrive at the SVL Simulator Sign In page. If you already have an account, you can use your credentials to sign in. If you don\u2019t already have an account, click Sign Up to create an account. To create an account, fill out the Sign Up form. You will need to confirm your email address to be able to continue. Click the \"Verify email\" link in the \"Complete registration\" email. If you don't receive the confirmation email, please check your spam filter or add \"contact@svlsimulator.com\" to your approved senders list. After signing in (or signing up), you will be logged in and are almost ready to start creating and running simulations. Before you can create a simulation, you'll need to link your local simulator instance to your cloud account. You should now see the Clusters view, where you need to enter a cluster name (e.g. \"local-sim\") for your local machine which is running SVL Simulator and click Create cluster . If you wish to link additional machines into a cluster for distributed simulation, refer to Clusters Tab for more information. If you don't see the New Cluster form or get an error when Creating the Cluster , your running simulator may be offline. Return to the SVL Simulator program window and click the Offline button to re-connect the simulator to the cloud, then click Link to Cloud to return to the Clusters view. You should now be able to enter a name for your local machine (e.g. \"local-sim\") and click Create cluster . At this point your local machine should now be linked to the cloud in your new default (\"local-sim\") cluster. You are now ready to run simulations. Follow the instructions in Running the Simulator to run your first simulation.","title":"Installation procedure"},{"location":"installation-guide/installing-simulator/#graphicsdrivers","text":"","title":"Graphics Drivers"},{"location":"installation-guide/installing-simulator/#windows","text":"Download driver manually from NVIDIA's website and install.","title":"Windows"},{"location":"installation-guide/installing-simulator/#linux","text":"You can check if the NVIDIA drivers are already installed, and their version by running in terminal: $ nvidia-smi The output should be similar to: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.57 Driver Version: 450.57 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:01:00.0 On | N/A | | 0% 36C P8 11W / 280W | 202MiB / 11169MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1942 G /usr/lib/xorg/Xorg 114MiB | | 0 N/A N/A 2292 G /usr/bin/gnome-shell 84MiB | +-----------------------------------------------------------------------------+ If you do not have the latest NVIDIA drivers installed, then install the drivers for your system. For example: Run ubuntu-drivers devices to check the highest supported driver version for your system: == /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 == modalias : pci:v000010DEd00001B06sv00001462sd00003607bc03sc00i00 vendor : NVIDIA Corporation model : GP102 [GeForce GTX 1080 Ti] driver : nvidia-driver-435 - distro non-free driver : nvidia-driver-390 - distro non-free driver : nvidia-driver-450 - third-party free recommended driver : nvidia-driver-440 - distro non-free driver : xserver-xorg-video-nouveau - distro free builtin ... Install the appropriate driver: $ sudo apt install nvidia-driver-450","title":"Linux"},{"location":"installation-guide/installing-simulator/#otherlinuxrequirements","text":"","title":"Other Linux Requirements"},{"location":"installation-guide/installing-simulator/#vulkan","text":"Make sure the Vulkan userspace library is installed: $ sudo apt install libvulkan1 NOTE: If you also have mesa-vulkan-drivers installed (e.g. for Intel motherboard video) they can prevent the simulator from being able to access your NVIDIA GPU. If the simulator crashes when starting, try removing the mesa vulkan drivers: $ sudo apt remove mesa-vulkan-drivers","title":"Vulkan"},{"location":"installation-guide/installing-simulator/#docker","text":"Docker v19.03 or later must be installed on Linux in order to run local simulations that use the Python API or VSE runtime templates . Follow the Docker installation instructions to install Docker.","title":"Docker"},{"location":"installation-guide/installing-simulator/#installing-simulator","text":"The Simulator release can be downloaded as a prebuilt binary from the simulator distribution . Download and extract the simulator binary zip file for your OS to a desired location. For Linux: svlsimulator-linux64-{release-version}.zip For Windows: svlsimulator-windows64-{release-version}.zip Enter the svlsimulator-{operating-system}64-{version-number} folder and run the executable application named simulator ( simulator.exe on Windows) by double-clicking it.","title":"Installing the simulator"},{"location":"installation-guide/installing-simulator/#linktocloud","text":"When the SVL Simulator program launches, it will connect to the SVL Simulator Cloud. Click Open Browser to open the simulator user interface in a web browser. If the Link to Cloud button is not available, check in the upper right corner to see if the simulator is Online or Offline . If offline, click the Offline button to re-connect the simulator to the cloud, then click Link to Cloud . You should arrive at the SVL Simulator Sign In page. If you already have an account, you can use your credentials to sign in. If you don\u2019t already have an account, click Sign Up to create an account. To create an account, fill out the Sign Up form. You will need to confirm your email address to be able to continue. Click the \"Verify email\" link in the \"Complete registration\" email. If you don't receive the confirmation email, please check your spam filter or add \"contact@svlsimulator.com\" to your approved senders list. After signing in (or signing up), you will be logged in and are almost ready to start creating and running simulations. Before you can create a simulation, you'll need to link your local simulator instance to your cloud account. You should now see the Clusters view, where you need to enter a cluster name (e.g. \"local-sim\") for your local machine which is running SVL Simulator and click Create cluster . If you wish to link additional machines into a cluster for distributed simulation, refer to Clusters Tab for more information. If you don't see the New Cluster form or get an error when Creating the Cluster , your running simulator may be offline. Return to the SVL Simulator program window and click the Offline button to re-connect the simulator to the cloud, then click Link to Cloud to return to the Clusters view. You should now be able to enter a name for your local machine (e.g. \"local-sim\") and click Create cluster . At this point your local machine should now be linked to the cloud in your new default (\"local-sim\") cluster. You are now ready to run simulations. Follow the instructions in Running the Simulator to run your first simulation.","title":"Link to Cloud"},{"location":"installation-guide/running-with-wsl2/","text":"Running with Windows Subsystem for Linux Introduction top # Some of the tools commonly used with SVL Simulator, like Apollo or Autoware.Auto , might require Linux operating system to run. If you don't have Linux available or prefer using Windows, it's now possible to run Linux-exclusive programs using Windows Subsystem for Linux (WSL). Features available in WSL are dependent on its version, which is tied to the version of Windows 10 . Starting with build 20149, WSL supports GPU Paravirtualization (GPU-PV) that will let you run CUDA applications inside your WSL instance and in Docker containers. Build 21362 adds support for GUI applications (as an alternative, you can also set up desktop environment ). Installation top # NOTE: At the time of writing, most of the drivers and software required to use GPU-PV in WSL are still in preview. This might require using their pre-release versions. Details can be found in specific sub-sections. Verify Windows 10 version top # In the Windows command line, enter: winver Reported OS Build should be 20149 or higher. If the build number is lower, and your system is up to date, required Windows version is not yet a part of public release. To use it, you will have to join Windows Insider Program on Dev Channel that will let you use early builds of Windows 10. If you're interested, please follow official instructions and update your Windows version. Install WSL 2 top # To install WSL 2, please refer to the official documentation . We recommend using Ubuntu 18.04 or Ubuntu 20.04 Linux distribution. This tutorial will assume Ubuntu 20.04 is installed. If you have installed WSL previously, make sure you're using WSL version 2. To check for version, enter: wsl -l -v The distribution you're planning to use should report 2 under VERSION . If you're using WSL 1, update it to WSL 2. Make sure your Linux kernel version is up to date. You can update it through an elevated Windows command line, by entering: wsl --update Install NVIDIA drivers top # At the time of writing, NVIDIA drivers for CUDA on WSL are still in public preview. Version 470.14 (with CUDA 11.3) or higher is required. You can check your current driver version through Windows command line, by entering: nvidia-smi If your driver version is older, you can download required version from official NVIDIA web page . To verify that your GPU is working inside WSL, you can build and run one of default CUDA examples using WSL terminal: cd /usr/local/cuda/samples/4_Finance/BlackScholes make ./BlackScholes Your console should provide you with output similar to the one shown below. This means that your GPU is accessible inside of WSL and can be used to run CUDA programs. [./BlackScholes] - Starting... GPU Device 0: \"Pascal\" with compute capability 6.1 Initializing data... ...allocating CPU memory for options. ...allocating GPU memory for options. ...generating input data in CPU mem. ...copying input data to GPU mem. Data init done. Executing Black-Scholes GPU kernel (512 iterations)... Options count : 8000000 BlackScholesGPU() time : 0.363584 msec Effective memory bandwidth: 220.031695 GB/s Gigaoptions per second : 22.003170 BlackScholes, Throughput = 22.0032 GOptions/s, Time = 0.00036 s, Size = 8000000 options, NumDevsUsed = 1, Workgroup = 128 Reading back GPU results... Checking the results... ...running CPU calculations. Comparing the results... L1 norm: 1.741792E-07 Max absolute error: 1.192093E-05 Shutting down... ...releasing GPU memory. ...releasing CPU memory. Shutdown done. [BlackScholes] - Test Summary NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. Test passed Install Docker Desktop for Windows top # If you plan to use Docker inside your WSL 2 distro, we suggest to install Docker Desktop by following official documentation . Alternatively, you can decide to skip Docker Desktop and use Docker and NVIDIA Container Toolkit installed directly from your WSL 2. We recommend the first option - this tutorial will assume Docker Desktop for Windows is used. If you insist on using the second option, you can find instructions in official NVIDIA documentation . If you're already using Docker Desktop for Windows, make sure version 3.1 or higher is installed. After installing and launching Docker Desktop, navigate to Settings -> General and make sure that the option Use the WSL 2 based engine is enabled. After that, navigate to Settings -> Resources -> WSL integration and make sure that integration with your WSL 2 distro is enabled. Whenever you're using Docker from WSL, the Docker Desktop application must be running on your Windows machine - otherwise WSL won't be able to recognize service docker . To verify that Docker environment is running properly, enter your WSL terminal and launch a sample CUDA docker image from NVIDIA: docker run --rm -it --gpus=all --env NVIDIA_DISABLE_REQUIRE=1 nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark NOTE: Parameter --env NVIDIA_DISABLE_REQUIRE=1 disables CUDA version check. This is required at the time of writing due to bug in NVIDIA drivers (CUDA version reported in WSL is lower than installed). Your console should provide you with output similar to the one shown below. This means that your GPU is accessible inside of Docker containers and can be used to run CUDA programs. Run \"nbody -benchmark [-numbodies=<numBodies>]\" to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=<N> (number of bodies (>= 1) to run in simulation) -device=<d> (where d=0,1,2.... for the CUDA device to use) -numdevices=<i> (where i=(number of CUDA devices > 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=<file.bin> (load a tipsy model file for simulation) NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. > Windowed mode > Simulation data stored in video memory > Single precision floating point simulation > 1 Devices used for simulation GPU Device 0: \"Pascal\" with compute capability 6.1 > Compute 6.1 CUDA device: [NVIDIA GeForce GTX 1080] 20480 bodies, total time for 10 iterations: 16.788 ms = 249.832 billion interactions per second = 4996.645 single-precision GFLOP/s at 20 flops per interaction Set up desktop environment (optional) top # If you want to run any GUI-based applications inside your WSL or Docker environment, you have configure X Window System. By default, display options inside of WSL are not configured and no valid output device is registered. This means not only that GUI will not be displayed, but also that any program attempting to output something to screen might not work correctly or fail to launch. If you have Windows 10 build 21362 or higher, you don't have to do anything - along your WSL instance, WSLg should have been automatically installed. WSLg pipes X11 and Wayland (used to run Linux GUI applications) directly into Windows graphical user interface. This means that any GUI application launched inside WSL will simply open on your Windows desktop. To verify that's the case, run any GUI-based application from your WSL instance. As an example, you can use one of OpenGL test applications. In your WSL terminal, enter: glxgears This should open new window with three spinning gears. If your Linux distro does not have this installed by default, you can get glxgears application from mesa-utils package: sudo apt install mesa-utils If you are able to launch Linux GUI applications directly on your Windows desktop, you can safely skip the rest of this section. However, if your Windows build version is lower than 21362 and you don't have access to WLSg, you can still use GUI applications through remote desktop. Remote desktop solution shown here will use xfce4 desktop environment for Linux and xrdp RDP server. Both modified configuration files ( /etc/xrdp/xrdp.ini and /etc/xrdp/startwm.sh ) are backed up with .bak extension if you ever need their original version. To install and configure the remote desktop for WSL, run commands shown below in your WSL terminal: sudo apt-get install xrdp sudo apt -y install xfce4 sudo cp /etc/xrdp/xrdp.ini /etc/xrdp/xrdp.ini.bak sudo sed -i 's/3389/3390/g' /etc/xrdp/xrdp.ini sudo sed -i 's/max_bpp=32/#max_bpp=32\\nmax_bpp=128/g' /etc/xrdp/xrdp.ini sudo sed -i 's/xserverbpp=24/#xserverbpp=24\\nxserverbpp=128/g' /etc/xrdp/xrdp.ini sudo cp /etc/xrdp/startwm.sh /etc/xrdp/startwm.sh.bak sudo sed -i 's+test -x /etc/X11/Xsession \\&\\& exec /etc/X11/Xsession+#test -x /etc/X11/Xsession \\&\\& exec /etc/X11 Xsession+g' /etc/xrdp/startwm.sh sudo sed -i 's+exec /bin/sh /etc/X11/Xsession+#exec /bin/sh /etc/X11/Xsession\\nstartxfce4+g' /etc/xrdp/startwm.sh To start RDP server that will allow you to connect to WSL desktop, run: sudo /etc/init.d/xrdp start After the setup, run Remote Desktop Connection application in your Windows environment ( mstsc from Windows start menu). Enter localhost:3390 as the address and click Connect . You will be welcomed with xrdp login screen. Enter your WSL credentials and click OK to connect. You should now be seeing the xfce4 desktop of your Linux distro. You can use it to run any GUI-based applications. Networking considerations top # Compared to using Docker on Linux, using it on Windows through Docker Desktop application with WSL 2 backend has some significant differences in networking. Whenever you want to use any kind of networking functionality in container running on Docker Desktop, make sure to take points below into account. --net=host option for docker run will not behave as expected. Docker daemon runs inside an isolated network namespace, which from the perspective of Docker container is a host network. You won't be able to access containers started with this option through usual means. Docker Desktop provides special host entry ( host.docker.internal ) that resolves to your host machine. It always resolves to reachable IP from the container, and resolves to 127.0.0.1 on the host. If you can't connect to your container through localhost , try using host.docker.internal instead. Since you can't use --net=host option, all of the ports that will be used to communicate with the container have to be explicitly exposed using -p flag (see official documentation for details). They will be tunneled both to WSL 2 network namespace and Windows host.","title":"Running with WSL2"},{"location":"installation-guide/running-with-wsl2/#introduction","text":"Some of the tools commonly used with SVL Simulator, like Apollo or Autoware.Auto , might require Linux operating system to run. If you don't have Linux available or prefer using Windows, it's now possible to run Linux-exclusive programs using Windows Subsystem for Linux (WSL). Features available in WSL are dependent on its version, which is tied to the version of Windows 10 . Starting with build 20149, WSL supports GPU Paravirtualization (GPU-PV) that will let you run CUDA applications inside your WSL instance and in Docker containers. Build 21362 adds support for GUI applications (as an alternative, you can also set up desktop environment ).","title":"Introduction"},{"location":"installation-guide/running-with-wsl2/#installation","text":"NOTE: At the time of writing, most of the drivers and software required to use GPU-PV in WSL are still in preview. This might require using their pre-release versions. Details can be found in specific sub-sections.","title":"Installation"},{"location":"installation-guide/running-with-wsl2/#verify-windows-10-version","text":"In the Windows command line, enter: winver Reported OS Build should be 20149 or higher. If the build number is lower, and your system is up to date, required Windows version is not yet a part of public release. To use it, you will have to join Windows Insider Program on Dev Channel that will let you use early builds of Windows 10. If you're interested, please follow official instructions and update your Windows version.","title":"Verify Windows 10 version"},{"location":"installation-guide/running-with-wsl2/#install-wsl-2","text":"To install WSL 2, please refer to the official documentation . We recommend using Ubuntu 18.04 or Ubuntu 20.04 Linux distribution. This tutorial will assume Ubuntu 20.04 is installed. If you have installed WSL previously, make sure you're using WSL version 2. To check for version, enter: wsl -l -v The distribution you're planning to use should report 2 under VERSION . If you're using WSL 1, update it to WSL 2. Make sure your Linux kernel version is up to date. You can update it through an elevated Windows command line, by entering: wsl --update","title":"Install WSL 2"},{"location":"installation-guide/running-with-wsl2/#install-nvidia-drivers","text":"At the time of writing, NVIDIA drivers for CUDA on WSL are still in public preview. Version 470.14 (with CUDA 11.3) or higher is required. You can check your current driver version through Windows command line, by entering: nvidia-smi If your driver version is older, you can download required version from official NVIDIA web page . To verify that your GPU is working inside WSL, you can build and run one of default CUDA examples using WSL terminal: cd /usr/local/cuda/samples/4_Finance/BlackScholes make ./BlackScholes Your console should provide you with output similar to the one shown below. This means that your GPU is accessible inside of WSL and can be used to run CUDA programs. [./BlackScholes] - Starting... GPU Device 0: \"Pascal\" with compute capability 6.1 Initializing data... ...allocating CPU memory for options. ...allocating GPU memory for options. ...generating input data in CPU mem. ...copying input data to GPU mem. Data init done. Executing Black-Scholes GPU kernel (512 iterations)... Options count : 8000000 BlackScholesGPU() time : 0.363584 msec Effective memory bandwidth: 220.031695 GB/s Gigaoptions per second : 22.003170 BlackScholes, Throughput = 22.0032 GOptions/s, Time = 0.00036 s, Size = 8000000 options, NumDevsUsed = 1, Workgroup = 128 Reading back GPU results... Checking the results... ...running CPU calculations. Comparing the results... L1 norm: 1.741792E-07 Max absolute error: 1.192093E-05 Shutting down... ...releasing GPU memory. ...releasing CPU memory. Shutdown done. [BlackScholes] - Test Summary NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. Test passed","title":"Install NVIDIA drivers"},{"location":"installation-guide/running-with-wsl2/#install-docker-desktop-for-windows","text":"If you plan to use Docker inside your WSL 2 distro, we suggest to install Docker Desktop by following official documentation . Alternatively, you can decide to skip Docker Desktop and use Docker and NVIDIA Container Toolkit installed directly from your WSL 2. We recommend the first option - this tutorial will assume Docker Desktop for Windows is used. If you insist on using the second option, you can find instructions in official NVIDIA documentation . If you're already using Docker Desktop for Windows, make sure version 3.1 or higher is installed. After installing and launching Docker Desktop, navigate to Settings -> General and make sure that the option Use the WSL 2 based engine is enabled. After that, navigate to Settings -> Resources -> WSL integration and make sure that integration with your WSL 2 distro is enabled. Whenever you're using Docker from WSL, the Docker Desktop application must be running on your Windows machine - otherwise WSL won't be able to recognize service docker . To verify that Docker environment is running properly, enter your WSL terminal and launch a sample CUDA docker image from NVIDIA: docker run --rm -it --gpus=all --env NVIDIA_DISABLE_REQUIRE=1 nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark NOTE: Parameter --env NVIDIA_DISABLE_REQUIRE=1 disables CUDA version check. This is required at the time of writing due to bug in NVIDIA drivers (CUDA version reported in WSL is lower than installed). Your console should provide you with output similar to the one shown below. This means that your GPU is accessible inside of Docker containers and can be used to run CUDA programs. Run \"nbody -benchmark [-numbodies=<numBodies>]\" to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=<N> (number of bodies (>= 1) to run in simulation) -device=<d> (where d=0,1,2.... for the CUDA device to use) -numdevices=<i> (where i=(number of CUDA devices > 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=<file.bin> (load a tipsy model file for simulation) NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. > Windowed mode > Simulation data stored in video memory > Single precision floating point simulation > 1 Devices used for simulation GPU Device 0: \"Pascal\" with compute capability 6.1 > Compute 6.1 CUDA device: [NVIDIA GeForce GTX 1080] 20480 bodies, total time for 10 iterations: 16.788 ms = 249.832 billion interactions per second = 4996.645 single-precision GFLOP/s at 20 flops per interaction","title":"Install Docker Desktop for Windows"},{"location":"installation-guide/running-with-wsl2/#set-up-desktop-environment-(optional)","text":"If you want to run any GUI-based applications inside your WSL or Docker environment, you have configure X Window System. By default, display options inside of WSL are not configured and no valid output device is registered. This means not only that GUI will not be displayed, but also that any program attempting to output something to screen might not work correctly or fail to launch. If you have Windows 10 build 21362 or higher, you don't have to do anything - along your WSL instance, WSLg should have been automatically installed. WSLg pipes X11 and Wayland (used to run Linux GUI applications) directly into Windows graphical user interface. This means that any GUI application launched inside WSL will simply open on your Windows desktop. To verify that's the case, run any GUI-based application from your WSL instance. As an example, you can use one of OpenGL test applications. In your WSL terminal, enter: glxgears This should open new window with three spinning gears. If your Linux distro does not have this installed by default, you can get glxgears application from mesa-utils package: sudo apt install mesa-utils If you are able to launch Linux GUI applications directly on your Windows desktop, you can safely skip the rest of this section. However, if your Windows build version is lower than 21362 and you don't have access to WLSg, you can still use GUI applications through remote desktop. Remote desktop solution shown here will use xfce4 desktop environment for Linux and xrdp RDP server. Both modified configuration files ( /etc/xrdp/xrdp.ini and /etc/xrdp/startwm.sh ) are backed up with .bak extension if you ever need their original version. To install and configure the remote desktop for WSL, run commands shown below in your WSL terminal: sudo apt-get install xrdp sudo apt -y install xfce4 sudo cp /etc/xrdp/xrdp.ini /etc/xrdp/xrdp.ini.bak sudo sed -i 's/3389/3390/g' /etc/xrdp/xrdp.ini sudo sed -i 's/max_bpp=32/#max_bpp=32\\nmax_bpp=128/g' /etc/xrdp/xrdp.ini sudo sed -i 's/xserverbpp=24/#xserverbpp=24\\nxserverbpp=128/g' /etc/xrdp/xrdp.ini sudo cp /etc/xrdp/startwm.sh /etc/xrdp/startwm.sh.bak sudo sed -i 's+test -x /etc/X11/Xsession \\&\\& exec /etc/X11/Xsession+#test -x /etc/X11/Xsession \\&\\& exec /etc/X11 Xsession+g' /etc/xrdp/startwm.sh sudo sed -i 's+exec /bin/sh /etc/X11/Xsession+#exec /bin/sh /etc/X11/Xsession\\nstartxfce4+g' /etc/xrdp/startwm.sh To start RDP server that will allow you to connect to WSL desktop, run: sudo /etc/init.d/xrdp start After the setup, run Remote Desktop Connection application in your Windows environment ( mstsc from Windows start menu). Enter localhost:3390 as the address and click Connect . You will be welcomed with xrdp login screen. Enter your WSL credentials and click OK to connect. You should now be seeing the xfce4 desktop of your Linux distro. You can use it to run any GUI-based applications.","title":"Set up desktop environment (optional)"},{"location":"installation-guide/running-with-wsl2/#networking-considerations","text":"Compared to using Docker on Linux, using it on Windows through Docker Desktop application with WSL 2 backend has some significant differences in networking. Whenever you want to use any kind of networking functionality in container running on Docker Desktop, make sure to take points below into account. --net=host option for docker run will not behave as expected. Docker daemon runs inside an isolated network namespace, which from the perspective of Docker container is a host network. You won't be able to access containers started with this option through usual means. Docker Desktop provides special host entry ( host.docker.internal ) that resolves to your host machine. It always resolves to reachable IP from the container, and resolves to 127.0.0.1 on the host. If you can't connect to your container through localhost , try using host.docker.internal instead. Since you can't use --net=host option, all of the ports that will be used to communicate with the container have to be explicitly exposed using -p flag (see official documentation for details). They will be tunneled both to WSL 2 network namespace and Windows host.","title":"Networking considerations"},{"location":"installation-guide/system-requirements/","text":"System requirements The recommended system specification for running SVL Simulator (locally) are as follows: at least 4 GHz Quad core CPU NVIDIA GTX 1080 (8GB memory) or higher Windows 10 (64-bit), Ubuntu 18.04 (64-bit), or Ubuntu 20.04 (64-bit) While lower-specification hardware may be able to run SVL Simulator, there may be issues with the performance required to interface properly with a user\u2019s System Under Test. SVL Simulator is currently supported for Windows (64-bit) and Linux (64-bit). For optimal performance, Windows is recommended. Currently, the full functionality of SVL Simulator in Developer Mode (in Unity Editor) is supported on Windows only. End-to-end automatic simulations using PythonAPI Runtime template or Visual Scenario Runtime template are supported on Linux only. On Windows, SVL Simulator requires a graphics card that supports DirectX 11. On Linux, SVL Simulator requires a graphics card that supports Vulkan 1.1. If running Apollo or Autoware on the same system as the Simulator, it is recommended that the GPU have at least 10GB of memory. If running Apollo or Autoware on a different system as the Simulator, a gigabit connection between the systems is required (a gigabit switch is sufficient, gigabit internet is not required). For best results top # System and graphics performance varies tremendously from lightweight gaming laptops to high end graphics workstations. There are many different hardware factors that can influence overall performance including CPU model, clock frequency, number of cores, system RAM, GPU model and available GPU memory. In addition, the Simulator configuration (environment, vehicle, number and type of sensors) can affect performance, as well as the autonomous software configuration (e.g. enabled modules). Keep an eye on CPU load, system memory, and GPU memory. Running out of any of these can cause a variety of problems such as low frame rates or autonomous software modules being unable to function. The minimum goal for real time simulation should be 15fps (or the desired sensor frame rate) since that will avoid dropping frames for critical sensors like camera and LiDAR sensors. For complex multi-sensor simulations, refer to the Distributed Simulation docs for information on multi-machine (multi-GPU) distributed simulation. While it is possible to run sophisticated autonomous software stacks like Apollo or Autoware on the same machine that runs SVL Simulator, it will challenge even the highest performing systems. For best results, the Simulator should be run on a separate machine from the autonomous software. If it is not practical or possible to run autonomous software on a separate machine then you might want to consider using ground truth sensors in place of perception and traffic signal modules, as documented in Modular Testing . This will greatly reduce the CPU and GPU requirements of an autonomous stack such as Apollo and increase the likelihood that you can run it on the same machine with SVL Simulator.","title":"System requirements"},{"location":"installation-guide/system-requirements/#for-best-results","text":"System and graphics performance varies tremendously from lightweight gaming laptops to high end graphics workstations. There are many different hardware factors that can influence overall performance including CPU model, clock frequency, number of cores, system RAM, GPU model and available GPU memory. In addition, the Simulator configuration (environment, vehicle, number and type of sensors) can affect performance, as well as the autonomous software configuration (e.g. enabled modules). Keep an eye on CPU load, system memory, and GPU memory. Running out of any of these can cause a variety of problems such as low frame rates or autonomous software modules being unable to function. The minimum goal for real time simulation should be 15fps (or the desired sensor frame rate) since that will avoid dropping frames for critical sensors like camera and LiDAR sensors. For complex multi-sensor simulations, refer to the Distributed Simulation docs for information on multi-machine (multi-GPU) distributed simulation. While it is possible to run sophisticated autonomous software stacks like Apollo or Autoware on the same machine that runs SVL Simulator, it will challenge even the highest performing systems. For best results, the Simulator should be run on a separate machine from the autonomous software. If it is not practical or possible to run autonomous software on a separate machine then you might want to consider using ground truth sensors in place of perception and traffic signal modules, as documented in Modular Testing . This will greatly reduce the CPU and GPU requirements of an autonomous stack such as Apollo and increase the likelihood that you can run it on the same machine with SVL Simulator.","title":"For best results"},{"location":"plugins/bridge-plugins/","text":"Bridge Plugins Bridge plugins are custom bridge implementations that can be used by an ego vehicle to send and receive sensor data. Bridge plugins must be built by the simulator and the resulting bundle named bridge_XXX must be placed in the AssetBundles/Bridges folder. If running the binary, this folder is included in the downloaded .zip file. If running in Unity Editor (Developer Mode), the sensor will be built into the folder directly. This must be done before running the simulator (running the executable or pressing Play in the Editor). Building bridge plugins to a bundle is done as below: Open Simulator -> Build... menu item Select bridge plugins in Bridges section of build window Build plugins with Build button To make a bridge plugin, create a folder in Assets/External/Bridges , for example Assets/External/Bridge/CustomBridge . Inside this folder you must place all the scripts that will be used by the simulator to build C# managed assembly. Implementation top # A bridge plugin must provide two separate classes inheriting the following interfaces: 1) IBridgeFactory 2) IBridgeInstance Factory class is used to provide meta-information about bridge and a few factory methods. It will be created only once. Instance class represents the actual bridge implementation that is able to send and/or receive data. It will be created once for every vehicle where it is used. Bridge Factory top # public interface IBridgeFactory { IBridgeInstance CreateInstance(); void Register(IBridgePlugin plugin); void RegPublisher<DataType, BridgeType>(IBridgePlugin plugin, Func<DataType, BridgeType> converter); void RegSubscriber<DataType, BridgeType>(IBridgePlugin plugin, Func<BridgeType, DataType> converter); } This interface requires the class to expose four public methods: 1) CreateInstance - called to create new bridge instance. Typically this method simply returns new object of class that implements IBridgeInstance interface. 2) Register - called by simulator allow factory register supported functionality with plugin interface. See the section below on plugin registration. 3) RegPublisher and RegSubscriber - called by custom sensors to register custom data types for publisher or subscriber functionality. Factory can use this method itself to register its supported data types for simulator built-in sensors. Classes inheriting from this interface must have BridgeName attribute applied to inform name that will be used to identify this bridge. For example: [BridgeName(\"MyCustomBridge\")] public class MyCustomBridgeFactory : IBridgeFactory { ... } Bridge Instance top # public interface IBridgeInstance { Status Status { get; } void Connect(string connection); void Disconnect(); } This interface requires the class to expose three public methods: 1) Status - getter that will be called by simulator to query status of bridge. For example, sensors will be sending data only if Status is set to Connected state. 1) Connect - called when simulator wants to initate new connection. Connection string is passed unmodified from web interface where it can be set to anything. It is responsibility of bridge implementation to validate then accept or reject this string. 2) Disconnect - called when simulator wants to close connection. Typically called upon unloading vehicle from scene. Bridge Plugin Registration top # Upon creating a bridge factory, the simulator will call Register method to allow factory and pass plugin instance that factory should use to register available data types and publishers/subscribers. This information is not static as custom sensor plugins may want to register their custom types with one or multiple bridge factories. Simulator provides many different data types, for example ImageData or PointCloudData that represents specific instances of data published or subscribed by sensors. Factory must inform plugin about which types it supports. This is performed by calling AddType<DataType>(string bridgeTypeName) on plugin instance. DataType is one of the data types simulator supports. bridgeTypeName is an arbitrary string used to show name of this data type in UI. For each subscriber or publisher bridge factory should call AddSubscriberCreator<DataType> or AddPublisherCreator<DataType> on plugin instance. These methods should provide delegate that can be used to create subscriber or publisher instances for each sensor that wants to use specific data type. void AddSubscriberCreator<DataType>(SubscriberCreator<DataType> subscriber); void AddPublisherCreator<DataType>(PublisherCreator<DataType> publisher); See IBridgePlugin.cs source code for more information on delegate arguments and return values. Typically, the bridge will call AddType + AddSubscriberCreator from RegSubscriber method, and AddType + AddPublisherCreator from RegPublisher method. This should be done for all default simulator data types simulator supports. Additionally, custom sensor plugins may choose to call your bridge RegPublisher and RegSubscriber methods to add custom data types they which to publish or subscribe. Please refer to ComfortSensor and LaneFollowingSensor sensor plugins for examples of this functionality. Registering Plugin with Web UI top # All bridge plugins must be registered with the SVL Simulator Web UI to be usable. This currently needs to be done manually by contacting us . Examples top # Open-source examples are available: ROS Bridge - ROS bridge implementation for Autoware ROSApollo Bridge - ROS bridge implementation for Apollo ROS2 Bridge - ROS2 bridge implementation CyberRT Bridge - CyberRT bridge implementation Logging Bridge - simple bridge that logs all published data to a .txt file.","title":"Bridge plugins"},{"location":"plugins/bridge-plugins/#implementation","text":"A bridge plugin must provide two separate classes inheriting the following interfaces: 1) IBridgeFactory 2) IBridgeInstance Factory class is used to provide meta-information about bridge and a few factory methods. It will be created only once. Instance class represents the actual bridge implementation that is able to send and/or receive data. It will be created once for every vehicle where it is used.","title":"Implementation"},{"location":"plugins/bridge-plugins/#bridge-factory","text":"public interface IBridgeFactory { IBridgeInstance CreateInstance(); void Register(IBridgePlugin plugin); void RegPublisher<DataType, BridgeType>(IBridgePlugin plugin, Func<DataType, BridgeType> converter); void RegSubscriber<DataType, BridgeType>(IBridgePlugin plugin, Func<BridgeType, DataType> converter); } This interface requires the class to expose four public methods: 1) CreateInstance - called to create new bridge instance. Typically this method simply returns new object of class that implements IBridgeInstance interface. 2) Register - called by simulator allow factory register supported functionality with plugin interface. See the section below on plugin registration. 3) RegPublisher and RegSubscriber - called by custom sensors to register custom data types for publisher or subscriber functionality. Factory can use this method itself to register its supported data types for simulator built-in sensors. Classes inheriting from this interface must have BridgeName attribute applied to inform name that will be used to identify this bridge. For example: [BridgeName(\"MyCustomBridge\")] public class MyCustomBridgeFactory : IBridgeFactory { ... }","title":"Bridge Factory"},{"location":"plugins/bridge-plugins/#bridge-instance","text":"public interface IBridgeInstance { Status Status { get; } void Connect(string connection); void Disconnect(); } This interface requires the class to expose three public methods: 1) Status - getter that will be called by simulator to query status of bridge. For example, sensors will be sending data only if Status is set to Connected state. 1) Connect - called when simulator wants to initate new connection. Connection string is passed unmodified from web interface where it can be set to anything. It is responsibility of bridge implementation to validate then accept or reject this string. 2) Disconnect - called when simulator wants to close connection. Typically called upon unloading vehicle from scene.","title":"Bridge Instance"},{"location":"plugins/bridge-plugins/#bridge-plugin-registration","text":"Upon creating a bridge factory, the simulator will call Register method to allow factory and pass plugin instance that factory should use to register available data types and publishers/subscribers. This information is not static as custom sensor plugins may want to register their custom types with one or multiple bridge factories. Simulator provides many different data types, for example ImageData or PointCloudData that represents specific instances of data published or subscribed by sensors. Factory must inform plugin about which types it supports. This is performed by calling AddType<DataType>(string bridgeTypeName) on plugin instance. DataType is one of the data types simulator supports. bridgeTypeName is an arbitrary string used to show name of this data type in UI. For each subscriber or publisher bridge factory should call AddSubscriberCreator<DataType> or AddPublisherCreator<DataType> on plugin instance. These methods should provide delegate that can be used to create subscriber or publisher instances for each sensor that wants to use specific data type. void AddSubscriberCreator<DataType>(SubscriberCreator<DataType> subscriber); void AddPublisherCreator<DataType>(PublisherCreator<DataType> publisher); See IBridgePlugin.cs source code for more information on delegate arguments and return values. Typically, the bridge will call AddType + AddSubscriberCreator from RegSubscriber method, and AddType + AddPublisherCreator from RegPublisher method. This should be done for all default simulator data types simulator supports. Additionally, custom sensor plugins may choose to call your bridge RegPublisher and RegSubscriber methods to add custom data types they which to publish or subscribe. Please refer to ComfortSensor and LaneFollowingSensor sensor plugins for examples of this functionality.","title":"Bridge Plugin Registration"},{"location":"plugins/bridge-plugins/#registering-plugin-with-web-ui","text":"All bridge plugins must be registered with the SVL Simulator Web UI to be usable. This currently needs to be done manually by contacting us .","title":"Registering Plugin with Web UI"},{"location":"plugins/bridge-plugins/#examples","text":"Open-source examples are available: ROS Bridge - ROS bridge implementation for Autoware ROSApollo Bridge - ROS bridge implementation for Apollo ROS2 Bridge - ROS2 bridge implementation CyberRT Bridge - CyberRT bridge implementation Logging Bridge - simple bridge that logs all published data to a .txt file.","title":"Examples"},{"location":"plugins/chargingstation-controllable/","text":"Controllable - ChargingStation This contains detailed documentation on the ChargingStation controllable plugin. To use this Controllable Plugin: 1) Clone the repo into Assets/External/Controllables/ChargingStation inside of your Simulator Unity Project 2) Build the Controllable Plugin for use with the Simulator, navigate to the Simulator -> Build Controllables Unity Editor menu item. Select ChargingStation controllable and build. Output bundle will be in AssetBundles/Controllables folder in root of Simulator Unity Project 3) Simulator will load, at runtime, all custom Controllable Plugin bundles in AssetBundles/Controllables directory Custom Logic top # To implement custom logic, contained in a given Controllable Plugin project there must be an IControllable implementation. An example of this is in TrafficCone.cs The interface requires the following to be implemented: public bool Spawned { get; set; } = false; public string UID { get; set; } public string GUID => UID; public string ControlType { get; set; } = \"chargingstation\"; public string CurrentState { get; set; } public string[] ValidStates { get; set; } = new[] { \"on\", \"off\" }; public string[] ValidActions { get; set; } = new string[] { }; public List<ControlAction> DefaultControlPolicy { get; set; } = new List<ControlAction> { new ControlAction { Action = \"state\", Value = \"off\" } }; public List<ControlAction> CurrentControlPolicy { get; set; } public Control(List<ControlAction> controlActions) { // } On Awake() CurrentControlPolicy and CurrentState must be set, e.g. private void Awake() { Lights.AddRange(GetComponentsInChildren<Light>()); SetLights(false); ChargingStationRenderer = GetComponent<Renderer>(); foreach (var mat in ChargingStationRenderer.materials) { if (mat.name.Contains(\"Emission\")) { EmissionMaterials.Add(mat); } } CurrentControlPolicy = DefaultControlPolicy; Control(CurrentControlPolicy); } Control checks the parsed ControlActions and sets the CurrentState public void Control(List<ControlAction> controlActions) { foreach (var action in controlActions) { switch (action.Action) { case \"state\": SetChargingStationState(action.Value); break; default: Debug.LogError($\"'{action.Value}' is an invalid action for '{ControlType}'\"); break; } } } public void SetChargingStationState(string value) { if (!ValidStates.Contains(value)) { Debug.LogError($\"'{value}' is an invalid state for '{ControlType}'\"); return; } CurrentState = value; switch (CurrentState) { case \"on\": EmissionMaterials?.ForEach(x => x.SetColor(\"_EmissiveColor\", Color.white)); EmissionMaterials?.ForEach(x => x.SetFloat(\"_EmissiveIntensity\", 3f)); SetLights(true); break; case \"off\": EmissionMaterials?.ForEach(x => x.SetColor(\"_EmissiveColor\", Color.black)); EmissionMaterials?.ForEach(x => x.SetFloat(\"_EmissiveIntensity\", 0f)); SetLights(false); break; default: break; } } private void SetLights(bool state) { Lights.ForEach(l => l.enabled = state); } Python API example top # Each controllable object has its own valid actions (e.g., green, yellow, red, trigger, wait, loop, on, off, \"\") that it can take and is controlled based on control policy , which defines rules for control actions. To get a list of controllable objects in a scene: controllables = sim.get_controllables() For a controllable object of interest, you can get following information: station = controllables[0] print(\"Type:\", station.type) print(\"Transform:\", station.transform) print(\"Current state:\", station.current_state) print(\"Valid actions:\", station.valid_actions) For control policy, each controllable object always has default control policy (read-only). When you load a scene for the first time or reset a scene to the initial state, a controllable object resets current control policy to default one follows it. You can get default control policy and current control policy as follows: print(\"Default control policy:\", station.default_control_policy) print(\"Current control policy:\", station.control_policy) To change a current control policy, you can create a new control policy and call control function as below: control_policy = \"on\" station.control(control_policy) To add a plugin controllable and set object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0,0,0) state.transform.rotation = lgsvl.Vector(0,0,0) state.velocity = lgsvl.Vector(0,10,0) state.angular_velocity = lgsvl.Vector(6.5,0,0) cone = sim.controllable_add(\"ChargingStation\", state) To get plugin controllable object state station.object_state To set plugin controllable object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0, 0, -10) station.object_state = state Controllables can also have a Unity RigidBody component at the root and apply velocity from the API. If no rigidbody exists then the velocity is ignored. #!/usr/bin/env python3 # # Copyright (c) 2020-2021 LG Electronics, Inc. # # This software contains code licensed as described in LICENSE. # from environs import Env import lgsvl sim = lgsvl.Simulator(env.str(\"LGSVL__SIMULATOR_HOST\", lgsvl.wise.SimulatorSettings.simulator_host), env.int(\"LGSVL__SIMULATOR_PORT\", lgsvl.wise.SimulatorSettings.simulator_port)) if sim.current_scene == lgsvl.wise.DefaultAssets.map_borregasave: sim.reset() else: sim.load(lgsvl.wise.DefaultAssets.map_borregasave) spawns = sim.get_spawn() state = lgsvl.AgentState() forward = lgsvl.utils.transform_to_forward(spawns[0]) right = lgsvl.utils.transform_to_right(spawns[0]) up = lgsvl.utils.transform_to_up(spawns[0]) state.transform = spawns[0] ego = sim.add_agent(env.str(\"LGSVL__VEHICLE_0\", lgsvl.wise.DefaultAssets.ego_lincoln2017mkz_apollo5), lgsvl.AgentType.EGO, state) print(\"Python API Quickstart #28: How to Add/Control Charging Station\") obj_state = lgsvl.ObjectState() obj_state.transform.position = lgsvl.Vector(38, 0, 7) obj_state.transform.rotation = lgsvl.Vector(0, 180, 0) station = sim.controllable_add(\"ChargingStation\", obj_state) station = sim.get_controllable(lgsvl.Vector(38, 0, 7), \"chargingstation\") print(\"\\n# Charging Station of interest:\") print(station) seconds = 1 input(\"\\nPress Enter to run simulation for {} seconds\".format(seconds)) print(\"\\nRunning simulation for {} seconds...\".format(seconds)) sim.run(seconds) # Get current controllable state print(\"\\n# Current charging station control policy:\") print(station.control_policy) print(\"\\n# Current charging station object state\") print(station.object_state) print(\"\\n# Update charging station object state\") new_state = lgsvl.ObjectState() new_state.transform.position = lgsvl.Vector(38, 0, 7) new_state.transform.rotation = lgsvl.Vector(0, 180, 0) station.object_state = new_state print(\"\\n# New object state\") print(station.object_state) # Set time of day for light effects sim.set_time_of_day(19.0) print(sim.time_of_day) # Create a new control policy control_policy = \"on\" # Control this traffic light with a new control policy station.control(control_policy) print(\"\\n# Updated control policy:\") print(station.control_policy) # Get current state of charging station print(\"\\n# Current station state:\") print(station.current_state) print(\"\\nDone!\")","title":"Controllable - ChargingStation"},{"location":"plugins/chargingstation-controllable/#custom-logic","text":"To implement custom logic, contained in a given Controllable Plugin project there must be an IControllable implementation. An example of this is in TrafficCone.cs The interface requires the following to be implemented: public bool Spawned { get; set; } = false; public string UID { get; set; } public string GUID => UID; public string ControlType { get; set; } = \"chargingstation\"; public string CurrentState { get; set; } public string[] ValidStates { get; set; } = new[] { \"on\", \"off\" }; public string[] ValidActions { get; set; } = new string[] { }; public List<ControlAction> DefaultControlPolicy { get; set; } = new List<ControlAction> { new ControlAction { Action = \"state\", Value = \"off\" } }; public List<ControlAction> CurrentControlPolicy { get; set; } public Control(List<ControlAction> controlActions) { // } On Awake() CurrentControlPolicy and CurrentState must be set, e.g. private void Awake() { Lights.AddRange(GetComponentsInChildren<Light>()); SetLights(false); ChargingStationRenderer = GetComponent<Renderer>(); foreach (var mat in ChargingStationRenderer.materials) { if (mat.name.Contains(\"Emission\")) { EmissionMaterials.Add(mat); } } CurrentControlPolicy = DefaultControlPolicy; Control(CurrentControlPolicy); } Control checks the parsed ControlActions and sets the CurrentState public void Control(List<ControlAction> controlActions) { foreach (var action in controlActions) { switch (action.Action) { case \"state\": SetChargingStationState(action.Value); break; default: Debug.LogError($\"'{action.Value}' is an invalid action for '{ControlType}'\"); break; } } } public void SetChargingStationState(string value) { if (!ValidStates.Contains(value)) { Debug.LogError($\"'{value}' is an invalid state for '{ControlType}'\"); return; } CurrentState = value; switch (CurrentState) { case \"on\": EmissionMaterials?.ForEach(x => x.SetColor(\"_EmissiveColor\", Color.white)); EmissionMaterials?.ForEach(x => x.SetFloat(\"_EmissiveIntensity\", 3f)); SetLights(true); break; case \"off\": EmissionMaterials?.ForEach(x => x.SetColor(\"_EmissiveColor\", Color.black)); EmissionMaterials?.ForEach(x => x.SetFloat(\"_EmissiveIntensity\", 0f)); SetLights(false); break; default: break; } } private void SetLights(bool state) { Lights.ForEach(l => l.enabled = state); }","title":"Custom Logic"},{"location":"plugins/chargingstation-controllable/#python-api-example","text":"Each controllable object has its own valid actions (e.g., green, yellow, red, trigger, wait, loop, on, off, \"\") that it can take and is controlled based on control policy , which defines rules for control actions. To get a list of controllable objects in a scene: controllables = sim.get_controllables() For a controllable object of interest, you can get following information: station = controllables[0] print(\"Type:\", station.type) print(\"Transform:\", station.transform) print(\"Current state:\", station.current_state) print(\"Valid actions:\", station.valid_actions) For control policy, each controllable object always has default control policy (read-only). When you load a scene for the first time or reset a scene to the initial state, a controllable object resets current control policy to default one follows it. You can get default control policy and current control policy as follows: print(\"Default control policy:\", station.default_control_policy) print(\"Current control policy:\", station.control_policy) To change a current control policy, you can create a new control policy and call control function as below: control_policy = \"on\" station.control(control_policy) To add a plugin controllable and set object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0,0,0) state.transform.rotation = lgsvl.Vector(0,0,0) state.velocity = lgsvl.Vector(0,10,0) state.angular_velocity = lgsvl.Vector(6.5,0,0) cone = sim.controllable_add(\"ChargingStation\", state) To get plugin controllable object state station.object_state To set plugin controllable object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0, 0, -10) station.object_state = state Controllables can also have a Unity RigidBody component at the root and apply velocity from the API. If no rigidbody exists then the velocity is ignored. #!/usr/bin/env python3 # # Copyright (c) 2020-2021 LG Electronics, Inc. # # This software contains code licensed as described in LICENSE. # from environs import Env import lgsvl sim = lgsvl.Simulator(env.str(\"LGSVL__SIMULATOR_HOST\", lgsvl.wise.SimulatorSettings.simulator_host), env.int(\"LGSVL__SIMULATOR_PORT\", lgsvl.wise.SimulatorSettings.simulator_port)) if sim.current_scene == lgsvl.wise.DefaultAssets.map_borregasave: sim.reset() else: sim.load(lgsvl.wise.DefaultAssets.map_borregasave) spawns = sim.get_spawn() state = lgsvl.AgentState() forward = lgsvl.utils.transform_to_forward(spawns[0]) right = lgsvl.utils.transform_to_right(spawns[0]) up = lgsvl.utils.transform_to_up(spawns[0]) state.transform = spawns[0] ego = sim.add_agent(env.str(\"LGSVL__VEHICLE_0\", lgsvl.wise.DefaultAssets.ego_lincoln2017mkz_apollo5), lgsvl.AgentType.EGO, state) print(\"Python API Quickstart #28: How to Add/Control Charging Station\") obj_state = lgsvl.ObjectState() obj_state.transform.position = lgsvl.Vector(38, 0, 7) obj_state.transform.rotation = lgsvl.Vector(0, 180, 0) station = sim.controllable_add(\"ChargingStation\", obj_state) station = sim.get_controllable(lgsvl.Vector(38, 0, 7), \"chargingstation\") print(\"\\n# Charging Station of interest:\") print(station) seconds = 1 input(\"\\nPress Enter to run simulation for {} seconds\".format(seconds)) print(\"\\nRunning simulation for {} seconds...\".format(seconds)) sim.run(seconds) # Get current controllable state print(\"\\n# Current charging station control policy:\") print(station.control_policy) print(\"\\n# Current charging station object state\") print(station.object_state) print(\"\\n# Update charging station object state\") new_state = lgsvl.ObjectState() new_state.transform.position = lgsvl.Vector(38, 0, 7) new_state.transform.rotation = lgsvl.Vector(0, 180, 0) station.object_state = new_state print(\"\\n# New object state\") print(station.object_state) # Set time of day for light effects sim.set_time_of_day(19.0) print(sim.time_of_day) # Create a new control policy control_policy = \"on\" # Control this traffic light with a new control policy station.control(control_policy) print(\"\\n# Updated control policy:\") print(station.control_policy) # Get current state of charging station print(\"\\n# Current station state:\") print(station.current_state) print(\"\\nDone!\")","title":"Python API example"},{"location":"plugins/controllable-plugins/","text":"Controllable Plugins Controllable plugins are custom controllables that can be added to a scene at runtime with the API. Before running the simulator (running the executable or pressing Play in the Editor) controllable plugins must be built by the simulator using Simulator -> Build... menu and the built asset placed in the AssetBundles/Controllables folder. Open-source example: TrafficCone Table of Contents Building Controllable Plugins Creating Controllable Plugins Controllable Logic Building Controllable Plugins top # Open Simulator -> Build... menu item Select controllable plugins in \"Controllables\" section of build window Build plugins with \"Build\" button The bundle named controllable_XXX will be placed in the AssetBundles/Controllables folder. If building the binary, this folder is included in the target destination. See build instructions for more details. Creating Controllable Plugins top # Create a folder in Assets/External/Controllables , e.g. Assets/External/Controllables/TrafficCone. Inside this folder you must place the controllable prefab with same name ( TrafficCone.prefab ) that will be used by simulator to instantiate at runtime, the controllable logic script and Models folder with materials and textures. This prefab must have a logic script that inherits interface IControllable and added to the root of the prefab. The controllable tag and layer for traffic cone is set to Obstacle. If collisions are desired, then add a collider components. A Rigidbody component can be added if velocity changes are desired, if not, velocity change commands will be ignored. To create a prefab: Right-click in the scene hierarchy and select Create Empty Change the name to the name of the controllable (e.g. TrafficCone ) In the Inspector for this object, select Add Component Search for the controllable script Add Rigidbody if needed Add Collider if needed Drag this object from the scene hierarchy into the project folder to create a new prefab, delete prefab in Hierarchy panel Controllable Logic top # Additionally place a C# script which will be compiled & bundled with prefab, as well as any additional Unity resources (shaders, materials, textures, etc...). This is required even for simple objects such as a Traffic Cone. Controllable scripts must inherit interface IControllable which allows controllables to receive API commands. In addition, it must implement all interface variables and methods. See the below code block from TrafficCone.cs: namespace Simulator.Controllable { public class TrafficCone : MonoBehaviour, IControllable { public bool Spawned { get; set; } public string UID { get; set; } public string GUID => UID; public string ControlType { get; set; } = \"cone\"; public string CurrentState { get; set; } public string[] ValidStates { get; set; } = new string[] { }; public string[] ValidActions { get; set; } = new string[] { }; public List<ControlAction> DefaultControlPolicy { get; set; } = new List<ControlAction> { new ControlAction { Action = \"state\", Value = \"\" } }; public List<ControlAction> CurrentControlPolicy { get; set; } private void Awake() { CurrentControlPolicy = DefaultControlPolicy; Control(CurrentControlPolicy); } protected void OnDestroy() { Resources.UnloadUnusedAssets(); } public void Control(List<ControlAction> controlActions) { foreach (var action in controlActions) { switch (action.Action) { case \"state\": CurrentState = action.Value; break; default: Debug.LogError($\"'{action.Action}' is an invalid action for '{ControlType}'\"); break; } } } } }","title":"Controllable plugins"},{"location":"plugins/controllable-plugins/#building-controllable-plugins","text":"Open Simulator -> Build... menu item Select controllable plugins in \"Controllables\" section of build window Build plugins with \"Build\" button The bundle named controllable_XXX will be placed in the AssetBundles/Controllables folder. If building the binary, this folder is included in the target destination. See build instructions for more details.","title":"Building Controllable Plugins"},{"location":"plugins/controllable-plugins/#creating-controllable-plugins","text":"Create a folder in Assets/External/Controllables , e.g. Assets/External/Controllables/TrafficCone. Inside this folder you must place the controllable prefab with same name ( TrafficCone.prefab ) that will be used by simulator to instantiate at runtime, the controllable logic script and Models folder with materials and textures. This prefab must have a logic script that inherits interface IControllable and added to the root of the prefab. The controllable tag and layer for traffic cone is set to Obstacle. If collisions are desired, then add a collider components. A Rigidbody component can be added if velocity changes are desired, if not, velocity change commands will be ignored. To create a prefab: Right-click in the scene hierarchy and select Create Empty Change the name to the name of the controllable (e.g. TrafficCone ) In the Inspector for this object, select Add Component Search for the controllable script Add Rigidbody if needed Add Collider if needed Drag this object from the scene hierarchy into the project folder to create a new prefab, delete prefab in Hierarchy panel","title":"Creating Controllable Plugins"},{"location":"plugins/controllable-plugins/#controllable-logic","text":"Additionally place a C# script which will be compiled & bundled with prefab, as well as any additional Unity resources (shaders, materials, textures, etc...). This is required even for simple objects such as a Traffic Cone. Controllable scripts must inherit interface IControllable which allows controllables to receive API commands. In addition, it must implement all interface variables and methods. See the below code block from TrafficCone.cs: namespace Simulator.Controllable { public class TrafficCone : MonoBehaviour, IControllable { public bool Spawned { get; set; } public string UID { get; set; } public string GUID => UID; public string ControlType { get; set; } = \"cone\"; public string CurrentState { get; set; } public string[] ValidStates { get; set; } = new string[] { }; public string[] ValidActions { get; set; } = new string[] { }; public List<ControlAction> DefaultControlPolicy { get; set; } = new List<ControlAction> { new ControlAction { Action = \"state\", Value = \"\" } }; public List<ControlAction> CurrentControlPolicy { get; set; } private void Awake() { CurrentControlPolicy = DefaultControlPolicy; Control(CurrentControlPolicy); } protected void OnDestroy() { Resources.UnloadUnusedAssets(); } public void Control(List<ControlAction> controlActions) { foreach (var action in controlActions) { switch (action.Action) { case \"state\": CurrentState = action.Value; break; default: Debug.LogError($\"'{action.Action}' is an invalid action for '{ControlType}'\"); break; } } } } }","title":"Controllable Logic"},{"location":"plugins/lidar-plugin/","text":"LiDAR Sensor Plugin This page introduces the Velodyne LiDAR Sensor plugin, as well as how to build your own LiDAR sensor plugin. Table of Contents Velodyne LiDAR Sensor Plugin Velodyne LiDAR Sensor JSON options Velodyne LiDAR Sensor Usage Running with Autoware Running with Apollo 5.0 Build Your Own LiDAR Sensor Plugin Velodyne LiDAR Sensor Plugin top # This sensor plugin is for Velodyne LiDAR . VLP-16, VLP-32C and VLS-128 are currently supported. The built asset bundle of this plugin (named sensor_VelodyneLidarSensor ) can be found in AssetBundles/Sensors folder when you unzip the downloaded SVL Simulator (i.e. in the same level of the Simulator executable). The Velodyne LiDAR Sensor is implemented following exact intrinsics of real Velodyne LiDAR, such as elevation angles and azimuth offsets. Particularly, each laser beam in Velodyne LiDAR sensor has azimuth offset same as the real LiDAR, while the normal LiDAR Sensor assumes all laser beams are on same vertical line (i.e. no azimuth offset). In contrast to the standard LiDAR Sensor , which generates point cloud and publishes it via bridge, Velodyne LiDAR sensor generates data packets and position packets and sends them out via UDP socket. Velodyne driver running on the host machine (the machine which receives the packets) is responsible for converting these packets into point cloud and publish it out. This will greatly alleviate the burden on bridge bandwidth, so that the simulation can support more sensors (e.g. camera sensors) simultaneously. See this issue for an example of exhausted bridge bandwidth. Velodyne LiDAR Sensor JSON options top # Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF VelodyneLidarType defines type of Velodyne LiDAR String VLP_16 HostName IP address of host String UDPPortData UDP port for data packets Int 2368 UDPPortPosition UDP port for position packets Int 8308 * Most of parameters except the last four are same as LiDAR Sensor . Details of last four parameters are as follows: Value of VelodyneLidarType can only be \"VLP_16\", \"VLP_32C\" or \"VLS_128\". Note that it uses underscore ('_') not dash ('-'). HostName is the IP address of the machine which receives the UDP packets (a.k.a. host machine). UDPPortData and UDPPortPosition are UPD ports for data packets and position packets. If more than one Velodyne LiDAR plugin is used, each one should have a unique port. VerticalRayAngles , LaserCount , FieldOfView , and CenterAngle will be ignored for Velodyne LiDAR since they will be set internally following the corresponding model spec. VLP-32C configuration sample: { \"type\": \"VelodyneLidarSensor\", \"name\": \"Velodyne VLP-32C\", \"params\": { \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\", \"VelodyneLidarType\": \"VLP_32C\", \"HostName\": \"127.0.0.1\", \"UdpPortData\": 2368, \"UdpPortPosition\": 8308 }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Velodyne LiDAR Sensor Usage top # Running with Autoware top # Autoware is based on ROS. For ROS-based systems, ROS Velodyne driver can be used. Detailed steps of running ROS Velodyne driver are as follows: 1. Create a workspace folder and enter it mkdir velodyne_ws && cd velodyne_ws 2. Clone ROS Velodyne driver into src folder git clone https://github.com/ros-drivers/velodyne.git src 3. Build the Velodyne drive as a ROS node catkin_make 4. Setup running environment source /opt/$ROS_DISTRO/setup.bash source devel/setup.bash 5. Configuration of device IP Before running the Velodyne driver, you need to modify the launch files to setup device IP (i.e. the IP of the machine where the SVL Simulator is running). For VLP-16, edit velodyne_ws/src/velodyne/velodyne_pointcloud/launch/VLP16_points.launch and put the device IP after device_ip . For VLP-32, edit velodyne_ws/src/velodyne/velodyne_pointcloud/launch/VLP-32C_points.launch and put the device IP after device_ip . ROS Velodyne driver does not support VLS-128 for now. For more details please refer to the official page . 6. Launch Velodyne driver For VLP-16: roslaunch velodyne_pointcloud VLP16_points.launch For VLP-32C: roslaunch velodyne_pointcloud VLP-32C_points.launch If you have SVL Simulator running on client machine, you should be able to see UDP packets received on both data port and position port, and ROS topic /velodyne_points is published by the driver. You can also use RViz to visualize the point cloud in that topic. Fig. 1 shows point cloud of VLP-32C visualized in the simulator, Fig. 1: Visualized point clouds of VLP-32C LiDAR in SVL Simulator. and Fig. 2 shows the same point cloud visualized in RViz (click to see in full resolution): Fig. 2: Visualized point clouds of VLP-32C LiDAR in RViz. Note that the output topic name ( /velodyne_points ) of ROS Velodyne driver is hard-coded and not configurable, while Autoware assumes point cloud published into ROS topic /points_raw . To have ROS Velodyne driver running with Autoware, you have to either use <remap> tag in Autoware launch files to may /velodyne_points to /points_raw , or modify the topic name in the source code of ROS Velodyne driver and rebuild it. Running with Apollo 5.0 top # Apollo 5.0 is based on CyberRT and comes with its own Velodyne driver . Detailed steps of running ROS Velodyne driver are as follows: 1. Follow these instructions to start Apollo 5.0 and launch bridge. 2. (optional) Configure the LiDAR model if your LiDAR setting is different to the default setting of Apollo 5.0. To configure the LiDAR model, you can edit velodyne.dag file. Note that if more than one LiDAR is used, each has different data port and position port (configured in their corresponding .conf files . You need to set UDPPortData and UDPPortPosition for each Velodyne LiDAR sensor accordingly. The default launch file and dag file of Apollo 5.0 use VLS-128 and VLP-16 LiDARs. If you need to use VLP-32C, in addition to add VLP-32C to dag file, you may need to modify static_transform_conf.pb.txt to include your own VLP-32C extrinsics if you want to get compensated point cloud. 3. Launch GPS , Localization , Transform , and Velodyne modules in Module Controller page of Dreamview. Fig. 3 shows the Dreamview web interface: Fig. 3: Dreamview web interface. On the Simulator side, you can add Velodyne LiDAR sensor into our sample JSON . If you have SVL Simulator running on client machine, you should be able to see UDP packets received on both data port and position port, and cyber_monitor should shows point clouds published into corresponding Cyber channels. You can also use cyber_visualizer to visualize the point cloud in those channels. Fig. 4 shows the point cloud of VLP-32C LiDAR visualized in Cyber Visualizer (click to see in full resolution): Fig. 4: Visualized point clouds of VLP-32C LiDAR in Cyber Visualizer. Build Your Own LiDAR Sensor Plugin top # If you want to build your own LiDAR sensor plugin to support other LiDAR models, you can follow the general instructions on building sensor plugins. Instead of deriving your plugin class from SensorBase , you can derive your class from LidarSensorBase , so that you can reuse most of the code there, focusing only on raw data generation and sending.","title":"<a name=\"top\"></a> LiDAR Sensor Plugin"},{"location":"plugins/lidar-plugin/#velodyne-lidar-sensor-plugins","text":"This sensor plugin is for Velodyne LiDAR . VLP-16, VLP-32C and VLS-128 are currently supported. The built asset bundle of this plugin (named sensor_VelodyneLidarSensor ) can be found in AssetBundles/Sensors folder when you unzip the downloaded SVL Simulator (i.e. in the same level of the Simulator executable). The Velodyne LiDAR Sensor is implemented following exact intrinsics of real Velodyne LiDAR, such as elevation angles and azimuth offsets. Particularly, each laser beam in Velodyne LiDAR sensor has azimuth offset same as the real LiDAR, while the normal LiDAR Sensor assumes all laser beams are on same vertical line (i.e. no azimuth offset). In contrast to the standard LiDAR Sensor , which generates point cloud and publishes it via bridge, Velodyne LiDAR sensor generates data packets and position packets and sends them out via UDP socket. Velodyne driver running on the host machine (the machine which receives the packets) is responsible for converting these packets into point cloud and publish it out. This will greatly alleviate the burden on bridge bandwidth, so that the simulation can support more sensors (e.g. camera sensors) simultaneously. See this issue for an example of exhausted bridge bandwidth.","title":"Velodyne LiDAR Sensor Plugin"},{"location":"plugins/lidar-plugin/#velodyne-lidar-sensor-json-options","text":"Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF VelodyneLidarType defines type of Velodyne LiDAR String VLP_16 HostName IP address of host String UDPPortData UDP port for data packets Int 2368 UDPPortPosition UDP port for position packets Int 8308 * Most of parameters except the last four are same as LiDAR Sensor . Details of last four parameters are as follows: Value of VelodyneLidarType can only be \"VLP_16\", \"VLP_32C\" or \"VLS_128\". Note that it uses underscore ('_') not dash ('-'). HostName is the IP address of the machine which receives the UDP packets (a.k.a. host machine). UDPPortData and UDPPortPosition are UPD ports for data packets and position packets. If more than one Velodyne LiDAR plugin is used, each one should have a unique port. VerticalRayAngles , LaserCount , FieldOfView , and CenterAngle will be ignored for Velodyne LiDAR since they will be set internally following the corresponding model spec. VLP-32C configuration sample: { \"type\": \"VelodyneLidarSensor\", \"name\": \"Velodyne VLP-32C\", \"params\": { \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\", \"VelodyneLidarType\": \"VLP_32C\", \"HostName\": \"127.0.0.1\", \"UdpPortData\": 2368, \"UdpPortPosition\": 8308 }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Velodyne LiDAR Sensor JSON options"},{"location":"plugins/lidar-plugin/#velodyne-lidar-sensor-usage","text":"","title":"Velodyne LiDAR Sensor Usage"},{"location":"plugins/lidar-plugin/#running-with-autoware","text":"Autoware is based on ROS. For ROS-based systems, ROS Velodyne driver can be used. Detailed steps of running ROS Velodyne driver are as follows: 1. Create a workspace folder and enter it mkdir velodyne_ws && cd velodyne_ws 2. Clone ROS Velodyne driver into src folder git clone https://github.com/ros-drivers/velodyne.git src 3. Build the Velodyne drive as a ROS node catkin_make 4. Setup running environment source /opt/$ROS_DISTRO/setup.bash source devel/setup.bash 5. Configuration of device IP Before running the Velodyne driver, you need to modify the launch files to setup device IP (i.e. the IP of the machine where the SVL Simulator is running). For VLP-16, edit velodyne_ws/src/velodyne/velodyne_pointcloud/launch/VLP16_points.launch and put the device IP after device_ip . For VLP-32, edit velodyne_ws/src/velodyne/velodyne_pointcloud/launch/VLP-32C_points.launch and put the device IP after device_ip . ROS Velodyne driver does not support VLS-128 for now. For more details please refer to the official page . 6. Launch Velodyne driver For VLP-16: roslaunch velodyne_pointcloud VLP16_points.launch For VLP-32C: roslaunch velodyne_pointcloud VLP-32C_points.launch If you have SVL Simulator running on client machine, you should be able to see UDP packets received on both data port and position port, and ROS topic /velodyne_points is published by the driver. You can also use RViz to visualize the point cloud in that topic. Fig. 1 shows point cloud of VLP-32C visualized in the simulator, Fig. 1: Visualized point clouds of VLP-32C LiDAR in SVL Simulator. and Fig. 2 shows the same point cloud visualized in RViz (click to see in full resolution): Fig. 2: Visualized point clouds of VLP-32C LiDAR in RViz. Note that the output topic name ( /velodyne_points ) of ROS Velodyne driver is hard-coded and not configurable, while Autoware assumes point cloud published into ROS topic /points_raw . To have ROS Velodyne driver running with Autoware, you have to either use <remap> tag in Autoware launch files to may /velodyne_points to /points_raw , or modify the topic name in the source code of ROS Velodyne driver and rebuild it.","title":"Running with Autoware"},{"location":"plugins/lidar-plugin/#running-with-apollo-5.0","text":"Apollo 5.0 is based on CyberRT and comes with its own Velodyne driver . Detailed steps of running ROS Velodyne driver are as follows: 1. Follow these instructions to start Apollo 5.0 and launch bridge. 2. (optional) Configure the LiDAR model if your LiDAR setting is different to the default setting of Apollo 5.0. To configure the LiDAR model, you can edit velodyne.dag file. Note that if more than one LiDAR is used, each has different data port and position port (configured in their corresponding .conf files . You need to set UDPPortData and UDPPortPosition for each Velodyne LiDAR sensor accordingly. The default launch file and dag file of Apollo 5.0 use VLS-128 and VLP-16 LiDARs. If you need to use VLP-32C, in addition to add VLP-32C to dag file, you may need to modify static_transform_conf.pb.txt to include your own VLP-32C extrinsics if you want to get compensated point cloud. 3. Launch GPS , Localization , Transform , and Velodyne modules in Module Controller page of Dreamview. Fig. 3 shows the Dreamview web interface: Fig. 3: Dreamview web interface. On the Simulator side, you can add Velodyne LiDAR sensor into our sample JSON . If you have SVL Simulator running on client machine, you should be able to see UDP packets received on both data port and position port, and cyber_monitor should shows point clouds published into corresponding Cyber channels. You can also use cyber_visualizer to visualize the point cloud in those channels. Fig. 4 shows the point cloud of VLP-32C LiDAR visualized in Cyber Visualizer (click to see in full resolution): Fig. 4: Visualized point clouds of VLP-32C LiDAR in Cyber Visualizer.","title":"Running with Apollo 5.0"},{"location":"plugins/lidar-plugin/#build-your-own-lidar-sensor-plugin","text":"If you want to build your own LiDAR sensor plugin to support other LiDAR models, you can follow the general instructions on building sensor plugins. Instead of deriving your plugin class from SensorBase , you can derive your class from LidarSensorBase , so that you can reuse most of the code there, focusing only on raw data generation and sending.","title":"Build Your Own LiDAR Sensor Plugin"},{"location":"plugins/npc-plugins/","text":"NPC Plugins Non ego traffic vehicles (NPCs) are plugins that have the ability to be customized in appearance and behavior in your simulations. This allows you to add local variations of vehicle styles and brands or implement encounters with vehicles displaying distinguished behaviors, such as erratic driving or vehicles stopping frequently (e.g., delivery vehicles, trash pick up, bicycles). NOTE: Default NPCs are no longer included in simulator source and must be built locally from source for custom binaries. See build instructions . Table of Contents NPC Models NPC Animations Building NPC Plugins NPC Behaviors Creating NPC Behavior Plugins Creating NPC Model Plugins NPC Meta Data NPC Models top # NPC Models allow customization of NPC visuals, most notably the 3D model of vehicles, but also the material used for the car paint or windows. With supporting custom NPC models we also added the support for motorbikes and vehicles with more two axles as well as more than one steering axle as is found on heavy vehicles. NPC models must include a named Collider node that is a simplified mesh to attach a Unity mesh collider. The mesh renderer is NOT active and the collider must be marked Convex. We classify NPCs into size categories which decide the spawning frequency as well as which paint colors are more common for which NPC size type. The following table lists all currently implemented size types and the weight that correlates to their relative probability of encountering this type. NPC Size Type Spawn Weight Compact 5 MidSize 6 Luxury 2 Sport 1 LightTruck 2 SUV 4 MiniVan 3 Large 2 Emergency 1 Bus 1 Trailer 0 Motorcycle 1 Bicycle 1 In this example, trailer type has a weight of zero, indicating that it should not spawn by itself, while MidSize type has the highest weight, indicating that it is the most frequent vehicle on the road. The NPC Type is configured via a Component added to the root of the prefab . Similarly, colors for each spawned vehicle is picked from a table for each vehicle type. NPC asset bundles are detected from <simulator installation path>/AssetBundles/NPCs and added to the simulation automatically. NPC Animations top # NPC animations are supported in SVL Simulator. They must be included in the model data and require an AnimationController.controller asset. This controller is the animation state machine that is referenced in the Animator component on the model root object. In the animation controller, the animation parameter named speed must be setup and set as a transition condition. With this parameter, npc animation speed and transitions can be effected by the npc rigidbody speed. This is the only default parameter supported in the npc controller. Building NPC Plugins top # NPCs can be grouped into collections to allow you to import regional collections of vehicles. Place NPCs in Assets/External/NPCs/CollectionFolder/ in separate named folders to have all CollectionFolder vehicles show up in the build menu in a group. To build an external NPC, Open Simulator -> Build... menu item Select an NPC in the build window Click Build The bundle named npc_XXX will be placed in the AssetBundles/NPCs/CollectionFolder folder in source code. This is where simulator loads NPC bundles at runtime. To have NPCs load in a binary, they must be placed in the AssetBundles/NPCs/ folder in that binary. See build instructions for more details. NPC Behaviors top # NPC Behaviors allow NPC Vehicles to behave in custom ways different from the provided built-in NPC behavior and are enabled and configured through the Python API. Before running the simulator (running the executable or pressing Play in the Editor) NPC behavior plugins must be built by the simulator using Simulator -> Build... menu and the built asset placed in the AssetBundles/NPCs folder in the source code or binary. Open-source example: DrunkDriver Creating NPC Behavior Plugins top # Create a folder in under your collection folder for each behavior, eg. Assets/External/NPCs/BehavioursNPC/DrunkDriver/ . If your plugin is a pure behavior, that is it has no visual components, we require one C# file to have the same name as the folder under the collection folder, for example note how the name DrunkDriver repeats in both the folder name and file name: Assets/External/NPCs/BehavioursNPC/DrunkDriver/DrunkDriver.cs . The main class has to inherit from NPCBehaviourBase and implement its abstract methods to hook into the NPC simulation framework. It is also possible to inherit from NPCLaneFollowingBehaviour if you want to implement a slight modification of the default lane following behavior, such as this simple DrunkDriver example: using System.Collections; using System.Collections.Generic; using UnityEngine; using Simulator.Api; using Simulator.Map; using Simulator.Utilities; using SimpleJSON; // In this example, we try to simulate a driver under ther influence of alcohol. // We modify the default NPCLaneFollowingBehaviour because in principle this behavior // shuld just add variation the the default behavior. public class NPCDrunkDriverBehaviour : NPCLaneFollowBehaviour { public float steerCorrectionMinTime = 0.0f; public float steerCorrectionMaxTime = 0.4f; public float steerDriftMin = 0.00f; public float steerDriftMax = 0.09f; protected float currentSteerDrift = 0.0f; protected float nextSteerCorrection = 0; // This function in the base class controls the NPC steering protected override void SetTargetTurn() { // we reduce the frequency at which steering is updated to the target heading to // simulate loss of attention or reduced reaction time if(nextSteerCorrection < Time.fixedTime) { float steerCorrectionIn = RandomGenerator.NextFloat(steerCorrectionMinTime, steerCorrectionMaxTime); nextSteerCorrection = Time.fixedTime + steerCorrectionIn; // we add drift to the steering to simulate loss of fine motor skills currentSteerDrift = RandomGenerator.NextFloat(steerDriftMin, steerDriftMax); currentSteerDrift = currentSteerDrift * Mathf.Abs(RandomGenerator.NextFloat(-1.0f, 1.0f)); // we can reuse the base steering at reduced frequency base.SetTargetTurn(); } else { // steering drift correlating to driving speed currentTurn += currentSteerDrift * currentSpeed; } } } If you want to configure aspects of your NPC behavior from the Python API, you can add a class implementing the ICommand interface as shown in this example: class DrunkDriverControl : ICommand { public string Name => \"agent/drunk/config\"; public void Execute(JSONNode args) { var uid = args[\"uid\"].Value; var api = ApiManager.Instance; if (!api.Agents.TryGetValue(uid, out GameObject npc)) { api.SendError(this, $\"Agent '{uid}' not found\"); return; } var behaviour = npc.GetComponent<NPCDrunkDriverBehaviour>(); if (behaviour == null) { api.SendError(this, $\"Agent '{uid}' is not a drunk driving NPC agent\"); return; } if(args.HasKey(\"correctionMinTime\")) behaviour.steerCorrectionMinTime = args[\"correctionMinTime\"].AsFloat; if(args.HasKey(\"correctionMaxTime\")) behaviour.steerCorrectionMaxTime = args[\"correctionMaxTime\"].AsFloat; if(args.HasKey(\"steerDriftMin\")) behaviour.steerDriftMin = args[\"steerDriftMin\"].AsFloat; if(args.HasKey(\"steerDriftMax\")) behaviour.steerDriftMax = args[\"steerDriftMax\"].AsFloat; api.SendResult(this); } } example usage from Python: # common setup code omitted, check PythonAPI/quickstart for examples # you can check if it has been loaded: behaviours = sim.available_npc_behaviours for i in range(len(behaviours)): if behaviours[i][\"name\"]==\"NPCDrunkDriverBehaviour\": drunkDriverAvailable = True #later... npc = sim.add_agent(agent, lgsvl.AgentType.NPC, state) if drunkDriverAvailable: inp = input(\"make drunk driver? yN\") if (inp == \"y\" or inp == \"Y\"): npc.set_behaviour(\"NPCDrunkDriverBehaviour\") # usage of example command from C# plugin. npc.remote.command(\"agent/drunk/config\", { \"uid\": npc.uid, \"correctionMinTime\":0.0, \"correctionMaxTime\":0.6, \"steerDriftMin\": 0.00, \"steerDriftMax\":0.09}) # can still use lane following methods as those are inherited npc.follow_closest_lane(True, 5.6) Creating NPC Model Plugins top # Create a folder under your collection folder for each Model, eg. Assets/External/NPCs/YourCollection/YourNPC/ . All assets of a NPC such as textures, additional materials or scripts are expected to live inside its folder. When an NPC is spawned, a NPCController script is attached to the root of the prefab instance, and it will try to find several components to animate the vehicle. To identify the components, we look at the Name property of the Components as follows: GameObject Name Component Type Description LightHeadLeft Light Headlights which are controlled by the NPCController to off, regular, high beam etc states LightHeadRight Light Headlights which are controlled by the NPCController to off, regular, high beam etc states LightBrakeLeft Light Braking lights, controlled by the NPCController to reflect the vehicle braking LightBrakeRight Light Braking lights, controlled by the NPCController to reflect the vehicle braking IndicatorLeftFront Light Left indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorLeftRear Light Left indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorRightFront Light Right indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorRightRear Light Right indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorReverse Light Light turns on when NPC is reversing Body Renderer Main vehicle body mesh Body Material Material will have its _BaseColor changed to give each NPC a random color from a list Collider Renderer Simplified collision mesh set to convex and less than 256 polygons LightHead Material Material which is controlled to emit light in conjunction with headlight Lights LightBrake Material Material which is controlled to emit light in conjunction with brake Lights IndicatorLeft Material Material which is controlled to emit light in conjunction with left indicator Lights IndicatorRight Material Material which is controlled to emit light in conjunction with right indicator Lights IndicatorReverse Material Material which is controlled to emit light in conjunction with reversing Light Wheel Renderer Renderer is used to create WheelCollider. NPCs with less than four wheels are assumed to be bikes \"Wheel\" and \"Front\" or \"Rear\" Renderer Wheels that are supposed to turn when steering NPC Meta Data top # Additional information about the NPC prefab is added by attaching a Unity MonoBehaviour Component called NPCMetaData added to each prefab.","title":"NPC plugins"},{"location":"plugins/npc-plugins/#npc-models","text":"NPC Models allow customization of NPC visuals, most notably the 3D model of vehicles, but also the material used for the car paint or windows. With supporting custom NPC models we also added the support for motorbikes and vehicles with more two axles as well as more than one steering axle as is found on heavy vehicles. NPC models must include a named Collider node that is a simplified mesh to attach a Unity mesh collider. The mesh renderer is NOT active and the collider must be marked Convex. We classify NPCs into size categories which decide the spawning frequency as well as which paint colors are more common for which NPC size type. The following table lists all currently implemented size types and the weight that correlates to their relative probability of encountering this type. NPC Size Type Spawn Weight Compact 5 MidSize 6 Luxury 2 Sport 1 LightTruck 2 SUV 4 MiniVan 3 Large 2 Emergency 1 Bus 1 Trailer 0 Motorcycle 1 Bicycle 1 In this example, trailer type has a weight of zero, indicating that it should not spawn by itself, while MidSize type has the highest weight, indicating that it is the most frequent vehicle on the road. The NPC Type is configured via a Component added to the root of the prefab . Similarly, colors for each spawned vehicle is picked from a table for each vehicle type. NPC asset bundles are detected from <simulator installation path>/AssetBundles/NPCs and added to the simulation automatically.","title":"NPC Models"},{"location":"plugins/npc-plugins/#npc-animations","text":"NPC animations are supported in SVL Simulator. They must be included in the model data and require an AnimationController.controller asset. This controller is the animation state machine that is referenced in the Animator component on the model root object. In the animation controller, the animation parameter named speed must be setup and set as a transition condition. With this parameter, npc animation speed and transitions can be effected by the npc rigidbody speed. This is the only default parameter supported in the npc controller.","title":"NPC Animations"},{"location":"plugins/npc-plugins/#building-npc-plugins","text":"NPCs can be grouped into collections to allow you to import regional collections of vehicles. Place NPCs in Assets/External/NPCs/CollectionFolder/ in separate named folders to have all CollectionFolder vehicles show up in the build menu in a group. To build an external NPC, Open Simulator -> Build... menu item Select an NPC in the build window Click Build The bundle named npc_XXX will be placed in the AssetBundles/NPCs/CollectionFolder folder in source code. This is where simulator loads NPC bundles at runtime. To have NPCs load in a binary, they must be placed in the AssetBundles/NPCs/ folder in that binary. See build instructions for more details.","title":"Building NPC Plugins"},{"location":"plugins/npc-plugins/#npc-behaviors","text":"NPC Behaviors allow NPC Vehicles to behave in custom ways different from the provided built-in NPC behavior and are enabled and configured through the Python API. Before running the simulator (running the executable or pressing Play in the Editor) NPC behavior plugins must be built by the simulator using Simulator -> Build... menu and the built asset placed in the AssetBundles/NPCs folder in the source code or binary. Open-source example: DrunkDriver","title":"NPC Behaviors"},{"location":"plugins/npc-plugins/#creating-npc-behavior-plugins","text":"Create a folder in under your collection folder for each behavior, eg. Assets/External/NPCs/BehavioursNPC/DrunkDriver/ . If your plugin is a pure behavior, that is it has no visual components, we require one C# file to have the same name as the folder under the collection folder, for example note how the name DrunkDriver repeats in both the folder name and file name: Assets/External/NPCs/BehavioursNPC/DrunkDriver/DrunkDriver.cs . The main class has to inherit from NPCBehaviourBase and implement its abstract methods to hook into the NPC simulation framework. It is also possible to inherit from NPCLaneFollowingBehaviour if you want to implement a slight modification of the default lane following behavior, such as this simple DrunkDriver example: using System.Collections; using System.Collections.Generic; using UnityEngine; using Simulator.Api; using Simulator.Map; using Simulator.Utilities; using SimpleJSON; // In this example, we try to simulate a driver under ther influence of alcohol. // We modify the default NPCLaneFollowingBehaviour because in principle this behavior // shuld just add variation the the default behavior. public class NPCDrunkDriverBehaviour : NPCLaneFollowBehaviour { public float steerCorrectionMinTime = 0.0f; public float steerCorrectionMaxTime = 0.4f; public float steerDriftMin = 0.00f; public float steerDriftMax = 0.09f; protected float currentSteerDrift = 0.0f; protected float nextSteerCorrection = 0; // This function in the base class controls the NPC steering protected override void SetTargetTurn() { // we reduce the frequency at which steering is updated to the target heading to // simulate loss of attention or reduced reaction time if(nextSteerCorrection < Time.fixedTime) { float steerCorrectionIn = RandomGenerator.NextFloat(steerCorrectionMinTime, steerCorrectionMaxTime); nextSteerCorrection = Time.fixedTime + steerCorrectionIn; // we add drift to the steering to simulate loss of fine motor skills currentSteerDrift = RandomGenerator.NextFloat(steerDriftMin, steerDriftMax); currentSteerDrift = currentSteerDrift * Mathf.Abs(RandomGenerator.NextFloat(-1.0f, 1.0f)); // we can reuse the base steering at reduced frequency base.SetTargetTurn(); } else { // steering drift correlating to driving speed currentTurn += currentSteerDrift * currentSpeed; } } } If you want to configure aspects of your NPC behavior from the Python API, you can add a class implementing the ICommand interface as shown in this example: class DrunkDriverControl : ICommand { public string Name => \"agent/drunk/config\"; public void Execute(JSONNode args) { var uid = args[\"uid\"].Value; var api = ApiManager.Instance; if (!api.Agents.TryGetValue(uid, out GameObject npc)) { api.SendError(this, $\"Agent '{uid}' not found\"); return; } var behaviour = npc.GetComponent<NPCDrunkDriverBehaviour>(); if (behaviour == null) { api.SendError(this, $\"Agent '{uid}' is not a drunk driving NPC agent\"); return; } if(args.HasKey(\"correctionMinTime\")) behaviour.steerCorrectionMinTime = args[\"correctionMinTime\"].AsFloat; if(args.HasKey(\"correctionMaxTime\")) behaviour.steerCorrectionMaxTime = args[\"correctionMaxTime\"].AsFloat; if(args.HasKey(\"steerDriftMin\")) behaviour.steerDriftMin = args[\"steerDriftMin\"].AsFloat; if(args.HasKey(\"steerDriftMax\")) behaviour.steerDriftMax = args[\"steerDriftMax\"].AsFloat; api.SendResult(this); } } example usage from Python: # common setup code omitted, check PythonAPI/quickstart for examples # you can check if it has been loaded: behaviours = sim.available_npc_behaviours for i in range(len(behaviours)): if behaviours[i][\"name\"]==\"NPCDrunkDriverBehaviour\": drunkDriverAvailable = True #later... npc = sim.add_agent(agent, lgsvl.AgentType.NPC, state) if drunkDriverAvailable: inp = input(\"make drunk driver? yN\") if (inp == \"y\" or inp == \"Y\"): npc.set_behaviour(\"NPCDrunkDriverBehaviour\") # usage of example command from C# plugin. npc.remote.command(\"agent/drunk/config\", { \"uid\": npc.uid, \"correctionMinTime\":0.0, \"correctionMaxTime\":0.6, \"steerDriftMin\": 0.00, \"steerDriftMax\":0.09}) # can still use lane following methods as those are inherited npc.follow_closest_lane(True, 5.6)","title":"Creating NPC Behavior Plugins"},{"location":"plugins/npc-plugins/#creating-npc-model-plugins","text":"Create a folder under your collection folder for each Model, eg. Assets/External/NPCs/YourCollection/YourNPC/ . All assets of a NPC such as textures, additional materials or scripts are expected to live inside its folder. When an NPC is spawned, a NPCController script is attached to the root of the prefab instance, and it will try to find several components to animate the vehicle. To identify the components, we look at the Name property of the Components as follows: GameObject Name Component Type Description LightHeadLeft Light Headlights which are controlled by the NPCController to off, regular, high beam etc states LightHeadRight Light Headlights which are controlled by the NPCController to off, regular, high beam etc states LightBrakeLeft Light Braking lights, controlled by the NPCController to reflect the vehicle braking LightBrakeRight Light Braking lights, controlled by the NPCController to reflect the vehicle braking IndicatorLeftFront Light Left indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorLeftRear Light Left indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorRightFront Light Right indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorRightRear Light Right indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorReverse Light Light turns on when NPC is reversing Body Renderer Main vehicle body mesh Body Material Material will have its _BaseColor changed to give each NPC a random color from a list Collider Renderer Simplified collision mesh set to convex and less than 256 polygons LightHead Material Material which is controlled to emit light in conjunction with headlight Lights LightBrake Material Material which is controlled to emit light in conjunction with brake Lights IndicatorLeft Material Material which is controlled to emit light in conjunction with left indicator Lights IndicatorRight Material Material which is controlled to emit light in conjunction with right indicator Lights IndicatorReverse Material Material which is controlled to emit light in conjunction with reversing Light Wheel Renderer Renderer is used to create WheelCollider. NPCs with less than four wheels are assumed to be bikes \"Wheel\" and \"Front\" or \"Rear\" Renderer Wheels that are supposed to turn when steering","title":"Creating NPC Model Plugins"},{"location":"plugins/npc-plugins/#npc-meta-data","text":"Additional information about the NPC prefab is added by attaching a Unity MonoBehaviour Component called NPCMetaData added to each prefab.","title":"NPC Meta Data"},{"location":"plugins/pedestrian-plugins/","text":"Pedestrian Plugins Pedestrians are plugins that have the ability to be customized in appearance, animation and behavior in your simulations. This allows you to add variations of pedestrian types. Asset source must be cloned and built locally. See build instructions . Pedestrian asset bundles are detected from <simulator installation path>/AssetBundles/Pedestrians and must be placed there manually for custom binaries or in source code for developer mode. They are loaded automatically on editor or simulation start. Table of Contents Pedestrian Models Pedestrian Animations Pedestrian Ground Truth Building Pedestrian Plugins Pedestrian Models top # Pedestrian models allow customization of pedestrian meshes and animations. We support bipedal, and quadrupedal rigs. Create a new collection folder in Assets/External/Pedestrians/YourPedestrianCollection . Create a folder for each type of pedestrian. Place all textures, meshes, animation controller and one prefab in this folder. Be sure the prefab is named the same as the containing folder. Pedestrian Animations top # SVL Simulator supports human and bird bipedal, and quadrupedal animations. They must be included in the model data and require an AnimationController.controller asset. This controller is the animation state machine that is referenced in the Animator component on the prefab root object. In the animation controller, the animation parameter named speed must be setup and set as a transition condition. With this parameter, pedestrian animation speed and transitions, can be effected by the pedestrian rigidbody speed. This is the only default parameter supported in the pedestrian controller. Pedestrian Ground Truth top # SVL Simulator supports run-time creation of ground truth bounding boxes for pedestrians with skinned mesh renderers. Any other render will use default values for ground truth. Users must adjust the skinned mesh renderer bounds to best fit the mesh. Building Pedestrian Plugins top # Pedestrians are grouped into collections to organize each type. Place pedestrians groups in Assets/External/Pedestrians/CollectionFolder/ in separate named folders to have all CollectionFolder vehicles show up in the build menu in a group. To build a pedestrian, Open Simulator -> Build... menu item Select a pedestrian in the build window Click Build The bundle named npc_XXX will be placed in the AssetBundles/Pedestrians/CollectionFolder folder in source code. This is where simulator loads pedestrian bundles at runtime. To have pedestrians load in a binary, they must be placed in the AssetBundles/Pedestrians/ folder in that binary. See build instructions for more details.","title":"Pedestrian plugins"},{"location":"plugins/pedestrian-plugins/#pedestrian-models","text":"Pedestrian models allow customization of pedestrian meshes and animations. We support bipedal, and quadrupedal rigs. Create a new collection folder in Assets/External/Pedestrians/YourPedestrianCollection . Create a folder for each type of pedestrian. Place all textures, meshes, animation controller and one prefab in this folder. Be sure the prefab is named the same as the containing folder.","title":"Pedestrian Models"},{"location":"plugins/pedestrian-plugins/#pedestrian-animations","text":"SVL Simulator supports human and bird bipedal, and quadrupedal animations. They must be included in the model data and require an AnimationController.controller asset. This controller is the animation state machine that is referenced in the Animator component on the prefab root object. In the animation controller, the animation parameter named speed must be setup and set as a transition condition. With this parameter, pedestrian animation speed and transitions, can be effected by the pedestrian rigidbody speed. This is the only default parameter supported in the pedestrian controller.","title":"Pedestrian Animations"},{"location":"plugins/pedestrian-plugins/#pedestrian-ground-truth","text":"SVL Simulator supports run-time creation of ground truth bounding boxes for pedestrians with skinned mesh renderers. Any other render will use default values for ground truth. Users must adjust the skinned mesh renderer bounds to best fit the mesh.","title":"Pedestrian Ground Truth"},{"location":"plugins/pedestrian-plugins/#building-pedestrian-plugins","text":"Pedestrians are grouped into collections to organize each type. Place pedestrians groups in Assets/External/Pedestrians/CollectionFolder/ in separate named folders to have all CollectionFolder vehicles show up in the build menu in a group. To build a pedestrian, Open Simulator -> Build... menu item Select a pedestrian in the build window Click Build The bundle named npc_XXX will be placed in the AssetBundles/Pedestrians/CollectionFolder folder in source code. This is where simulator loads pedestrian bundles at runtime. To have pedestrians load in a binary, they must be placed in the AssetBundles/Pedestrians/ folder in that binary. See build instructions for more details.","title":"Building Pedestrian Plugins"},{"location":"plugins/sensor-plugins/","text":"Sensor Plugins Sensor plugins are custom sensors that can be added to a vehicle configuration. Sensor plugins must be built by the simulator and the resultant bundle named sensor_XXX must be placed in the AssetBundles/Sensors folder. If running the binary, this folder is included in the downloaded .zip. If running in Editor, the sensor will be built into the folder directly. This must be done before running the simulator (running the executable or pressing Play in the Editor). The sensor can be added to a vehicle configuration just like other sensors, see here Building sensor plugins to bundle is done as below 1. Open Simulator -> Build... menu item 2. Select sensor plugins in \"Sensor\" section of build window 3. Build plugins with \"Build\" button To make sensor plugin, create folder in Assets/External/Sensors , for example Assets/External/Sensors/CustomCameraSensor . Inside this folder you must place sensor prefab with same name ( CustomCameraSensor.prefab ) that will be used by simulator to instantiate at runtime. This prefab must have the sensor script added to the root of the prefab. To create a prefab: Right-click in the scene hierarchy and select Create Empty Change the name to the name of the sensor (e.g. CustomCameraSensor ) In the Inspector for this object, select Add Component Search for the sensor script Drag this object from the scene hierarchy into the project folder Additionally you can place C# scripts which will be compiled & bundled with prefab, as well as any additional Unity resources (shaders, materials, textures, etc...). Sensor plugins must have SensorType attribute which specifies the kind of sensor being implemented as well as the type of data that the sensor sends over the bridge. In addition, it must have SensorBase as the base class and must implement the Initialize , Deinitialize , OnBridgeSetup , OnVisualize , and OnVisualizeToggle methods. Sensors can optionally include CheckVisible method to prevent NPC or Pedestrians from spawning in bounds of the sensor. If the sensor can be simulated on the client machine in a distributed simulation, see sensors distribution to learn how to configure it. See the below codeblock from the ColorCamera sensor: namespace Simulator.Sensors { // The SensorType's name will match the `type` when defining a sensor in the JSON configuration of a vehicle // The requiredType list is required if data will be sent over the bridge. It can otherwise be empty. // Publishable data types are: // CanBusData, CLockData, Detected2DObjectData, Detected3DObjectData, DetectedRadarObjectData, // GpsData, ImageData, ImuData, PointCloudData, SignalData, VehicleControlData [SensorType(\"Custom Color Camera\", new[] { typeof(ImageData)})] // Inherits Monobehavior // SensorBase also defines the parameters Name, Topic, and Frame public partial class CustomCameraSensor : SensorBase { private Camera Camera; IBridge Bridge; IWriter<ImageData> Writer; // These public variables can be set in the JSON configuration [SensorParameter] [Range(1, 128)] public int JpegQuality = 75; // By adding the AnalysisMeasurement tag, the last recorded value of this field // will be reported in the analysis tab in WISE. MeasurementType allows you to // control what units the field will be represented with [AnalysisMeasurement(MeasurementType.Fps)] public float TargetFPS = 0f; // Called to initialize the sensor plugin protected override void Initialize() { } // Called to deinitialize the sensor plugin protected override void Deinitialize() { } //Sets up the bridge to send this sensor's data public override void OnBridgeSetup(IBridge bridge) { Bridge = bridge; Writer = bridge.AddWriter<ImageData>(Topic); } // Defines how the sensor data will be visualized in the simulator public override void OnVisualize(Visualizer visualizer) { Debug.Assert(visualizer != null); visualizer.UpdateRenderTexture(Camera.activeTexture, Camera.aspect); } // Called when user toggles visibility of sensor visualization // This function needs to be implemented, but otherwise can be empty public override void OnVisualizeToggle(bool state) { } // Called when NPC and Pedestrian managers need to check if visible by sensor // camera or bounds before placing object in scene public override void CheckVisible(Bounds bounds) { var activeCameraPlanes = GeometryUtility.CalculateFrustumPlanes(Camera); return GeometryUtility.TestPlanesAABB(activeCameraPlanes, bounds); } // Sensors can implement custom analysis event callbacks for failing criteria, // collect relevant data and log it in the post simulation analysis. id should // be ego vehicle id which all sensors are childed to private void LowFPSEvent(uint id) { Hashtable data = new Hashtable { { \"Id\", id }, { \"Type\", \"LowFPS\" }, { \"Time\", SimulatorManager.Instance.GetSessionElapsedTimeSpan().ToString() }, }; SimulatorManager.Instance.AnalysisManager.AddEvent(data); } } } SensorBase in inherited from Unity's Monobehavior so any of the Messages can be used to control how and when the sensor collects data. Open-source examples are available: Comfort Sensor","title":"Sensor plugins"},{"location":"python-api/api-quickstart-descriptions/","text":"Python API Quickstart Script Descriptions This document describes the example Python scripts that use the SVL Simulator Python API. These scripts are located here . You can find the documentation on the API here . 01-connecting-to-simulator.py : How to connect to an already running instance of the simulator and some information you can get about the instance 02-loading-scene-show-spawns.py : How to load a scene and get the scene's predefined spawn transforms 03-raycast.py : How to create an EGO vehicle and do raycasting from a point 04-ego-drive-straight.py : How to create an agent with a velocity and then run the simulator for a set amount of time 05-ego-drive-in-circle.py : How to apply control to an EGO vehicle and then run the simulator indefinitely 06-save-camera-image.py : How to save a camera image in different formats and with various settings 07-save-lidar-point-cloud.py : How to save a LiDAR point cloud 08-create-npc.py : How to create several types of NPC vehicles and spawn them in different positions 09-reset-scene.py : How to empty the scene of all EGOs, NPCs, and Pedestrians, but keep the scene loaded 10-npc-follow-the-lane.py : How to create NPCs and then let them drive in the nearest annotated lane 11-collision-callbacks.py : How to setup the simulator so that whenever the 3 created agents collide with anything, the name of the agent and the collision point is printed 12-create-npc-on-lane.py : How to create NPC vehicles in random position in a radius around the EGO vehicle, but the NPCs are placed on the nearest lane to the initial random position 13-npc-follow-waypoints.py : How to create a list of waypoints with fixed wait times and direct an NPC to follow them 14-create-pedestrians.py : How to create pedestrians in rows in front of the spawn position 15-pedestrian-walk-randomly.py : How to start and stop a pedestrian walking randomly on the sidewalk 16-pedestrian-follow-waypoints.py : How to create a list of waypoints and direct a pedestrian to follow them 17-many-pedestrians-walking.py : How to generate an army of pedestrians and have them walk back and forth 18-weather-effects.py : How to get the current weather state of the simulator and how to adjust the various settings 19-time-of-day.py : How to get the time of date in the simulator and how to set it to a fixed time and a moving time 20-enable-sensors.py : How to enable a specific sensor so that it can send data over a bridge 21-map-coordinates.py : How to convert from simulator coordinates to GPS coordinates and back. Latitude/Longitude and Northing/Easting are supported along with altitude and orientation 22-connecting-bridge.py : How to command an EGO vehicle to connect to a bridge at a specific IP address and port and then wait for the connection to be established 23-npc-callbacks.py : How to setup the simulator so that whenever an NPC reaches a stopline or changes lane, the name of the NPC is printed 24-ego-drive-straight-non-realtime.py : How to run the simulator at non-realtime. 25-waypoint-flying-npc.py : How to use waypoints to define customized motion for NPC. 26-npc-trigger-waypoints.py : How to use trigger waypoints that pause npc motion until an ego vehicle approaches. 27-control-traffic-lights.py : How to get and set the control policy of a controllable object (e.g., changing a traffic light signal) 28-control-traffic-cone.py : How to add and move a controllable object (e.g. a traffic cone) 29-add-random-agents.py : How to use random NPCs and pedestrians in a simulation 30-time-to-collision-trigger.py How to use time-to-collision triggers 31-wait-for-distance-trigger.py How to use wait-for-distance triggers 32-pedestrian-time-to-collision.py How to use time-to-collision on pedestrians 33-ego-drive-stepped.py How to run a stepped simulation using the Python Api 34-simulator-cam-set.py How to set up fixed camera positions in a simulation 98-npc-behaviour.py How to get and set available NPC behaviors 99-utils-examples.py : How to use several of the utility scripts to transform an arbitrary point to the coordinate system of a local transform (relative to sensor)","title":"Python API quickstart guide"},{"location":"python-api/dreamview-api/","text":"Dreamview API Overview top # The Dreamview API is a subpackage of the the SVL Simulator Python API which communicates with Apollo's Dreamview module. Apollo's Dreamview provides a web ui that allows users to enable/disable different modules within Apollo and to set destinations to navigate to. The Dreamview API enables users to automate this procedure with Python scripts. This document provides an overview of the API. Table of Contents Overview Requirements Usage Guide Connection Methods check_module_status disable_apollo disable_module enable_apollo enable_module get_current_map get_current_vehicle get_module_status reconnect set_destination set_hd_map set_setup_mode set_vehicle setup_apollo Example Requirements top # Installation of the Python API - requires Python 3.5 or later. Apollo 5.0 or later - instructions here . ApolloControl sensor plugin (optional) - This plugin is already included inside the SVL Simulator release build available on GitHub. For custom builds, the sensor plugin should be placed inside the AssetBundles/Sensors folder of the simulator. The sensor checks to see if Apollo is ready (sending control commands) before starting the simulation. CheckControl sensor plugin (optional) - This plugin is already included inside the SVL Simulator release build available on GitHub. For custom builds, the sensor plugin should be placed inside the AssetBundles/Sensors folder of the simulator. The sensor checks to see if Apollo is ready (sending control commands) before starting the simulation. Usage Guide top # Connection top # The main class in the Dreamview API is the Connection class that allows establishing a connection with Dreamview. To instantiate, the constructor may be called as follows: dreamview.Connection(simulator, ego_agent, ip='localhost', port='8888') where: simulator is an lgsvl.Simulator object ego_agent is an lgsvl.EgoVehicle object - this is intended to be used with a vehicle equipped with Apollo 5.0 ip : address of the machine where the Apollo stack is running (defaults to 'localhost') port : the port number for Dreamview (defaults to '8888') Methods top # check_module_status top # check_module_status(self, modules) Checks if all modules in a provided list are enabled. disable_apollo top # disable_apollo(self) Disables all Apollo modules. disable_module top # disable_module(self, module) Disables a specific Apollo module. module is the name of the Apollo 5.0 module as seen in the \"Module Controller\" tab of Dreamview. enable_apollo top # enable_apollo(self, dest_x, dest_z, modules) Enables a list of modules and then sets the destination. enable_module top # enable_module(self, module) Enables a specific Apollo module. module is the name of the Apollo 5.0 module as seen in the \"Module Controller\" tab of Dreamview. get_current_map top # get_current_map(self) Returns the current HD map loaded in Dreamview. get_current_vehicle top # get_current_vehicle(self) Returns the current Vehicle configuration loaded in Dreamview. get_module_status top # get_module_status(self) Returns a dict where the key is the name of the module and value is a bool based on the module's current status. reconnect top # reconnect(self) Closes the websocket connection and re-creates it so that data can be received again. set_destination top # set_destination(self, x_long_east, z_lat_north, y=0, coord_type=CoordType.Unity) Sends a RoutingRequest to Apollo for the provided coordinates. This function can accept a variety of Coordinate systems. If using Unity World Coordinate System: x_long_east = x z_lat_north = z y = y If using Latitude/Longitude: x_long_east = Longitude z_lat_north = Latitude If using Easting/Northing: x_long_east = Easting z_lat_north = Northing set_hd_map top # set_hd_map(self, map) Selects the provided HD map. Folders in /apollo/modules/map/data/ are the available HD maps. Map options in Dreamview are the folder names with the following changes: underscores (_) are replaced with spaces the first letter of each word is capitalized map parameter is the modified folder name. map should match one of the options in the right-most drop down in the top-right corner of Dreamview. set_setup_mode top # set_setup_mode(self, mode) mode is the name of the Apollo 5.0 mode as seen in the left-most drop down in the top-right corner of Dreamview. set_vehicle top # set_vehicle(self, vehicle, gps_offset_x=0.0, gps_offset_y=0.0, gps_offset_z=-1.348) Selects the provided vehicle configuration. Folders in /apollo/modules/calibration/data/ are the available vehicle calibrations. Vehicle options in Dreamview are the folder names with the following changes: underscores (_) are replaced with spaces the first letter of each word is capitalized vehicle parameter is the modified folder name. vehicle should match one of the options in the middle drop down in the top-right corner of Dreamview. setup_apollo top # setup_apollo(self, dest_x, dest_z, modules, default_timeout=60.0) Starts a list of Apollo modules and sets the destination. Will wait for Control module to send a message before returning. Control sending a message indicates that all modules are working and Apollo is ready to continue. Example top # This example will start a simulation that will drive an ego vehicle to a destination using Apollo. The simulation will start once Apollo is ready. To run this example you will need to have an Apollo vehicle with the ApolloControl sensor added. To add the sensor to the vehicle, add the following to the vehicle configuration JSON file: { \"type\" : \"ApolloControlSensor\", \"name\" : \"Apollo Control Sensor\", \"params\": { \"Topic\": \"/apollo/control\" } } Before running the Python script, start the Apollo docker container (see instructions ) and start dreamview and the cyber bridge using the following commands: bootstrap.sh bridge.sh You should not start any modules or interact with dreamview manually. You can view dreamview by navigating to localhost:8888 in a web browser. Now you can start an API only simulation and run the Python script below: import os import lgsvl SIMULATOR_HOST = os.environ.get(\"SIMULATOR_HOST\", \"127.0.0.1\") SIMULATOR_PORT = int(os.environ.get(\"SIMULATOR_PORT\", 8181)) BRIDGE_HOST = os.environ.get(\"BRIDGE_HOST\", \"127.0.0.1\") BRIDGE_PORT = int(os.environ.get(\"BRIDGE_PORT\", 9090)) sim = lgsvl.Simulator(SIMULATOR_HOST, SIMULATOR_PORT) if sim.current_scene == \"BorregasAve\": sim.reset() else: sim.load(\"BorregasAve\") spawns = sim.get_spawn() state = lgsvl.AgentState() state.transform = spawns[0] ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO, state) ego.connect_bridge(BRIDGE_HOST, BRIDGE_PORT) # Dreamview setup dv = lgsvl.dreamview.Connection(sim, ego, BRIDGE_HOST) dv.set_hd_map('Borregas Ave') dv.set_vehicle('Lincoln2017MKZ') modules = [ 'Localization', 'Perception', 'Transform', 'Routing', 'Prediction', 'Planning', 'Camera', 'Traffic Light', 'Control' ] destination = spawns[0].destinations[0] dv.setup_apollo(destination.position.x, destination.position.z, modules) sim.run() Upon successful execution, the Ego vehicle should navigate to a destination on the far side of the map. Note that Apollo setup can take quite a while (often up to 30 seconds) before it is ready for the simulation to be run.","title":"Dreamview API"},{"location":"python-api/dreamview-api/#overview","text":"The Dreamview API is a subpackage of the the SVL Simulator Python API which communicates with Apollo's Dreamview module. Apollo's Dreamview provides a web ui that allows users to enable/disable different modules within Apollo and to set destinations to navigate to. The Dreamview API enables users to automate this procedure with Python scripts. This document provides an overview of the API.","title":"Overview"},{"location":"python-api/dreamview-api/#requirements","text":"Installation of the Python API - requires Python 3.5 or later. Apollo 5.0 or later - instructions here . ApolloControl sensor plugin (optional) - This plugin is already included inside the SVL Simulator release build available on GitHub. For custom builds, the sensor plugin should be placed inside the AssetBundles/Sensors folder of the simulator. The sensor checks to see if Apollo is ready (sending control commands) before starting the simulation. CheckControl sensor plugin (optional) - This plugin is already included inside the SVL Simulator release build available on GitHub. For custom builds, the sensor plugin should be placed inside the AssetBundles/Sensors folder of the simulator. The sensor checks to see if Apollo is ready (sending control commands) before starting the simulation.","title":"Requirements"},{"location":"python-api/dreamview-api/#usage-guide","text":"","title":"Usage Guide"},{"location":"python-api/dreamview-api/#connection","text":"The main class in the Dreamview API is the Connection class that allows establishing a connection with Dreamview. To instantiate, the constructor may be called as follows: dreamview.Connection(simulator, ego_agent, ip='localhost', port='8888') where: simulator is an lgsvl.Simulator object ego_agent is an lgsvl.EgoVehicle object - this is intended to be used with a vehicle equipped with Apollo 5.0 ip : address of the machine where the Apollo stack is running (defaults to 'localhost') port : the port number for Dreamview (defaults to '8888')","title":"Connection"},{"location":"python-api/dreamview-api/#methods","text":"","title":"Methods"},{"location":"python-api/dreamview-api/#check_module_status","text":"check_module_status(self, modules) Checks if all modules in a provided list are enabled.","title":"check_module_status"},{"location":"python-api/dreamview-api/#disable_apollo","text":"disable_apollo(self) Disables all Apollo modules.","title":"disable_apollo"},{"location":"python-api/dreamview-api/#disable_module","text":"disable_module(self, module) Disables a specific Apollo module. module is the name of the Apollo 5.0 module as seen in the \"Module Controller\" tab of Dreamview.","title":"disable_module"},{"location":"python-api/dreamview-api/#enable_apollo","text":"enable_apollo(self, dest_x, dest_z, modules) Enables a list of modules and then sets the destination.","title":"enable_apollo"},{"location":"python-api/dreamview-api/#enable_module","text":"enable_module(self, module) Enables a specific Apollo module. module is the name of the Apollo 5.0 module as seen in the \"Module Controller\" tab of Dreamview.","title":"enable_module"},{"location":"python-api/dreamview-api/#get_current_map","text":"get_current_map(self) Returns the current HD map loaded in Dreamview.","title":"get_current_map"},{"location":"python-api/dreamview-api/#get_current_vehicle","text":"get_current_vehicle(self) Returns the current Vehicle configuration loaded in Dreamview.","title":"get_current_vehicle"},{"location":"python-api/dreamview-api/#get_module_status","text":"get_module_status(self) Returns a dict where the key is the name of the module and value is a bool based on the module's current status.","title":"get_module_status"},{"location":"python-api/dreamview-api/#reconnect","text":"reconnect(self) Closes the websocket connection and re-creates it so that data can be received again.","title":"reconnect"},{"location":"python-api/dreamview-api/#set_destination","text":"set_destination(self, x_long_east, z_lat_north, y=0, coord_type=CoordType.Unity) Sends a RoutingRequest to Apollo for the provided coordinates. This function can accept a variety of Coordinate systems. If using Unity World Coordinate System: x_long_east = x z_lat_north = z y = y If using Latitude/Longitude: x_long_east = Longitude z_lat_north = Latitude If using Easting/Northing: x_long_east = Easting z_lat_north = Northing","title":"set_destination"},{"location":"python-api/dreamview-api/#set_hd_map","text":"set_hd_map(self, map) Selects the provided HD map. Folders in /apollo/modules/map/data/ are the available HD maps. Map options in Dreamview are the folder names with the following changes: underscores (_) are replaced with spaces the first letter of each word is capitalized map parameter is the modified folder name. map should match one of the options in the right-most drop down in the top-right corner of Dreamview.","title":"set_hd_map"},{"location":"python-api/dreamview-api/#set_setup_mode","text":"set_setup_mode(self, mode) mode is the name of the Apollo 5.0 mode as seen in the left-most drop down in the top-right corner of Dreamview.","title":"set_setup_mode"},{"location":"python-api/dreamview-api/#set_vehicle","text":"set_vehicle(self, vehicle, gps_offset_x=0.0, gps_offset_y=0.0, gps_offset_z=-1.348) Selects the provided vehicle configuration. Folders in /apollo/modules/calibration/data/ are the available vehicle calibrations. Vehicle options in Dreamview are the folder names with the following changes: underscores (_) are replaced with spaces the first letter of each word is capitalized vehicle parameter is the modified folder name. vehicle should match one of the options in the middle drop down in the top-right corner of Dreamview.","title":"set_vehicle"},{"location":"python-api/dreamview-api/#setup_apollo","text":"setup_apollo(self, dest_x, dest_z, modules, default_timeout=60.0) Starts a list of Apollo modules and sets the destination. Will wait for Control module to send a message before returning. Control sending a message indicates that all modules are working and Apollo is ready to continue.","title":"setup_apollo"},{"location":"python-api/dreamview-api/#example","text":"This example will start a simulation that will drive an ego vehicle to a destination using Apollo. The simulation will start once Apollo is ready. To run this example you will need to have an Apollo vehicle with the ApolloControl sensor added. To add the sensor to the vehicle, add the following to the vehicle configuration JSON file: { \"type\" : \"ApolloControlSensor\", \"name\" : \"Apollo Control Sensor\", \"params\": { \"Topic\": \"/apollo/control\" } } Before running the Python script, start the Apollo docker container (see instructions ) and start dreamview and the cyber bridge using the following commands: bootstrap.sh bridge.sh You should not start any modules or interact with dreamview manually. You can view dreamview by navigating to localhost:8888 in a web browser. Now you can start an API only simulation and run the Python script below: import os import lgsvl SIMULATOR_HOST = os.environ.get(\"SIMULATOR_HOST\", \"127.0.0.1\") SIMULATOR_PORT = int(os.environ.get(\"SIMULATOR_PORT\", 8181)) BRIDGE_HOST = os.environ.get(\"BRIDGE_HOST\", \"127.0.0.1\") BRIDGE_PORT = int(os.environ.get(\"BRIDGE_PORT\", 9090)) sim = lgsvl.Simulator(SIMULATOR_HOST, SIMULATOR_PORT) if sim.current_scene == \"BorregasAve\": sim.reset() else: sim.load(\"BorregasAve\") spawns = sim.get_spawn() state = lgsvl.AgentState() state.transform = spawns[0] ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO, state) ego.connect_bridge(BRIDGE_HOST, BRIDGE_PORT) # Dreamview setup dv = lgsvl.dreamview.Connection(sim, ego, BRIDGE_HOST) dv.set_hd_map('Borregas Ave') dv.set_vehicle('Lincoln2017MKZ') modules = [ 'Localization', 'Perception', 'Transform', 'Routing', 'Prediction', 'Planning', 'Camera', 'Traffic Light', 'Control' ] destination = spawns[0].destinations[0] dv.setup_apollo(destination.position.x, destination.position.z, modules) sim.run() Upon successful execution, the Ego vehicle should navigate to a destination on the far side of the map. Note that Apollo setup can take quite a while (often up to 30 seconds) before it is ready for the simulation to be run.","title":"Example"},{"location":"python-api/python-api/","text":"Python API Guide Overview top # SVL Simulator exposes runtime functionality to a Python API which you can use to manipulate object placement and vehicle movement in a loaded scene, retrieve sensor configuration and data, control weather, time state, and more. The interface to listen for incoming API calls is defined in config.yml . See Configuration File and Command-Line Parameters for more information. Table of Contents Overview Requirements Quickstart Core Concepts Simulation Non-real-time Simulation Agents EGO Vehicle NPC Vehicles Pedestrians Callbacks Agent Callbacks 'EgoVehicle NpcVehicle Callbacks Pedestrian Callbacks Sensors Camera Sensor LiDAR Sensor IMU Sensor GPS Sensor Radar Sensor CAN bus Video Recording Sensor Weather and Time of Day Control Controllable Objects Helper Functions Changelog Requirements top # Using Python API requires Python version 3.6 or later. Quickstart top # Python API is available in separate repository: https://github.com/lgsvl/PythonAPI After cloning or downloading it from the git repository follow these steps: Run the following command to install Python files and necessary dependencies: pip3 install --user -e . Now launch the simulator, either using the binary file or from Unity Editor. The simulator by default listens for connections on port 8181 on localhost . See the guide for Running SVL Simulator for more information on how to set up and run the simulator. Click the Open Browser button to open the Simulator UI, then click Sign in . Enter login credentials or click Sign Up to create an account. You will need to add assets such as maps and vehicles to your library to use in simulations. These assets can either be added from the Store or uploaded to the cloud. Quickstart scripts require the BorregasAve map and the Lincoln2017MKZ vehicle, which are included in your library by default. You will need to run a simulation using the API Only runtime template to run Python API scripts on your host machine to control the simulation. See the document on Simulations for information on how to create a simulation. Select the newly created Simulation and click the Run Simulation button. Run the following example to see the API in action: ./quickstart/05-ego-drive-in-circle.py This will load the BorregasAve.unity scene, instantiate one EGO vehicle, then ask you to press Enter to start driving EGO vehicle in a circle. When the script is running, it will apply throttle and steering commands to make the car move. Core concepts top # The Simulator and API communicate by sending json over a websocket server running on port 8181 . The API client can either be on the same machine or on any other machine on the same network. API exposes the following main types: Simulator - main object for connecting to simulator and creating other objects Agent - superclass of vehicles and pedestrian EgoVehicle - EGO vehicle with accurate physics simulation and sensors NpcVehicle - NPC vehicle with simplified physics, useful for creating many background vehicles Pedestrian - pedestrian walking on sidewalks Vehicles and Pedestrian are a subclasses of Agent which has common properties like transform , position , and velocity . All coordinates in the API return values in the Unity coordinate system. This coordinate system uses meters as a unit of distance and is a left-handed coordinate system where x points left, z points forward, and y points up. The Simulator class provides helper methods to convert coordinates to and from latitude/longitude and northing/easting values. Simulation top # To connect to the simulator you need to an instance of the Simulator class: import lgsvl sim = lgsvl.Simulator(address = \"localhost\", port = 8181) You can specify a different address as hostname or IP address. By default only port 8181 is used for API connection. Only one client can be connected to simulator at a time. Next, load the scene (sometimes called map). This is done by the load method: sim.load(scene = \"aae03d2a-b7ca-4a88-9e41-9035287a12cc\", seed = 650387) scene is a string representing the UUID of the Map in the Web UI. Scenes can be added either from the map tab under Store or by manually uploading. Some of the well-known scenes available in the Map tab under Store are: Map name UUID Description BorregasAve aae03d2a-b7ca-4a88-9e41-9035287a12cc a Digital Twin of a real-world suburban street block in Sunnyvale, CA AutonomouStuff 2aae5d39-a11c-4516-87c4-cdc9ca784551 small office parking lot in SanJose, CA Shalun 97128028-33c7-4411-b1ec-d693ed35071f an autonomous vehicle testing facility in Taiwan (More information here ) SanFrancisco 5d272540-f689-4355-83c7-03bf11b6865f a real world urban environment from San Francisco, CA GoMentum Station 979dd7f3-b25b-47f0-ab10-a6effb370138 a Digital Twin of a real-world autonomous vehicle testing facility in Concord, CA CubeTown 06773677-1ce3-492f-9fe2-b3147e126e27 a virtual environment with block obstacles used to perform basic testing of vehicles SingleLaneRoad a6e2d149-6a18-4b83-9029-4411d7b2e69a a simple two-way single-lane road Seed (optional) is an Integer (-2,147,483,648 - 2,147,483,647) that determines the \"random\" behavior of the NPC vehicles and rain effects. Check out the Store in the Web UI for list of available scenes. Once a scene is loaded you can instantiate agents and run simulations. See the Agents section on how to create vehicles and pedestrians. Loading scenes takes a while, to reset a scene to the initial state without reloading it call the reset method: sim.reset() This will remove any vehicles or callbacks currently registered. After setting up the scene in a desired state you can start advancing time. During Python code execution time is stopped in the simulator. To run the simulator in realtime, call the run method: sim.run(time_limit = 5.0) run accepts an optional argument for a time limit specifying how long to run. The default value of 0 will run infinitely. Diagram illustrating API execution: Non-real-time Simulation top # The simulator can be run at faster-than-real-time speeds depending on the performance of the computer running the simulator. This is done by calling the run method with the time_scale argument: sim.run(time_limit = 6, time_scale = 2) run takes a 2nd optional argument specifying how much faster to run. In the above example, if the computer is fast enough, the run call will finish in 3 seconds (6 divided by 2), but 6 virtual seconds of data would be generated. If only time_scale is specified or time_limit = 0, then simulation will run continuously at non-realtime speed. The value of time_scale can be lower than 1 which gives ability to run simulation in slower than real time. Agents top # You can create vehicles and pedestrians by calling the add_agent method of the Simulator object. Example: ego = sim.add_agent(name = \"511086bd-97ad-4109-b0ad-654ba662fbcf\", \\ agent_type = lgsvl.AgentType.EGO, state = None) This will create the Lincoln2017MKZ vehicle with the Keyboard Control sensor configuration. This vehicle and sensor configuration are available by default in My Library . In this instance the UUID for the desired sensor configuration is entered for the name argument. The currently available AgentTypes are: AgentType.EGO - EGO vehicle AgentType.NPC - NPC vehicle AgentType.PEDESTRIAN - pedestrian Ego agents are called by the UUIDs of their sensor configurations in the WebUI. To access the UUID of a sensor configuration click on a particular vehicle in My Library to expand the detailed view and click on the ID icon for the desired sensor configuration to copy its UUID to the clipboard. NPC agents are called by their name directly. Available NPC vehicles: Sedan SUV Jeep Hatchback SchoolBus BoxTruck Similarly, pedestrian agents are also called by their names directly. Available pedestrian types: Bob EntrepreneurFemale Howard Johny Pamela Presley Red Robin Stephen Zoe If an incorrect name is entered, a Python exception will be thrown. Optionally you can create agents in specific positions and orientations in the scene. For this you need to use the AgentState class. For example: state = lgsvl.AgentState() state.transform.position = lgsvl.Vector(10, 0, 30) state.transform.rotation.y = 90 ego = sim.add_agent(\"511086bd-97ad-4109-b0ad-654ba662fbcf\", lgsvl.AgentType.EGO, state) This will create a vehicle at position x = 10 , z = 30 which is rotated 90 degrees around the vertical axis. The position and rotation are set in the world coordinates space. You can always adjust the position, rotation, velocity and angular velocity of the agent at any later time: s = ego.state s.velocity.x = -50 ego.state = s This will set x component of velocity (in world coordinate space) to -50 meters per second and leave y and z components of velocity unmodified. All agents have the following common functionality: state - property to get or set agent state (position, velocity, ...) transform - property to get transform member of the state (shortcut for state.transform ) bounding_box - property to get bounding box in local coordinate space. Note that bounding box is not centered around (0, 0, 0) - it depends on the actual geometry of the agent. on_collision - method to set a callback function to be called when the agent collides with something (other agent or static obstacle), see callbacks section for more information. EGO vehicle top # EGO vehicle has following additional functionality: apply_control - method to apply specified throttle, break, steering or other actions to vehicle. Pass sticky=True to apply these values on every simulation update iteration. get_sensors - method to return list of Python-Api compatible sensors connect_bridge - method to connect to ROS or Cyber RT bridge bridge_connected - bool property, True if bridge is connected You can control the movement of the EGO vehicle either by manually specifying state, applying manual control, or connecting through the bridge. Example to apply constant 20% throttle to EGO vehicle: ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) c = lgsvl.VehicleControl() c.throttle = 0.2 ego.apply_control(c, True) NPC vehicles top # You can create multiple NPC vehicles on the map to drive along the lanes or follow specific waypoints on the map. NPC vehicle has the following additional functionality: change_lane - method to make the vehicle change lanes follow - method to make vehicle follow specific waypoints follow_closest_lane - method to make vehicle follow lanes on_waypoint_reached - method to set callback function which is called for every waypoint the vehicle reaches on_stop_line - method to set callback function which is called when vehicle reaches a stop line at intersection on_lane_change - method to set callback function which is called when vehicle decides to change lanes You can control the movement of an NPC vehicle either by manually specifying state, or instructing it to follow waypoints or lanes. To make an NPC follow waypoints prepare a list of DriveWaypoint objects and call the follow method for the npc vehicle: npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) waypoints = [ lgsvl.DriveWaypoint(lgsvl.Vector(1,0,3), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(5,0,3), 10, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(1,0,5), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, a desired velocity in m/s, a desired angular orientation as a vector of Euler angles, an optional wait-time for the vehicle to stay idle, an bool on whether the NPC should be active while idling, and an optional trigger distance. The NPC will ignore all traffic rules and will not avoid collisions to try to get to the next waypoint. The angular orientation of the NPC will be interpolated in such a manner that it will pass through the waypoint at the angle specified in the DriveWaypoint . The trigger distance, if used, provides a method to pause the NPC until an ego vehicle approaches. The NPC will begin to drive as soon as its distance to an ego vehicle drops below the value specified as trigger distance in the DriveWaypoint . You can receive information on progress by setting the on_waypoint_reached callback. Example (see callbacks for more details): npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) def on_waypoint(agent, index): print(\"waypoint {} reached\".format(index)) npc.follow(waypoints, loop=True) npc.on_waypoint_reached(on_waypoint) sim.run() follow_closest_lane will make the NPC vehicle follow whatever lane is the closest. Upon reaching intersections it will randomly decide to either drive straight or turn. You can also spawn a pool of NPC vehicles with the same behavior as NPCs added to a non-API simulation. They will follow the map annotations, obey speed limits, obey traffic signals, and attempt to avoid accidents. These NPCs cannot be directly controlled. sim.add_random_agents(lgsvl.AgentType.NPC) Pedestrians top # You can create Pedestrian agents that will allow you to create pedestrians on sidewalks and make them walk. Pedestrians have the following additional functionality: walk_randomly - method to make pedestrian walk randomly on the sidewalk follow - method to make pedestrian follow specific waypoints on_waypoint_reached - method to set callback function which is called for every waypoint reached You can control the movement of pedestrians either by manually specifying state, or instructing them to follow waypoints or walk randomly. To make pedestrians follow waypoints prepare a list of WalkWaypoint objects and call the follow method for pedestrians: npc = sim.add_agent(\"Bob\", lgsvl.AgentType.PEDESTRIAN) waypoints = [ lgsvl.WalkWaypoint(lgsvl.Vector(1,0,3), 5, 0), lgsvl.WalkWaypoint(lgsvl.Vector(5,0,3), 10, 0), lgsvl.WalkWaypoint(lgsvl.Vector(1,0,5), 5, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, an optional idle time that the pedestrian will spend standing in-place when it reaches the waypoint, and an optional trigger distance. You can receive information on progress by setting the on_waypoint_reached callback. You can also spawn a pool of pedestrians with the same behavior as pedestrians added to a non-API simulation. They will follow the map annotations and path find. These pedestrians cannot be directly controlled. sim.add_random_agents(lgsvl.AgentType.PEDESTRIAN) Callbacks top # The Python API can invoke callbacks to inform you of specific events that occur during simulator runtime. Callbacks are invoked from inside the Simulator.run() method and while a callback is running the simulation time is paused. Once the callback finishes time is resumed and the simulation resumes execution. You can call Simulator.stop() to stop further execution and return immediately from the callback. The internals of this process are illustrated in the following sequence diagram: Here the code resumes simulation after the first callback, but stops execution when the second callback is handled. You set callback functions by calling on_NAME method of object, see information below. Agent Callbacks top # on_collision - called when agent collides with something (other agent or stationary obstacle). Example usage: def on_collision(agent1, agent2, contact): name1 = \"STATIC OBSTACLE\" if agent1 is None else agent1.name name2 = \"STATIC OBSTACLE\" if agent2 is None else agent2.name print(\"{} collided with {} at {}\".format(name1, name2, contact)) ego.on_collision(on_collision) Callback receives three arguments: (agent1, agent2, contact) - the first two are the agents that collide, one of them can be None if it is a stationary obstacle like a building or a traffic light pole, and the third is the world position of the contact point. EgoVehicle Callbacks top # In addition to Agent callbacks, EgoVehicle has one extra callback. on_custom - called when a Sensor Plugin sends a callback; accepts three arguments: (agent, kind, context) - agent instance, kind of sensor plugin as string, JSON context See Sensor Plugins for more information. NpcVehicle Callbacks top # In addition to Agent callbacks, NpcVehicle has three extra callbacks: on_waypoint_reached - called when vehicle reaches a waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer on_stop_line - called when vehicle stops at a stop line for a traffic light or stop sign; accepts one argument: (agent) - agent instance on_lane_change - called when vehicle starts changing lane; accepts one argument: (agent) - agent instance Pedestrian Callbacks top # In addition to Agent callbacks, Pedestrian has one extra callback. on_waypoint_reached - called when pedestrian reaches waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer. Sensors top # EGO vehicles have sensors attached. You can view the configuration of the sensors in the Web Ui. The following sensor classes have been defined to facilitate their use with the Python Api. These classes can only be used if the sensor configuration of the ego vehicle includes the sensor. CameraSensor - see Camera sensor LidarSensor - see LiDAR sensor ImuSensor - see IMU sensor GpsSensor - see GPS sensor RadarSensor - see Radar sensor CanBusSensor - see CAN bus sensor VideoRecordingSensor - see Video Recording sensor Each sensor has the following common members: name - name of sensor, to differentiate sensors of the same type, for example, to choose one out of multiple cameras attached to EgoVehicle transform - property that contains position and rotation of a sensor relative to the agent transform enabled - bool property, set to True if sensor is enabled for capturing and sending data to ROS or Cyber bridge Camera Sensor top # The Camera sensor has the following read only properties: frequency - rate at which images are captured & sent to ROS or Cyber bridge width - image width height - image height fov - vertical field of view in degrees near_plane - distance of near plane far_plane - distance of far plane format - format of image (\"RGB\" for 24-bit color image, \"DEPTH\" - 8-bit grayscale depth buffer, \"SEGMENTATION\" - 24-bit color image with semantic/instance segmentation) Camera image can be saved to disk by calling save : ego = sim.add_agent(\"47b529db-0593-4908-b3e7-4b24a32a0f70\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Main Camera\": sensor.save(\"main-camera.png\", compression=0) save method accepts a path relative to the running simulator, and an optional compression for png files (0...9) or quality (0..100) for jpeg files. LiDAR Sensor top # LiDAR sensor has following read only properties: min_distance - minimal distance for capturing points max_distance - maximum distance for capturing points rays - how many laser rays (vertically) to use rotations - frequency of rotation, typically 10Hz measurements - how many measurements per rotation each ray is taking fov - vertical field of view (bottom to top ray) in degrees angle - angle LiDAR is tilted (middle of fov view) compensated - bool, whether LiDAR point cloud is compensated LiDAR point cloud can be saved to disk as a .pcd file by calling save : ego = sim.add_agent(\"47b529db-0593-4908-b3e7-4b24a32a0f70\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Lidar\": sensor.save(\"lidar.pcd\") A .pcd file is in the binary Point Cloud Data format where each point has x/y/z coordinates as 4-byte floats and a 1-byte unsigned int as intensity (0...255). IMU Sensor top # You can use the IMU sensor to get its position in the vehicle. All measurements an IMU would provide can be obtained by using the transform property of the agent. GPS Sensor top # You can retrieve the current GPS location from the GPS sensor by calling data : data = gps_sensor.data() print(\"Latitude:\", data.latitude) Returned data will contain following fields: latitude longitude northing easting altitude orientation - rotation around up-axis in degrees Radar Sensor top # Currently the Radar sensor can be used only to get its position and rotation in the vehicle. Radar measurements can be received in ROS or Cyber by setting the enabled property of the sensor. CAN bus top # Currently CAN bus can be used only to get its position and rotation in the vehicle. CAN bus messages can be received in ROS or Cyber by setting the enabled property of the sensor. Video Recording Sensor top # The Video Recording sensor is used to record a video of test cases for playback afterward. The following parameters can be set to configure the video recording: width - width of the video in pixels height - height of the video in pixels framerate - the number of frames per second in the video min_distance - the minimum distance from the camera for which objects are rendered max_distance - the maximum distance from the camera for which objects are rendered fov - the vertical field of view of the camera in degrees quality - the target constant quality level for VBR rate control (0 to 51, 0 means automatic) bitrate - the average number of bits per second max_bitrate - the maximum number of bits per second Weather and Time of Day Control top # You can control the weather properties of the simulation by reading or writing to the weather property. You can set rain , fog , wetness , cloudiness , and damage (referring to road damage) as a float in 0..1 range. Example: w = sim.weather w.rain = 0.5 # set rain to 50% sim.weather = w Changing time of day allows to control whether the loaded scene appears as day or night. To get the current time read the time_of_day property: print(\"Current time of day:\", sim.time_of_day) It will return a float between 0 and 24. To set time of day call set_time_of_day : sim.set_time_of_day(10, fixed=True) This will set current time of day to 10 AM of the current date. The date and time of day are important because they determine the position of the sun and directly effect lighting in the scene. The optional bool argument fixed indicates whether the simulation should advance this time automatically or freeze it and not change it ( fixed=True ). For fine-tuned control of time of day, or to set a custom date along with the time of day call set_datetime which takes a Python datetime object as input: from datetime import datetime dt = datetime( year=2020, month=12, day=25, hour=13, minute = 0, second = 0 ) sim.set_datetime(dt) Controllable Objects top # A controllable object is an object that you can control by performing an action using Python APIs. Each controllable object has its own valid actions (e.g., green, yellow, red, trigger, wait, loop) that it can take and is controlled based on control policy , which defines rules for control actions. For example, a traffic light is a controllable object, and you can change its behavior by updating control policy: \"trigger=50;green=1;yellow=1.5;red=2;loop\" trigger=50 - Wait until an ego vehicle approaches this controllable object within 50 meters green=1 - Change current state to green and wait for 1 second yellow=1.5 - Change current state to yellow and wait for 1.5 second red=2 - Change current state to red and wait for 2 second loop - Loop over this control policy from the beginning Available controllable object types: signal cone All Controllable objects can be added or removed dynamically. When reset() is called, all Controllables are removed and the ones in the map (if any) are added back. Controllable objects can be loaded plugins at runtime. Plugin must include IControllable and be built using the Simulator build process from the Assets/External/Controllables folder. To get a list of controllable objects in a scene: controllables = sim.get_controllables() For a controllable object of interest, you can get following information: signal = controllables[0] print(\"Type:\", signal.type) print(\"Transform:\", signal.transform) print(\"Current state:\", signal.current_state) print(\"Valid actions:\", signal.valid_actions) For control policy, each controllable object always has default control policy (read-only). When you load a scene for the first time or reset a scene to the initial state, a controllable object resets current control policy to default one follows it. You can get default control policy and current control policy as follows: print(\"Default control policy:\", signal.default_control_policy) print(\"Current control policy:\", signal.control_policy) To change a current control policy, you can create a new control policy and call control function as below: control_policy = \"trigger=50;green=1;yellow=1.5;red=2;loop\" signal.control(control_policy) To add a plugin controllable and set object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0,0,0) state.transform.rotation = lgsvl.Vector(0,0,0) state.velocity = lgsvl.Vector(0,10,0) state.angular_velocity = lgsvl.Vector(6.5,0,0) cone = sim.controllable_add(\"TrafficCone\", state) To get plugin controllable object state cone.object_state To set plugin controllable object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0, 0, -10) cone.object_state = state Helper Functions top # Simulator class offers following helper functions: version - property that returns current version of simulator as string layer - property that returns all named Unity physics layers current_scene - property that returns currently loaded scene as string, None if none is loaded current_frame - property that returns currently simulated frame number as integer current_time - property that returns currently simulation time in seconds as float get_spawn - method that returns list of Spawn objects representing good positions to place vehicles in the map. This list can be empty, it depends on how the map is prepared in Unity. Returned Spawn objects contain a transform which holds position and rotation members as a Vector , as well as destinations which holds valid destination points for an ego vehicle starting at the spawn point as an array of transforms . get_agents - method that returns a list of currently available agent objects added with add_agent To map points in Unity coordinates to GPS coordinates the Simulator class offers the following two functions: map_to_gps - maps transform (position & rotation) to GPS location, returns same type as GPS Sensor data method map_from_gps - maps GPS location (latitude/longitude or northing/easting) to transform raycast - shoots a ray from specific location and returns closest object it hits map_from_gps accepts two different inputs - latitude/longitude or northing/easting. Examples: tr1 = sim.map_from_gps(latitude=10, longitude=-30) tr2 = sim.map_from_gps(northing=123455, easting=552341) Optionally you can pass altitude and orientation. raycast method can be used in following way: origin = lgsvl.Vector(10, 0, 20) direction = lgsvl.Vector(1, 0, 0) hit = sim.raycast(origin, direction, layer_mask=1) if hit: print(\"Distance right:\", hit.distance) This will shoot a ray in the positive x -axis direction from the (10, 0, 20) coordinates. A RaycastHit object with distance , point and normal fields is returned if something is hit, otherwise None is returned. When raycasting you should specify a layer_mask argument that specifies which objects to check collision with. It corresponds to layers in the Unity project - check the project for actual values or use the layer property. layer_mask Name Description 0 Default roads must use this layer* 1 TransparentFX used to ignore FX with transparency* 2 Ignore Raycast used to ignore any raycasts against* 4 Water not used* 5 UI used to cull UI in scene* 8 PostProcessing used to cull postprocessing effects 9 Agent ego vehicles 10 NPC npc vehicles 11 Pedestrian pedestrians 12 Obstacle sign poles, buildings 13 Sensor used to cull sensor effects 14 GroundTruthRange used to cull ground truth range 15 GroundTruth used to cull ground truth triggers 16 Lane used to cull lane triggers 31 SkyEffects used to cull clouds * Default Unity Physics Layers Changelog top # 2020-08-28 Added support for time-to-collision and distance-to-collision triggers for NPCs and pedestrians 2020-05-22 Added suport for setting pedestrian travel speed 2020-01-30 Extended controllable objects to support plugins - see controllable plugins 2019-09-05 Extended DriveWaypoint to support angle, idle time and trigger distance Added controllable objects - use Simulator.get_controllables 2019-08-12 Added time_scale argument to run function for running simulation in non-realtime Added seed argument to Simulator.load for deterministic NPCs 2019-04-19 initial release","title":"Python API guide"},{"location":"python-api/python-api/#overview","text":"SVL Simulator exposes runtime functionality to a Python API which you can use to manipulate object placement and vehicle movement in a loaded scene, retrieve sensor configuration and data, control weather, time state, and more. The interface to listen for incoming API calls is defined in config.yml . See Configuration File and Command-Line Parameters for more information.","title":"Overview"},{"location":"python-api/python-api/#requirements","text":"Using Python API requires Python version 3.6 or later.","title":"Requirements"},{"location":"python-api/python-api/#quickstart","text":"Python API is available in separate repository: https://github.com/lgsvl/PythonAPI After cloning or downloading it from the git repository follow these steps: Run the following command to install Python files and necessary dependencies: pip3 install --user -e . Now launch the simulator, either using the binary file or from Unity Editor. The simulator by default listens for connections on port 8181 on localhost . See the guide for Running SVL Simulator for more information on how to set up and run the simulator. Click the Open Browser button to open the Simulator UI, then click Sign in . Enter login credentials or click Sign Up to create an account. You will need to add assets such as maps and vehicles to your library to use in simulations. These assets can either be added from the Store or uploaded to the cloud. Quickstart scripts require the BorregasAve map and the Lincoln2017MKZ vehicle, which are included in your library by default. You will need to run a simulation using the API Only runtime template to run Python API scripts on your host machine to control the simulation. See the document on Simulations for information on how to create a simulation. Select the newly created Simulation and click the Run Simulation button. Run the following example to see the API in action: ./quickstart/05-ego-drive-in-circle.py This will load the BorregasAve.unity scene, instantiate one EGO vehicle, then ask you to press Enter to start driving EGO vehicle in a circle. When the script is running, it will apply throttle and steering commands to make the car move.","title":"Quickstart"},{"location":"python-api/python-api/#core-concepts","text":"The Simulator and API communicate by sending json over a websocket server running on port 8181 . The API client can either be on the same machine or on any other machine on the same network. API exposes the following main types: Simulator - main object for connecting to simulator and creating other objects Agent - superclass of vehicles and pedestrian EgoVehicle - EGO vehicle with accurate physics simulation and sensors NpcVehicle - NPC vehicle with simplified physics, useful for creating many background vehicles Pedestrian - pedestrian walking on sidewalks Vehicles and Pedestrian are a subclasses of Agent which has common properties like transform , position , and velocity . All coordinates in the API return values in the Unity coordinate system. This coordinate system uses meters as a unit of distance and is a left-handed coordinate system where x points left, z points forward, and y points up. The Simulator class provides helper methods to convert coordinates to and from latitude/longitude and northing/easting values.","title":"Core Concepts"},{"location":"python-api/python-api/#simulation","text":"To connect to the simulator you need to an instance of the Simulator class: import lgsvl sim = lgsvl.Simulator(address = \"localhost\", port = 8181) You can specify a different address as hostname or IP address. By default only port 8181 is used for API connection. Only one client can be connected to simulator at a time. Next, load the scene (sometimes called map). This is done by the load method: sim.load(scene = \"aae03d2a-b7ca-4a88-9e41-9035287a12cc\", seed = 650387) scene is a string representing the UUID of the Map in the Web UI. Scenes can be added either from the map tab under Store or by manually uploading. Some of the well-known scenes available in the Map tab under Store are: Map name UUID Description BorregasAve aae03d2a-b7ca-4a88-9e41-9035287a12cc a Digital Twin of a real-world suburban street block in Sunnyvale, CA AutonomouStuff 2aae5d39-a11c-4516-87c4-cdc9ca784551 small office parking lot in SanJose, CA Shalun 97128028-33c7-4411-b1ec-d693ed35071f an autonomous vehicle testing facility in Taiwan (More information here ) SanFrancisco 5d272540-f689-4355-83c7-03bf11b6865f a real world urban environment from San Francisco, CA GoMentum Station 979dd7f3-b25b-47f0-ab10-a6effb370138 a Digital Twin of a real-world autonomous vehicle testing facility in Concord, CA CubeTown 06773677-1ce3-492f-9fe2-b3147e126e27 a virtual environment with block obstacles used to perform basic testing of vehicles SingleLaneRoad a6e2d149-6a18-4b83-9029-4411d7b2e69a a simple two-way single-lane road Seed (optional) is an Integer (-2,147,483,648 - 2,147,483,647) that determines the \"random\" behavior of the NPC vehicles and rain effects. Check out the Store in the Web UI for list of available scenes. Once a scene is loaded you can instantiate agents and run simulations. See the Agents section on how to create vehicles and pedestrians. Loading scenes takes a while, to reset a scene to the initial state without reloading it call the reset method: sim.reset() This will remove any vehicles or callbacks currently registered. After setting up the scene in a desired state you can start advancing time. During Python code execution time is stopped in the simulator. To run the simulator in realtime, call the run method: sim.run(time_limit = 5.0) run accepts an optional argument for a time limit specifying how long to run. The default value of 0 will run infinitely. Diagram illustrating API execution:","title":"Simulation"},{"location":"python-api/python-api/#non-realtime-simulation","text":"The simulator can be run at faster-than-real-time speeds depending on the performance of the computer running the simulator. This is done by calling the run method with the time_scale argument: sim.run(time_limit = 6, time_scale = 2) run takes a 2nd optional argument specifying how much faster to run. In the above example, if the computer is fast enough, the run call will finish in 3 seconds (6 divided by 2), but 6 virtual seconds of data would be generated. If only time_scale is specified or time_limit = 0, then simulation will run continuously at non-realtime speed. The value of time_scale can be lower than 1 which gives ability to run simulation in slower than real time.","title":"Non-real-time Simulation"},{"location":"python-api/python-api/#agents","text":"You can create vehicles and pedestrians by calling the add_agent method of the Simulator object. Example: ego = sim.add_agent(name = \"511086bd-97ad-4109-b0ad-654ba662fbcf\", \\ agent_type = lgsvl.AgentType.EGO, state = None) This will create the Lincoln2017MKZ vehicle with the Keyboard Control sensor configuration. This vehicle and sensor configuration are available by default in My Library . In this instance the UUID for the desired sensor configuration is entered for the name argument. The currently available AgentTypes are: AgentType.EGO - EGO vehicle AgentType.NPC - NPC vehicle AgentType.PEDESTRIAN - pedestrian Ego agents are called by the UUIDs of their sensor configurations in the WebUI. To access the UUID of a sensor configuration click on a particular vehicle in My Library to expand the detailed view and click on the ID icon for the desired sensor configuration to copy its UUID to the clipboard. NPC agents are called by their name directly. Available NPC vehicles: Sedan SUV Jeep Hatchback SchoolBus BoxTruck Similarly, pedestrian agents are also called by their names directly. Available pedestrian types: Bob EntrepreneurFemale Howard Johny Pamela Presley Red Robin Stephen Zoe If an incorrect name is entered, a Python exception will be thrown. Optionally you can create agents in specific positions and orientations in the scene. For this you need to use the AgentState class. For example: state = lgsvl.AgentState() state.transform.position = lgsvl.Vector(10, 0, 30) state.transform.rotation.y = 90 ego = sim.add_agent(\"511086bd-97ad-4109-b0ad-654ba662fbcf\", lgsvl.AgentType.EGO, state) This will create a vehicle at position x = 10 , z = 30 which is rotated 90 degrees around the vertical axis. The position and rotation are set in the world coordinates space. You can always adjust the position, rotation, velocity and angular velocity of the agent at any later time: s = ego.state s.velocity.x = -50 ego.state = s This will set x component of velocity (in world coordinate space) to -50 meters per second and leave y and z components of velocity unmodified. All agents have the following common functionality: state - property to get or set agent state (position, velocity, ...) transform - property to get transform member of the state (shortcut for state.transform ) bounding_box - property to get bounding box in local coordinate space. Note that bounding box is not centered around (0, 0, 0) - it depends on the actual geometry of the agent. on_collision - method to set a callback function to be called when the agent collides with something (other agent or static obstacle), see callbacks section for more information.","title":"Agents"},{"location":"python-api/python-api/#ego-vehicle","text":"EGO vehicle has following additional functionality: apply_control - method to apply specified throttle, break, steering or other actions to vehicle. Pass sticky=True to apply these values on every simulation update iteration. get_sensors - method to return list of Python-Api compatible sensors connect_bridge - method to connect to ROS or Cyber RT bridge bridge_connected - bool property, True if bridge is connected You can control the movement of the EGO vehicle either by manually specifying state, applying manual control, or connecting through the bridge. Example to apply constant 20% throttle to EGO vehicle: ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) c = lgsvl.VehicleControl() c.throttle = 0.2 ego.apply_control(c, True)","title":"EGO Vehicle"},{"location":"python-api/python-api/#npc-vehicles","text":"You can create multiple NPC vehicles on the map to drive along the lanes or follow specific waypoints on the map. NPC vehicle has the following additional functionality: change_lane - method to make the vehicle change lanes follow - method to make vehicle follow specific waypoints follow_closest_lane - method to make vehicle follow lanes on_waypoint_reached - method to set callback function which is called for every waypoint the vehicle reaches on_stop_line - method to set callback function which is called when vehicle reaches a stop line at intersection on_lane_change - method to set callback function which is called when vehicle decides to change lanes You can control the movement of an NPC vehicle either by manually specifying state, or instructing it to follow waypoints or lanes. To make an NPC follow waypoints prepare a list of DriveWaypoint objects and call the follow method for the npc vehicle: npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) waypoints = [ lgsvl.DriveWaypoint(lgsvl.Vector(1,0,3), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(5,0,3), 10, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(1,0,5), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, a desired velocity in m/s, a desired angular orientation as a vector of Euler angles, an optional wait-time for the vehicle to stay idle, an bool on whether the NPC should be active while idling, and an optional trigger distance. The NPC will ignore all traffic rules and will not avoid collisions to try to get to the next waypoint. The angular orientation of the NPC will be interpolated in such a manner that it will pass through the waypoint at the angle specified in the DriveWaypoint . The trigger distance, if used, provides a method to pause the NPC until an ego vehicle approaches. The NPC will begin to drive as soon as its distance to an ego vehicle drops below the value specified as trigger distance in the DriveWaypoint . You can receive information on progress by setting the on_waypoint_reached callback. Example (see callbacks for more details): npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) def on_waypoint(agent, index): print(\"waypoint {} reached\".format(index)) npc.follow(waypoints, loop=True) npc.on_waypoint_reached(on_waypoint) sim.run() follow_closest_lane will make the NPC vehicle follow whatever lane is the closest. Upon reaching intersections it will randomly decide to either drive straight or turn. You can also spawn a pool of NPC vehicles with the same behavior as NPCs added to a non-API simulation. They will follow the map annotations, obey speed limits, obey traffic signals, and attempt to avoid accidents. These NPCs cannot be directly controlled. sim.add_random_agents(lgsvl.AgentType.NPC)","title":"NPC Vehicles"},{"location":"python-api/python-api/#pedestrians","text":"You can create Pedestrian agents that will allow you to create pedestrians on sidewalks and make them walk. Pedestrians have the following additional functionality: walk_randomly - method to make pedestrian walk randomly on the sidewalk follow - method to make pedestrian follow specific waypoints on_waypoint_reached - method to set callback function which is called for every waypoint reached You can control the movement of pedestrians either by manually specifying state, or instructing them to follow waypoints or walk randomly. To make pedestrians follow waypoints prepare a list of WalkWaypoint objects and call the follow method for pedestrians: npc = sim.add_agent(\"Bob\", lgsvl.AgentType.PEDESTRIAN) waypoints = [ lgsvl.WalkWaypoint(lgsvl.Vector(1,0,3), 5, 0), lgsvl.WalkWaypoint(lgsvl.Vector(5,0,3), 10, 0), lgsvl.WalkWaypoint(lgsvl.Vector(1,0,5), 5, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, an optional idle time that the pedestrian will spend standing in-place when it reaches the waypoint, and an optional trigger distance. You can receive information on progress by setting the on_waypoint_reached callback. You can also spawn a pool of pedestrians with the same behavior as pedestrians added to a non-API simulation. They will follow the map annotations and path find. These pedestrians cannot be directly controlled. sim.add_random_agents(lgsvl.AgentType.PEDESTRIAN)","title":"Pedestrians"},{"location":"python-api/python-api/#callbacks","text":"The Python API can invoke callbacks to inform you of specific events that occur during simulator runtime. Callbacks are invoked from inside the Simulator.run() method and while a callback is running the simulation time is paused. Once the callback finishes time is resumed and the simulation resumes execution. You can call Simulator.stop() to stop further execution and return immediately from the callback. The internals of this process are illustrated in the following sequence diagram: Here the code resumes simulation after the first callback, but stops execution when the second callback is handled. You set callback functions by calling on_NAME method of object, see information below.","title":"Callbacks"},{"location":"python-api/python-api/#agent-callbacks","text":"on_collision - called when agent collides with something (other agent or stationary obstacle). Example usage: def on_collision(agent1, agent2, contact): name1 = \"STATIC OBSTACLE\" if agent1 is None else agent1.name name2 = \"STATIC OBSTACLE\" if agent2 is None else agent2.name print(\"{} collided with {} at {}\".format(name1, name2, contact)) ego.on_collision(on_collision) Callback receives three arguments: (agent1, agent2, contact) - the first two are the agents that collide, one of them can be None if it is a stationary obstacle like a building or a traffic light pole, and the third is the world position of the contact point.","title":"Agent Callbacks"},{"location":"python-api/python-api/#egovehicle-callbacks","text":"In addition to Agent callbacks, EgoVehicle has one extra callback. on_custom - called when a Sensor Plugin sends a callback; accepts three arguments: (agent, kind, context) - agent instance, kind of sensor plugin as string, JSON context See Sensor Plugins for more information.","title":"'EgoVehicle"},{"location":"python-api/python-api/#npcvehicle-callbacks","text":"In addition to Agent callbacks, NpcVehicle has three extra callbacks: on_waypoint_reached - called when vehicle reaches a waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer on_stop_line - called when vehicle stops at a stop line for a traffic light or stop sign; accepts one argument: (agent) - agent instance on_lane_change - called when vehicle starts changing lane; accepts one argument: (agent) - agent instance","title":"NpcVehicle Callbacks"},{"location":"python-api/python-api/#pedestrian-callbacks","text":"In addition to Agent callbacks, Pedestrian has one extra callback. on_waypoint_reached - called when pedestrian reaches waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer.","title":"Pedestrian Callbacks"},{"location":"python-api/python-api/#sensors","text":"EGO vehicles have sensors attached. You can view the configuration of the sensors in the Web Ui. The following sensor classes have been defined to facilitate their use with the Python Api. These classes can only be used if the sensor configuration of the ego vehicle includes the sensor. CameraSensor - see Camera sensor LidarSensor - see LiDAR sensor ImuSensor - see IMU sensor GpsSensor - see GPS sensor RadarSensor - see Radar sensor CanBusSensor - see CAN bus sensor VideoRecordingSensor - see Video Recording sensor Each sensor has the following common members: name - name of sensor, to differentiate sensors of the same type, for example, to choose one out of multiple cameras attached to EgoVehicle transform - property that contains position and rotation of a sensor relative to the agent transform enabled - bool property, set to True if sensor is enabled for capturing and sending data to ROS or Cyber bridge","title":"Sensors"},{"location":"python-api/python-api/#camera-sensor","text":"The Camera sensor has the following read only properties: frequency - rate at which images are captured & sent to ROS or Cyber bridge width - image width height - image height fov - vertical field of view in degrees near_plane - distance of near plane far_plane - distance of far plane format - format of image (\"RGB\" for 24-bit color image, \"DEPTH\" - 8-bit grayscale depth buffer, \"SEGMENTATION\" - 24-bit color image with semantic/instance segmentation) Camera image can be saved to disk by calling save : ego = sim.add_agent(\"47b529db-0593-4908-b3e7-4b24a32a0f70\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Main Camera\": sensor.save(\"main-camera.png\", compression=0) save method accepts a path relative to the running simulator, and an optional compression for png files (0...9) or quality (0..100) for jpeg files.","title":"Camera Sensor"},{"location":"python-api/python-api/#lidar-sensor","text":"LiDAR sensor has following read only properties: min_distance - minimal distance for capturing points max_distance - maximum distance for capturing points rays - how many laser rays (vertically) to use rotations - frequency of rotation, typically 10Hz measurements - how many measurements per rotation each ray is taking fov - vertical field of view (bottom to top ray) in degrees angle - angle LiDAR is tilted (middle of fov view) compensated - bool, whether LiDAR point cloud is compensated LiDAR point cloud can be saved to disk as a .pcd file by calling save : ego = sim.add_agent(\"47b529db-0593-4908-b3e7-4b24a32a0f70\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Lidar\": sensor.save(\"lidar.pcd\") A .pcd file is in the binary Point Cloud Data format where each point has x/y/z coordinates as 4-byte floats and a 1-byte unsigned int as intensity (0...255).","title":"LiDAR Sensor"},{"location":"python-api/python-api/#imu-sensor","text":"You can use the IMU sensor to get its position in the vehicle. All measurements an IMU would provide can be obtained by using the transform property of the agent.","title":"IMU Sensor"},{"location":"python-api/python-api/#gps-sensor","text":"You can retrieve the current GPS location from the GPS sensor by calling data : data = gps_sensor.data() print(\"Latitude:\", data.latitude) Returned data will contain following fields: latitude longitude northing easting altitude orientation - rotation around up-axis in degrees","title":"GPS Sensor"},{"location":"python-api/python-api/#radar-sensor","text":"Currently the Radar sensor can be used only to get its position and rotation in the vehicle. Radar measurements can be received in ROS or Cyber by setting the enabled property of the sensor.","title":"Radar Sensor"},{"location":"python-api/python-api/#can-bus","text":"Currently CAN bus can be used only to get its position and rotation in the vehicle. CAN bus messages can be received in ROS or Cyber by setting the enabled property of the sensor.","title":"CAN bus"},{"location":"python-api/python-api/#video-recording","text":"The Video Recording sensor is used to record a video of test cases for playback afterward. The following parameters can be set to configure the video recording: width - width of the video in pixels height - height of the video in pixels framerate - the number of frames per second in the video min_distance - the minimum distance from the camera for which objects are rendered max_distance - the maximum distance from the camera for which objects are rendered fov - the vertical field of view of the camera in degrees quality - the target constant quality level for VBR rate control (0 to 51, 0 means automatic) bitrate - the average number of bits per second max_bitrate - the maximum number of bits per second","title":"Video Recording Sensor"},{"location":"python-api/python-api/#weather-and-time-of-day-control","text":"You can control the weather properties of the simulation by reading or writing to the weather property. You can set rain , fog , wetness , cloudiness , and damage (referring to road damage) as a float in 0..1 range. Example: w = sim.weather w.rain = 0.5 # set rain to 50% sim.weather = w Changing time of day allows to control whether the loaded scene appears as day or night. To get the current time read the time_of_day property: print(\"Current time of day:\", sim.time_of_day) It will return a float between 0 and 24. To set time of day call set_time_of_day : sim.set_time_of_day(10, fixed=True) This will set current time of day to 10 AM of the current date. The date and time of day are important because they determine the position of the sun and directly effect lighting in the scene. The optional bool argument fixed indicates whether the simulation should advance this time automatically or freeze it and not change it ( fixed=True ). For fine-tuned control of time of day, or to set a custom date along with the time of day call set_datetime which takes a Python datetime object as input: from datetime import datetime dt = datetime( year=2020, month=12, day=25, hour=13, minute = 0, second = 0 ) sim.set_datetime(dt)","title":"Weather and Time of Day Control"},{"location":"python-api/python-api/#controllable-objects","text":"A controllable object is an object that you can control by performing an action using Python APIs. Each controllable object has its own valid actions (e.g., green, yellow, red, trigger, wait, loop) that it can take and is controlled based on control policy , which defines rules for control actions. For example, a traffic light is a controllable object, and you can change its behavior by updating control policy: \"trigger=50;green=1;yellow=1.5;red=2;loop\" trigger=50 - Wait until an ego vehicle approaches this controllable object within 50 meters green=1 - Change current state to green and wait for 1 second yellow=1.5 - Change current state to yellow and wait for 1.5 second red=2 - Change current state to red and wait for 2 second loop - Loop over this control policy from the beginning Available controllable object types: signal cone All Controllable objects can be added or removed dynamically. When reset() is called, all Controllables are removed and the ones in the map (if any) are added back. Controllable objects can be loaded plugins at runtime. Plugin must include IControllable and be built using the Simulator build process from the Assets/External/Controllables folder. To get a list of controllable objects in a scene: controllables = sim.get_controllables() For a controllable object of interest, you can get following information: signal = controllables[0] print(\"Type:\", signal.type) print(\"Transform:\", signal.transform) print(\"Current state:\", signal.current_state) print(\"Valid actions:\", signal.valid_actions) For control policy, each controllable object always has default control policy (read-only). When you load a scene for the first time or reset a scene to the initial state, a controllable object resets current control policy to default one follows it. You can get default control policy and current control policy as follows: print(\"Default control policy:\", signal.default_control_policy) print(\"Current control policy:\", signal.control_policy) To change a current control policy, you can create a new control policy and call control function as below: control_policy = \"trigger=50;green=1;yellow=1.5;red=2;loop\" signal.control(control_policy) To add a plugin controllable and set object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0,0,0) state.transform.rotation = lgsvl.Vector(0,0,0) state.velocity = lgsvl.Vector(0,10,0) state.angular_velocity = lgsvl.Vector(6.5,0,0) cone = sim.controllable_add(\"TrafficCone\", state) To get plugin controllable object state cone.object_state To set plugin controllable object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0, 0, -10) cone.object_state = state","title":"Controllable Objects"},{"location":"python-api/python-api/#helper-functions","text":"Simulator class offers following helper functions: version - property that returns current version of simulator as string layer - property that returns all named Unity physics layers current_scene - property that returns currently loaded scene as string, None if none is loaded current_frame - property that returns currently simulated frame number as integer current_time - property that returns currently simulation time in seconds as float get_spawn - method that returns list of Spawn objects representing good positions to place vehicles in the map. This list can be empty, it depends on how the map is prepared in Unity. Returned Spawn objects contain a transform which holds position and rotation members as a Vector , as well as destinations which holds valid destination points for an ego vehicle starting at the spawn point as an array of transforms . get_agents - method that returns a list of currently available agent objects added with add_agent To map points in Unity coordinates to GPS coordinates the Simulator class offers the following two functions: map_to_gps - maps transform (position & rotation) to GPS location, returns same type as GPS Sensor data method map_from_gps - maps GPS location (latitude/longitude or northing/easting) to transform raycast - shoots a ray from specific location and returns closest object it hits map_from_gps accepts two different inputs - latitude/longitude or northing/easting. Examples: tr1 = sim.map_from_gps(latitude=10, longitude=-30) tr2 = sim.map_from_gps(northing=123455, easting=552341) Optionally you can pass altitude and orientation. raycast method can be used in following way: origin = lgsvl.Vector(10, 0, 20) direction = lgsvl.Vector(1, 0, 0) hit = sim.raycast(origin, direction, layer_mask=1) if hit: print(\"Distance right:\", hit.distance) This will shoot a ray in the positive x -axis direction from the (10, 0, 20) coordinates. A RaycastHit object with distance , point and normal fields is returned if something is hit, otherwise None is returned. When raycasting you should specify a layer_mask argument that specifies which objects to check collision with. It corresponds to layers in the Unity project - check the project for actual values or use the layer property. layer_mask Name Description 0 Default roads must use this layer* 1 TransparentFX used to ignore FX with transparency* 2 Ignore Raycast used to ignore any raycasts against* 4 Water not used* 5 UI used to cull UI in scene* 8 PostProcessing used to cull postprocessing effects 9 Agent ego vehicles 10 NPC npc vehicles 11 Pedestrian pedestrians 12 Obstacle sign poles, buildings 13 Sensor used to cull sensor effects 14 GroundTruthRange used to cull ground truth range 15 GroundTruth used to cull ground truth triggers 16 Lane used to cull lane triggers 31 SkyEffects used to cull clouds * Default Unity Physics Layers","title":"Helper Functions"},{"location":"python-api/python-api/#changelog","text":"2020-08-28 Added support for time-to-collision and distance-to-collision triggers for NPCs and pedestrians 2020-05-22 Added suport for setting pedestrian travel speed 2020-01-30 Extended controllable objects to support plugins - see controllable plugins 2019-09-05 Extended DriveWaypoint to support angle, idle time and trigger distance Added controllable objects - use Simulator.get_controllables 2019-08-12 Added time_scale argument to run function for running simulation in non-realtime Added seed argument to Simulator.load for deterministic NPCs 2019-04-19 initial release","title":"Changelog"},{"location":"release-notes/changelog/","text":"Changelog All notable changes and release notes for SVL Simulator will be documented in this file. [2021.2] - 2021-06-30 # Added # External Pedestrians and NPC (other traffic vehicles) with animation support New NPCs and Pedestrians like Scooters, Bicyclists and Animals External Bridge Support Multiple simulator asset bundle versions support Rain Collision effect Support for simulations without EGO ( Spectator Ego ) New \u201cDriver view\u201d camera state New API to set simulator camera state Annotation tool to set accurate values (position, rotation and scale) for MapOrigin New \u201cclear cache\u201d option Ability to use proxy server for cloud_url in config.yml New enhanced web user interface: Improved visual sensor editor Ability to clone Sensors in sensor configuration User warning messages for privilege based restriction Markdown support for Asset description Changed # Unity Upgrade to 2020.3.3f1 Revert FBX package to 3.2.1-preview.2 Remove overdraw with HDRP flare Improved NPC handling: Fixed rotating NPC wheels when using NPCWaypointBehaviour Pedestrian ground truth from bounds Improvements and bug fixes around Distributed Simulation with multi-ego support: Fixed API mode for distributed simulation on a single machine Fixed pedestrian animations on client machine Improved distributing VehicleActions Support for the ArticulationBody in distributed simulations Improved load balancing of the sensors Updated Sensor support: Fix TF visual and sensor hierarchy update input for g920 wheel for windows Publish camera intrinsic parameters to ROS and ROS2 Rework lidar to use cubemap Improved lighting and rain: Fix brown lighting Rotate sun and moon to match map rotation Rain collision improvements and fixes Fix overlapping rain chunks Enhanced Visual Scenario Editor support: Minor UX fixes Fixed changing agents' variants Fix issue with agent dropdown not refreshing on click Loading pedestrians from config Improvements around HD Map Mesh Generator : Improve data handling in HD map mesh builder Erosion and re-leveling for point cloud mesh Work in progress improvements on Developer mode: Rework developer settings Bundle version uptick and unity editor check on developer debug mode for bridges Sensor debug developer loading More enhanced and user friendly web user interface: Password Sensitivity improvements GLTF editor bug fixes and improvements HD map preview improvements Simulation invalid items management WCAG accessibility UI compatibility Live Test Results view in Simulation Profile Updated algorithm for Trending assets view Simulation form auto-save Misc Improvements: Support default light layers in point cloud renderer Loads assembly with the ego name from asset bundle Added public driver view transform to be set in the Inspector Asmdef added, loader fixed to deal with bundle/WISE name mismatch Loader: improve the error message about BundleFormat Make sure Simulator.Editor.Build.Run exits on any exception Trimming last slash in the cloud url if needed. get full path from data path provided [2021.1.1] - 2021-05-20 # Added # 3D Visualizer to help create sensor configuration on a vehicle HD map visualizer on map profile pages Ability to share test reports with other users \"Supported Simulator versions\" value on asset profile page Improved sharing feature for private assets and simulations Save and Exit button on sensor configuration page Tutorials to help new users on how to interact with the web user interface pages Email confirmation upon sensor plugin sharing Changed # Improved quick filters on Store and Library pages Redirect to existing asset profile page when uploading an already uploaded asset Request access button on private plugin pages Require adding a vehicle sensor configuration when creating a simulation Fixed issues with multi-vehicle simulation using same vehicle and sensor configuration Improvements to cluster page user experience Show proper names for missing sensors on invalid simulations Proper error messages for wrong information on registration page Proper error messages when maximum allowed cluster limit is reached [2021.1] - 2021-03-23 # Added # New cloud-based web user interface including: Asset Store for downloading assets, uploading and sharing assets publicly user account password management user assets management library support for private assets private assets/simulations sharing with registered users cluster/Simulation status handling SVL Simulator content from this release will be available in the Asset Store New and updated runtime templates for simulations with automatic bringup and execution: Random Traffic Visual Scenario Editor Python API API Only Visual Scenario Editor for creating scenarios including support for: traffic vehicle and pedestrian waypoints previews copy/paste/undo feature ability to specify ego vehicle target destination support for controllables with ability to edit default policy color selection for actors waypoint positions snapped to HD map lanes support for distance, time, and time-to-collision triggers traffic lights control Automatically generated test case reports of simulations including: success/failure scenario evaluation simulation statistics analytics data per sensor video recording of simulation multiple violation event detection: collision, stopline, stopsign, ego-stuck, sudden jerk/brake/turn, speed-limit, etc. PythonAPI: many improvements including: added cloudiness and road damage to weather parameters ability to specify date in environment configuration added Dreamview API as sub-module added callback agents_traversed_waypoints added ability to set speed at WalkWaypoints added waypoint trigger system ability to set simulator camera free-state position and rotation sample Python API scripts based on NHTSA-defined scenarios for end-to-end testing additional Python API test samples: random-traffic mode, cut-in, sudden-braking, etc. Environment configuration improvements: moon position in nighttime environment configurations Improved environment road wetness layer UI for FPS and SIM time Improved rain particles Increased types of traffic lights, model improvements Improvements in Developer Mode : developer settings to use ego vehicle within Developer Mode ability to select and add actors to simulation in Developer Settings added sensor debug mode added tool to rotate Editor scene view to -X north Features for traffic agents in simulation: support for bicyclist traffic agents support for custom traffic vehicle behavior plugins pedestrian crosswalk support with yield for traffic vehicles (SanFrancisco map) added direction for pedestrian crosswalk annotation tool spawning NPCs now raycasts target bounds from egos' positions New map/environment creation improvements: default ego destination points for LG-provided maps HD map road network environment generation tool test maps for NHTSA-like test cases: Straight1LanePedestrianCrosswalk, Straight1LaneSame, etc. General simulation improvements: ability to create and run a Distributed Simulation from web user interface offline mode for running saved simulation configurations improved cache control for downloaded assets Sensors improvements: new sensors : StopLine, HUDKeyboardControl, AutowareAIControl, LGSVLControl, etc. improved LiDAR simulation performance sun-flare, raindrops, greyscale, and video artifacts effects for sensors support for sensor post-distortion effects Default values for simple and complex sensor parameter types Enabled sensors to load texture assets Improvements around support for AD stacks: Optimized ROS2 interface and bridge Support for Apollo 6.0 and master Changed # Product branding changed from \"LGSVL Simulator\" to \"SVL Simulator\" Old local web user interface is now deprecated. Users need to migrate to new web user interface Old content website will be deprecated All content now managed on cloud - downloaded as needed by simulation configuration Unity update to 2019.4.18f1 All sensors are now sensor plugins - default sensors are part of user plugin library Sensor plugins no longer bundled with simulator executable Distributed simulation: fixed pedestrian movement animation bug Distributed simulation: video artifact fix Fixed wrong timestamp error in clock and IMU sensors Fix acceleration/angular velocity in Apollo imu/corrected_imu Fixed way of handling failed downloads Fixed bugs and issues in OpenDrive HD map importer and exporter Point cloud rendering: added alternative method for removing obscured points (reduces flickering, requires depth prepass) Point cloud rendering: Fill scarce depth map holes through neighborhood sampling Point Cloud Rendering: Fix skybox blending in unlit point cloud Improved traffic lights in environments Rain/clouds/lights improvements NPC spawn logic improvements Fix old NPC models and prefab Adjust camera distance from ego bounds HD map import: Fix keyError in lanelet2 and improve grouping lanes at intersections HD map import: Fix map origin update during OpenDrive map import [2020.06] - 2020-08-31 # Added # Support modular testing of planning with Apollo 5.0 Bridge plugin system with example ( LoggingBridge ) Added default ultrasonic distance sensor Sensor TF visualizer in sensor visualizations Added button in map annotation tool to add synthetic lane boundary lines automatically Python API: added callback agents_traversed_waypoints Python API: added ability to set speed at WalkWaypoints Python API: Added waypoint trigger system Changed # Fixed 90 degree orientation offset in GPS odometry sensor Replaced default ROS2 bridge to improved custom ROS2 bridge instead of ros2-web-bridge Unity update to 2019.3.15f1 Fixed timestamps in GPS, IMU Added timestamp to default LiDAR sensor PointCloud message Fixed bugs in OpenDrive map importer to correctly compute reference lines and lane widths for edge cases Improved OpenDrive map exporter to export parametric cubic curves instead of simple lines for each reference line segment [2020.05] - 2020-06-02 # Added # Support for custom traffic vehicle behavior plugins Support for Apollo 5.5 ( latest master ) Support for distributed simulation Velodyne VLS-128 support Python API function to spawn random non-ego vehicles and pedestrians Unlit shadows support for point cloud environment rendering AD system-agnostic message definitions to \"lgsvl_msgs\" for vehicle control, CAN bus and status data, etc. Changed # Refactored traffic agent Waypoint Mode with fixes for idle and trigger waypoints Changed gnss_odom message coordinate system to right-handed coordinate system Fixed \"ignore map origin\" option for gnss_odom message Fixed acceleration values in IMU sensor Fix sky flickering issue during point cloud rendering Fixed LiDAR readback error Removed and replaced documentation source files from main repository to separate Documentation repository [2020.03] - 2020-04-03 # Added # Velodyne VLP-16 and VLP-32C support through custom LiDAR sensor plugin Support for LiDAR sensor plugin with direct UDP socket interface Support for point cloud environment rendering Extensible vehicle dynamics Functional Mockup Interface 2.0 (FMI 2.0) interface for vehicle dynamics Support for fisheye camera lens distortion Support for sensor_msgs/NavSatFix message type for GPS messages in ROS2 Vehicle Odometry sensor for reporting steering angles and velocity in ROS2 Changed # Upgraded to Unity 2019.3.3f1 Fixed black artifacts in Linux bug Fixed display issues in Unity Editor bug Fixed camera sensor invalid data bug 3D Ground Truth sensor now generates different ID for vehicle after each respawn [2020.01] - 2020-01-31 # Added # Controllable Object plugin support, dynamically add and control Controllables with the Python API Ability to create LiDAR sensor with non-uniform beam distribution Ability to add distortion effects to Camera sensors Support for Autoware.Auto messages (ROS2) Ability to create custom message types for sensor plugins TransformSensor which can act as a parent for other sensors VehicleStateSensor which is needed by Autoware.Auto Changed # Added SanFrancisco map as a default map option Allow null texture bundles without an error Pedestrian mass reduced to 70kg Fixed lagging of 2D Ground Truth boxes (in visualization and data) ComfortSensor included with Simulator binary TrafficCone included with Simulator binary Fixed exported HD map lane relations Fixed ROS2 GPS [2019.12] - 2020-01-21 # Added # Custom callback for Python API, useful in sensor plugins. Ability to specify count and types of NPCs to spawn on the map. Changed # Optimized texture usage in maps - significantly reduces size on disk and GPU memory used. Allow boundary type to be optional for Apollo HD map import. Fix Python API to be able load maps & vehicles that are shared between multiple users. Separated bundle file format version for map and vehicle bundles. Improved headlights for EGO vehicles. Improved HD map annotation icons. Fixed Sedan NPC mesh. [2019.11] - 2019-11-19 # Added # OpenDrive 1.4 HD map import. Rain drops on ground when it is raining. Separated Apollo HD map export between version 3.0 and 5.0 to support 5.0 specific features. Cache vehicle prefabs when loading same vehicle multiple times into simulation. Ability to login to account via command-line. Ability for vehicle to have multiple interior lights. Fixes #474. Allow Color, Depth and Semantic cameras to have higher capture & publish rate. Building & loading custom sensors as plugins. Changed # Fixed traffic lights signal colors on Shalun map. Fixed exceptions when NPCs are despawned while still in intersection. Fixed errors when adding pedestrian to map without NavMesh. Fixed Lanelet2 boundary import and export. Multiple fixes for OpenDrive import and export. Fixed wrong raycast layers in 2D Ground Truth sensor to detect if NPC is visible. Fixed missing timestamp when publishing ROS/Cyber messages from 2D Ground Truth sensor. Limit number of spawned NPCs and vehicles for large maps. [2019.10] - 2019-10-28 # Added # Apollo HD map import Accurate sun position in sky based of map location. Including time of sunrise and sunset. Control calibration sensor to help calibrating AD stack control. Ported Shalun map from previous non-HDRP simulator version. Ground Truth sensor for traffic light. Python API method to get controllable object by position. Python API method to convert multiple map coordinates in single call. Python API method to perform multiple ray-casts in single call. Sensor for controlling vehicle with steering wheel (Logitech G920). Platform independent asset bundles (Windows and Linux). Allow to set custom data path for database and downloaded bundles. Visualize data values for non-image senors (GPS, IMU, etc). Populate scene with required objects when new scene is created in Unity Editor. Changed # Fixed exceptions in ROS Bridge where if it receives message on topic that it has not subscribed. Fixed 3D Ground Truth sensor to report correct NPC orientation angles. Fixed Radar sensor to visualize pedestrians. Fixed Color camera to render mountains in BorregaAve. Fixed EGO vehicle collision callback to Python API. Fixed WebUI redirect loop that happens if you are logged out. Fixed reported NPC vehicle speed. Fixes #347 and #317. Fixed gear shifting for EGO vehicle control. Fixes #389. Fixed NPC waypoint following where NPCs stopped if assigned speed is too low. Fixed semantic segmentation for vehicles and pedestrians added with Python API. Fixed ROS2 message publishing (seq field was missing in Header). Fixes #413. Fixed issue with database on some non-English locales. Fixes #381. Fixed point cloud generation in Unity Editor. Fixed browser loosing cookie when session ends in WebUI. Fixed slowness in Python API when running without access to Internet. Fixed issue when multiple users could not use same map url. Improved error messages when simulation fails to start. [2019.09] - 2019-09-06 # Added # Sensor visualization UI HD map export to OpenDrive 1.4 format ROS service support for ROS bridge Python API to support more robust waypoints for NPC vehicles Python API with ability to control traffic lights on map Hyundai Nexo vehicle Changed # Improved NPC movement and right turns on red traffic light Fixed NPC vehicle despawning logic so they don't get stuck in intersections Change NPC vehicles colliders from box to mesh to improves collision precision Updated generated protobuf message classes for latest Apollo 5.0 Fixed 3D Ground Truth message type for ROS Fixed 3D and 2D Ground Truth bounding box locations [2019.07] - 2019-08-09 # Added # Separate Asset Bundles for environments and vehicles Fully deterministic physics simulation Faster-than-real-time capability with Python API Lanelet2 HD map format import/export Ability to edit sensor configuration dynamically Multi-user support - account login allows different users to login to one machine running simulator BorregasAve 3D environment as a default provided map AutonomouStuff 3D environment as a default provided map of parking lot SingleLaneRoad 3D environment as a default provided map CubeTown 3D environment as a default provided map Lexus RX and Lincoln MKZ vehicle support LiDAR outputs intensity value based on reflectivity of material (instead of color) Support for Apollo 5.0 Support for Autoware 1.12 Light reflections on road from wetness Better sky rendering, including clouds Ability to import point cloud files for visualization Changed # User interface - use web UI for main user interaction Render pipeline - Unity High Definition Render Pipeline Significantly improved LiDAR simulation performance using multithreading Improved map annotation for easier use in Editor Improved point cloud generation from 3D environment in Editor Removed # Support for Duckiebot, EP_rigged, SF_rigged, Tugbot vehicles Support for SimpleLoop, SimpleMap, SimpleRoom, Duckietown, DuckieDownTown, SanFrancisco, Shalun maps [2019.05 and older] # Please see release notes for previous versions on our Github releases page.","title":"Release notes"},{"location":"release-notes/changelog/#20212-2021-06-30","text":"","title":"[2021.2] - 2021-06-30"},{"location":"release-notes/changelog/#added","text":"External Pedestrians and NPC (other traffic vehicles) with animation support New NPCs and Pedestrians like Scooters, Bicyclists and Animals External Bridge Support Multiple simulator asset bundle versions support Rain Collision effect Support for simulations without EGO ( Spectator Ego ) New \u201cDriver view\u201d camera state New API to set simulator camera state Annotation tool to set accurate values (position, rotation and scale) for MapOrigin New \u201cclear cache\u201d option Ability to use proxy server for cloud_url in config.yml New enhanced web user interface: Improved visual sensor editor Ability to clone Sensors in sensor configuration User warning messages for privilege based restriction Markdown support for Asset description","title":"Added"},{"location":"release-notes/changelog/#changed","text":"Unity Upgrade to 2020.3.3f1 Revert FBX package to 3.2.1-preview.2 Remove overdraw with HDRP flare Improved NPC handling: Fixed rotating NPC wheels when using NPCWaypointBehaviour Pedestrian ground truth from bounds Improvements and bug fixes around Distributed Simulation with multi-ego support: Fixed API mode for distributed simulation on a single machine Fixed pedestrian animations on client machine Improved distributing VehicleActions Support for the ArticulationBody in distributed simulations Improved load balancing of the sensors Updated Sensor support: Fix TF visual and sensor hierarchy update input for g920 wheel for windows Publish camera intrinsic parameters to ROS and ROS2 Rework lidar to use cubemap Improved lighting and rain: Fix brown lighting Rotate sun and moon to match map rotation Rain collision improvements and fixes Fix overlapping rain chunks Enhanced Visual Scenario Editor support: Minor UX fixes Fixed changing agents' variants Fix issue with agent dropdown not refreshing on click Loading pedestrians from config Improvements around HD Map Mesh Generator : Improve data handling in HD map mesh builder Erosion and re-leveling for point cloud mesh Work in progress improvements on Developer mode: Rework developer settings Bundle version uptick and unity editor check on developer debug mode for bridges Sensor debug developer loading More enhanced and user friendly web user interface: Password Sensitivity improvements GLTF editor bug fixes and improvements HD map preview improvements Simulation invalid items management WCAG accessibility UI compatibility Live Test Results view in Simulation Profile Updated algorithm for Trending assets view Simulation form auto-save Misc Improvements: Support default light layers in point cloud renderer Loads assembly with the ego name from asset bundle Added public driver view transform to be set in the Inspector Asmdef added, loader fixed to deal with bundle/WISE name mismatch Loader: improve the error message about BundleFormat Make sure Simulator.Editor.Build.Run exits on any exception Trimming last slash in the cloud url if needed. get full path from data path provided","title":"Changed"},{"location":"release-notes/changelog/#202111-2021-05-20","text":"","title":"[2021.1.1] - 2021-05-20"},{"location":"release-notes/changelog/#added_1","text":"3D Visualizer to help create sensor configuration on a vehicle HD map visualizer on map profile pages Ability to share test reports with other users \"Supported Simulator versions\" value on asset profile page Improved sharing feature for private assets and simulations Save and Exit button on sensor configuration page Tutorials to help new users on how to interact with the web user interface pages Email confirmation upon sensor plugin sharing","title":"Added"},{"location":"release-notes/changelog/#changed_1","text":"Improved quick filters on Store and Library pages Redirect to existing asset profile page when uploading an already uploaded asset Request access button on private plugin pages Require adding a vehicle sensor configuration when creating a simulation Fixed issues with multi-vehicle simulation using same vehicle and sensor configuration Improvements to cluster page user experience Show proper names for missing sensors on invalid simulations Proper error messages for wrong information on registration page Proper error messages when maximum allowed cluster limit is reached","title":"Changed"},{"location":"release-notes/changelog/#20211-2021-03-23","text":"","title":"[2021.1] - 2021-03-23"},{"location":"release-notes/changelog/#added_2","text":"New cloud-based web user interface including: Asset Store for downloading assets, uploading and sharing assets publicly user account password management user assets management library support for private assets private assets/simulations sharing with registered users cluster/Simulation status handling SVL Simulator content from this release will be available in the Asset Store New and updated runtime templates for simulations with automatic bringup and execution: Random Traffic Visual Scenario Editor Python API API Only Visual Scenario Editor for creating scenarios including support for: traffic vehicle and pedestrian waypoints previews copy/paste/undo feature ability to specify ego vehicle target destination support for controllables with ability to edit default policy color selection for actors waypoint positions snapped to HD map lanes support for distance, time, and time-to-collision triggers traffic lights control Automatically generated test case reports of simulations including: success/failure scenario evaluation simulation statistics analytics data per sensor video recording of simulation multiple violation event detection: collision, stopline, stopsign, ego-stuck, sudden jerk/brake/turn, speed-limit, etc. PythonAPI: many improvements including: added cloudiness and road damage to weather parameters ability to specify date in environment configuration added Dreamview API as sub-module added callback agents_traversed_waypoints added ability to set speed at WalkWaypoints added waypoint trigger system ability to set simulator camera free-state position and rotation sample Python API scripts based on NHTSA-defined scenarios for end-to-end testing additional Python API test samples: random-traffic mode, cut-in, sudden-braking, etc. Environment configuration improvements: moon position in nighttime environment configurations Improved environment road wetness layer UI for FPS and SIM time Improved rain particles Increased types of traffic lights, model improvements Improvements in Developer Mode : developer settings to use ego vehicle within Developer Mode ability to select and add actors to simulation in Developer Settings added sensor debug mode added tool to rotate Editor scene view to -X north Features for traffic agents in simulation: support for bicyclist traffic agents support for custom traffic vehicle behavior plugins pedestrian crosswalk support with yield for traffic vehicles (SanFrancisco map) added direction for pedestrian crosswalk annotation tool spawning NPCs now raycasts target bounds from egos' positions New map/environment creation improvements: default ego destination points for LG-provided maps HD map road network environment generation tool test maps for NHTSA-like test cases: Straight1LanePedestrianCrosswalk, Straight1LaneSame, etc. General simulation improvements: ability to create and run a Distributed Simulation from web user interface offline mode for running saved simulation configurations improved cache control for downloaded assets Sensors improvements: new sensors : StopLine, HUDKeyboardControl, AutowareAIControl, LGSVLControl, etc. improved LiDAR simulation performance sun-flare, raindrops, greyscale, and video artifacts effects for sensors support for sensor post-distortion effects Default values for simple and complex sensor parameter types Enabled sensors to load texture assets Improvements around support for AD stacks: Optimized ROS2 interface and bridge Support for Apollo 6.0 and master","title":"Added"},{"location":"release-notes/changelog/#changed_2","text":"Product branding changed from \"LGSVL Simulator\" to \"SVL Simulator\" Old local web user interface is now deprecated. Users need to migrate to new web user interface Old content website will be deprecated All content now managed on cloud - downloaded as needed by simulation configuration Unity update to 2019.4.18f1 All sensors are now sensor plugins - default sensors are part of user plugin library Sensor plugins no longer bundled with simulator executable Distributed simulation: fixed pedestrian movement animation bug Distributed simulation: video artifact fix Fixed wrong timestamp error in clock and IMU sensors Fix acceleration/angular velocity in Apollo imu/corrected_imu Fixed way of handling failed downloads Fixed bugs and issues in OpenDrive HD map importer and exporter Point cloud rendering: added alternative method for removing obscured points (reduces flickering, requires depth prepass) Point cloud rendering: Fill scarce depth map holes through neighborhood sampling Point Cloud Rendering: Fix skybox blending in unlit point cloud Improved traffic lights in environments Rain/clouds/lights improvements NPC spawn logic improvements Fix old NPC models and prefab Adjust camera distance from ego bounds HD map import: Fix keyError in lanelet2 and improve grouping lanes at intersections HD map import: Fix map origin update during OpenDrive map import","title":"Changed"},{"location":"release-notes/changelog/#202006-2020-08-31","text":"","title":"[2020.06] - 2020-08-31"},{"location":"release-notes/changelog/#added_3","text":"Support modular testing of planning with Apollo 5.0 Bridge plugin system with example ( LoggingBridge ) Added default ultrasonic distance sensor Sensor TF visualizer in sensor visualizations Added button in map annotation tool to add synthetic lane boundary lines automatically Python API: added callback agents_traversed_waypoints Python API: added ability to set speed at WalkWaypoints Python API: Added waypoint trigger system","title":"Added"},{"location":"release-notes/changelog/#changed_3","text":"Fixed 90 degree orientation offset in GPS odometry sensor Replaced default ROS2 bridge to improved custom ROS2 bridge instead of ros2-web-bridge Unity update to 2019.3.15f1 Fixed timestamps in GPS, IMU Added timestamp to default LiDAR sensor PointCloud message Fixed bugs in OpenDrive map importer to correctly compute reference lines and lane widths for edge cases Improved OpenDrive map exporter to export parametric cubic curves instead of simple lines for each reference line segment","title":"Changed"},{"location":"release-notes/changelog/#202005-2020-06-02","text":"","title":"[2020.05] - 2020-06-02"},{"location":"release-notes/changelog/#added_4","text":"Support for custom traffic vehicle behavior plugins Support for Apollo 5.5 ( latest master ) Support for distributed simulation Velodyne VLS-128 support Python API function to spawn random non-ego vehicles and pedestrians Unlit shadows support for point cloud environment rendering AD system-agnostic message definitions to \"lgsvl_msgs\" for vehicle control, CAN bus and status data, etc.","title":"Added"},{"location":"release-notes/changelog/#changed_4","text":"Refactored traffic agent Waypoint Mode with fixes for idle and trigger waypoints Changed gnss_odom message coordinate system to right-handed coordinate system Fixed \"ignore map origin\" option for gnss_odom message Fixed acceleration values in IMU sensor Fix sky flickering issue during point cloud rendering Fixed LiDAR readback error Removed and replaced documentation source files from main repository to separate Documentation repository","title":"Changed"},{"location":"release-notes/changelog/#202003-2020-04-03","text":"","title":"[2020.03] - 2020-04-03"},{"location":"release-notes/changelog/#added_5","text":"Velodyne VLP-16 and VLP-32C support through custom LiDAR sensor plugin Support for LiDAR sensor plugin with direct UDP socket interface Support for point cloud environment rendering Extensible vehicle dynamics Functional Mockup Interface 2.0 (FMI 2.0) interface for vehicle dynamics Support for fisheye camera lens distortion Support for sensor_msgs/NavSatFix message type for GPS messages in ROS2 Vehicle Odometry sensor for reporting steering angles and velocity in ROS2","title":"Added"},{"location":"release-notes/changelog/#changed_5","text":"Upgraded to Unity 2019.3.3f1 Fixed black artifacts in Linux bug Fixed display issues in Unity Editor bug Fixed camera sensor invalid data bug 3D Ground Truth sensor now generates different ID for vehicle after each respawn","title":"Changed"},{"location":"release-notes/changelog/#202001-2020-01-31","text":"","title":"[2020.01] - 2020-01-31"},{"location":"release-notes/changelog/#added_6","text":"Controllable Object plugin support, dynamically add and control Controllables with the Python API Ability to create LiDAR sensor with non-uniform beam distribution Ability to add distortion effects to Camera sensors Support for Autoware.Auto messages (ROS2) Ability to create custom message types for sensor plugins TransformSensor which can act as a parent for other sensors VehicleStateSensor which is needed by Autoware.Auto","title":"Added"},{"location":"release-notes/changelog/#changed_6","text":"Added SanFrancisco map as a default map option Allow null texture bundles without an error Pedestrian mass reduced to 70kg Fixed lagging of 2D Ground Truth boxes (in visualization and data) ComfortSensor included with Simulator binary TrafficCone included with Simulator binary Fixed exported HD map lane relations Fixed ROS2 GPS","title":"Changed"},{"location":"release-notes/changelog/#201912-2020-01-21","text":"","title":"[2019.12] - 2020-01-21"},{"location":"release-notes/changelog/#added_7","text":"Custom callback for Python API, useful in sensor plugins. Ability to specify count and types of NPCs to spawn on the map.","title":"Added"},{"location":"release-notes/changelog/#changed_7","text":"Optimized texture usage in maps - significantly reduces size on disk and GPU memory used. Allow boundary type to be optional for Apollo HD map import. Fix Python API to be able load maps & vehicles that are shared between multiple users. Separated bundle file format version for map and vehicle bundles. Improved headlights for EGO vehicles. Improved HD map annotation icons. Fixed Sedan NPC mesh.","title":"Changed"},{"location":"release-notes/changelog/#201911-2019-11-19","text":"","title":"[2019.11] - 2019-11-19"},{"location":"release-notes/changelog/#added_8","text":"OpenDrive 1.4 HD map import. Rain drops on ground when it is raining. Separated Apollo HD map export between version 3.0 and 5.0 to support 5.0 specific features. Cache vehicle prefabs when loading same vehicle multiple times into simulation. Ability to login to account via command-line. Ability for vehicle to have multiple interior lights. Fixes #474. Allow Color, Depth and Semantic cameras to have higher capture & publish rate. Building & loading custom sensors as plugins.","title":"Added"},{"location":"release-notes/changelog/#changed_8","text":"Fixed traffic lights signal colors on Shalun map. Fixed exceptions when NPCs are despawned while still in intersection. Fixed errors when adding pedestrian to map without NavMesh. Fixed Lanelet2 boundary import and export. Multiple fixes for OpenDrive import and export. Fixed wrong raycast layers in 2D Ground Truth sensor to detect if NPC is visible. Fixed missing timestamp when publishing ROS/Cyber messages from 2D Ground Truth sensor. Limit number of spawned NPCs and vehicles for large maps.","title":"Changed"},{"location":"release-notes/changelog/#201910-2019-10-28","text":"","title":"[2019.10] - 2019-10-28"},{"location":"release-notes/changelog/#added_9","text":"Apollo HD map import Accurate sun position in sky based of map location. Including time of sunrise and sunset. Control calibration sensor to help calibrating AD stack control. Ported Shalun map from previous non-HDRP simulator version. Ground Truth sensor for traffic light. Python API method to get controllable object by position. Python API method to convert multiple map coordinates in single call. Python API method to perform multiple ray-casts in single call. Sensor for controlling vehicle with steering wheel (Logitech G920). Platform independent asset bundles (Windows and Linux). Allow to set custom data path for database and downloaded bundles. Visualize data values for non-image senors (GPS, IMU, etc). Populate scene with required objects when new scene is created in Unity Editor.","title":"Added"},{"location":"release-notes/changelog/#changed_9","text":"Fixed exceptions in ROS Bridge where if it receives message on topic that it has not subscribed. Fixed 3D Ground Truth sensor to report correct NPC orientation angles. Fixed Radar sensor to visualize pedestrians. Fixed Color camera to render mountains in BorregaAve. Fixed EGO vehicle collision callback to Python API. Fixed WebUI redirect loop that happens if you are logged out. Fixed reported NPC vehicle speed. Fixes #347 and #317. Fixed gear shifting for EGO vehicle control. Fixes #389. Fixed NPC waypoint following where NPCs stopped if assigned speed is too low. Fixed semantic segmentation for vehicles and pedestrians added with Python API. Fixed ROS2 message publishing (seq field was missing in Header). Fixes #413. Fixed issue with database on some non-English locales. Fixes #381. Fixed point cloud generation in Unity Editor. Fixed browser loosing cookie when session ends in WebUI. Fixed slowness in Python API when running without access to Internet. Fixed issue when multiple users could not use same map url. Improved error messages when simulation fails to start.","title":"Changed"},{"location":"release-notes/changelog/#201909-2019-09-06","text":"","title":"[2019.09] - 2019-09-06"},{"location":"release-notes/changelog/#added_10","text":"Sensor visualization UI HD map export to OpenDrive 1.4 format ROS service support for ROS bridge Python API to support more robust waypoints for NPC vehicles Python API with ability to control traffic lights on map Hyundai Nexo vehicle","title":"Added"},{"location":"release-notes/changelog/#changed_10","text":"Improved NPC movement and right turns on red traffic light Fixed NPC vehicle despawning logic so they don't get stuck in intersections Change NPC vehicles colliders from box to mesh to improves collision precision Updated generated protobuf message classes for latest Apollo 5.0 Fixed 3D Ground Truth message type for ROS Fixed 3D and 2D Ground Truth bounding box locations","title":"Changed"},{"location":"release-notes/changelog/#201907-2019-08-09","text":"","title":"[2019.07] - 2019-08-09"},{"location":"release-notes/changelog/#added_11","text":"Separate Asset Bundles for environments and vehicles Fully deterministic physics simulation Faster-than-real-time capability with Python API Lanelet2 HD map format import/export Ability to edit sensor configuration dynamically Multi-user support - account login allows different users to login to one machine running simulator BorregasAve 3D environment as a default provided map AutonomouStuff 3D environment as a default provided map of parking lot SingleLaneRoad 3D environment as a default provided map CubeTown 3D environment as a default provided map Lexus RX and Lincoln MKZ vehicle support LiDAR outputs intensity value based on reflectivity of material (instead of color) Support for Apollo 5.0 Support for Autoware 1.12 Light reflections on road from wetness Better sky rendering, including clouds Ability to import point cloud files for visualization","title":"Added"},{"location":"release-notes/changelog/#changed_11","text":"User interface - use web UI for main user interaction Render pipeline - Unity High Definition Render Pipeline Significantly improved LiDAR simulation performance using multithreading Improved map annotation for easier use in Editor Improved point cloud generation from 3D environment in Editor","title":"Changed"},{"location":"release-notes/changelog/#removed","text":"Support for Duckiebot, EP_rigged, SF_rigged, Tugbot vehicles Support for SimpleLoop, SimpleMap, SimpleRoom, Duckietown, DuckieDownTown, SanFrancisco, Shalun maps","title":"Removed"},{"location":"release-notes/changelog/#201905-and-older","text":"Please see release notes for previous versions on our Github releases page.","title":"[2019.05 and older]"},{"location":"release-notes/limitations-license-notice/","text":"Limitations and License Notice Restrictions and limitations # This documentation website contains an overview of many features and aspects of SVL Simulator, including planned releases or architectures that may not be provided at the time of delivery of this documentation. Such architectures, descriptions, and other features and aspects may be subject to change in the future, in LG Electronics USA\u2019s sole discretion. License agreement conditions # All content in this documentation, as well as products and services mentioned or provided along with this document, unless explicitly stated otherwise by Zenith Electronics, LLC, a subsidiary of LG Electronics USA, Inc \"LG\"), in writing, are subject to the terms and conditions in the Terms of Use . Not part of this release # SVL Simulator includes the simulation platform, tools, and relevant content and services pertaining to simulation. While it may include possible reference implementations and modifications to open source autonomous driving systems, SVL Simulator does not officially provide autonomous vehicle systems that have been tested in the real world. LG Electronics USA disclaims any liability for issues that may arise related to open source autonomous driving systems in connection with user's evaluation and usage of SVL Simulator software and/or its further development.","title":"Limitations and license notice"},{"location":"release-notes/limitations-license-notice/#restrictions-and-limitations","text":"This documentation website contains an overview of many features and aspects of SVL Simulator, including planned releases or architectures that may not be provided at the time of delivery of this documentation. Such architectures, descriptions, and other features and aspects may be subject to change in the future, in LG Electronics USA\u2019s sole discretion.","title":"Restrictions and limitations"},{"location":"release-notes/limitations-license-notice/#license-agreement-conditions","text":"All content in this documentation, as well as products and services mentioned or provided along with this document, unless explicitly stated otherwise by Zenith Electronics, LLC, a subsidiary of LG Electronics USA, Inc \"LG\"), in writing, are subject to the terms and conditions in the Terms of Use .","title":"License agreement conditions"},{"location":"release-notes/limitations-license-notice/#not-part-of-this-release","text":"SVL Simulator includes the simulation platform, tools, and relevant content and services pertaining to simulation. While it may include possible reference implementations and modifications to open source autonomous driving systems, SVL Simulator does not officially provide autonomous vehicle systems that have been tested in the real world. LG Electronics USA disclaims any liability for issues that may arise related to open source autonomous driving systems in connection with user's evaluation and usage of SVL Simulator software and/or its further development.","title":"Not part of this release"},{"location":"release-notes/release-contents/","text":"Release contents SVL Simulator is delivered as several zip archives in the following directory layout: \u2514\u2500\u2500 2021.2 \u251c\u2500\u2500 svlsimulator-linux64-2021.2.zip \u251c\u2500\u2500 svlsimulator-windows64-2021.2.zip \u251c\u2500\u2500 svlsimulator-source-2021.2.zip \u251c\u2500\u2500 svlsimulator-source-2021.2.tar.gz svlsimulator-linux64-2021.2.zip : A zip archive of the Linux binary executable of SVL Simulator. This includes the scenario runner environment, which can be used to execute end-to-end Visual Scenario Editor and Python API runtime template-based local simulations through the web user interface. It also contains all the examples from the PythonApi repository. svlsimulator-windows64-2021.2.zip : A zip archive of the Windows binary executable of SVL Simulator. svlsimulator-source-2021.2.zip : A zip archive of the source code of SVL Simulator. This is needed to run the simulator in Developer Mode. svlsimulator-source-2021.2.tar.gz : A tarball archive of the source code of SVL Simulator. This is needed to run the simulator in Developer Mode.","title":"Contents of this release"},{"location":"release-notes/release-features/","text":"2021.2 Release Features Support for external Pedestrians and NPCs top # Pedestrians and NPCs are separated from simulator and can be loaded from asset bundles. User still has to build NPCs and Pedestrians as external assets with Simulator, as these are still not fully supported from web user interface . Now, users can build their own NPCs and pedestrians with animations more easily with the simulator. We have also added some new NPCs and Pedestrians like Scooters, Bicyclists and Animals. These are available through our binary builds ( win , linux ) only. Support for External Bridges top # Bridges from simulator to Autonomous-Driving or Robotics stack are also separated from simulator and can be loaded as asset bundles from the web user interface . Users can now build their own bridge to SVL Simulator and upload it as an asset bundle to user's library on web user interface to include in the simulation. Following bridges are supported as default: ROS , ROS2 , CyberRT and here is an example of a custom bridge implementation. Support for multiple versions of asset bundles on web user interface top # SVL Simulator's cloud based web user interface can now manage and serve content for different versions of SVL Simulator. User can now edit asset bundles for different simulator versions to already added assets on the web user interface instead of creating duplicate assets per version. The web user interface can confirm validity of asset versions against the simulator version under test before it is run, to avoid simulation failures due to mismatched asset versions. New improved distributed simulation with multi-ego support top # We have made several bug fixes, stability and performance improvements to support running multiple ego vehicles in a distributed simulation environment. Multiple improvements in the Interactive 3D sensor configuration editor top # With the 2021.2 release, we have significantly improved the user experience on our new 3D sensor configuration editor that helps you create or modify sensor configurations for your vehicles. Some improvements include: better zoom and centering of the vehicle, opacity and grid toggles, axes color updates, mouse based controls, and some critical bug fixes on parent-child sensor transform. Support for simulations without EGO vehicle (Spectator EGO) top # Spectator EGO is an empty Ego vehicle without colliders, geometry, and vehicle dynamics. Use this vehicle when there is no need for Ego to be present during simulation. You can still attach sensors or observe simulation. Significant performance improvements on Lidar Sensor top # Full rework of Lidar sensor - it will now use cubemap texture instead of a pool of tall, 2D textures. This change fixes artifacts due to tall textures and solid point cloud rendering. This change improves lidar scaling - high beam counts, scans per rotation and frequency, should yield significantly more FPS than previous versions. Note : Performance improvement is not noticeable if CyberRT is used, due to massive conversion performance cost. Bug fixes and improvements top # For a full list of bug fixes, improvements, and all other changes since the last release, please see the release notes .","title":"Release features"},{"location":"release-notes/release-features/#support-for-external-peds-npcs","text":"Pedestrians and NPCs are separated from simulator and can be loaded from asset bundles. User still has to build NPCs and Pedestrians as external assets with Simulator, as these are still not fully supported from web user interface . Now, users can build their own NPCs and pedestrians with animations more easily with the simulator. We have also added some new NPCs and Pedestrians like Scooters, Bicyclists and Animals. These are available through our binary builds ( win , linux ) only.","title":"Support for external pedestrians and NPCs"},{"location":"release-notes/release-features/#support-for-external-bridges","text":"Bridges from simulator to Autonomous-Driving or Robotics stack are also separated from simulator and can be loaded as asset bundles from the web user interface . Users can now build their own bridge to SVL Simulator and upload it as an asset bundle to user's library on web user interface to include in the simulation. Following bridges are supported as default: ROS , ROS2 , CyberRT and here is an example of a custom bridge implementation.","title":"Support for external Bridges"},{"location":"release-notes/release-features/#support-for-multiple-versions","text":"SVL Simulator's cloud based web user interface can now manage and serve content for different versions of SVL Simulator. User can now edit asset bundles for different simulator versions to already added assets on the web user interface instead of creating duplicate assets per version. The web user interface can confirm validity of asset versions against the simulator version under test before it is run, to avoid simulation failures due to mismatched asset versions.","title":"Support for multiple versions of asset bundles on web user interface"},{"location":"release-notes/release-features/#support-for-multi-ego","text":"We have made several bug fixes, stability and performance improvements to support running multiple ego vehicles in a distributed simulation environment.","title":"New improved distributed simulation with multi-ego support"},{"location":"release-notes/release-features/#improvements-in-sensor-editor","text":"With the 2021.2 release, we have significantly improved the user experience on our new 3D sensor configuration editor that helps you create or modify sensor configurations for your vehicles. Some improvements include: better zoom and centering of the vehicle, opacity and grid toggles, axes color updates, mouse based controls, and some critical bug fixes on parent-child sensor transform.","title":"Multiple improvements in the Interactive 3D sensor configuration editor"},{"location":"release-notes/release-features/#spectator-ego","text":"Spectator EGO is an empty Ego vehicle without colliders, geometry, and vehicle dynamics. Use this vehicle when there is no need for Ego to be present during simulation. You can still attach sensors or observe simulation.","title":"Support for simulations without EGO vehicle (Spectator EGO)"},{"location":"release-notes/release-features/#new-cubemap-lidar-improvements","text":"Full rework of Lidar sensor - it will now use cubemap texture instead of a pool of tall, 2D textures. This change fixes artifacts due to tall textures and solid point cloud rendering. This change improves lidar scaling - high beam counts, scans per rotation and frequency, should yield significantly more FPS than previous versions. Note : Performance improvement is not noticeable if CyberRT is used, due to massive conversion performance cost.","title":"Significant performance improvements on Lidar Sensor"},{"location":"release-notes/release-features/#bug-fixes-and-improvements","text":"For a full list of bug fixes, improvements, and all other changes since the last release, please see the release notes .","title":"Bug Fixes and Improvements"},{"location":"running-simulations/api-how-to-run-scenario/","text":"How To Run a Scenario or Test Case The following steps detail how to run the Vehicle Following scenario. This scenario and other example scenarios can be found on our examples page . These scenarios use the SVL Simulator Python API. The SVL Simulator Python API is available on GitHub: https://github.com/lgsvl/PythonAPI After cloning or downloading it from the git repository follow these steps: Run the following command to install Python files and necessary dependencies: pip3 install --user -e . Start the simulator. Set environment variables SIMULATOR_HOST and BRIDGE_HOST SIMULATOR_HOST is where the simulator will be run. The default value for this is \"localhost\" and does not need to be set if the simulator is running on the same machine that the Python script will be run from. BRIDGE_HOST is where the AD stack will be run. This is relative to where the simulator is run. The default value is \"localhost\" which is the same machine as the simulator. For example, if computer A will run the simulator and computer B will run the AD stack SIMULATOR_HOST should be set to the IP of computer A BRIDGE_HOST should be set to the IP of computer B To set the variables for the current terminal window use export SIMULATOR_HOST=192.168.1.100 export BRIDGE_HOST=192.168.1.101 Start your AD stack. The example scripts are written for Apollo 5.0. See below for how to edit the scripts to work with other AD stacks. Select the MKZ as the vehicle SingleLaneRoad for the map Start all modules and the bridge (if relevant) Run the script ./VF_S_25.py Set the destination for the AD stack. For this scenario, the destination is the end of the current lane. The AV should start driving forward towards the NPC. It should avoid crashing into the NPC. How to Edit the EGO vehicle top # In each of the example scenarios and test cases, there is a section that setups up the EGO vehicle: If using a different AD stack, the vehicle type needs to be changed. Change the agent name (orange string) to what is desired. e.g. For Autoware it might be \"Lexus2016RXHybrid (Autoware)\"","title":"How To Run a Scenario or Test Case"},{"location":"running-simulations/api-how-to-run-scenario/#how-to-edit-the-ego-vehicle","text":"In each of the example scenarios and test cases, there is a section that setups up the EGO vehicle: If using a different AD stack, the vehicle type needs to be changed. Change the agent name (orange string) to what is desired. e.g. For Autoware it might be \"Lexus2016RXHybrid (Autoware)\"","title":"How to Edit the EGO vehicle"},{"location":"running-simulations/developer-mode/","text":"Developer Mode Some functionality of SVL Simulator is only available in Developer Mode, by opening the SVL Simulator source code as a project in Unity Editor. The available functionality includes: HD map annotation tool ( tutorial ) Import/export of supported HD map formats HD map mesh generation ( tutorial ) Developing and building environments ( tutorial ) Developing and building vehicles ( tutorial ) Developing and building custom plugins ( sensor plugin , controllable plugin ) Building custom simulator binary executables ( tutorial ) Ability to customize vehicle dynamics with FMI All other functionalities of SVL Simulator are available by running the binary executable and do not require Developer Mode. Setup top # There are a few steps needed to set up the development environment for Developer Mode, which is officially supported only on Windows. While much of the SVL Simulator Developer Mode functionality may work on Linux, we do not guarantee Developer Mode to be fully supported on Linux. Download and install Unity Hub ( https://unity3d.com/get-unity/download ) Ubuntu: You may need to allow the downloaded AppImage to be run as an executable Right-click the AppImage Select Properties Go to the Permissions tab Check Allow executing file as program Alternatively, in the terminal run sudo chmod +x UnityHub.AppImage Download and install Unity 2020.3.3f1 with Windows and Linux build support modules ( https://unity3d.com/get-unity/download/archive ) IMPORTANT include support for both Windows and Linux when installing Unity (Optional) include support for Visual Studio for easier debugging Unity Download Archive Click the Unity Hub button to have Unity Hub handle the installation process Download and install Node.js Version 12.13.0 LTS version is fine Make sure you have git-lfs installed before cloning this repository . Instructions for installation are here Verify installation with: $ git lfs install Git LFS initialized. Clone simulator from GitHub: $ git clone --single-branch https://github.com/lgsvl/simulator.git Be sure to clone latest release branch, not master. Run Unity Hub In the Projects tab, click Add and select the folder that the Simulator was cloned to In the Installs tab, click Locate and choose the Unity launcher in the Unity2020.3.3f1 folder In the Projects tab, verify that the Simulator is using Unity Version 2020.3.3f1 from the dropdown Double-click the name of the project to launch Unity Editor If this is an updated Unity version, let Unity recompile shaders and then close Unity and open the project again.","title":"Developer mode"},{"location":"running-simulations/developer-mode/#setup","text":"There are a few steps needed to set up the development environment for Developer Mode, which is officially supported only on Windows. While much of the SVL Simulator Developer Mode functionality may work on Linux, we do not guarantee Developer Mode to be fully supported on Linux. Download and install Unity Hub ( https://unity3d.com/get-unity/download ) Ubuntu: You may need to allow the downloaded AppImage to be run as an executable Right-click the AppImage Select Properties Go to the Permissions tab Check Allow executing file as program Alternatively, in the terminal run sudo chmod +x UnityHub.AppImage Download and install Unity 2020.3.3f1 with Windows and Linux build support modules ( https://unity3d.com/get-unity/download/archive ) IMPORTANT include support for both Windows and Linux when installing Unity (Optional) include support for Visual Studio for easier debugging Unity Download Archive Click the Unity Hub button to have Unity Hub handle the installation process Download and install Node.js Version 12.13.0 LTS version is fine Make sure you have git-lfs installed before cloning this repository . Instructions for installation are here Verify installation with: $ git lfs install Git LFS initialized. Clone simulator from GitHub: $ git clone --single-branch https://github.com/lgsvl/simulator.git Be sure to clone latest release branch, not master. Run Unity Hub In the Projects tab, click Add and select the folder that the Simulator was cloned to In the Installs tab, click Locate and choose the Unity launcher in the Unity2020.3.3f1 folder In the Projects tab, verify that the Simulator is using Unity Version 2020.3.3f1 from the dropdown Double-click the name of the project to launch Unity Editor If this is an updated Unity version, let Unity recompile shaders and then close Unity and open the project again.","title":"Setup"},{"location":"running-simulations/offline-mode/","text":"Offline Mode Offline Mode allows you to run simulations without needing an active Internet connection. Simulations that have been run at least once on the cluster are cached locally, allowing you to run them again directly from the main SVL Simulator executable without needing to open your browser and be online. On a local cluster running SVL Simulator that is linked to your account, you can see the \"Online\" status at the top right of the main menu. Clicking the \"Online\" status button reveals an option to \"Go Offline\". Once offline, your local cluster is still linked to your account, but can now run cached simulations without being online. You will be able to view a list of simulations that have previously been run on the cluster. Click the Play button after selecting the simulation to run. In order to create a new simulation and run it, or change any settings, you will need to switch back online and modify from your account in the web user interface.","title":"Offline mode"},{"location":"running-simulations/python-runtime/","text":"Simulator Python API Runtime Table of Contents Using the Python API runtime Reading Logs Example Python Script Environment Variables Using the Python API runtime top # The simulator provides a Python API runtime to allow running Python scripts to control the simulation directly in the WebUI. To use the Python API runtime: Go to the Simulations tab Click Add New Fill out the Simulation Name , Select Cluster , and optionally a description and click Next In the Runtime Template dropdown menu, select Python API Select the Map and Vehicle and Sensor Configuration you will be using in from the drop-down menus. This will set the LGSVL__MAP and LGSVL__VEHICLE_0 environment variables to the UUID of the selected map and vehicle. These environment variables can be used in the Python script to load the correct map and vehicle sensor configuration. In the text box titled Python script , enter your Python code directly and click Next Any maps or vehicles used in the Python script must be added to the users library An example Python script is available below , which requires the BorregasAve map and Lincoln2017MKZ vehicle to be added to the user library There will be no need to specify an Autopilot; click Next again Publish the new simulation to use it Once published, the new simulation will appear in your Simulations page and can be run by clicking on the Run Simulation button. Note: The Python script should not prompt for user input, otherwise it will result in a timeout as there is no way to get user input after the simulation has started. For example, all of the Python API examples in the quickstart folder in the Python API repository ask for user input before starting the simulation. To use one of the scripts from quickstart all of the input(...) calls should be removed. Reading Logs top # In cases where the Python script throws an error the simulator will not be able to run the script properly (or at all) and errors will be written to Playor.log located in the simulator's persistent data path. By default this would be ~/.config/unity3d/LGElectronics/SVLSimulator/Player.log . Note: Player.log contains all logs from the simulator (not just Python API runtime logs). Example Python Script top # Below is an example script to use with the Python API runtime. This script requires the BorregasAve map and the Lincoln2017MKZ vehicle to be added to the user's library. After running the script, the ego vehicle will be spawned at the center of an intersection and an NPC vehicle will pass it on its right, pausing for half a second at every waypoint. import os import lgsvl import copy sim = lgsvl.Simulator(os.environ.get(\"LGSVL__SIMULATOR_HOST\"), 8181) # BorregasAve default map must be added to your personal library sim.load(os.environ.get(\"LGSVL__MAP\")) spawns = sim.get_spawn() forward = lgsvl.utils.transform_to_forward(spawns[0]) right = lgsvl.utils.transform_to_right(spawns[0]) # EGO state = lgsvl.AgentState() state.transform = spawns[0] ego_state = copy.deepcopy(state) ego_state.transform.position += 50 * forward ego_state.transform.position -= 3 * right # The Lincoln2017MKZ default vehicle must be added to your vehicle library a = sim.add_agent(os.environ.get(\"LGSVL__VEHICLE_0\"), lgsvl.AgentType.EGO, ego_state) # NPC npc_state = copy.deepcopy(state) npc_state.transform.position += 10 * forward npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC, npc_state) vehicles = { a: \"EGO\", npc: \"Sedan\", } # This block creates the list of waypoints that the NPC will follow # Each waypoint is an position vector paired with the speed that the NPC will drive to it waypoints = [] x_max = 2 z_delta = 12 layer_mask = 0 layer_mask |= 1 << 0 # 0 is the layer for the road (default) for i in range(20): speed = 24# if i % 2 == 0 else 12 px = 0 pz = (i + 1) * z_delta # Waypoint angles are input as Euler angles (roll, pitch, yaw) angle = spawns[0].rotation # Raycast the points onto the ground because BorregasAve is not flat hit = sim.raycast(spawns[0].position + pz * forward, lgsvl.Vector(0,-1,0), layer_mask) # NPC will wait for 1 second at each waypoint wp = lgsvl.DriveWaypoint(hit.point, speed, angle, 1) waypoints.append(wp) # The NPC needs to be given the list of waypoints. # A bool can be passed as the 2nd argument that controls whether or not the NPC loops over the waypoints (default false) npc.follow(waypoints) sim.run(20) Environment Variables top # The simulator uses a set of predefined environment variables to configure the simulation. These variables are listed below. Environment Variable Description LGSVL__SIMULATOR_HOST SVL Simulator hostname or IP LGSVL__SIMULATOR_PORT SVL Simulator port LGSVL__MAP UUID of map to be loaded in Simulator LGSVL__VEHICLE_0 UUID of the first EGO vehicle/sensor configuration to be loaded in Simulator. Subsequent EGO vehicles can be accessed by incrementing the number, e.g. LGSVL_VEHICLE_1 . LGSVL__AUTOPILOT_0_HOST Autopilot bridge hostname or IP. If SVL Simulator is running on a different host than Autopilot, this must be set. LGSVL__AUTOPILOT_0_PORT Autopilot bridge port LGSVL__AUTOPILOT_0_VEHICLE_CONFIG Vehicle configuration to be loaded in Dreamview (Capitalization and spacing must match the dropdown in Dreamview) (e.g. Lincoln2017MKZ) LGSVL__AUTOPILOT_0_VEHICLE_MODULES Comma-separated list of modules to be enabled in Dreamview. Items must be enclosed by double-quotes and there must not be spaces between the double-quotes and commas. (Capitalization and space must match the sliders in Dreamview) LGSVL__AUTOPILOT_HD_MAP HD map to be loaded in Dreamview (Capitalization and spacing must match the dropdown in Dreamview) LGSVL__DATE_TIME Date and time to start simulation at. Time is the local time in the time zone of the map origin. Format 'YYYY-mm-ddTHH:MM:SS' LGSVL__ENVIRONMENT_CLOUDINESS Value of clouds weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_DAMAGE Value of road damage effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_FOG Value of fog weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_RAIN Value of rain weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_WETNESS Value of wetness weather effect, clamped to [0.0, 1.0] LGSVL__RANDOM_SEED Seed used to determine random factors (e.g. NPC type, color, behaviour) LGSVL__SIMULATION_DURATION_SECS The time length of the simulation [int] LGSVL__SPAWN_BICYCLES Whether or not to spawn bicycles [boolean] (not yet supported) LGSVL__SPAWN_PEDESTRIANS Whether or not to spawn pedestrians [boolean] LGSVL__SPAWN_TRAFFIC Whether or not to spawn NPC vehicles [boolean] LGSVL__TIME_STATIC Whether or not time should remain static [boolean] ( True = time is static, False = time moves forward)","title":"Simulator Python API Runtime[](#top)"},{"location":"running-simulations/python-runtime/#using-the-python-api-runtime","text":"The simulator provides a Python API runtime to allow running Python scripts to control the simulation directly in the WebUI. To use the Python API runtime: Go to the Simulations tab Click Add New Fill out the Simulation Name , Select Cluster , and optionally a description and click Next In the Runtime Template dropdown menu, select Python API Select the Map and Vehicle and Sensor Configuration you will be using in from the drop-down menus. This will set the LGSVL__MAP and LGSVL__VEHICLE_0 environment variables to the UUID of the selected map and vehicle. These environment variables can be used in the Python script to load the correct map and vehicle sensor configuration. In the text box titled Python script , enter your Python code directly and click Next Any maps or vehicles used in the Python script must be added to the users library An example Python script is available below , which requires the BorregasAve map and Lincoln2017MKZ vehicle to be added to the user library There will be no need to specify an Autopilot; click Next again Publish the new simulation to use it Once published, the new simulation will appear in your Simulations page and can be run by clicking on the Run Simulation button. Note: The Python script should not prompt for user input, otherwise it will result in a timeout as there is no way to get user input after the simulation has started. For example, all of the Python API examples in the quickstart folder in the Python API repository ask for user input before starting the simulation. To use one of the scripts from quickstart all of the input(...) calls should be removed.","title":"Using the Python API runtime"},{"location":"running-simulations/python-runtime/#reading-logs","text":"In cases where the Python script throws an error the simulator will not be able to run the script properly (or at all) and errors will be written to Playor.log located in the simulator's persistent data path. By default this would be ~/.config/unity3d/LGElectronics/SVLSimulator/Player.log . Note: Player.log contains all logs from the simulator (not just Python API runtime logs).","title":"Reading Logs"},{"location":"running-simulations/python-runtime/#example-python-script","text":"Below is an example script to use with the Python API runtime. This script requires the BorregasAve map and the Lincoln2017MKZ vehicle to be added to the user's library. After running the script, the ego vehicle will be spawned at the center of an intersection and an NPC vehicle will pass it on its right, pausing for half a second at every waypoint. import os import lgsvl import copy sim = lgsvl.Simulator(os.environ.get(\"LGSVL__SIMULATOR_HOST\"), 8181) # BorregasAve default map must be added to your personal library sim.load(os.environ.get(\"LGSVL__MAP\")) spawns = sim.get_spawn() forward = lgsvl.utils.transform_to_forward(spawns[0]) right = lgsvl.utils.transform_to_right(spawns[0]) # EGO state = lgsvl.AgentState() state.transform = spawns[0] ego_state = copy.deepcopy(state) ego_state.transform.position += 50 * forward ego_state.transform.position -= 3 * right # The Lincoln2017MKZ default vehicle must be added to your vehicle library a = sim.add_agent(os.environ.get(\"LGSVL__VEHICLE_0\"), lgsvl.AgentType.EGO, ego_state) # NPC npc_state = copy.deepcopy(state) npc_state.transform.position += 10 * forward npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC, npc_state) vehicles = { a: \"EGO\", npc: \"Sedan\", } # This block creates the list of waypoints that the NPC will follow # Each waypoint is an position vector paired with the speed that the NPC will drive to it waypoints = [] x_max = 2 z_delta = 12 layer_mask = 0 layer_mask |= 1 << 0 # 0 is the layer for the road (default) for i in range(20): speed = 24# if i % 2 == 0 else 12 px = 0 pz = (i + 1) * z_delta # Waypoint angles are input as Euler angles (roll, pitch, yaw) angle = spawns[0].rotation # Raycast the points onto the ground because BorregasAve is not flat hit = sim.raycast(spawns[0].position + pz * forward, lgsvl.Vector(0,-1,0), layer_mask) # NPC will wait for 1 second at each waypoint wp = lgsvl.DriveWaypoint(hit.point, speed, angle, 1) waypoints.append(wp) # The NPC needs to be given the list of waypoints. # A bool can be passed as the 2nd argument that controls whether or not the NPC loops over the waypoints (default false) npc.follow(waypoints) sim.run(20)","title":"Example Python Script"},{"location":"running-simulations/python-runtime/#environment-variables","text":"The simulator uses a set of predefined environment variables to configure the simulation. These variables are listed below. Environment Variable Description LGSVL__SIMULATOR_HOST SVL Simulator hostname or IP LGSVL__SIMULATOR_PORT SVL Simulator port LGSVL__MAP UUID of map to be loaded in Simulator LGSVL__VEHICLE_0 UUID of the first EGO vehicle/sensor configuration to be loaded in Simulator. Subsequent EGO vehicles can be accessed by incrementing the number, e.g. LGSVL_VEHICLE_1 . LGSVL__AUTOPILOT_0_HOST Autopilot bridge hostname or IP. If SVL Simulator is running on a different host than Autopilot, this must be set. LGSVL__AUTOPILOT_0_PORT Autopilot bridge port LGSVL__AUTOPILOT_0_VEHICLE_CONFIG Vehicle configuration to be loaded in Dreamview (Capitalization and spacing must match the dropdown in Dreamview) (e.g. Lincoln2017MKZ) LGSVL__AUTOPILOT_0_VEHICLE_MODULES Comma-separated list of modules to be enabled in Dreamview. Items must be enclosed by double-quotes and there must not be spaces between the double-quotes and commas. (Capitalization and space must match the sliders in Dreamview) LGSVL__AUTOPILOT_HD_MAP HD map to be loaded in Dreamview (Capitalization and spacing must match the dropdown in Dreamview) LGSVL__DATE_TIME Date and time to start simulation at. Time is the local time in the time zone of the map origin. Format 'YYYY-mm-ddTHH:MM:SS' LGSVL__ENVIRONMENT_CLOUDINESS Value of clouds weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_DAMAGE Value of road damage effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_FOG Value of fog weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_RAIN Value of rain weather effect, clamped to [0.0, 1.0] LGSVL__ENVIRONMENT_WETNESS Value of wetness weather effect, clamped to [0.0, 1.0] LGSVL__RANDOM_SEED Seed used to determine random factors (e.g. NPC type, color, behaviour) LGSVL__SIMULATION_DURATION_SECS The time length of the simulation [int] LGSVL__SPAWN_BICYCLES Whether or not to spawn bicycles [boolean] (not yet supported) LGSVL__SPAWN_PEDESTRIANS Whether or not to spawn pedestrians [boolean] LGSVL__SPAWN_TRAFFIC Whether or not to spawn NPC vehicles [boolean] LGSVL__TIME_STATIC Whether or not time should remain static [boolean] ( True = time is static, False = time moves forward)","title":"Environment Variables"},{"location":"running-simulations/python-testcase/","text":"Python Test Cases Video # Introduction # Test Case mode is a new feature in the SVL Simulator which executes test cases written in Python using the LGSVL Python API . Python scripts can be used to implement many different kinds of tests to run in the SVL Simulator. Among the general use-cases of Python Test Case mode are: Generating and testing random variations of various driving test cases Performing automated (e.g. acceptance) tests based on a set of repeatable test cases Performing automated regressions tests Python Test Case mode provides the ability to configure and execute several sample test cases available under \"Available from Others\" view in Simulations, using Apollo for Autonomous Driving. Refer to Python Test Case Simulation for an illustrated step-by-step walk through on selecting and configuring a python test case simulation. This document describes how to set up and run these test cases with Apollo. For example, one provided example is a \"cut-in\" test case which is performed on the San Francisco map (shown below). Another provided example is a \"sudden braking\" test case which is performed on the SingleLaneRoad map (shown below). Additional examples demonstrate pedestrian-crossing test case which can be performed on the Straight1LanePedestrianCrosswalk map and the red-light runner test case which is performed on the Borregas Ave map. (Screenshot of the \"cut-in\" test case on the San Francisco map, above) (Screenshot of the \"sudden-braking\" test case on the SingleLandRoad map, above) Requirements # Python Test Case Mode has these dependencies: SVL Simulator -- the release tag of the simulator should match the release tag of Python Runner to ensure compatibility Apollo 5.0 lgsvl fork Docker and Docker-compose Downloading and launching SVL Simulator top # The Simulator release can be downloaded as a prebuilt binary from the github release page . Download and extract the zip archive at the desired location. In the unzipped directory, run the executable file named simulator to launch the SVL Simulator. The main window of the simulator will open. Click on the \"Link to Cloud\" or \"Open Browser\" button to launch the simulator Web UI in a web browser. The Web UI controls all aspects of the simulator. You will need to create an account to login to the Web UI. Once logged in you will be able to link your simulator into a cluster, select maps, configure vehicles, and create simulations to run in the simulator. For more information on initial setup of the simulator, refer to the Installing Simuator and Link to Cloud sections of the Running SVL Simulator documentation for an illustrated step-by-step walk through of this process. Installing and Building Apollo 5.0 top # Please follow the instructions on the SVL Simulator documentation website to for Running Apollo 5.0 with SVL Simulator if you have not already done so. This will involve installing Docker CE, Nvidia Docker and the LGSVL Docker image, then cloning the lgsvl fork of the Apollo 5.0 sources, and finally building Apollo and the bridge. Important : Confirm that Apollo is able to drive the car to a selected destination when running in a \"Random Interactive\" simulation before attempting to use Test Case Mode. Refer to Random Interactive Simulation for instructions on setting up a random interactive simulation you can use to test Apollo functionality with the simulator. Python Test Case Mode workflow # Configure Test Case simulation in Web UI Launch Apollo, configure Dreamview, and enable modules Start SVL Simulator in Test Case mode Configure ego destination in Dreamview Configure Test Case simulation in Web UI (if needed) top # From the simulator Web UI, you will need to locate in the Store, a map and vehicle, and add each of them to your library. Refer to the Store for information on how to add the \"SanFrancisco\" map, \"Lincoln2017MKZ\" vehicle and sensors needed for \"Apollo 5.0 (full analysis)\" sensor configuration on this vehicle. Refer to Python Test Case Simulation for an illustrated step-by-step walk through on selecting and configuring a Python test case simulation. Note : The public \"Cut-in Scenario\" test case simulation should be ready to use if Apollo is running on the same machine as the simulator but if Apollo is running elsewhere, be sure to update the BRIDGE_HOST as described in the Python Test Case Simulation walk through. Launch Apollo, Configure Dreamview, and Enable Modules top # Open a terminal, change directory to the apollo-5.0 project (lgsvl fork: https://github.com/lgsvl/apollo-5.0), and type: ./docker/dev_start.sh to start apollo container ./docker/dev_into.sh to enter the container Make sure you have built Apollo with GPU enabled options; for more details, refer to Running Apollo 5.0 with SVL Simulator . bootstrap.sh && bridge.sh & to start the Apollo Dreamview Web UI and cyber_bridge. Open browser, enter address localhost:8888 to load Dreamview, and select Lincoln2017MKZ from the vehicle menu. Be sure to also select the correct map in Dreamview for the desired test case: For the Cut In scenario, select San Francisco from the map menu. For the Pedestrian Crossing scenario, select Straight 1 Lane Pedestrian Crosswalk from the map menu. For the Red Light Runner scenario, select Borregas Ave from the map menu. For the Sudden Braking scenario, select Single Lane Road from the map menu. Enable required modules in the Module Controller view before using the Python Runner including Camera , Localization , Perception , Planning , Prediction , Routing , Traffic Light , and Transform . Enable the Control module in the Module Controller view as well. Note : The ego vehicle will not drive until a destination is selected and a routing request is made in the Route Editing view. However, a destination cannot be selected until the simulation is started and Apollo is able to localize the ego on the map. Note : When changing the map selection in Dreamview, all modules will be disabled and will need to be re-enabled. Start SVL Simulator in Test Case mode top # In the Simulations tab in the Web UI locate the Cut-in Scenario scenario simulation. Then click the red \"Run Simulation\" button under the Cut-in Scenario to start the simulation. Note : The ego vehicle will not drive until a destination is selected and a routing request is made in the Route Editing view. However, a destination cannot be selected until the simulation is started and Apollo is able to localize the ego on the map. Configure Ego Destination in Dreamview top # After starting the simulation, Apollo should receive sensor information and localize the car onto the selected map. In Dreamview, switch to Route Editing view, zoom out, scroll if needed, and click to set the destination in the same ego lane before the next intersection. Then click \"Send Routing Request\" to plan the route. In Module View make sure all required modules are still enabled. The ego vehicle should begin driving. The NPC should cut in front of the ego vehicle, and the ego vehicle should slow or stop to avoid hitting the NPC. This test case can be repeated by clicking the red \"Run\" (triangle/play) button in the Web UI. Since the destination and route plan are already configured, re-starting the simulation will reset the ego start location and Apollo should immediately being driving. Example Test Cases # There are several example test cases provided in this preview release. Each test case has a required map, and a suggested destination, as summarized in the following table: Test Case Vehicle Sensor configuration Map Destination cut-in.py Lincoln2017MKZ Apollo 5.0 (full analysis) San Francisco same lane before next intersection ped-crossing.py Lincoln2017MKZ Apollo 5.0 (full analysis) Straight1LanePedestrianCrosswalk end of the map red-light-runner.py Lincoln2017MKZ Apollo 5.0 (full analysis) Borregas Ave other side of intersection sudden-braking.py Lincoln2017MKZ Apollo 5.0 (full analysis) SingleLaneRoad end of the map Refer to the Store for information on how to add the \"SanFrancisco\" map, \"Lincoln2017MKZ\" vehicle and sensors needed for \"Apollo 5.0 (full analysis)\" sensor configuration. Refer to Python Test Case Simulation for an illustrated step-by-step walk through on selecting and configuring a python test case simulation. Note that each map (environment) must be available in the Simulator Maps view, and the corresponding Apollo HD map must be selected in Dreamview. Also note that changing maps in Dreamview will disable the modules so you will need to re-enable them as described above. Details for each of the test cases are provided below. Cut-in Test Case top # On a two lane road, an NPC cuts in front of ego vehicle from an adjacent lane while maintaining previous speed, with a small enough distance in front of ego vehicle such that ego vehicle will need to react by either changing speed (braking) or turning or swerving away. This test case runs on the San Francisco map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the San Francisco map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Cut In\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination in the same ego lane before the next intersection. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled. Pedestrian Crossing Test Case top # A pedestrian begins crossing a crosswalk as the ego vehicle approaches the intersection. The ego vehicle will need to slow down or come to a stop to wait for the pedestrian to finish crossing. This test case runs on the Straight1LanePedestrianCrosswalk map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the Straight1LanePedestrianCrosswalk map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Pedestrian Crossing\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination at the end of the map. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled. Red Light Runner Test Case top # While the ego vehicle attempts to cross a 4-way intersection with a green traffic light, an NPC crossing the intersection from the right runs a red light and crosses the intersection. The Ego must react by braking, stopping, or turning in order to avoid a collision. This test case runs on the Borregas Ave map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the Borregas Ave map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Red Light Runner\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination on the other side of the intersection. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled. Sudden Braking Test Case top # The ego vehicle follows an NPC vehicle that is traveling at the speed limit. The NPC suddenly brakes and comes to a stop. The ego vehicle will need to greatly reduce its speed to avoid a collision. After a few seconds, the NPC speeds back up gradually to the speed limit. This test case runs on the SingleLaneRoad map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the Single Lane Road map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Sudden Braking\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination at the end of the map. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled. Known issues top # The NPC usually waits until the ego vehicle starts driving but in some cases it might start before a route has been requested for the ego. In that case wait for the simulation to finish and then run it again while leaving the modules enabled and the route to the destination already specified. If errors appear due to previously-downloaded (maps or vehicle) asset bundles from a previous version the Simulator, delete the Simulator persistent data folder located at /home/[username]/.config/unity3d/LGElectronics/SVLSimulator . If Straight 1 Lane Pedestrian Crosswalk is not available in the Dreamview map menu, try updating your apollo-5.0 sources (from the lgsvl fork ), then stop and re-start Apollo, and re-load Dreamview. The AD stack (e.g. Apollo) is not currently controlled by the Python Runner, so you will need to manually set a destination in Dreamview. You can control the AD stack through your Python Script using the python dreamview-API module. Dreamview-API module only supports Apollo at the moment. Using these APIs, you can configure hd_map and vehicle settings in Apollo Dreamview, enable/disable Apollo modules, set destination, etc..","title":"Python testcase"},{"location":"running-simulations/python-testcase/#video","text":"","title":"Video"},{"location":"running-simulations/python-testcase/#introduction","text":"Test Case mode is a new feature in the SVL Simulator which executes test cases written in Python using the LGSVL Python API . Python scripts can be used to implement many different kinds of tests to run in the SVL Simulator. Among the general use-cases of Python Test Case mode are: Generating and testing random variations of various driving test cases Performing automated (e.g. acceptance) tests based on a set of repeatable test cases Performing automated regressions tests Python Test Case mode provides the ability to configure and execute several sample test cases available under \"Available from Others\" view in Simulations, using Apollo for Autonomous Driving. Refer to Python Test Case Simulation for an illustrated step-by-step walk through on selecting and configuring a python test case simulation. This document describes how to set up and run these test cases with Apollo. For example, one provided example is a \"cut-in\" test case which is performed on the San Francisco map (shown below). Another provided example is a \"sudden braking\" test case which is performed on the SingleLaneRoad map (shown below). Additional examples demonstrate pedestrian-crossing test case which can be performed on the Straight1LanePedestrianCrosswalk map and the red-light runner test case which is performed on the Borregas Ave map. (Screenshot of the \"cut-in\" test case on the San Francisco map, above) (Screenshot of the \"sudden-braking\" test case on the SingleLandRoad map, above)","title":"Introduction"},{"location":"running-simulations/python-testcase/#requirements","text":"Python Test Case Mode has these dependencies: SVL Simulator -- the release tag of the simulator should match the release tag of Python Runner to ensure compatibility Apollo 5.0 lgsvl fork Docker and Docker-compose","title":"Requirements"},{"location":"running-simulations/python-testcase/#downloading-and-launching-svl-simulator","text":"The Simulator release can be downloaded as a prebuilt binary from the github release page . Download and extract the zip archive at the desired location. In the unzipped directory, run the executable file named simulator to launch the SVL Simulator. The main window of the simulator will open. Click on the \"Link to Cloud\" or \"Open Browser\" button to launch the simulator Web UI in a web browser. The Web UI controls all aspects of the simulator. You will need to create an account to login to the Web UI. Once logged in you will be able to link your simulator into a cluster, select maps, configure vehicles, and create simulations to run in the simulator. For more information on initial setup of the simulator, refer to the Installing Simuator and Link to Cloud sections of the Running SVL Simulator documentation for an illustrated step-by-step walk through of this process.","title":"Downloading and launching SVL Simulator"},{"location":"running-simulations/python-testcase/#installing-and-building-apollo-5.0","text":"Please follow the instructions on the SVL Simulator documentation website to for Running Apollo 5.0 with SVL Simulator if you have not already done so. This will involve installing Docker CE, Nvidia Docker and the LGSVL Docker image, then cloning the lgsvl fork of the Apollo 5.0 sources, and finally building Apollo and the bridge. Important : Confirm that Apollo is able to drive the car to a selected destination when running in a \"Random Interactive\" simulation before attempting to use Test Case Mode. Refer to Random Interactive Simulation for instructions on setting up a random interactive simulation you can use to test Apollo functionality with the simulator.","title":"Installing and Building Apollo 5.0"},{"location":"running-simulations/python-testcase/#python-test-case-mode-workflow","text":"Configure Test Case simulation in Web UI Launch Apollo, configure Dreamview, and enable modules Start SVL Simulator in Test Case mode Configure ego destination in Dreamview","title":"Python Test Case Mode workflow"},{"location":"running-simulations/python-testcase/#configure-test-case-simulation-in-web-ui-(if-needed)","text":"From the simulator Web UI, you will need to locate in the Store, a map and vehicle, and add each of them to your library. Refer to the Store for information on how to add the \"SanFrancisco\" map, \"Lincoln2017MKZ\" vehicle and sensors needed for \"Apollo 5.0 (full analysis)\" sensor configuration on this vehicle. Refer to Python Test Case Simulation for an illustrated step-by-step walk through on selecting and configuring a Python test case simulation. Note : The public \"Cut-in Scenario\" test case simulation should be ready to use if Apollo is running on the same machine as the simulator but if Apollo is running elsewhere, be sure to update the BRIDGE_HOST as described in the Python Test Case Simulation walk through.","title":"Configure Test Case simulation in Web UI (if needed)"},{"location":"running-simulations/python-testcase/#launch-apollo,-configure-dreamview,-and-enable-modules","text":"Open a terminal, change directory to the apollo-5.0 project (lgsvl fork: https://github.com/lgsvl/apollo-5.0), and type: ./docker/dev_start.sh to start apollo container ./docker/dev_into.sh to enter the container Make sure you have built Apollo with GPU enabled options; for more details, refer to Running Apollo 5.0 with SVL Simulator . bootstrap.sh && bridge.sh & to start the Apollo Dreamview Web UI and cyber_bridge. Open browser, enter address localhost:8888 to load Dreamview, and select Lincoln2017MKZ from the vehicle menu. Be sure to also select the correct map in Dreamview for the desired test case: For the Cut In scenario, select San Francisco from the map menu. For the Pedestrian Crossing scenario, select Straight 1 Lane Pedestrian Crosswalk from the map menu. For the Red Light Runner scenario, select Borregas Ave from the map menu. For the Sudden Braking scenario, select Single Lane Road from the map menu. Enable required modules in the Module Controller view before using the Python Runner including Camera , Localization , Perception , Planning , Prediction , Routing , Traffic Light , and Transform . Enable the Control module in the Module Controller view as well. Note : The ego vehicle will not drive until a destination is selected and a routing request is made in the Route Editing view. However, a destination cannot be selected until the simulation is started and Apollo is able to localize the ego on the map. Note : When changing the map selection in Dreamview, all modules will be disabled and will need to be re-enabled.","title":"Launch Apollo, Configure Dreamview, and Enable Modules"},{"location":"running-simulations/python-testcase/#start-svl-simulator-in-test-case-mode","text":"In the Simulations tab in the Web UI locate the Cut-in Scenario scenario simulation. Then click the red \"Run Simulation\" button under the Cut-in Scenario to start the simulation. Note : The ego vehicle will not drive until a destination is selected and a routing request is made in the Route Editing view. However, a destination cannot be selected until the simulation is started and Apollo is able to localize the ego on the map.","title":"Start SVL Simulator in Test Case mode"},{"location":"running-simulations/python-testcase/#configure-ego-destination-in-dreamview","text":"After starting the simulation, Apollo should receive sensor information and localize the car onto the selected map. In Dreamview, switch to Route Editing view, zoom out, scroll if needed, and click to set the destination in the same ego lane before the next intersection. Then click \"Send Routing Request\" to plan the route. In Module View make sure all required modules are still enabled. The ego vehicle should begin driving. The NPC should cut in front of the ego vehicle, and the ego vehicle should slow or stop to avoid hitting the NPC. This test case can be repeated by clicking the red \"Run\" (triangle/play) button in the Web UI. Since the destination and route plan are already configured, re-starting the simulation will reset the ego start location and Apollo should immediately being driving.","title":"Configure Ego Destination in Dreamview"},{"location":"running-simulations/python-testcase/#example-test-cases","text":"There are several example test cases provided in this preview release. Each test case has a required map, and a suggested destination, as summarized in the following table: Test Case Vehicle Sensor configuration Map Destination cut-in.py Lincoln2017MKZ Apollo 5.0 (full analysis) San Francisco same lane before next intersection ped-crossing.py Lincoln2017MKZ Apollo 5.0 (full analysis) Straight1LanePedestrianCrosswalk end of the map red-light-runner.py Lincoln2017MKZ Apollo 5.0 (full analysis) Borregas Ave other side of intersection sudden-braking.py Lincoln2017MKZ Apollo 5.0 (full analysis) SingleLaneRoad end of the map Refer to the Store for information on how to add the \"SanFrancisco\" map, \"Lincoln2017MKZ\" vehicle and sensors needed for \"Apollo 5.0 (full analysis)\" sensor configuration. Refer to Python Test Case Simulation for an illustrated step-by-step walk through on selecting and configuring a python test case simulation. Note that each map (environment) must be available in the Simulator Maps view, and the corresponding Apollo HD map must be selected in Dreamview. Also note that changing maps in Dreamview will disable the modules so you will need to re-enable them as described above. Details for each of the test cases are provided below.","title":"Example Test Cases"},{"location":"running-simulations/python-testcase/#cut-in-test-case","text":"On a two lane road, an NPC cuts in front of ego vehicle from an adjacent lane while maintaining previous speed, with a small enough distance in front of ego vehicle such that ego vehicle will need to react by either changing speed (braking) or turning or swerving away. This test case runs on the San Francisco map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the San Francisco map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Cut In\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination in the same ego lane before the next intersection. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled.","title":"Cut-in Test Case"},{"location":"running-simulations/python-testcase/#pedestrian-crossing-test-case","text":"A pedestrian begins crossing a crosswalk as the ego vehicle approaches the intersection. The ego vehicle will need to slow down or come to a stop to wait for the pedestrian to finish crossing. This test case runs on the Straight1LanePedestrianCrosswalk map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the Straight1LanePedestrianCrosswalk map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Pedestrian Crossing\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination at the end of the map. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled.","title":"Pedestrian Crossing Test Case"},{"location":"running-simulations/python-testcase/#red-light-runner-test-case","text":"While the ego vehicle attempts to cross a 4-way intersection with a green traffic light, an NPC crossing the intersection from the right runs a red light and crosses the intersection. The Ego must react by braking, stopping, or turning in order to avoid a collision. This test case runs on the Borregas Ave map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the Borregas Ave map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Red Light Runner\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination on the other side of the intersection. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled.","title":"Red Light Runner Test Case"},{"location":"running-simulations/python-testcase/#sudden-braking-test-case","text":"The ego vehicle follows an NPC vehicle that is traveling at the speed limit. The NPC suddenly brakes and comes to a stop. The ego vehicle will need to greatly reduce its speed to avoid a collision. After a few seconds, the NPC speeds back up gradually to the speed limit. This test case runs on the SingleLaneRoad map with the Lincoln MKZ ego vehicle driven by Apollo. Be sure that the Single Lane Road map is selected in Dreamview, and that all required modules are enabled. To run the Cut-in test case, select the \"Sudden Braking\" test case file as described above. Then switch to the Route Editing view of Dreamview. Scroll (or zoom out) and click to set a destination at the end of the map. Then click Send Routing Request and return to the Module Controller view to confirm that Control (and other modules) are enabled.","title":"Sudden Braking Test Case"},{"location":"running-simulations/python-testcase/#known-issues","text":"The NPC usually waits until the ego vehicle starts driving but in some cases it might start before a route has been requested for the ego. In that case wait for the simulation to finish and then run it again while leaving the modules enabled and the route to the destination already specified. If errors appear due to previously-downloaded (maps or vehicle) asset bundles from a previous version the Simulator, delete the Simulator persistent data folder located at /home/[username]/.config/unity3d/LGElectronics/SVLSimulator . If Straight 1 Lane Pedestrian Crosswalk is not available in the Dreamview map menu, try updating your apollo-5.0 sources (from the lgsvl fork ), then stop and re-start Apollo, and re-load Dreamview. The AD stack (e.g. Apollo) is not currently controlled by the Python Runner, so you will need to manually set a destination in Dreamview. You can control the AD stack through your Python Script using the python dreamview-API module. Dreamview-API module only supports Apollo at the moment. Using these APIs, you can configure hd_map and vehicle settings in Apollo Dreamview, enable/disable Apollo modules, set destination, etc..","title":"Known issues"},{"location":"running-simulations/python-vse-runner/","text":"TestCaseRunner Video # Introduction # TestCaseRunner is a tool for the SVL Simulator which executes test cases created with the Visual Scenario Editor (as well as other Python tests). These tests are executed by the TestCaseRunner using the LGSVL Python API . Note : This tool is only supported on Linux. Package overview top # Python Runner is delivered as part of the svlsimulator-linux64-{release-version}.zip archive with the following directory layout: svlsimulator-linux64-{release-version} \u251c\u2500\u2500 TestCaseRunner \u2502 \u2514\u2500\u2500 scenarioRunner \u2502 \u2514\u2500\u2500 docker \u2502 \u2514\u2500\u2500 scenario_runner.sh The docker directory contains a docker image saved as a tarball. scenario_runner.sh along with the above docker image is used for Python API / Visual scenario editor a.k.a VSE \" runtime templates in web UI to run the scenarios automatically. Visual Scenario runtime template is a way to upload and run VSE test cases directly from web UI. scenario_runner.sh is the Python Runner script that executes VSE test cases in the simulator through the simulator's Python API. Also, if you want to run the VSE scenario manually using \"API only\" runtime template, then you can use this file. Python Runner workflow top # Setup simulator in web UI Launch simulator in API mode Run the Python runner script Start the ego vehicle Requirements # Python Runner has these dependencies: SVL Simulator -- the release tag of the simulator should match the release tag of Python Runner to ensure compatibility. Apollo 5.0 lgsvl fork Docker and Docker-compose Downloading and launching the SVL Simulator top # The Simulator release can be downloaded as a prebuilt binary from the github releases page . Download and extract the zip archive at the desired location. In the unzipped directory, run the executable file named simulator to launch the SVL Simulator. The main window of the simulator will open. Click on the \"Link to Cloud\" or \"Open Browser\" button to launch the simulator Web UI in a web browser. The Web UI controls all aspects of the simulator. You will need to create an account to login to the Web UI. Once logged in you will be able to link your simulator into a cluster, select maps, configure vehicles, and create simulations to run in the simulator. For more information on initial setup of the simulator, refer to the Installing Simuator and Link to Cloud sections of the Running SVL Simulator documentation for an illustrated step-by-step walk through of this process. Configure API Only mode in Web UI (if needed) top # From the simulator Web UI, you can find any needed maps and vehicles in the Store and add each of them to your library. Or, if you want to use a private map to create VSE tests, you will need to upload your map's asset bundle (refer to Adding a map for how to upload your own maps). Similarly, you can also use your own private vehicle (refer to Adding a vehicle for how to upload your own vehicles). Note for any Apollo compatible vehicle you use, you need to have ApolloControl sensor included in order to correctly start simulation. Refer to Apollo Control sensor configuration for more information. Refer to the Store for information on how to add the \"BorregasAve\" and \"SanFrancisco\" maps and \"Lincoln2017MKZ (Apollo 5.0)\" vehicle. Refer to API Only Simulation for an illustrated step-by-step walk through on creating an API Only simulation. Start SVL Simulator in API Only mode top # In the Simulations tab in the Web UI locate the API Only simulation, you need to create a simulation with API Only if you don't have it (for steps refer to Runtime Template: API Only ). Then click the red \"Run Simulation\" button under the API Only simulation to start the simulation. Launching Apollo alongside the Simulator # Installing (and Building) Apollo 5.0 top # Please follow the instructions on the SVL Simulator documentation website to for Running Apollo 5.0 with SVL Simulator if you have not already done so. This will involve installing Docker CE, Nvidia Docker and the LGSVL Docker image, then cloning the lgsvl fork of the Apollo 5.0 sources, and finally building Apollo and the bridge. Start Apollo 5.0 top # Open a terminal, change directory to the apollo-5.0 project (lgsvl fork: https://github.com/lgsvl/apollo-5.0), and type: ./docker/dev_start.sh to start apollo container ./docker/dev_into.sh to enter the container Make sure you have built Apollo with GPU enabled options; for more details, refer to Running Apollo 5.0 with SVL Simulator . Make sure you have HD map files and vehicle calibration files for your VSE test case in apollo repository. bootstrap.sh && bridge.sh & to start the Apollo Dreamview Web UI and cyber_bridge. Open browser, enter address localhost:8888 to view Dreamview. Using the Python Runner for VSE tests # Python Runner top # Navigate to the unzipped directory of the SVL Simulator Linux package in a terminal window. The Python Runner script can be run from the svlsimulator/TestCaseRunner/scenarioRunner directory as ./scenario_runner.sh Python Runner Commands top # The Python Runner supports the following commands: scenario_runner.sh help for help using the Python Runner scenario_runner.sh env to print useful environment variables scenario_runner.sh run to run a test case (with optional parameters) scenario_runner.sh run --help for help running a test case scenario_runner.sh version to print version information Python Runner Parameters top # The Python Runner supports the following command line parameters when running a VSE test case: -d, --duration DURATION Maximum scenario duration in seconds. (default: 20.0) -f, --force-duration Force simulation to end after given duration. If not set, simulation will end by given duration or at the time when all NPCs' waypoints have been reached. Python Runner Environment Variables top # The Python Runner supports the following environment variables (and default values) when running a test case: SIMULATOR_HOST=localhost SIMULATOR_PORT=8181 BRIDGE_HOST=localhost BRIDGE_PORT=9090 SCENARIOS_DIR If simulator is being run on a separate machine than the Python Runner make sure to set the SIMULATOR_HOST environment variable with the IP address of the machine running the simulator; for example export SIMULATOR_HOST=192.168.0.2 . The BRIDGE_HOST machine is the machine which is running Apollo (and the CyberRT bridge) and is most likely also the same machine running Python Runner. If simulator and Apollo are running on the same machine, use the default value of localhost . Otherwise, set BRIDGE_HOST to the IP address of the machine running Apollo; for example export BRIDGE_HOST=192.168.0.1 . You may set SCENARIOS_DIR in order to mount a local directory which contains a VSE script that is not available inside the Python Runner container; for example export SCENARIOS_DIR=\"/home/<user>/my-vse-scripts/\" . Or you can specify the path to the script on the command line. Usage Example # Create a test scenario using the Visual Scenario Editor and make note of the directory to which it is saved. After placing an EgoAgent in the Visual Scenario Editor scene, click on the ego agent, then click the \"Edit\" button and select the desired ego vehicle from the scrolling list that appears. If the vehicle will be controlled by Apollo then select a vehicle that is configured for Apollo, e.g. \"Lincoln2017MKZ (Apollo 5.0)\", which is configured with Apollo sensors and also configured for test case analytics. You also need to select a destination point for the ego agent so that the VSE runner will set up apollo automatically, i.e., connect with apollo, start required modules and set destination. Also note that changing maps in Dreamview will disable the modules so you will need to re-enable them as described above. Running the VSE Scenario top # To run a locally-generated VSE test case called my-vse-test.json which is located in the ~/my-vse-scripts directory, with a test duration of 30 seconds, navigate to the svlsimulator/TestCaseRunner/scenarioRunner directory and type: $ export SCENARIOS_DIR=/home/<user>/my-vse-scripts/ $ ./scenario_runner.sh run my-vse-test.json -d 30 Setting SCENARIOS_DIR is not necessary if you specify the path to the script. In addition, when finished testing in API-Only mode, click on the red \"Stop\" (square/stop) button (at the bottom of the Web UI) to end the API-only simulation. Running VSE runtime template top # The same VSE scenario file can be uploaded to web UI and run automatically. See instructions here . Note : Visual Scenario runtime template based simulations can only be run on Linux simulator build. This is not supported on Windows. Known issues top # The VSE runner won't turn off apollo modules when the scenario finishes. You need to disable the modules once you finish the testing. If your runner is not working correctly, please make sure apollo has corresponding HD maps and vehicle calibration files, and bridge is running.","title":"Python vse runner"},{"location":"running-simulations/python-vse-runner/#video","text":"","title":"Video"},{"location":"running-simulations/python-vse-runner/#introduction","text":"TestCaseRunner is a tool for the SVL Simulator which executes test cases created with the Visual Scenario Editor (as well as other Python tests). These tests are executed by the TestCaseRunner using the LGSVL Python API . Note : This tool is only supported on Linux.","title":"Introduction"},{"location":"running-simulations/python-vse-runner/#package-overview","text":"Python Runner is delivered as part of the svlsimulator-linux64-{release-version}.zip archive with the following directory layout: svlsimulator-linux64-{release-version} \u251c\u2500\u2500 TestCaseRunner \u2502 \u2514\u2500\u2500 scenarioRunner \u2502 \u2514\u2500\u2500 docker \u2502 \u2514\u2500\u2500 scenario_runner.sh The docker directory contains a docker image saved as a tarball. scenario_runner.sh along with the above docker image is used for Python API / Visual scenario editor a.k.a VSE \" runtime templates in web UI to run the scenarios automatically. Visual Scenario runtime template is a way to upload and run VSE test cases directly from web UI. scenario_runner.sh is the Python Runner script that executes VSE test cases in the simulator through the simulator's Python API. Also, if you want to run the VSE scenario manually using \"API only\" runtime template, then you can use this file.","title":"Package overview"},{"location":"running-simulations/python-vse-runner/#python-runner-workflow","text":"Setup simulator in web UI Launch simulator in API mode Run the Python runner script Start the ego vehicle","title":"Python Runner workflow"},{"location":"running-simulations/python-vse-runner/#requirements","text":"Python Runner has these dependencies: SVL Simulator -- the release tag of the simulator should match the release tag of Python Runner to ensure compatibility. Apollo 5.0 lgsvl fork Docker and Docker-compose","title":"Requirements"},{"location":"running-simulations/python-vse-runner/#downloading-and-launching-the-svl-simulator","text":"The Simulator release can be downloaded as a prebuilt binary from the github releases page . Download and extract the zip archive at the desired location. In the unzipped directory, run the executable file named simulator to launch the SVL Simulator. The main window of the simulator will open. Click on the \"Link to Cloud\" or \"Open Browser\" button to launch the simulator Web UI in a web browser. The Web UI controls all aspects of the simulator. You will need to create an account to login to the Web UI. Once logged in you will be able to link your simulator into a cluster, select maps, configure vehicles, and create simulations to run in the simulator. For more information on initial setup of the simulator, refer to the Installing Simuator and Link to Cloud sections of the Running SVL Simulator documentation for an illustrated step-by-step walk through of this process.","title":"Downloading and launching the SVL Simulator"},{"location":"running-simulations/python-vse-runner/#configure-api-only-mode-in-web-ui-(if-needed)","text":"From the simulator Web UI, you can find any needed maps and vehicles in the Store and add each of them to your library. Or, if you want to use a private map to create VSE tests, you will need to upload your map's asset bundle (refer to Adding a map for how to upload your own maps). Similarly, you can also use your own private vehicle (refer to Adding a vehicle for how to upload your own vehicles). Note for any Apollo compatible vehicle you use, you need to have ApolloControl sensor included in order to correctly start simulation. Refer to Apollo Control sensor configuration for more information. Refer to the Store for information on how to add the \"BorregasAve\" and \"SanFrancisco\" maps and \"Lincoln2017MKZ (Apollo 5.0)\" vehicle. Refer to API Only Simulation for an illustrated step-by-step walk through on creating an API Only simulation.","title":"Configure API Only mode in Web UI (if needed)"},{"location":"running-simulations/python-vse-runner/#start-svl-simulator-in-api-only-mode","text":"In the Simulations tab in the Web UI locate the API Only simulation, you need to create a simulation with API Only if you don't have it (for steps refer to Runtime Template: API Only ). Then click the red \"Run Simulation\" button under the API Only simulation to start the simulation.","title":"Start SVL Simulator in API Only mode"},{"location":"running-simulations/python-vse-runner/#launching-apollo-alongside-the-simulator","text":"","title":"Launching Apollo alongside the Simulator"},{"location":"running-simulations/python-vse-runner/#installing-(and-building)-apollo-5.0","text":"Please follow the instructions on the SVL Simulator documentation website to for Running Apollo 5.0 with SVL Simulator if you have not already done so. This will involve installing Docker CE, Nvidia Docker and the LGSVL Docker image, then cloning the lgsvl fork of the Apollo 5.0 sources, and finally building Apollo and the bridge.","title":"Installing (and Building) Apollo 5.0"},{"location":"running-simulations/python-vse-runner/#start-apollo-5.0","text":"Open a terminal, change directory to the apollo-5.0 project (lgsvl fork: https://github.com/lgsvl/apollo-5.0), and type: ./docker/dev_start.sh to start apollo container ./docker/dev_into.sh to enter the container Make sure you have built Apollo with GPU enabled options; for more details, refer to Running Apollo 5.0 with SVL Simulator . Make sure you have HD map files and vehicle calibration files for your VSE test case in apollo repository. bootstrap.sh && bridge.sh & to start the Apollo Dreamview Web UI and cyber_bridge. Open browser, enter address localhost:8888 to view Dreamview.","title":"Start Apollo 5.0"},{"location":"running-simulations/python-vse-runner/#using-the-python-runner-for-vse-tests","text":"","title":"Using the Python Runner for VSE tests"},{"location":"running-simulations/python-vse-runner/#python-runner","text":"Navigate to the unzipped directory of the SVL Simulator Linux package in a terminal window. The Python Runner script can be run from the svlsimulator/TestCaseRunner/scenarioRunner directory as ./scenario_runner.sh","title":"Python Runner"},{"location":"running-simulations/python-vse-runner/#python-runner-commands","text":"The Python Runner supports the following commands: scenario_runner.sh help for help using the Python Runner scenario_runner.sh env to print useful environment variables scenario_runner.sh run to run a test case (with optional parameters) scenario_runner.sh run --help for help running a test case scenario_runner.sh version to print version information","title":"Python Runner Commands"},{"location":"running-simulations/python-vse-runner/#python-runner-parameters","text":"The Python Runner supports the following command line parameters when running a VSE test case: -d, --duration DURATION Maximum scenario duration in seconds. (default: 20.0) -f, --force-duration Force simulation to end after given duration. If not set, simulation will end by given duration or at the time when all NPCs' waypoints have been reached.","title":"Python Runner Parameters"},{"location":"running-simulations/python-vse-runner/#python-runner-environment-variables","text":"The Python Runner supports the following environment variables (and default values) when running a test case: SIMULATOR_HOST=localhost SIMULATOR_PORT=8181 BRIDGE_HOST=localhost BRIDGE_PORT=9090 SCENARIOS_DIR If simulator is being run on a separate machine than the Python Runner make sure to set the SIMULATOR_HOST environment variable with the IP address of the machine running the simulator; for example export SIMULATOR_HOST=192.168.0.2 . The BRIDGE_HOST machine is the machine which is running Apollo (and the CyberRT bridge) and is most likely also the same machine running Python Runner. If simulator and Apollo are running on the same machine, use the default value of localhost . Otherwise, set BRIDGE_HOST to the IP address of the machine running Apollo; for example export BRIDGE_HOST=192.168.0.1 . You may set SCENARIOS_DIR in order to mount a local directory which contains a VSE script that is not available inside the Python Runner container; for example export SCENARIOS_DIR=\"/home/<user>/my-vse-scripts/\" . Or you can specify the path to the script on the command line.","title":"Python Runner Environment Variables"},{"location":"running-simulations/python-vse-runner/#usage-example","text":"Create a test scenario using the Visual Scenario Editor and make note of the directory to which it is saved. After placing an EgoAgent in the Visual Scenario Editor scene, click on the ego agent, then click the \"Edit\" button and select the desired ego vehicle from the scrolling list that appears. If the vehicle will be controlled by Apollo then select a vehicle that is configured for Apollo, e.g. \"Lincoln2017MKZ (Apollo 5.0)\", which is configured with Apollo sensors and also configured for test case analytics. You also need to select a destination point for the ego agent so that the VSE runner will set up apollo automatically, i.e., connect with apollo, start required modules and set destination. Also note that changing maps in Dreamview will disable the modules so you will need to re-enable them as described above.","title":"Usage Example"},{"location":"running-simulations/python-vse-runner/#running-the-vse-scenario","text":"To run a locally-generated VSE test case called my-vse-test.json which is located in the ~/my-vse-scripts directory, with a test duration of 30 seconds, navigate to the svlsimulator/TestCaseRunner/scenarioRunner directory and type: $ export SCENARIOS_DIR=/home/<user>/my-vse-scripts/ $ ./scenario_runner.sh run my-vse-test.json -d 30 Setting SCENARIOS_DIR is not necessary if you specify the path to the script. In addition, when finished testing in API-Only mode, click on the red \"Stop\" (square/stop) button (at the bottom of the Web UI) to end the API-only simulation.","title":"Running the VSE Scenario"},{"location":"running-simulations/python-vse-runner/#running-vse-runtime-template","text":"The same VSE scenario file can be uploaded to web UI and run automatically. See instructions here . Note : Visual Scenario runtime template based simulations can only be run on Linux simulator build. This is not supported on Windows.","title":"Running VSE runtime template"},{"location":"running-simulations/python-vse-runner/#known-issues","text":"The VSE runner won't turn off apollo modules when the scenario finishes. You need to disable the modules once you finish the testing. If your runner is not working correctly, please make sure apollo has corresponding HD maps and vehicle calibration files, and bridge is running.","title":"Known issues"},{"location":"running-simulations/running-simulator/","text":"Running SVL Simulator Before running SVL Simulator make sure to go through the guide for installing and starting it for the first time. This guide assumes that SVL Simulator is installed and you are ready to run a simulation. This guide will walk you through the steps of adding assets from the Store to My Library , and creating and running following types of simulations: A random interactive simulation An API-only simulation A Python Test Case simulation A Visual scenario editor based simulation Table of Contents Video Add Assets from the Store Review Assets in your Library Create and run a random interactive simulation API Only Simulation Creating and running a Python Test Case Simulation Creating and running a Visual Scenario Editor based Simulation Video top # Add Assets from the Store top # Click tabs under Store to view available maps, vehicles, and sensors that are available for you to use. You'll need to add maps and vehicles from the Store to your own library (or upload maps and vehicles of your own) before you can use them in a simulation. Scroll through the list of all Free maps available, or click Trending to see most commonly downloaded maps or click Shared with me to see private maps shared to you by other users. Locate the BorregasAve map and click \"+\" to add it to your library if not already added (for Random Interactive Simulation below). Locate the SanFrancisco map and click \"+\" to add it to your library if not already added (for Python Test Case Simulation below). Click any tab under Store (if needed) to return to the Store view. Scroll through the list of all Free vehicles available, or click Trending to see most commonly downloaded maps or click Shared with me to see private maps shared to you by other users. Locate the Lincoln2017MKZ vehicle and click \"+\" to add it to your library if not already added (for Random Interactive Simulation or Python Test Case Simulation below). Locate the added vehicle in your library and click on the \"Apollo 5.0\" sensor configuration to confirm if there are any missing sensors in your library (for Random Interactive Simulation below). Locate the added vehicle in your library and click on the \"Apollo 5.0 (full analysis)\" sensor configuration to confirm if there are any missing sensors in your library (for Python Test Case Simulation below). If any sensors are missing, then locate them under \"Sensors\" tab under Store and add them to your library OR follow the instructions on the sensor configuration page. NOTE: All test scripts load map by name or UUID and vehicle's sensor configuration by UUID. So take care to add the specific vehicles, sensors and maps to your library that are needed for the scripts or test cases that you intend to run. Please use UUID instead of name in your Python Test Case scripts. You can get UUID from the url when you open the asset profile in your browser. Review Assets in your Library top # Click Library and then click Maps , Vehicles and Sensors to view the map, vehicle and sensors that you just added to your library. NOTE : You may notice some maps, vehicles and sensors in your library that you did not explicitly add; these were automatically added when your account was created. Create and run a random interactive simulation top # In this section we will create an Interactive Mode simulation which allows you to control the vehicle and interact with the environment while the simulation is running. Random Traffic Scenarios are simulations in which NPCs and pedestrians exhibit randomly generated behavior. This random behavior is deterministic, meaning that the same behavior will be repeated if the same random seed is used in the simulation set up. Click Simulations . Click Add New to create a new simulation. On the New Simulation General pane, enter a name for your new simulation (e.g. \"Random Interactive\"), and select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Interactive mode to enable interactive mode for this simulation, if you wish to be able to play/pause the simulation and change settings after the simulation has started. Click the switch next to Headless mode if you do not wish the simulation to be rendered on the screen. Click the switch next to Create test report to enable cloud to create a test report at the end of simulation run with analytics and event data from sensors on your vehicle. Click the Next button to advance to the Test Case pane. On the New Simulation Test Case pane, select Random Traffic from the Runtime Template list. Next select the BorregasAve map, the Lincoln2017MKZ vehicle and Apollo 5.0 sensor configuration from the dropdown lists. IMPORTANT: If you have not already added a Map, Vehicle or needed Sensors to your library, refer to the Store above for information on how to add them. (Optional) Click + Add another vehicle to add additional Ego vehicles. Note that each ego vehicle will need to be controlled by a separate AD stack and must have it's own bridge connection which would be specified in the Autopilot tab. You could also specify date, time, and environmental settings here but since we are creating an interactive simulation those can interactively be modified while the simulation is running. Click the switches next to Random Traffic and Random Pedestrians to enable these NPCs (non-player characters) in this random simulation. The Random Bicyclists feature is not yet supported and will be available in future releases. Enable Use Pre-defined Seed and manually enter a number to be used as the seed for all random behavior in the simulator. Using a pre-defined seed makes it possible to have deterministic and repeatable random simulations. Click the Next button to advance to the Autopilot pane. On the New Simulation Autopilot pane, select Apollo 5.0 from the Autopilot list. For all ego vehicles in your simulation, enter the details of where can simulator connect to Autopilot. In the Bridge IP, enter \"localhost:9090\" or you can replace \"localhost\" with the IP address of your autopilot system if it is different from the machine on which SVL Simulator is running. Click the Next button to advance to the Publish pane. On the New Simulation Publish pane, click Publish to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created Random Interactive simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button from drop-down menu on top-right corner to re-connect the simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the random interactive simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message indicating that any newly added maps, vehicles and sensors are downloading, after which the simulation should initialize. Since this is an interactive simulation, click the Play button at the bottom left corner of the screen to begin the simulation. You should see the random traffic driving and random pedestrians walking. Click the joystick icon at the bottom of the screen to get a list of keyboard controls. Click the settings icon to adjust the Environment settings. Click the eye icon to enable or disable individual sensor visualizations. If Apollo 5.0 is available and running, then SVL Simulator should connect to the Apollo bridge, and Apollo should be able to drive the ego vehicle in it. Refer to Running Apollo 5.0 with SVL Simulator for more information. To stop the simulation, switch back to the SVL Simulator browser window, click Simulations and click Stop Simulation . API Only Simulation top # In this section we will create an API Only simulation which allows you to control the simulation from the command line using a Python script or Python scenario runner. Click Simulations . Click Add New to create a new simulation. On the New Simulation General pane, enter a name for your new simulation (e.g. \"API Only Mode\"), and select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Create test report if you wish to generate a test report to review for each simulation. Click the Next button to advance to the Test Case pane. On the New Simulation Test Case pane, select API Only from the Runtime Template list. In API Only mode, there are no additional parameters to specify. Click the Next button to advance to the Autopilot pane. Click the Next button to advance to the Publish pane. On the New Simulation Publish pane, click Publish to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created API Only simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button in drop-down menu on top-right corner to re-connect SVL Simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the API Only simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message \"API ready!\" indicating that simulator is ready to connect to your API script. You may now control the simulation using any Python script. Refer to Python API for information on using the Python API to control simulation. To stop the simulation, switch back to the SVL Simulator browser window, click Simulations and click Stop Simulation . Creating and running a Python Test Case Simulation top # In this section we will create and run a Python Test Case simulation available under \"Available from Others\" view in Simulations tab. Note that Python Test Case simulation currently requires the binary build of the latest SVL Simulator running on Linux. Contact us if you need to use this with a self-built or editor build of SVL Simulator. Click Simulations tab and then switch to Available from Others view to see simulations that are available for you to use. Type test-case into the search field and hit enter to show available test-case simulations. Locate the \"Cut-in Scenario\" simulation and click on it to view the details. You could of course select any test-case or other simulation that matches the autopilot software and version that you might be using, but this test-case simulation will work fine to demonstrate how to create a test-case simulation. Click Customize and Add Simulation button to add it to your library. Known Issue : Currently, Customize and Add Simulation doesn't clone the settings of \"Create Test Report\" button and \"Sensor Configuration\" value under vehicle to the new simulation. On the simulation General pane, you may customize the name of the simulation, description, and tags (if desired) but it is not necessary to do so. Select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Create test report to generate a test report to review after running this simulation. Click the Next button to advance to the Test Case pane. On the simulation Test Case pane, you will be see the Python script that controls this test case. Review the Python script and take note of the map specified in the sim.load statement (e.g. SanFrancisco ) as well as the ego vehicles' sensor configurations specified in ego = sim.add_agent statements (e.g. Lincoln2017MKZ vehicle with Apollo 5.0 (full analysis) sensor configuration ). Make sure that these appear in the Map , Vehicle and Sensor configuration fields. If they are not present in the drop down menus, add them to your library following the instructions in Store above. For this cut-in scenario simulation, there are no additional parameters to specify. Click the Next button to advance to the Autopilot pane, select Apollo 5.0 from the Autopilot list. In the Bridge IP, enter \"localhost:9090\" or you can replace \"localhost\" with the IP address of your autopilot system if it is different from the machine on which SVL Simulator is running. Click the Next button to advance to the Publish pane. On the simulation Publish pane, click \"Publish\" to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created Cut-in Scenario simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button in the drop-down menu on top right corner to re-connect SVL Simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the Cut-in Scenario simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message indicating that any newly added maps, vehicles and sensors are downloading, after which the simulation should initialize. At this point you should configure Apollo in Dreamview for the correct map and vehicle, enable the required modules, set a destination point, and send a routing request. Refer to Python Test Cases and Apollo 5.0 or Apollo master if needed for information on executing test case scenarios like cut-in with Apollo. The simulation should stop automatically when finished (or if it times out). When the simulation is finished, switch back to the SVL Simulator browser window. Click Test Results to view results of simulation runs. Review the list of test results and click View to view test results from a specific simulation run. Troubleshooting tips for Python runtime template based simulations If python script throws errors in simulation, follow guide here . Many examples of pythonAPI runtime scenarios can be found in PythonAPI repository on github. You can also find a quick example script here . For Python API simulation to work properly, it should include proper environment variables . Creating and running a Visual Scenario Editor based Simulation top # In this section we will create and run a Visual Scenario Editor based simulation. Note that Visual Scenario Editor based simulation currently requires the binary build of the latest SVL Simulator running on Linux. Contact us if you need to use this with a self-built or editor build of SVL Simulator. Launch SVL Simulator binary and create a scenario using Visual Scenario Editor and save the scenario as \"sample-vse-test-case.json\". Now on the web UI, click Simulations tab in the left panel. Click Add New to create a new simulation. On the New Simulation General pane, enter a name for your new simulation (e.g. \"VSE Simulation\"), and select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Interactive mode to disable interactive mode for this simulation as we want the runner to control the simulation execution automatically. Click the switch next to Create test report to enable cloud to create a test report at the end of simulation run with analytics and event data from sensors on your vehicle. Click the Next button to advance to the Test Case pane. On the New Simulation Test Case pane, select Visual scenario editor from the Runtime Template list. Next select the cloud shaped upload button for \"Scenario\" field and upload the Visual scenario editor created json file \"sample-vse-test-case.json\" Next make sure that map is automatically set properly to what you used when creating the scenario in Visual scenario editor tool. eg: BorregasAve Next make sure that vehicle is automatically set to what you used when creating the scenario in Visual scenario editor tool. eg: Lincoln2017MKZ vehicle and Apollo 5.0 sensor configuration IMPORTANT: If you have not already added a Map, Vehicle or needed Sensors to your library, refer to the Store above for information on how to add them. You could also specify date, time, and environmental settings here. Click the Next button to advance to the Autopilot pane. On the New Simulation Autopilot pane, select Apollo 5.0 from the Autopilot list. In the Bridge IP, enter \"localhost:9090\" or you can replace \"localhost\" with the IP address of your autopilot system if it is different from the machine on which SVL Simulator is running. Click the Next button to advance to the Publish pane. On the New Simulation Publish pane, click Publish to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created Visual scenario editor based simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button from drop-down menu on top-right corner to re-connect the simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the Visual scenario editor based simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message indicating that any newly added maps, vehicles and sensors are downloading, after which the simulation should initialize. Since this is Visual scenario editor based simulation, \"TestCaseRunner\" in background will take care of configuring, enabling & disabling the Autopilot automatically. If Apollo 5.0 is available and running, then SVL Simulator should connect to the Apollo bridge, and Apollo should be able to drive the ego vehicle in it. Refer to Running Apollo 5.0 with SVL Simulator for more information. The simulation should stop automatically when finished (or if it times out or errors). When the simulation is finished, switch back to the SVL Simulator browser window. Click Test Results to view results of simulation runs. Review the list of test results and click View to view test results from a specific simulation run. Known issues with Visual scenario editor based simulations The TestCaseRunner for VSE won't turn off apollo modules when the scenario finishes. You need to disable the modules once you finish the testing. If your VSE based simulation is not working correctly, please make sure apollo has corresponding HD maps (HD map folder name should match the map name) and vehicle calibration files, and bridge is running. Visual scenario editor template simulations with Autopilot only work on Linux at the moment. We are working on adding the Windows support. VSE based simulations only support Apollo 5.0 and above at the moment as it is based on dreamview-python-api .","title":"Running Simulator"},{"location":"running-simulations/running-simulator/#video","text":"","title":"Video"},{"location":"running-simulations/running-simulator/#store","text":"Click tabs under Store to view available maps, vehicles, and sensors that are available for you to use. You'll need to add maps and vehicles from the Store to your own library (or upload maps and vehicles of your own) before you can use them in a simulation. Scroll through the list of all Free maps available, or click Trending to see most commonly downloaded maps or click Shared with me to see private maps shared to you by other users. Locate the BorregasAve map and click \"+\" to add it to your library if not already added (for Random Interactive Simulation below). Locate the SanFrancisco map and click \"+\" to add it to your library if not already added (for Python Test Case Simulation below). Click any tab under Store (if needed) to return to the Store view. Scroll through the list of all Free vehicles available, or click Trending to see most commonly downloaded maps or click Shared with me to see private maps shared to you by other users. Locate the Lincoln2017MKZ vehicle and click \"+\" to add it to your library if not already added (for Random Interactive Simulation or Python Test Case Simulation below). Locate the added vehicle in your library and click on the \"Apollo 5.0\" sensor configuration to confirm if there are any missing sensors in your library (for Random Interactive Simulation below). Locate the added vehicle in your library and click on the \"Apollo 5.0 (full analysis)\" sensor configuration to confirm if there are any missing sensors in your library (for Python Test Case Simulation below). If any sensors are missing, then locate them under \"Sensors\" tab under Store and add them to your library OR follow the instructions on the sensor configuration page. NOTE: All test scripts load map by name or UUID and vehicle's sensor configuration by UUID. So take care to add the specific vehicles, sensors and maps to your library that are needed for the scripts or test cases that you intend to run. Please use UUID instead of name in your Python Test Case scripts. You can get UUID from the url when you open the asset profile in your browser.","title":"Add Assets from the Store"},{"location":"running-simulations/running-simulator/#mylibrary","text":"Click Library and then click Maps , Vehicles and Sensors to view the map, vehicle and sensors that you just added to your library. NOTE : You may notice some maps, vehicles and sensors in your library that you did not explicitly add; these were automatically added when your account was created.","title":"Review Assets in your Library"},{"location":"running-simulations/running-simulator/#randominteractivesimulation","text":"In this section we will create an Interactive Mode simulation which allows you to control the vehicle and interact with the environment while the simulation is running. Random Traffic Scenarios are simulations in which NPCs and pedestrians exhibit randomly generated behavior. This random behavior is deterministic, meaning that the same behavior will be repeated if the same random seed is used in the simulation set up. Click Simulations . Click Add New to create a new simulation. On the New Simulation General pane, enter a name for your new simulation (e.g. \"Random Interactive\"), and select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Interactive mode to enable interactive mode for this simulation, if you wish to be able to play/pause the simulation and change settings after the simulation has started. Click the switch next to Headless mode if you do not wish the simulation to be rendered on the screen. Click the switch next to Create test report to enable cloud to create a test report at the end of simulation run with analytics and event data from sensors on your vehicle. Click the Next button to advance to the Test Case pane. On the New Simulation Test Case pane, select Random Traffic from the Runtime Template list. Next select the BorregasAve map, the Lincoln2017MKZ vehicle and Apollo 5.0 sensor configuration from the dropdown lists. IMPORTANT: If you have not already added a Map, Vehicle or needed Sensors to your library, refer to the Store above for information on how to add them. (Optional) Click + Add another vehicle to add additional Ego vehicles. Note that each ego vehicle will need to be controlled by a separate AD stack and must have it's own bridge connection which would be specified in the Autopilot tab. You could also specify date, time, and environmental settings here but since we are creating an interactive simulation those can interactively be modified while the simulation is running. Click the switches next to Random Traffic and Random Pedestrians to enable these NPCs (non-player characters) in this random simulation. The Random Bicyclists feature is not yet supported and will be available in future releases. Enable Use Pre-defined Seed and manually enter a number to be used as the seed for all random behavior in the simulator. Using a pre-defined seed makes it possible to have deterministic and repeatable random simulations. Click the Next button to advance to the Autopilot pane. On the New Simulation Autopilot pane, select Apollo 5.0 from the Autopilot list. For all ego vehicles in your simulation, enter the details of where can simulator connect to Autopilot. In the Bridge IP, enter \"localhost:9090\" or you can replace \"localhost\" with the IP address of your autopilot system if it is different from the machine on which SVL Simulator is running. Click the Next button to advance to the Publish pane. On the New Simulation Publish pane, click Publish to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created Random Interactive simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button from drop-down menu on top-right corner to re-connect the simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the random interactive simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message indicating that any newly added maps, vehicles and sensors are downloading, after which the simulation should initialize. Since this is an interactive simulation, click the Play button at the bottom left corner of the screen to begin the simulation. You should see the random traffic driving and random pedestrians walking. Click the joystick icon at the bottom of the screen to get a list of keyboard controls. Click the settings icon to adjust the Environment settings. Click the eye icon to enable or disable individual sensor visualizations. If Apollo 5.0 is available and running, then SVL Simulator should connect to the Apollo bridge, and Apollo should be able to drive the ego vehicle in it. Refer to Running Apollo 5.0 with SVL Simulator for more information. To stop the simulation, switch back to the SVL Simulator browser window, click Simulations and click Stop Simulation .","title":"Create and run a random interactive simulation"},{"location":"running-simulations/running-simulator/#apionlysimulation","text":"In this section we will create an API Only simulation which allows you to control the simulation from the command line using a Python script or Python scenario runner. Click Simulations . Click Add New to create a new simulation. On the New Simulation General pane, enter a name for your new simulation (e.g. \"API Only Mode\"), and select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Create test report if you wish to generate a test report to review for each simulation. Click the Next button to advance to the Test Case pane. On the New Simulation Test Case pane, select API Only from the Runtime Template list. In API Only mode, there are no additional parameters to specify. Click the Next button to advance to the Autopilot pane. Click the Next button to advance to the Publish pane. On the New Simulation Publish pane, click Publish to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created API Only simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button in drop-down menu on top-right corner to re-connect SVL Simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the API Only simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message \"API ready!\" indicating that simulator is ready to connect to your API script. You may now control the simulation using any Python script. Refer to Python API for information on using the Python API to control simulation. To stop the simulation, switch back to the SVL Simulator browser window, click Simulations and click Stop Simulation .","title":"API Only Simulation"},{"location":"running-simulations/running-simulator/#pythontestcasesimulation","text":"In this section we will create and run a Python Test Case simulation available under \"Available from Others\" view in Simulations tab. Note that Python Test Case simulation currently requires the binary build of the latest SVL Simulator running on Linux. Contact us if you need to use this with a self-built or editor build of SVL Simulator. Click Simulations tab and then switch to Available from Others view to see simulations that are available for you to use. Type test-case into the search field and hit enter to show available test-case simulations. Locate the \"Cut-in Scenario\" simulation and click on it to view the details. You could of course select any test-case or other simulation that matches the autopilot software and version that you might be using, but this test-case simulation will work fine to demonstrate how to create a test-case simulation. Click Customize and Add Simulation button to add it to your library. Known Issue : Currently, Customize and Add Simulation doesn't clone the settings of \"Create Test Report\" button and \"Sensor Configuration\" value under vehicle to the new simulation. On the simulation General pane, you may customize the name of the simulation, description, and tags (if desired) but it is not necessary to do so. Select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Create test report to generate a test report to review after running this simulation. Click the Next button to advance to the Test Case pane. On the simulation Test Case pane, you will be see the Python script that controls this test case. Review the Python script and take note of the map specified in the sim.load statement (e.g. SanFrancisco ) as well as the ego vehicles' sensor configurations specified in ego = sim.add_agent statements (e.g. Lincoln2017MKZ vehicle with Apollo 5.0 (full analysis) sensor configuration ). Make sure that these appear in the Map , Vehicle and Sensor configuration fields. If they are not present in the drop down menus, add them to your library following the instructions in Store above. For this cut-in scenario simulation, there are no additional parameters to specify. Click the Next button to advance to the Autopilot pane, select Apollo 5.0 from the Autopilot list. In the Bridge IP, enter \"localhost:9090\" or you can replace \"localhost\" with the IP address of your autopilot system if it is different from the machine on which SVL Simulator is running. Click the Next button to advance to the Publish pane. On the simulation Publish pane, click \"Publish\" to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created Cut-in Scenario simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button in the drop-down menu on top right corner to re-connect SVL Simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the Cut-in Scenario simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message indicating that any newly added maps, vehicles and sensors are downloading, after which the simulation should initialize. At this point you should configure Apollo in Dreamview for the correct map and vehicle, enable the required modules, set a destination point, and send a routing request. Refer to Python Test Cases and Apollo 5.0 or Apollo master if needed for information on executing test case scenarios like cut-in with Apollo. The simulation should stop automatically when finished (or if it times out). When the simulation is finished, switch back to the SVL Simulator browser window. Click Test Results to view results of simulation runs. Review the list of test results and click View to view test results from a specific simulation run. Troubleshooting tips for Python runtime template based simulations If python script throws errors in simulation, follow guide here . Many examples of pythonAPI runtime scenarios can be found in PythonAPI repository on github. You can also find a quick example script here . For Python API simulation to work properly, it should include proper environment variables .","title":"Creating and running a Python Test Case Simulation"},{"location":"running-simulations/running-simulator/#vsesimulation","text":"In this section we will create and run a Visual Scenario Editor based simulation. Note that Visual Scenario Editor based simulation currently requires the binary build of the latest SVL Simulator running on Linux. Contact us if you need to use this with a self-built or editor build of SVL Simulator. Launch SVL Simulator binary and create a scenario using Visual Scenario Editor and save the scenario as \"sample-vse-test-case.json\". Now on the web UI, click Simulations tab in the left panel. Click Add New to create a new simulation. On the New Simulation General pane, enter a name for your new simulation (e.g. \"VSE Simulation\"), and select the cluster you created during installation (e.g. \"local-sim\") from the cluster list. Click the switch next to Interactive mode to disable interactive mode for this simulation as we want the runner to control the simulation execution automatically. Click the switch next to Create test report to enable cloud to create a test report at the end of simulation run with analytics and event data from sensors on your vehicle. Click the Next button to advance to the Test Case pane. On the New Simulation Test Case pane, select Visual scenario editor from the Runtime Template list. Next select the cloud shaped upload button for \"Scenario\" field and upload the Visual scenario editor created json file \"sample-vse-test-case.json\" Next make sure that map is automatically set properly to what you used when creating the scenario in Visual scenario editor tool. eg: BorregasAve Next make sure that vehicle is automatically set to what you used when creating the scenario in Visual scenario editor tool. eg: Lincoln2017MKZ vehicle and Apollo 5.0 sensor configuration IMPORTANT: If you have not already added a Map, Vehicle or needed Sensors to your library, refer to the Store above for information on how to add them. You could also specify date, time, and environmental settings here. Click the Next button to advance to the Autopilot pane. On the New Simulation Autopilot pane, select Apollo 5.0 from the Autopilot list. In the Bridge IP, enter \"localhost:9090\" or you can replace \"localhost\" with the IP address of your autopilot system if it is different from the machine on which SVL Simulator is running. Click the Next button to advance to the Publish pane. On the New Simulation Publish pane, click Publish to save (and publish ) the simulation to your library. The simulation will be private and only accessible from your account. You should now see the Simulations view, with the newly created Visual scenario editor based simulation. The simulation should indicate Idle as shown. If instead it says Offline , return to the SVL Simulator program window and click the Online button from drop-down menu on top-right corner to re-connect the simulator to the cloud. You can then switch back to the SVL Simulator browser window or click Open Browser in the Simulator (and then click Simulations to return to the Simulations view). To run the Visual scenario editor based simulation you just created, click Run Simulation . Switch back to the SVL Simulator and you may see a message indicating that any newly added maps, vehicles and sensors are downloading, after which the simulation should initialize. Since this is Visual scenario editor based simulation, \"TestCaseRunner\" in background will take care of configuring, enabling & disabling the Autopilot automatically. If Apollo 5.0 is available and running, then SVL Simulator should connect to the Apollo bridge, and Apollo should be able to drive the ego vehicle in it. Refer to Running Apollo 5.0 with SVL Simulator for more information. The simulation should stop automatically when finished (or if it times out or errors). When the simulation is finished, switch back to the SVL Simulator browser window. Click Test Results to view results of simulation runs. Review the list of test results and click View to view test results from a specific simulation run. Known issues with Visual scenario editor based simulations The TestCaseRunner for VSE won't turn off apollo modules when the scenario finishes. You need to disable the modules once you finish the testing. If your VSE based simulation is not working correctly, please make sure apollo has corresponding HD maps (HD map folder name should match the map name) and vehicle calibration files, and bridge is running. Visual scenario editor template simulations with Autopilot only work on Linux at the moment. We are working on adding the Windows support. VSE based simulations only support Apollo 5.0 and above at the moment as it is based on dreamview-python-api .","title":"Creating and running a Visual Scenario Editor based Simulation"},{"location":"running-simulations/runtime-container/","text":"Runtime Container","title":"Runtime Container"},{"location":"running-simulations/runtime-templates/","text":"Runtime Templates The SVL Simulator has four Runtime Templates . When creating a simulation, a user must choose one of these templates to build their simulation on. The four runtime templates are: Random Traffic - Simulations where traffic (NPCs and pedestrians) behavior is determined by a random seed. Visual Scenario Editor - A template for running simulations created with the Visual Scenario Editor . The simulation is created by uploading a scenario file outputted by VSE directly to the Web UI. Visual Scenario Editor tool is supported on both Windows and Linux, but the end-to-end VSE runtime simulation from webUI is currently only supported on Linux. Python API - A template for running Python API simulations directly from the Web UI. Users can paste Python code directly into the Web UI to quickly create simulations/test cases. Python Api template is supported on both Windows and Linux; however, end-to-end integration with an AD stack such as Apollo using python dreamview-api is only supported on Linux. API-Only - A template for creating a simulation that is controlled by Python scripts located on the users machine.","title":"Runtime templates"},{"location":"simulation-content/add-destinations/","text":"Adding Ego Vehicle Destinations to a Map It is possible to define fixed destination points in a map. These fixed destination points can be fetched by the Python API and passed on to the AD stack. This enables developers to run a quick test case on any given map. This guide provides instructions on how to properly define destination points. The guide is for users who will be using the SVL Simulator in Developer Mode , creating custom maps, or editing existing maps in the Unity Editor. Choosing the location for the destination point top # All locations on a given map are not valid destination points for an AD stack. Given a starting point and a destination, AD stacks will search through the HD map to find a route that will connect these two points. Although intuitively it may seem that any point on a road would be a valid destination, this is not necessarily the case since all roads in the maps are not necessarily connected, and even if they are connected they may require violation of traffic rules. To solve this problem, destination points are linked to spawn points in the simulator. Each spawn point maintains a list of destinations that are valid for that particular spawn point (details will be covered later). To confirm that a location is accessible from a particular starting point on the map: Open the map annotations tools by selecting Annotate HD Map under the Simulator menu in the Unity editor or use the keyboard shortcut Shift + Alt + M . In the new window, click the View All button to visualize all annotations. Trace a path between each spawn and destination point along the map annotations to confirm a path exists. Below is an example of map annotations in the simulator. The cyan colored markings are the lane center annotations with the arrows indicating the direction of travel. These markings can be traced to validate that a destination point is valid for a given spawn point. Bear in mind that arriving at a given destination may only be possible with one or more lane changes. Creating a destination object top # Once a location is selected, to create a destination point, and empty GameObject must be created at the desired location. Give the GameObject a name that identifies it as a destination object such as DestInfo . Some AD stacks take the orientation of destination points into account in addition to their position; therefore, it is important to make sure that the Z-axis of the GameObject is aligned to the direction the vehicle should be pointing when it arrives at the destination. You should also make sure that the Y-axis position (altitude) of the GameObject places it on the road surface. Below is a top-down view of a destination point (the blue and red axes) correctly aligned to the underlying map annotations. In the 'Inspector' tab, click 'Add Component' and select the Destinationinfo script as seen below. Repeat this step for any other destination point you wish to create on the map. Linking destinations to spawn points top # Spawn points are defined by the SpawnInfo GameObjects in the scene. Each SpawnInfo can hold a list of references to destination points that an ego vehicle should be able to drive to from the spawn point. A single destination point can be referenced by multiple spawn points. To add a destination to the list of destinations for a particular spawn point, select the spawn point and increment the size of its destinations list by one in the Inspector tab: An element will be added to the list ( Element 0 in the image above). Drag the desired destination object from the hierarchy tree to the new element: Repeat this step to assign any destination point to any spawn point. Once all destinations have been added and linked to their coorresponding spawn points, save the scene and move on to building the scene and uploading it to the cloud for use with the simulator. Accessing the destinations using Python API top # The Python API exposes the list of destinations for each Spawn object in Python. Each destination is represented as a transform object that has position and rotation attributes. Example: spawns = sim.get_spawn() # returns a list of spawn points destinations = spawns[0].destinations # list of destinations for the first spawn point","title":"Adding destinations to a map"},{"location":"simulation-content/add-destinations/#choosing-the-location-for-the-destination-point","text":"All locations on a given map are not valid destination points for an AD stack. Given a starting point and a destination, AD stacks will search through the HD map to find a route that will connect these two points. Although intuitively it may seem that any point on a road would be a valid destination, this is not necessarily the case since all roads in the maps are not necessarily connected, and even if they are connected they may require violation of traffic rules. To solve this problem, destination points are linked to spawn points in the simulator. Each spawn point maintains a list of destinations that are valid for that particular spawn point (details will be covered later). To confirm that a location is accessible from a particular starting point on the map: Open the map annotations tools by selecting Annotate HD Map under the Simulator menu in the Unity editor or use the keyboard shortcut Shift + Alt + M . In the new window, click the View All button to visualize all annotations. Trace a path between each spawn and destination point along the map annotations to confirm a path exists. Below is an example of map annotations in the simulator. The cyan colored markings are the lane center annotations with the arrows indicating the direction of travel. These markings can be traced to validate that a destination point is valid for a given spawn point. Bear in mind that arriving at a given destination may only be possible with one or more lane changes.","title":"Choosing the location for the destination point"},{"location":"simulation-content/add-destinations/#creating-a-destination-object","text":"Once a location is selected, to create a destination point, and empty GameObject must be created at the desired location. Give the GameObject a name that identifies it as a destination object such as DestInfo . Some AD stacks take the orientation of destination points into account in addition to their position; therefore, it is important to make sure that the Z-axis of the GameObject is aligned to the direction the vehicle should be pointing when it arrives at the destination. You should also make sure that the Y-axis position (altitude) of the GameObject places it on the road surface. Below is a top-down view of a destination point (the blue and red axes) correctly aligned to the underlying map annotations. In the 'Inspector' tab, click 'Add Component' and select the Destinationinfo script as seen below. Repeat this step for any other destination point you wish to create on the map.","title":"Creating a destination object"},{"location":"simulation-content/add-destinations/#linking-destinations-to-spawn-points","text":"Spawn points are defined by the SpawnInfo GameObjects in the scene. Each SpawnInfo can hold a list of references to destination points that an ego vehicle should be able to drive to from the spawn point. A single destination point can be referenced by multiple spawn points. To add a destination to the list of destinations for a particular spawn point, select the spawn point and increment the size of its destinations list by one in the Inspector tab: An element will be added to the list ( Element 0 in the image above). Drag the desired destination object from the hierarchy tree to the new element: Repeat this step to assign any destination point to any spawn point. Once all destinations have been added and linked to their coorresponding spawn points, save the scene and move on to building the scene and uploading it to the cloud for use with the simulator.","title":"Linking destinations to spawn points"},{"location":"simulation-content/add-destinations/#accessing-the-destinations-using-python-api","text":"The Python API exposes the list of destinations for each Spawn object in Python. Each destination is represented as a transform object that has position and rotation attributes. Example: spawns = sim.get_spawn() # returns a list of spawn points destinations = spawns[0].destinations # list of destinations for the first spawn point","title":"Accessing the destinations using Python API"},{"location":"simulation-content/add-new-ego-vehicle/","text":"How to Add a New Ego Vehicle This document describes how to create a new ego vehicle in the SVL Simulator. Video top # ( Link ) Adding a new ego vehicle in SVL Simulator. Asset Preparation top # Open vehicle mesh in 3D modeling software such as Blender or Maya. Edit root node to origin z forward. Blender will need .fbx options changed to correct for different axis orientation. Unity is Z forward and Y up. Create a simple mesh from the main body mesh and name it collider. This needs to be less than 256 polygons. Name the body mesh 'Body'. Separate wheel meshes and name them FrontRightWheel, FrontLeftWheel, RearLeftWheel, RearRightWheel. Center pivots on each wheel mesh z forward. Create an empty node called 'LightsGroup' to hold all light emitting meshes, e.g., HeadLights, BrakeLights, RightTurnIndicator, LeftTurnIndicator, ReverseIndicator. These must be separate meshes with separate materials and named exactly the same. Export mesh as an .fbx. If you have applied textures and materials in the modeling software, check the embed media option so these files are included in the file and you can extract in Unity. Getting Started top # Launch SVL Simulator from the Unity Editor (as described here ). Open any scene to work in or create a new one. Create a folder for the new vehicle Assets/External/Vehicles/YourNewEgoVehicle/. Create two folders in this new folder Assets/External/Vehicles/YourNewEgoVehicle/Models/Materials/. Import the vehicle .fbx in Assets/External/Vehicles/YourNewEgoVehicle/Models. In the Unity mesh importer, toggle off all animations, rigs. Export materials and textures into Assets/External/Vehicles/YourNewEgoVehicle/Models/Materials. Convert all LightsGroup mesh materials shaders to Shader Graphs/VehicleLightsShader. You may need to adjust the emission color. Rename the LightsGroup mesh materials to HeadLightsMat, LeftIndicatorMat, RightIndicatorMat, ReverseIndicatorMat, BrakeLightsMat. Setup the Vehicle top # Create a new empty root game object for your vehicle and give it a name. YourNewEgoVehicle Reset transform. Unity can place the game object at random places so it is best to have the object at origin. Place vehicle meshes as a child of the root game object. Be sure that it is at local position origin. Drag YourNewEgoVehicle from the inspector panel into the project panel in Assets/External/Vehicles/YourNewEgoVehicle. This creates a prefab in Unity. Now if you make changes to the mesh, it will stay separate so you won't need to remake the prefab. Toggle off the mesh renderer Unity component for the Collider mesh. Add a mesh collider and toggle convex. Add the Jaguar2015XE open source vehicle to existing ego vehicle prefab to the hierarchy. This makes it easy to copy components to the new vehicle. Make sure the new vehicle prefab is at origin and overlaps with the Jaguar2015XE vehicle. This makes it easy to align light objects and wheel colliders. Unpack Jaguar2015XE prefab completely. We will use the needed components from this vehicle. Move the WheelColliderHolder object to the new vehicle as a child of the parent object. Move HeadLights, BrakeLights, IndicatorsLeft, IndicatorsRight, IndicatorsReverse and InteriorLights holder objects as a child of the new vehicle parent object. Copy all Unity components from the root object of the Jaguar2015XE to the root object of your new vehicle. Right click on each component on the reference prefab root, right click to copy and then right click on the root of the new vehicle to paste component as new. Do this for every component. Delete the Jaguar2015XE from the scene, it is no longer needed. DO NOT SAVE CHANGES in the Jaguar2015XE Reset transforms for WheelColliderHolder, HeadLights, BrakeLights, IndicatorsLeft, IndicatorsRight, IndicatorsReverse and InteriorLights objects so they align to the new vehicle. Align each wheel collider object to each wheel and align lights to the new vehicle light meshes. Create an empty object called BaseLink as a child of the root object of the new vehicle. Move BaseLink so it aligns with the center of the back axle of the new vehicle. Apply the component BaseLink to the BaseLink object. NOTE Not in video. Drag the BaseLink object to the BaseLink public reference in the VehicleSMI component. NOTE Be sure you have a MeshCollider component on the Collider mesh, the Collision mesh renderer component disabled and the collider has convex enabled. Apply the Correct References top # We need to apply the correct references to the vehicle scripts since the public variables are still referencing the other vehicle: Note: Be sure to click the Apply button in the Inspector after each component change. This saves the change to the prefab in the project. Check that no variable references are still bold after updating the references. VehicleSMI.cs top # For the VehicleSMI script, reference the following colliders and meshes in the Axles dropdown. Reference the FL WheelCollider in Axles Element 0 (Left) Reference the FR WheelCollider in Axles Element 0 (Right) Reference the RL WheelCollider in Axles Element 1 (Left) Reference the RR WheelCollider in Axles Element 1 (Right) Reference the FrontLeftWheel wheel mesh in Axles Element 0 (Left Visuals) Reference the FrontRightWheel wheel mesh in Axles Element 0 (Right Visuals) Reference the RearLeftWheel wheel mesh in Axles Element 1 (Left Visuals) Reference the RearRightWheel wheel mesh in Axles Element 1 (Right Visuals) VehicleActions.cs top # Duplicate the High and Low beam textures in the Jaguar2015XE and place in Assets/External/Vehicles/YourNewEgoVehicle/Models/Materials. Rename the prefix to the name of your vehicle, e.g., YourNewEgoVehicleHeadLightHighBeamCookie. NOTE Be sure the texture in set to 256 compression in the texture settings. Drag the textures into the VehicleActions.cs public references in the inspector panel on the new vehicle. Apply changes to the vehicle prefab. Final Steps top # Set the vehicle root and all child objects to the Agent layer. Set the vehicle root to the Player tag Apply changes to the vehicle prefab. Open Simulator -> Build... menu at the top of the Unity editor Be sure you have both Windows and Linux support to make asset bundles. Check the new vehicle and press build. When completed, in the root of the repo, YourNewEgoVehicle bundle will be in Simulator/AssetBundles/Vehicles/ Run simulator, open Library -> Vehicles tab in WebUI and select Add New . Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Create a new sensor configuration. Publish the NewEgoVehicle. Congratulations! You have successfully added a new ego vehicle! It can now be used with any simulation you create.","title":"Creating a new ego vehicle"},{"location":"simulation-content/add-new-ego-vehicle/#video","text":"( Link ) Adding a new ego vehicle in SVL Simulator.","title":"Video"},{"location":"simulation-content/add-new-ego-vehicle/#asset-preparation","text":"Open vehicle mesh in 3D modeling software such as Blender or Maya. Edit root node to origin z forward. Blender will need .fbx options changed to correct for different axis orientation. Unity is Z forward and Y up. Create a simple mesh from the main body mesh and name it collider. This needs to be less than 256 polygons. Name the body mesh 'Body'. Separate wheel meshes and name them FrontRightWheel, FrontLeftWheel, RearLeftWheel, RearRightWheel. Center pivots on each wheel mesh z forward. Create an empty node called 'LightsGroup' to hold all light emitting meshes, e.g., HeadLights, BrakeLights, RightTurnIndicator, LeftTurnIndicator, ReverseIndicator. These must be separate meshes with separate materials and named exactly the same. Export mesh as an .fbx. If you have applied textures and materials in the modeling software, check the embed media option so these files are included in the file and you can extract in Unity.","title":"Asset Preparation"},{"location":"simulation-content/add-new-ego-vehicle/#getting-started","text":"Launch SVL Simulator from the Unity Editor (as described here ). Open any scene to work in or create a new one. Create a folder for the new vehicle Assets/External/Vehicles/YourNewEgoVehicle/. Create two folders in this new folder Assets/External/Vehicles/YourNewEgoVehicle/Models/Materials/. Import the vehicle .fbx in Assets/External/Vehicles/YourNewEgoVehicle/Models. In the Unity mesh importer, toggle off all animations, rigs. Export materials and textures into Assets/External/Vehicles/YourNewEgoVehicle/Models/Materials. Convert all LightsGroup mesh materials shaders to Shader Graphs/VehicleLightsShader. You may need to adjust the emission color. Rename the LightsGroup mesh materials to HeadLightsMat, LeftIndicatorMat, RightIndicatorMat, ReverseIndicatorMat, BrakeLightsMat.","title":"Getting Started"},{"location":"simulation-content/add-new-ego-vehicle/#setup-the-vehicle","text":"Create a new empty root game object for your vehicle and give it a name. YourNewEgoVehicle Reset transform. Unity can place the game object at random places so it is best to have the object at origin. Place vehicle meshes as a child of the root game object. Be sure that it is at local position origin. Drag YourNewEgoVehicle from the inspector panel into the project panel in Assets/External/Vehicles/YourNewEgoVehicle. This creates a prefab in Unity. Now if you make changes to the mesh, it will stay separate so you won't need to remake the prefab. Toggle off the mesh renderer Unity component for the Collider mesh. Add a mesh collider and toggle convex. Add the Jaguar2015XE open source vehicle to existing ego vehicle prefab to the hierarchy. This makes it easy to copy components to the new vehicle. Make sure the new vehicle prefab is at origin and overlaps with the Jaguar2015XE vehicle. This makes it easy to align light objects and wheel colliders. Unpack Jaguar2015XE prefab completely. We will use the needed components from this vehicle. Move the WheelColliderHolder object to the new vehicle as a child of the parent object. Move HeadLights, BrakeLights, IndicatorsLeft, IndicatorsRight, IndicatorsReverse and InteriorLights holder objects as a child of the new vehicle parent object. Copy all Unity components from the root object of the Jaguar2015XE to the root object of your new vehicle. Right click on each component on the reference prefab root, right click to copy and then right click on the root of the new vehicle to paste component as new. Do this for every component. Delete the Jaguar2015XE from the scene, it is no longer needed. DO NOT SAVE CHANGES in the Jaguar2015XE Reset transforms for WheelColliderHolder, HeadLights, BrakeLights, IndicatorsLeft, IndicatorsRight, IndicatorsReverse and InteriorLights objects so they align to the new vehicle. Align each wheel collider object to each wheel and align lights to the new vehicle light meshes. Create an empty object called BaseLink as a child of the root object of the new vehicle. Move BaseLink so it aligns with the center of the back axle of the new vehicle. Apply the component BaseLink to the BaseLink object. NOTE Not in video. Drag the BaseLink object to the BaseLink public reference in the VehicleSMI component. NOTE Be sure you have a MeshCollider component on the Collider mesh, the Collision mesh renderer component disabled and the collider has convex enabled.","title":"Setup the Vehicle"},{"location":"simulation-content/add-new-ego-vehicle/#apply-the-correct-references","text":"We need to apply the correct references to the vehicle scripts since the public variables are still referencing the other vehicle: Note: Be sure to click the Apply button in the Inspector after each component change. This saves the change to the prefab in the project. Check that no variable references are still bold after updating the references.","title":"Apply the Correct References"},{"location":"simulation-content/add-new-ego-vehicle/#vehiclesmi.cs","text":"For the VehicleSMI script, reference the following colliders and meshes in the Axles dropdown. Reference the FL WheelCollider in Axles Element 0 (Left) Reference the FR WheelCollider in Axles Element 0 (Right) Reference the RL WheelCollider in Axles Element 1 (Left) Reference the RR WheelCollider in Axles Element 1 (Right) Reference the FrontLeftWheel wheel mesh in Axles Element 0 (Left Visuals) Reference the FrontRightWheel wheel mesh in Axles Element 0 (Right Visuals) Reference the RearLeftWheel wheel mesh in Axles Element 1 (Left Visuals) Reference the RearRightWheel wheel mesh in Axles Element 1 (Right Visuals)","title":"VehicleSMI.cs"},{"location":"simulation-content/add-new-ego-vehicle/#vehicleactions.cs","text":"Duplicate the High and Low beam textures in the Jaguar2015XE and place in Assets/External/Vehicles/YourNewEgoVehicle/Models/Materials. Rename the prefix to the name of your vehicle, e.g., YourNewEgoVehicleHeadLightHighBeamCookie. NOTE Be sure the texture in set to 256 compression in the texture settings. Drag the textures into the VehicleActions.cs public references in the inspector panel on the new vehicle. Apply changes to the vehicle prefab.","title":"VehicleActions.cs"},{"location":"simulation-content/add-new-ego-vehicle/#final-steps","text":"Set the vehicle root and all child objects to the Agent layer. Set the vehicle root to the Player tag Apply changes to the vehicle prefab. Open Simulator -> Build... menu at the top of the Unity editor Be sure you have both Windows and Linux support to make asset bundles. Check the new vehicle and press build. When completed, in the root of the repo, YourNewEgoVehicle bundle will be in Simulator/AssetBundles/Vehicles/ Run simulator, open Library -> Vehicles tab in WebUI and select Add New . Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Create a new sensor configuration. Publish the NewEgoVehicle. Congratulations! You have successfully added a new ego vehicle! It can now be used with any simulation you create.","title":"Final Steps"},{"location":"simulation-content/add-new-map/","text":"How to Add a New Map This document will describe how to create a new map in the SVL Simulator. Video top # ( Link ) Adding a new map in SVL Simulator. Getting Started top # The following text is a list of the steps described in the above video. Launch SVL Simulator from the Unity Editor (as described here ). In the top menu, select File -> New Scene or Ctrl+N Create your new map folders in Assets/External/Environments/NewMap with /Models/Materials and /Prefabs in this root folder It is helpful to clone the CubeTown repository as an example scene to see how all Simulator features are setup and assets organized. Import Assets top # Import assets you created or obtained into the Unity Editor. Move models, textures, materials and any other assets you need into the /NewMap folders. The new map should have no asset dependencies from folders outside of /NewMap. Do not include script files or non-HDRP shaders. If you need scripts in your scene, you will need to place in the source files of the SVL Simulator and build your own binaries. If your assets have non-standard shaders you will need to change them to the Unity Standard shader before converting to HDRP. Once all non Standard shaders are changed, you can convert the materials to HDRP materials. Select all materials. Select Edit -> Render Pipeline -> Upgrade Selected Materials to High Definition Materials. If the materials are all using Standard shaders, all shader settings will be converted with little need for adjustment. Be sure all materials have GPU instancing checked. Now we can start building the new map. Creating the scene top # Delete the MapHolder object in the Hierarchy Panel. This will be added back with the annotation tool later. Create an Object to hold the mesh data for your map. Name it the same as your folder, e.g. NewMap. Zero out the transform position/rotation: 0,0,0 and scale: 1,1,1. Save the scene in the root of the /NewMap, naming exactly the same, NewMap.scene. Delete or move any other scene files out of this root folder. Open the Annotation Tool, select Simulator -> Annotation Tool, and click Create Map Holder. This will add the correct holder for annotation named already the same as your map scene name. Save the scene again. Do this often. You should always be hitting Ctrl+S during development. Create the scene by adding meshes and colliders to the NewMap mesh holder object as children. You can further organize by making secondary holder objects, e.g., Roads, Buildings, Props. Just be sure to zero transforms for holder objects. Once organized and positioned, select the root holder object and check static in the top right of the Inspector Panel. Then, open the static menu and uncheck Off Mesh Link Generation and Batching Static. HDRP supports GPU instancing materials OR Batching, not both. GPU instancing is more performant. Then select all mesh holder root objects that you do not want pedestrians to walk on, e.g., Props, Buildings, Signs, and uncheck Navigation Static. This will make Nav Meshes ignore these mesh types. Add the TimeOfDayLight.cs component to all meshes that will emit light, not building windows. Then add a light component object as a child of this light. Add TimeOfDayBuilding.cs to the building holder. Be sure you apply black textures to the emission component of the meshes you don't want to emit and create an emission map for the ones you do want to light during the night. Set semantic tags for the holder objects OR the objects themselves, not both. Code will search and apply tags to materials from parent, down to all child objects. Set physics layers for all meshes to default. Be sure to do this from the root object so all child objects get set. Purchased assets may have very odd tags and physics layers that you will need to fix. Change all road material shaders to ShaderGraphs -> EnvironmentSimulation for environmental effects to work on road surfaces. Add mesh colliders to all meshes that need physics collisions, especially roads. Extra Steps not in video top # Select Window -> AI -> Navigation. Select Object tab in this panel. Select groups of meshes, not their holders, e.g., road, sidewalk, grass, curb meshes. In the Object tab in Navigation, set the meshes to the types. Select the Bake tab. Click the Bake button. This creates a Nav mesh that gets saved in the /NewMap folder. Select Window -> Rendering -> Occlusion Culling. Select Bake tab. Click the Bake button. This creates an occlusion object that helps with frame rate. Finalize other map features top # Select MapOrigin object in the Hierarchy Panel. Set Northing, Easting, UTM and TimeZone. Set NPC settings for the map. Edit SpawnInfo position and rotation. This object(s) will determine where the EGO vehicle(s) will spawn. If you want multiple spawn points, duplicate this object and move to another position on the map. If importing or creating HDMap annotations, rotate all map objects so map North faces in the -X coordinate in Unity Scene View (see here ). Use the simulator tool to rotate the scene to the correct position at Simulator -> Editor Tools -> Scene Type -> RotateSceneView and then the Run button. Make all scene objects a child of a empty game object at origin, rotate and then remove parent. Save the scene. Select Simulator -> Build... Select NewMap and click the Build button.","title":"Creating a new map"},{"location":"simulation-content/add-new-map/#video","text":"( Link ) Adding a new map in SVL Simulator.","title":"Video"},{"location":"simulation-content/add-new-map/#getting-started","text":"The following text is a list of the steps described in the above video. Launch SVL Simulator from the Unity Editor (as described here ). In the top menu, select File -> New Scene or Ctrl+N Create your new map folders in Assets/External/Environments/NewMap with /Models/Materials and /Prefabs in this root folder It is helpful to clone the CubeTown repository as an example scene to see how all Simulator features are setup and assets organized.","title":"Getting Started"},{"location":"simulation-content/add-new-map/#import-assets","text":"Import assets you created or obtained into the Unity Editor. Move models, textures, materials and any other assets you need into the /NewMap folders. The new map should have no asset dependencies from folders outside of /NewMap. Do not include script files or non-HDRP shaders. If you need scripts in your scene, you will need to place in the source files of the SVL Simulator and build your own binaries. If your assets have non-standard shaders you will need to change them to the Unity Standard shader before converting to HDRP. Once all non Standard shaders are changed, you can convert the materials to HDRP materials. Select all materials. Select Edit -> Render Pipeline -> Upgrade Selected Materials to High Definition Materials. If the materials are all using Standard shaders, all shader settings will be converted with little need for adjustment. Be sure all materials have GPU instancing checked. Now we can start building the new map.","title":"Import Assets"},{"location":"simulation-content/add-new-map/#creating-the-scene","text":"Delete the MapHolder object in the Hierarchy Panel. This will be added back with the annotation tool later. Create an Object to hold the mesh data for your map. Name it the same as your folder, e.g. NewMap. Zero out the transform position/rotation: 0,0,0 and scale: 1,1,1. Save the scene in the root of the /NewMap, naming exactly the same, NewMap.scene. Delete or move any other scene files out of this root folder. Open the Annotation Tool, select Simulator -> Annotation Tool, and click Create Map Holder. This will add the correct holder for annotation named already the same as your map scene name. Save the scene again. Do this often. You should always be hitting Ctrl+S during development. Create the scene by adding meshes and colliders to the NewMap mesh holder object as children. You can further organize by making secondary holder objects, e.g., Roads, Buildings, Props. Just be sure to zero transforms for holder objects. Once organized and positioned, select the root holder object and check static in the top right of the Inspector Panel. Then, open the static menu and uncheck Off Mesh Link Generation and Batching Static. HDRP supports GPU instancing materials OR Batching, not both. GPU instancing is more performant. Then select all mesh holder root objects that you do not want pedestrians to walk on, e.g., Props, Buildings, Signs, and uncheck Navigation Static. This will make Nav Meshes ignore these mesh types. Add the TimeOfDayLight.cs component to all meshes that will emit light, not building windows. Then add a light component object as a child of this light. Add TimeOfDayBuilding.cs to the building holder. Be sure you apply black textures to the emission component of the meshes you don't want to emit and create an emission map for the ones you do want to light during the night. Set semantic tags for the holder objects OR the objects themselves, not both. Code will search and apply tags to materials from parent, down to all child objects. Set physics layers for all meshes to default. Be sure to do this from the root object so all child objects get set. Purchased assets may have very odd tags and physics layers that you will need to fix. Change all road material shaders to ShaderGraphs -> EnvironmentSimulation for environmental effects to work on road surfaces. Add mesh colliders to all meshes that need physics collisions, especially roads.","title":"Creating the scene"},{"location":"simulation-content/add-new-map/#extra-steps-not-in-video","text":"Select Window -> AI -> Navigation. Select Object tab in this panel. Select groups of meshes, not their holders, e.g., road, sidewalk, grass, curb meshes. In the Object tab in Navigation, set the meshes to the types. Select the Bake tab. Click the Bake button. This creates a Nav mesh that gets saved in the /NewMap folder. Select Window -> Rendering -> Occlusion Culling. Select Bake tab. Click the Bake button. This creates an occlusion object that helps with frame rate.","title":"Extra Steps not in video"},{"location":"simulation-content/add-new-map/#finalize-other-map-features","text":"Select MapOrigin object in the Hierarchy Panel. Set Northing, Easting, UTM and TimeZone. Set NPC settings for the map. Edit SpawnInfo position and rotation. This object(s) will determine where the EGO vehicle(s) will spawn. If you want multiple spawn points, duplicate this object and move to another position on the map. If importing or creating HDMap annotations, rotate all map objects so map North faces in the -X coordinate in Unity Scene View (see here ). Use the simulator tool to rotate the scene to the correct position at Simulator -> Editor Tools -> Scene Type -> RotateSceneView and then the Run button. Make all scene objects a child of a empty game object at origin, rotate and then remove parent. Save the scene. Select Simulator -> Build... Select NewMap and click the Build button.","title":"Finalize other map features"},{"location":"simulation-content/assets/","text":"Adding Assets The main repository for the SVL Simulator does not contain art assets. We have moved these assets to external repositories so users can easily add their own. Adding assets is only supported when cloning simulator source with Unity Editor in developer mode . Currently there are several open-source examples. Environments: CubeTown SingleLaneRoad Shalun SanFrancisco Vehicles: Jaguar2015XE NPCs: DefaultNPC Pedestrians: Walkers Table of Contents Adding an Asset Setup an Asset Building Assets Adding an Asset top # All assets have been removed from Simulator source code. When working in developer mode and building custom binaries, users must build assets locally for npcs and pedestrians. Assets need to be cloned into a specific location in the project: simulator/Assets/External/Environments for Environments simulator/Assets/External/Vehicles for Vehicles simulator/Assets/External/NPCs for NPCs and Bicycles simulator/Assets/External/Pedestrians for Walkers, Scooters and Animals Clone the desired asset into the appropriate folder. Do not change the name of the folder that the asset is cloned into, it must match the name of the asset. For environments: simulator/Assets/External/Environments/Mars must contain simulator/Assets/External/Environments/Mars/Mars.unity For vehicles: simulator/Assets/External/Vehicles/Rover must contain simulator/Assets/External/Vehicles/Rover/Rover.prefab For npcs: simulator/Assets/External/NPCs/DefaultNPC must contain simulator/Assets/External/NPCs/DefaultNPC/Jeep/Jeep.prefab For pedestrians: simulator/Assets/External/Pedestrians/Walkers must contain simulator/Assets/External/Pedestrians/Walkers/Bob/Bob.prefab Setup an Asset top # For map assets setup see here . For ego vehicle asset setup see here . For npc asset setup see here . For pedestrian asset setup see here . Building Assets top # Assets are built using the same build script as the simulator. Follow the build instructions through step 17. NPCs and Pedestrians must be built locally for custom binaries. They must be added to AssetBundles folder to be loaded by the simulator binary or in editor source in developer mode . IMPORTANT Windows and Linux support must be installed with Unity to build assetbundles","title":"Building content"},{"location":"simulation-content/assets/#adding-an-asset","text":"All assets have been removed from Simulator source code. When working in developer mode and building custom binaries, users must build assets locally for npcs and pedestrians. Assets need to be cloned into a specific location in the project: simulator/Assets/External/Environments for Environments simulator/Assets/External/Vehicles for Vehicles simulator/Assets/External/NPCs for NPCs and Bicycles simulator/Assets/External/Pedestrians for Walkers, Scooters and Animals Clone the desired asset into the appropriate folder. Do not change the name of the folder that the asset is cloned into, it must match the name of the asset. For environments: simulator/Assets/External/Environments/Mars must contain simulator/Assets/External/Environments/Mars/Mars.unity For vehicles: simulator/Assets/External/Vehicles/Rover must contain simulator/Assets/External/Vehicles/Rover/Rover.prefab For npcs: simulator/Assets/External/NPCs/DefaultNPC must contain simulator/Assets/External/NPCs/DefaultNPC/Jeep/Jeep.prefab For pedestrians: simulator/Assets/External/Pedestrians/Walkers must contain simulator/Assets/External/Pedestrians/Walkers/Bob/Bob.prefab","title":"Adding an Asset"},{"location":"simulation-content/assets/#setup-an-asset","text":"For map assets setup see here . For ego vehicle asset setup see here . For npc asset setup see here . For pedestrian asset setup see here .","title":"Setup an Asset"},{"location":"simulation-content/assets/#building-assets","text":"Assets are built using the same build script as the simulator. Follow the build instructions through step 17. NPCs and Pedestrians must be built locally for custom binaries. They must be added to AssetBundles folder to be loaded by the simulator binary or in editor source in developer mode . IMPORTANT Windows and Linux support must be installed with Unity to build assetbundles","title":"Building Assets"},{"location":"simulation-content/control-calibration/","text":"How to Collect Data with Control Calibration Sensor Control Calibration Sensor is for collecting control data to generate control calibration table which can be referred by control module to decide throttle, brake and steering command. Setup top # Check if there is Wide Flat Map ls /apollo/modules/map/data/wide_flat_map Instructions top # Add WideFlatMap into WebUI. The assetbundle is available on the content store Create a vehicle with a CyberRT bridge type and add total control calibration criteria into ego vehicle's sensor parameters . Start Apollo docker container /apollo/docker/scripts/dev_start.sh Go into Apollo docker container with /apollo/docker/scripts/dev_into.sh Run Cyber bridge /apollo/scripts/bridge.sh Run localization module mainboard -d /apollo/modules/localization/dag/dag_streaming_rtk_localization.dag Run dreamview bootstrap.sh In a browser, navigate to Dreamview at http://localhost:8888/ Choose vehicle model. Choose Map as Wide Flat Map. Choose setup mode to Vehicle Calibration. In Others tab, choose Data Collection Monitor In Modules tab, turn Recorder on. In the Simulator's WebUI, create a simulation with WideFlatMap and the vehicle created in Step 2 (with the bridge connection string to the computer running Apollo). In Simulator's WebUI, start simulation. You can see progress bar filled as Apollo collects data. Once all progress bars are filled, vehicle control calibration is done.","title":"How to Collect Data with Control Calibration Sensor"},{"location":"simulation-content/control-calibration/#setup","text":"Check if there is Wide Flat Map ls /apollo/modules/map/data/wide_flat_map","title":"Setup"},{"location":"simulation-content/control-calibration/#instructions","text":"Add WideFlatMap into WebUI. The assetbundle is available on the content store Create a vehicle with a CyberRT bridge type and add total control calibration criteria into ego vehicle's sensor parameters . Start Apollo docker container /apollo/docker/scripts/dev_start.sh Go into Apollo docker container with /apollo/docker/scripts/dev_into.sh Run Cyber bridge /apollo/scripts/bridge.sh Run localization module mainboard -d /apollo/modules/localization/dag/dag_streaming_rtk_localization.dag Run dreamview bootstrap.sh In a browser, navigate to Dreamview at http://localhost:8888/ Choose vehicle model. Choose Map as Wide Flat Map. Choose setup mode to Vehicle Calibration. In Others tab, choose Data Collection Monitor In Modules tab, turn Recorder on. In the Simulator's WebUI, create a simulation with WideFlatMap and the vehicle created in Step 2 (with the bridge connection string to the computer running Apollo). In Simulator's WebUI, start simulation. You can see progress bar filled as Apollo collects data. Once all progress bars are filled, vehicle control calibration is done.","title":"Instructions"},{"location":"simulation-content/ego-vehicle-dynamics/","text":"EGO Vehicle Dynamics SVL Simulator supports multiple dynamics models for EGO vehicles. The default dynamics model is a C# based model that uses Unity's PhysX physics engine and components. The model receives controller input and applies force to the Unity wheel colliders. For users who would like to change or replace vehicle dynamics with their own models, the simulator offers the following ways to do so. Simple Model Interface - pure C# dynamics Full Model Interface - FMI 2.0 supported dynamics Simple Model Interface top # Simple Model Interface is our pure C# dynamic model. SVL Simulator vehicles that are provided, use the VehicleSMI class. It is located in Assets -> Scripts -> Dynamics -> Examples. This class inherits IVehicleDynamics and has required methods for simulation. Users can make any pure C# dynamic model class as long as it inherits IVehicleDynamics and is compiled in SVL Simulator run-time executable. If creating a new SMI class, place in Assets -> Scripts -> Dynamics folder. Future updates will create a dll of the class so it won't need to be compiled in the simulator executable. Simple Model Interface Setup top # Create or select an EGO vehicle prefab in the Project panel only, not in scene Double click this EGO vehicle prefab. This opens the prefab editor scene Remove VehicleSMI.cs from the vehicle component list in the Inspector panel. This is the component that you are replacing with your own Add your new c# dynamics class component that inherits IVehicleDynamics If you plan to use Unity's physics engine, be sure to look at how VehicleSMI caches references to the wheel colliders and wheel meshes. This AxleInfo, in the C# example, enables you to apply force to the wheels and match the movement to the wheel models. Set any public references needed for your new dynamics model and save the prefab, Ctrl-S Open Simulator -> Build... to open the SVL Simulator bundle creation window Select Ego vehicle Select Build and after it is completed, in the root of the SVL Simulator you will have a folder called AssetBundles where the new bundles will be created. The new vehicle will be in AssetBundles/Vehicles folder. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser button in the Game panel view Under Library -> Vehicles select Add New Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Be sure to rename the vehicle so it isn't confused with other dynamics. Create a new sensor configuration. Publish the new vehicle. Create new simulation and select this new vehicle. Full Model Interface top # Full Model Interface supports FMI2.0 Functional Mock-up Interface . Since user FMU's can vary greatly, we have provided an exampleFMU.fmu for testing in Windows only. It can be found in the source code in Assets -> Resources. ExampleVehicleFMU.cs is provided to see how the Full Model Interface system works. It is located in Assets -> Scripts -> Dynamics -> Examples. Users can create their own FMU class that inherits from IVehicleDynamics but will need to be compiled with the simulator. Create or select an EGO vehicle prefab in the Project panel only, not in scene Double click this EGO vehicle prefab. This opens the prefab editor scene Remove VehicleSMI.cs from the vehicle component list in the Inspector panel. This is the component that you are replacing Add Component ExampleVehicleFMU.cs ExampleFMU.fmu requires Unity physics solver Toggle button Non Unity Physics to Unity Physics Set Axles size to 2 Setup Axles data. You will be dragging gameobjects from the Hierarchy panel to the Inspector panel Drag the correct wheel colliders and wheel meshes to public references from the prefab, see SMI setup Enable Motor and Steering for the front axle Set Brake Bias to 0.5f for front and back axles Import the ExampleFMU.fmu Toggle button Import FMU Choose ExampleFMU.fmu in Assets -> Resources -> ExampleFMU.fmu FMU will unpack in the repository vehicle folder in External -> Vehicles -> VehicleName -> FMUName folder FMUImporter.cs will parse the XML file into FMUData in the VehicleFMU class instance on prefab Model Variables will be listed so users can reference by index Each model variable has multiple values that are displayed in the scroll area Open Simulator -> Build... to open the SVL Simulator bundle creation window Select the new Ego vehicle to build Select Build and after it is completed, in the root of the SVL Simulator you will have a folder called AssetBundles/Vehicles where the new bundles will be created. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser... button in the Game panel view Under Library -> Vehicles select Add New Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Be sure to rename the vehicle so it isn't confused with other dynamics. Create a new sensor configuration. Publish the new vehicle. Create new simulation and select this new vehicle. Full Model Interface Run-time behavior top # SVL Simulator only supports one FMU vehicle and for Windows only.","title":"Vehicle dynamics"},{"location":"simulation-content/ego-vehicle-dynamics/#simple-model-interface","text":"Simple Model Interface is our pure C# dynamic model. SVL Simulator vehicles that are provided, use the VehicleSMI class. It is located in Assets -> Scripts -> Dynamics -> Examples. This class inherits IVehicleDynamics and has required methods for simulation. Users can make any pure C# dynamic model class as long as it inherits IVehicleDynamics and is compiled in SVL Simulator run-time executable. If creating a new SMI class, place in Assets -> Scripts -> Dynamics folder. Future updates will create a dll of the class so it won't need to be compiled in the simulator executable.","title":"Simple Model Interface"},{"location":"simulation-content/ego-vehicle-dynamics/#simple-model-interface-setup","text":"Create or select an EGO vehicle prefab in the Project panel only, not in scene Double click this EGO vehicle prefab. This opens the prefab editor scene Remove VehicleSMI.cs from the vehicle component list in the Inspector panel. This is the component that you are replacing with your own Add your new c# dynamics class component that inherits IVehicleDynamics If you plan to use Unity's physics engine, be sure to look at how VehicleSMI caches references to the wheel colliders and wheel meshes. This AxleInfo, in the C# example, enables you to apply force to the wheels and match the movement to the wheel models. Set any public references needed for your new dynamics model and save the prefab, Ctrl-S Open Simulator -> Build... to open the SVL Simulator bundle creation window Select Ego vehicle Select Build and after it is completed, in the root of the SVL Simulator you will have a folder called AssetBundles where the new bundles will be created. The new vehicle will be in AssetBundles/Vehicles folder. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser button in the Game panel view Under Library -> Vehicles select Add New Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Be sure to rename the vehicle so it isn't confused with other dynamics. Create a new sensor configuration. Publish the new vehicle. Create new simulation and select this new vehicle.","title":"Simple Model Interface Setup"},{"location":"simulation-content/ego-vehicle-dynamics/#full-model-interface","text":"Full Model Interface supports FMI2.0 Functional Mock-up Interface . Since user FMU's can vary greatly, we have provided an exampleFMU.fmu for testing in Windows only. It can be found in the source code in Assets -> Resources. ExampleVehicleFMU.cs is provided to see how the Full Model Interface system works. It is located in Assets -> Scripts -> Dynamics -> Examples. Users can create their own FMU class that inherits from IVehicleDynamics but will need to be compiled with the simulator. Create or select an EGO vehicle prefab in the Project panel only, not in scene Double click this EGO vehicle prefab. This opens the prefab editor scene Remove VehicleSMI.cs from the vehicle component list in the Inspector panel. This is the component that you are replacing Add Component ExampleVehicleFMU.cs ExampleFMU.fmu requires Unity physics solver Toggle button Non Unity Physics to Unity Physics Set Axles size to 2 Setup Axles data. You will be dragging gameobjects from the Hierarchy panel to the Inspector panel Drag the correct wheel colliders and wheel meshes to public references from the prefab, see SMI setup Enable Motor and Steering for the front axle Set Brake Bias to 0.5f for front and back axles Import the ExampleFMU.fmu Toggle button Import FMU Choose ExampleFMU.fmu in Assets -> Resources -> ExampleFMU.fmu FMU will unpack in the repository vehicle folder in External -> Vehicles -> VehicleName -> FMUName folder FMUImporter.cs will parse the XML file into FMUData in the VehicleFMU class instance on prefab Model Variables will be listed so users can reference by index Each model variable has multiple values that are displayed in the scroll area Open Simulator -> Build... to open the SVL Simulator bundle creation window Select the new Ego vehicle to build Select Build and after it is completed, in the root of the SVL Simulator you will have a folder called AssetBundles/Vehicles where the new bundles will be created. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser... button in the Game panel view Under Library -> Vehicles select Add New Drag the new bundle into the target area to upload the vehicle or select from file folders. Set the vehicle details. Be sure to rename the vehicle so it isn't confused with other dynamics. Create a new sensor configuration. Publish the new vehicle. Create new simulation and select this new vehicle.","title":"Full Model Interface"},{"location":"simulation-content/ego-vehicle-dynamics/#full-model-interface-run-time-behavior","text":"SVL Simulator only supports one FMU vehicle and for Windows only.","title":"Full Model Interface Run-time behavior"},{"location":"simulation-content/hd-map-mesh-generation/","text":"HD Map Mesh Generation Video top # Introduction top # SVL Simulator provides an option to automatically generate meshes for imported HD map data, allowing you to build and upload custom environments for your simulations. Depending on your needs this tool can prepare textured road meshes for rendering, mesh colliders, or both. The mesh builder does not use HD map files directly, but instead operates on Simulator's unified internal data structure for map annotations. Some of the most popular file formats can be imported through HD Map Importer tool. After import is finished, you can use that data to generate meshes. Accessing the mesh builder top # To access the mesh builder window, open Simulator project in Unity editor, then navigate into Simulator/Build HD Map Mesh on the menu bar. Mesh builder window will be opened. Builder settings top # Mesh builder window offers two non-exclusive options for mesh generation: colliders and renderers. Selecting colliders will create MeshCollider component for each of the roads. These colliders will be used by all entities in simulation, including agents and NPCs. Selecting renderers will create MeshRenderer component (along with MeshFilter ) for each of the roads. Meshes created this way will be visible in simulation, textured, and will include lane-lines. Mesh Settings top # Settings in this tab will affect general shape of the mesh, both for colliders and for rendering. Parameter Name Description Snap Lane Ends If this option is enabled, end vertices from subsequent lanes will be snapped together within distance threshold to remove holes. Snap Threshold Maximum distance (in meters) within which lane ends can be snapped together. Only available if Snap Lane Ends is enabled. Push Outer Verts If this option is enabled, external lane boundaries will be pushed out to create roadside. Separate Outer Mesh If this option is enabled, pushed out boundaries will be split into separate mesh. Only available if Push Outer Verts is enabled. Push Distance Distance (in meters) to push external lane boundaries out. Only available if Push Outer Verts is enabled. Snap Lane Ends top # Option for snapping lane ends can improve overall mesh quality if lanes in the original HD map file are not perfectly aligned. If you notice any holes like the one shown below, try enabling this option. If gaps are larger, you can try increasing snap threshold. Push Outer Verts top # This option is useful if you want to create additional roadside that was not defined in original HD map file. Lane-lines are not affected by this, and only outer lane boundaries are expanded. This can help prevent vehicles from falling off the road when even slightly crossing the side line. Separate Outer Mesh top # When this option is enabled, all roadsides created by using Push Outer Verts option will be split into separate meshes, instead of being an extension of the road. This will result in more complex geometry, but can be useful if you want to use different materials for roads and roadsides. Using separate meshes can also fix artifacts that rarely occur when tight corners are pushed out by a significant amount. Rendering Settings top # Settings in this tab will affect visuals of generated meshes. If Create Renderers option is disabled, this tab will not be available. Parameter Name Description Road UV Unit Distance (in meters) defining size of a single UV coordinates tile for roads. Increasing this value will stretch road texture over larger area. Line UV Unit Distance (in meters) defining size of a single UV coordinates tile for lane-lines. Increasing this value will stretch line texture over longer distance. Line Width Width (in meters) of a single lane-line. Double lines will be scaled proportionally. Line Bump Vertical distance (in meters) between road and lane-lines. This elevates lines slightly to avoid clipping. Material Settings top # Materials used on generated meshes are not exposed in mesh builder window. Instead, they are defined in an asset that can be found under Assets/Resources/Editor/HDMapMaterials.asset . Please note that even though it's possible to replace default materials with your own, it's generally not advised to use different shaders. Custom materials and shaders will render properly, but some additional features, like weather effects, might not be compatible and will not show. Building HD map environment top # It's possible to prepare a simple, usable environment for simulation from HD map data only. To create environment bundle from HD map data, follow the steps below. Launch SVL Simulator from the Unity Editor (as described here ). Create new scene through File/New Scene menu item (note: avoid creating scene through context menu - it will lack required components). Access HD Map Importer tool and import your HD map data. Access HD Map Mesh Builder tool and generate both colliders and renderers. Locate SpawnInfo object in scene hierarchy and move it to the desired EGO vehicle spawn point. Create new directory with your desired environment name under Assets/External/Environments/ . Save the scene under new directory. Final path of the newly created scene should be Assets/External/Environments/SceneName/SceneName.unity . (optional) Create NavMesh to enable pedestrians. See NavMesh section for details. Your environment is now ready to be built. Follow the build instructions for assets . Creating NavMesh for pedestrians top # If you plan to use pedestrians on generated environment, you'll have to bake navigation meshes for pathfinding. After road meshes are created and the scene have been saved, follow the steps below to prepare NavMesh. NOTE: Pedestrian annotations must be created for pedestrians to navigate the NavMesh. See Map Annotation . Access Navigation window through Window/AI/Navigation menu item. In the Object tab, select Mesh Renderers filtering option. Select all generated mesh renderers in scene hierarchy. Enable Navigation Static option for all selected mesh renderers and change their layer to Road . Note: If you separate outer meshes , you can assign different layer to them, e.g. Sidewalk . All of these objects have _roadside postfix. In the Bake tab, click Bake button to generate the NavMesh. Save the scene.","title":"Road network generation"},{"location":"simulation-content/hd-map-mesh-generation/#video","text":"","title":"Video"},{"location":"simulation-content/hd-map-mesh-generation/#introduction","text":"SVL Simulator provides an option to automatically generate meshes for imported HD map data, allowing you to build and upload custom environments for your simulations. Depending on your needs this tool can prepare textured road meshes for rendering, mesh colliders, or both. The mesh builder does not use HD map files directly, but instead operates on Simulator's unified internal data structure for map annotations. Some of the most popular file formats can be imported through HD Map Importer tool. After import is finished, you can use that data to generate meshes.","title":"Introduction"},{"location":"simulation-content/hd-map-mesh-generation/#accessing-the-mesh-builder","text":"To access the mesh builder window, open Simulator project in Unity editor, then navigate into Simulator/Build HD Map Mesh on the menu bar. Mesh builder window will be opened.","title":"Accessing the mesh builder"},{"location":"simulation-content/hd-map-mesh-generation/#builder-settings","text":"Mesh builder window offers two non-exclusive options for mesh generation: colliders and renderers. Selecting colliders will create MeshCollider component for each of the roads. These colliders will be used by all entities in simulation, including agents and NPCs. Selecting renderers will create MeshRenderer component (along with MeshFilter ) for each of the roads. Meshes created this way will be visible in simulation, textured, and will include lane-lines.","title":"Builder settings"},{"location":"simulation-content/hd-map-mesh-generation/#mesh-settings","text":"Settings in this tab will affect general shape of the mesh, both for colliders and for rendering. Parameter Name Description Snap Lane Ends If this option is enabled, end vertices from subsequent lanes will be snapped together within distance threshold to remove holes. Snap Threshold Maximum distance (in meters) within which lane ends can be snapped together. Only available if Snap Lane Ends is enabled. Push Outer Verts If this option is enabled, external lane boundaries will be pushed out to create roadside. Separate Outer Mesh If this option is enabled, pushed out boundaries will be split into separate mesh. Only available if Push Outer Verts is enabled. Push Distance Distance (in meters) to push external lane boundaries out. Only available if Push Outer Verts is enabled.","title":"Mesh Settings"},{"location":"simulation-content/hd-map-mesh-generation/#snap-lane-ends","text":"Option for snapping lane ends can improve overall mesh quality if lanes in the original HD map file are not perfectly aligned. If you notice any holes like the one shown below, try enabling this option. If gaps are larger, you can try increasing snap threshold.","title":"Snap Lane Ends"},{"location":"simulation-content/hd-map-mesh-generation/#push-outer-verts","text":"This option is useful if you want to create additional roadside that was not defined in original HD map file. Lane-lines are not affected by this, and only outer lane boundaries are expanded. This can help prevent vehicles from falling off the road when even slightly crossing the side line.","title":"Push Outer Verts"},{"location":"simulation-content/hd-map-mesh-generation/#separate-outer-mesh","text":"When this option is enabled, all roadsides created by using Push Outer Verts option will be split into separate meshes, instead of being an extension of the road. This will result in more complex geometry, but can be useful if you want to use different materials for roads and roadsides. Using separate meshes can also fix artifacts that rarely occur when tight corners are pushed out by a significant amount.","title":"Separate Outer Mesh"},{"location":"simulation-content/hd-map-mesh-generation/#rendering-settings","text":"Settings in this tab will affect visuals of generated meshes. If Create Renderers option is disabled, this tab will not be available. Parameter Name Description Road UV Unit Distance (in meters) defining size of a single UV coordinates tile for roads. Increasing this value will stretch road texture over larger area. Line UV Unit Distance (in meters) defining size of a single UV coordinates tile for lane-lines. Increasing this value will stretch line texture over longer distance. Line Width Width (in meters) of a single lane-line. Double lines will be scaled proportionally. Line Bump Vertical distance (in meters) between road and lane-lines. This elevates lines slightly to avoid clipping.","title":"Rendering Settings"},{"location":"simulation-content/hd-map-mesh-generation/#material-settings","text":"Materials used on generated meshes are not exposed in mesh builder window. Instead, they are defined in an asset that can be found under Assets/Resources/Editor/HDMapMaterials.asset . Please note that even though it's possible to replace default materials with your own, it's generally not advised to use different shaders. Custom materials and shaders will render properly, but some additional features, like weather effects, might not be compatible and will not show.","title":"Material Settings"},{"location":"simulation-content/hd-map-mesh-generation/#building-hd-map-environment","text":"It's possible to prepare a simple, usable environment for simulation from HD map data only. To create environment bundle from HD map data, follow the steps below. Launch SVL Simulator from the Unity Editor (as described here ). Create new scene through File/New Scene menu item (note: avoid creating scene through context menu - it will lack required components). Access HD Map Importer tool and import your HD map data. Access HD Map Mesh Builder tool and generate both colliders and renderers. Locate SpawnInfo object in scene hierarchy and move it to the desired EGO vehicle spawn point. Create new directory with your desired environment name under Assets/External/Environments/ . Save the scene under new directory. Final path of the newly created scene should be Assets/External/Environments/SceneName/SceneName.unity . (optional) Create NavMesh to enable pedestrians. See NavMesh section for details. Your environment is now ready to be built. Follow the build instructions for assets .","title":"Building HD map environment"},{"location":"simulation-content/hd-map-mesh-generation/#creating-navmesh-for-pedestrians","text":"If you plan to use pedestrians on generated environment, you'll have to bake navigation meshes for pathfinding. After road meshes are created and the scene have been saved, follow the steps below to prepare NavMesh. NOTE: Pedestrian annotations must be created for pedestrians to navigate the NavMesh. See Map Annotation . Access Navigation window through Window/AI/Navigation menu item. In the Object tab, select Mesh Renderers filtering option. Select all generated mesh renderers in scene hierarchy. Enable Navigation Static option for all selected mesh renderers and change their layer to Road . Note: If you separate outer meshes , you can assign different layer to them, e.g. Sidewalk . All of these objects have _roadside postfix. In the Bake tab, click Bake button to generate the NavMesh. Save the scene.","title":"Creating NavMesh for pedestrians"},{"location":"simulation-content/lane-line-detector/","text":"Lane-line Detector Overview top # The lane-line detector tool can used to prepare lane-line data for the segmentation and lane-line sensors. Depending on the desired result, it can use data from map annotations and/or road intensity maps. If your annotation data is precise and aligns with the environment well, it's suggested to use it as the only input for this tool. On the other hand, if environment and annotations are slightly misaligned, you might want to try using the optional feature that will attempt to detect lines on intensity maps and correct the annotations based on this. Refer to the main options section for more details. The main output from this tool is a set of meshes matching lane-lines on the road. These meshes are not visible for normal cameras, but will be used by the segmentation sensor to differentiate between road and lane-lines. If Generate Line Sensor Data in main options is enabled, output will also include correction data for the lane-line sensor. You can find more details about this on lane-line sensor page. If line visibility for segmentation camera is unnecessary or undesired, you can safely ignore this tool. Comparison of segmentation image with and without lane-lines is shown below. Accessing the Detector top # To open the lane-line detector window, open a Simulator project in Unity editor, then select Simulator/Detect Lane-lines on the menu bar. Main Options top # There are two main options that must be set before any other, more detailed, settings are available. The Line Source dropdown describes which data should be used to generate lines that will be visible by segmentation sensor. Three options are available: - HD Map - only annotation data will be used and all generated lines will follow lines from the high-definition (HD) map. - Intensity Map - only road intensity maps will be used. This method is based on image processing and results may vary depending on the quality of the intensity maps. - Corrected HD Map - annotation data will be used, but the tool will attempt to correct any misaligned lines based on road intensity maps. The Generate Line Sensor Data checkbox determines whether the annotations should be corrected for the lane-line sensor . Correction is based on road intensity maps, similar to the Corrected HD Map option in the Line Source setting. If this option is disabled, raw annotation data will be used instead. Note that this setting is separate to Line Source , and both sensors can use differently processed data. After generation is done, a new object named LineData_generated will be created on the scene. When it is selected, the scene view will show outlines of the generated lines. If you're not satisfied with the result, try using a different mode, or changing the advanced settings. Line-detection Settings top # This section is only visible if any of the main options was configured to use line-detection based on intensity maps. Grouping Settings top # Parameter Name Description Line Distance Threshold The maximum distance between line segments that can be classified as a single lane-line when classifying lines detected on intensity maps. Line Angle Threshold The maximum angle between line segments that can be classified as a single lane-line when classifying lines detected on intensity maps. Max Line Segment Length The maximum length of a single detected line segment. Longer lines will be split into multiple segments. Worst Fit Threshold The maximum valid distance between a line center and the segments creating it. This is used to filter out large clusters of spread-parallel line-segments. Min Width Threshold The maximum distance between segments that should be considered parts of a single line. This is used to filter out large clusters of spread-parallel line-segments. Joint Line Threshold The minimum viable width of a lane-line. This is used to filter out linear artifacts from detected line-segments. Postprocessing Settings top # Parameter Name Description World Space Snap Distance The maximum distance between detected lines that could be considered parts of a single curve. Lines below this threshold will have their ends snap together to create a better curve approximation. World Dotted Line Distance The maximum distance between detected lines that could be considered separate parts of a single dotted line. World Space Snap Angle The maximum angle between detected lines that could be considered parts of a single curve (solid or dotted). Line Mesh Generation Settings top # This section is only visible if Line Source , in the main options, was configured to use map annotations. Parameter Name Description Snap Lane Ends If this option is enabled, end vertices from subsequent lanes will be snapped together, within the Snap Threshold, to remove holes. Snap Threshold The maximum distance (in meters) within which lane ends can be snapped together. This is only available if Snap Lane Ends is enabled. Line UV Unit The distance, in meters, defining the size of a single UV coordinates-tile for lane-lines. Increasing this value will stretch the line texture over a longer distance. Line Width The width, in meters, of a single lane-line. Double lines will be scaled proportionally. Line Bump The vertical distance, in meters, between road and lane-lines. This elevates lines slightly to avoid clipping.","title":"Lane-line detector"},{"location":"simulation-content/lane-line-detector/#overview","text":"The lane-line detector tool can used to prepare lane-line data for the segmentation and lane-line sensors. Depending on the desired result, it can use data from map annotations and/or road intensity maps. If your annotation data is precise and aligns with the environment well, it's suggested to use it as the only input for this tool. On the other hand, if environment and annotations are slightly misaligned, you might want to try using the optional feature that will attempt to detect lines on intensity maps and correct the annotations based on this. Refer to the main options section for more details. The main output from this tool is a set of meshes matching lane-lines on the road. These meshes are not visible for normal cameras, but will be used by the segmentation sensor to differentiate between road and lane-lines. If Generate Line Sensor Data in main options is enabled, output will also include correction data for the lane-line sensor. You can find more details about this on lane-line sensor page. If line visibility for segmentation camera is unnecessary or undesired, you can safely ignore this tool. Comparison of segmentation image with and without lane-lines is shown below.","title":"Overview"},{"location":"simulation-content/lane-line-detector/#accessing-the-detector","text":"To open the lane-line detector window, open a Simulator project in Unity editor, then select Simulator/Detect Lane-lines on the menu bar.","title":"Accessing the Detector"},{"location":"simulation-content/lane-line-detector/#main-options","text":"There are two main options that must be set before any other, more detailed, settings are available. The Line Source dropdown describes which data should be used to generate lines that will be visible by segmentation sensor. Three options are available: - HD Map - only annotation data will be used and all generated lines will follow lines from the high-definition (HD) map. - Intensity Map - only road intensity maps will be used. This method is based on image processing and results may vary depending on the quality of the intensity maps. - Corrected HD Map - annotation data will be used, but the tool will attempt to correct any misaligned lines based on road intensity maps. The Generate Line Sensor Data checkbox determines whether the annotations should be corrected for the lane-line sensor . Correction is based on road intensity maps, similar to the Corrected HD Map option in the Line Source setting. If this option is disabled, raw annotation data will be used instead. Note that this setting is separate to Line Source , and both sensors can use differently processed data. After generation is done, a new object named LineData_generated will be created on the scene. When it is selected, the scene view will show outlines of the generated lines. If you're not satisfied with the result, try using a different mode, or changing the advanced settings.","title":"Main Options"},{"location":"simulation-content/lane-line-detector/#line-detection-settings","text":"This section is only visible if any of the main options was configured to use line-detection based on intensity maps.","title":"Line-detection Settings"},{"location":"simulation-content/lane-line-detector/#grouping-settings","text":"Parameter Name Description Line Distance Threshold The maximum distance between line segments that can be classified as a single lane-line when classifying lines detected on intensity maps. Line Angle Threshold The maximum angle between line segments that can be classified as a single lane-line when classifying lines detected on intensity maps. Max Line Segment Length The maximum length of a single detected line segment. Longer lines will be split into multiple segments. Worst Fit Threshold The maximum valid distance between a line center and the segments creating it. This is used to filter out large clusters of spread-parallel line-segments. Min Width Threshold The maximum distance between segments that should be considered parts of a single line. This is used to filter out large clusters of spread-parallel line-segments. Joint Line Threshold The minimum viable width of a lane-line. This is used to filter out linear artifacts from detected line-segments.","title":"Grouping Settings"},{"location":"simulation-content/lane-line-detector/#postprocessing-settings","text":"Parameter Name Description World Space Snap Distance The maximum distance between detected lines that could be considered parts of a single curve. Lines below this threshold will have their ends snap together to create a better curve approximation. World Dotted Line Distance The maximum distance between detected lines that could be considered separate parts of a single dotted line. World Space Snap Angle The maximum angle between detected lines that could be considered parts of a single curve (solid or dotted).","title":"Postprocessing Settings"},{"location":"simulation-content/lane-line-detector/#line-mesh-generation-settings","text":"This section is only visible if Line Source , in the main options, was configured to use map annotations. Parameter Name Description Snap Lane Ends If this option is enabled, end vertices from subsequent lanes will be snapped together, within the Snap Threshold, to remove holes. Snap Threshold The maximum distance (in meters) within which lane ends can be snapped together. This is only available if Snap Lane Ends is enabled. Line UV Unit The distance, in meters, defining the size of a single UV coordinates-tile for lane-lines. Increasing this value will stretch the line texture over a longer distance. Line Width The width, in meters, of a single lane-line. Double lines will be scaled proportionally. Line Bump The vertical distance, in meters, between road and lane-lines. This elevates lines slightly to avoid clipping.","title":"Line Mesh Generation Settings"},{"location":"simulation-content/lane-line-sensor/","text":"Lane-line Sensor Overview top # The lane-line sensor is used to extract and publish data describing the position and curvature of road lines on the lane which EGO vehicle currently occupies. It uses ground truth data from map annotations , which can be optionally corrected. You can read more about how annotation data is used in the input data section. Data format top # The lane-line sensor currently only supports the Cyber bridge, and publishes data in a format compatible with Apollo 5.0. Each message follows the perception_lane format and, aside from the header, contains data about one or more lines in a format compatible with CameraLaneLine . Fields populated and published by the simulator are described in the table below. Parameter Description type Describes the color and shape of the line (white/yellow, solid/dotted) pos_type Describes the position of the line in relation to the EGO vehicle (right/left, ego/adjacent/third/etc.) curve_camera_coord The line curve in sensor space defined as third-degree polynomial * * See Curve definition section for details Please note that even though lane-line sensor visualization in the simulator shows detected lines as an overlay for a color image, the image itself is not part of published data and is only shown as a visual aid. Curve Definition top # Each lane-curve is described as third-degree polynomial, with coordinate space being centered on the sensor, the x axis pointing towards its front, and the y axis pointing towards its right side. This coordinate space uses only two dimensions, ignoring altitude. Image below shows the described coordinate space. Each curve is defined by six values - a, b, c, d, longitude_min, longitude_max - as defined in LaneLineCubicCurve . On the referenced image, longitude_min and longitude_max are described as MinX and MaxX , respectively). Given these parameters, the curve function f(x) can be defined as: f(x) = a + b * x + c * x^2 + d * x^3 for x \u2208 <longitude_min, longitude_max> It's important to note that polynomial coefficients are calculated via polynomial regression, which means the final function is an approximation and might not match the input data (red dots) perfectly. This is mostly noticeable on steep curves. Input data for the polynomial regression is sampled directly from map annotations for the given environment, trimmed to the sensor's field of view ( FOV on the referenced image) and defined visibility range (see JSON parameters for more information). Details about how the points are sampled can be found in the input data section. Input Data top # By default, the lane-line sensor will use lines defined in the map's annotation data. All of the spatial data and metadata for published lines will be based on this, which eliminates issues related to image processing, but requires precise annotations. Since annotation data can be relatively scarce, each segment is resampled along its length before the polynomial regression step. Sampling density depends on SampleDelta parameter that can be defined in JSON parameters . An example of how the annotation data is resampled before final processing is shown on the image below. Points used for curve approximation (red dots) are based on white and yellow annotation lines. In some cases, annotation data imported from external sources might not match the environment perfectly. If you have no option to improve the alignment, you might want to try using the automated correction option. This uses image processing and will attempt to align annotation keypoints with lines detected on road intensity maps. Please note that the results may vary based on the intensity map's quality. To use automatic line-correction, open the lane-line detector tool and use it with the Generate Line Sensor Data option enabled. An example of how offset map annotation data can look before (blue and yellow lines) and after automated correction (green lines) is shown below. Note that this correction will only affect the sensor visibility and will not change the annotation data itself. Testing with Apollo top # Lane-line sensor currently only supports CyberRT message types. If you want to verify that lane-line data is properly detected and received by Apollo, follow the steps below. Follow the instructions for running Apollo 5.0 with SVL Simulator. Don't start any simulation, but make sure that bridge is running. Using the Web User Interface, add the map that you want to use for testing to your library. You can either choose a map from Store , or create and upload your own. Note: your map can use raw or corrected annotation data. See input data section for more details. Add Lane Line Sensor plugin to your library. Add any vehicle to your library. Create new sensor configuration for your vehicle. In the configuration options, add lane-line sensor to the list of used sensors. Make sure that Topic and Frame properties are not empty. Choose CyberRT as the bridge used by this configuration. Create new simulation using the map, vehicle and sensor configuration that you just created. On the Autopilot page, select Apollo 5.0 and provide the bridge IP address. Start the simulation. In Apollo's Docker environment, run cyber_monitor tool. Lane-line sensor topic should be reporting received data. You can inspect the details by selecting the topic name.","title":"Lane-line sensor"},{"location":"simulation-content/lane-line-sensor/#overview","text":"The lane-line sensor is used to extract and publish data describing the position and curvature of road lines on the lane which EGO vehicle currently occupies. It uses ground truth data from map annotations , which can be optionally corrected. You can read more about how annotation data is used in the input data section.","title":"Overview"},{"location":"simulation-content/lane-line-sensor/#data-format","text":"The lane-line sensor currently only supports the Cyber bridge, and publishes data in a format compatible with Apollo 5.0. Each message follows the perception_lane format and, aside from the header, contains data about one or more lines in a format compatible with CameraLaneLine . Fields populated and published by the simulator are described in the table below. Parameter Description type Describes the color and shape of the line (white/yellow, solid/dotted) pos_type Describes the position of the line in relation to the EGO vehicle (right/left, ego/adjacent/third/etc.) curve_camera_coord The line curve in sensor space defined as third-degree polynomial * * See Curve definition section for details Please note that even though lane-line sensor visualization in the simulator shows detected lines as an overlay for a color image, the image itself is not part of published data and is only shown as a visual aid.","title":"Data format"},{"location":"simulation-content/lane-line-sensor/#curve-definition","text":"Each lane-curve is described as third-degree polynomial, with coordinate space being centered on the sensor, the x axis pointing towards its front, and the y axis pointing towards its right side. This coordinate space uses only two dimensions, ignoring altitude. Image below shows the described coordinate space. Each curve is defined by six values - a, b, c, d, longitude_min, longitude_max - as defined in LaneLineCubicCurve . On the referenced image, longitude_min and longitude_max are described as MinX and MaxX , respectively). Given these parameters, the curve function f(x) can be defined as: f(x) = a + b * x + c * x^2 + d * x^3 for x \u2208 <longitude_min, longitude_max> It's important to note that polynomial coefficients are calculated via polynomial regression, which means the final function is an approximation and might not match the input data (red dots) perfectly. This is mostly noticeable on steep curves. Input data for the polynomial regression is sampled directly from map annotations for the given environment, trimmed to the sensor's field of view ( FOV on the referenced image) and defined visibility range (see JSON parameters for more information). Details about how the points are sampled can be found in the input data section.","title":"Curve Definition"},{"location":"simulation-content/lane-line-sensor/#input-data","text":"By default, the lane-line sensor will use lines defined in the map's annotation data. All of the spatial data and metadata for published lines will be based on this, which eliminates issues related to image processing, but requires precise annotations. Since annotation data can be relatively scarce, each segment is resampled along its length before the polynomial regression step. Sampling density depends on SampleDelta parameter that can be defined in JSON parameters . An example of how the annotation data is resampled before final processing is shown on the image below. Points used for curve approximation (red dots) are based on white and yellow annotation lines. In some cases, annotation data imported from external sources might not match the environment perfectly. If you have no option to improve the alignment, you might want to try using the automated correction option. This uses image processing and will attempt to align annotation keypoints with lines detected on road intensity maps. Please note that the results may vary based on the intensity map's quality. To use automatic line-correction, open the lane-line detector tool and use it with the Generate Line Sensor Data option enabled. An example of how offset map annotation data can look before (blue and yellow lines) and after automated correction (green lines) is shown below. Note that this correction will only affect the sensor visibility and will not change the annotation data itself.","title":"Input Data"},{"location":"simulation-content/lane-line-sensor/#testing-with-apollo","text":"Lane-line sensor currently only supports CyberRT message types. If you want to verify that lane-line data is properly detected and received by Apollo, follow the steps below. Follow the instructions for running Apollo 5.0 with SVL Simulator. Don't start any simulation, but make sure that bridge is running. Using the Web User Interface, add the map that you want to use for testing to your library. You can either choose a map from Store , or create and upload your own. Note: your map can use raw or corrected annotation data. See input data section for more details. Add Lane Line Sensor plugin to your library. Add any vehicle to your library. Create new sensor configuration for your vehicle. In the configuration options, add lane-line sensor to the list of used sensors. Make sure that Topic and Frame properties are not empty. Choose CyberRT as the bridge used by this configuration. Create new simulation using the map, vehicle and sensor configuration that you just created. On the Autopilot page, select Apollo 5.0 and provide the bridge IP address. Start the simulation. In Apollo's Docker environment, run cyber_monitor tool. Lane-line sensor topic should be reporting received data. You can inspect the details by selecting the topic name.","title":"Testing with Apollo"},{"location":"simulation-content/map-annotation/","text":"Map Annotation SVL Simulator supports creating, editing, and exporting of HD maps of existing 3D environments (Unity scenes) in Developer Mode . The maps can be saved in the currently supported Apollo, Autoware or Lanelet2 formats. Map annotation objects contain a combination of data for all export formats and internal NPC navigation. The classes are organized in a specific way to better support NPC navigation. Not all data contained in an HD map import is kept in the annotation objects. To better understand map annotation objects and how they are organized, please review existing open-source maps before attempting to create complex map annotation scenes. Table of Contents Creating a New Map Annotate Lanes Create Parent Object Make Lanes Make Boundary Lines Annotate Intersections Create Parent Object Create Intersection Lanes Create Traffic Signals Create Traffic Signs Create Stop Lines Create Pole Annotate Self-Reversing Lanes Annotation Information Annotate Self-Reversing Lanes under Traffic Lanes Annotate Self-Reversing Lanes under Intersections Annotate Other Features Create Pedestrian NavMesh Create Pedestrian Path Sidewalk Create Pedestrian Path Crosswalk Create Junction Create Crosswalk Create Clear Area Create Parking Space Create Speed Bump Export Map Annotations Import Map Annotations Map Formats Creating a New Map top # Make sure your roads belong to the layer of Default since waypoints will be only created on this layer. Turn on Gizmos in Unity Editor Scene panel so annotation will be visible. Make sure your environment has Mesh Collider Unity components added. Open HD Map Annotation in Unity : Simulator -> Annotate HD Map By default, map annotation is not shown. Click View All under View Modes to show all existing map annotations. Or you can click Lock View Selected to only show the annotations you are selecting. NOTE Large maps with heavy annotation will slow the Unity Editor; be sure to only enable what you need to work with. You can also use the Unity Editor features to hide objects that are not needed in the Hierarchy panel. Before annotating, drag in the correct Parent Object , for example TrafficLanes . Then every new object you create will be under TrafficLanes object. The expected object hierarchy is as follows: Map - This prefab which will be the object containing all the HD Annotations. The MapHolder script should be added to this prefab TrafficLanes - This object will hold all of the MapLaneSection and MapLane which cannot be grouped under MapLaneSection MapLaneSection - This will hold all of the lanes in a section of road MapLane - A single annotated lane Intersections - This object will hold all of the MapIntersection MapIntersection - This object will hold all of the annotations for an intersection MapLane - A single annotated lane MapLine - A single annotated line (e.g. a stop line) MapSignal - A single annotated traffic signal MapSign - A single annotated traffic sign (e.g. a stop sign) BoundaryLines - This object will hold all of boundary lines More notes on HD Map Annotation window: Under Utilities , there are three Utility buttons Snap all which will snap all your annotations to ground layer. NOTE Colliders above the annotation will catch the raycast; be sure to turn off before using this tool. Remove lines which will remove all extra boundary lines in the map annotations. If two boundary lines' end points are both overlapping, one of the lines will be removed and the other will be shared. Create lines which will create fake boundary lines for all lanes which miss boundary lines using fixed width. To quick annotate a map, you can first annotate lanes, click this button to create boundary lines, and then adjust them. Under Create Intersection / Lane Section Holder , there are two buttons Intersection and Lane Section to create these two objects quickly and attach the corresponding scripts. To make Map a prefab, drag it from the scene Hierarchy into the Project folder After annotation is done, remember to save: select the Map prefab, in the Inspector click Overrides -> Apply All Annotate Lanes top # Create Parent Object top # In the Map prefab (if you don't have one, you can create an empty game object, attaching MapHolder script and make it a prefab), create a new object and name it \"TrafficLanes\" In the Inspector of the Map , drag the new TrafficLanes object into the Traffic Lanes holder In TrafficLanes , create a new object Add the MapLaneSection script to the object and position it close the section of road that will be annotated You can also create a MapLaneSection object using the Lane Section button in HD Map Annotation window. MapLaneSection script is needed for sections that contain more than 1 lane. For sections with only 1 lane, they can be left as a child object of the TrafficLanes object or grouped with the boundary lines under a different parent object without the MapLaneSection script. Each MapLaneSection will contain parallel lanes with similar lengths 1 MapLane per lane of road as well as 1 MapLine per boundary line (optional, you can use the utility button Create lines to generate fake boundary lines) If the annotations will be broken up into multiple MapLane (e.g. a straight section and a curved section), multiple MapLaneSection are required A MapLane cannot begin or end in the middle of another MapLane (e.g. a lane splits or merges). This situation would require a 2nd MapLaneSection to be created where the split/merge begins. The other lanes on the road will also need to be broken up into multiple MapLane to be included in the multiple MapLaneSection MapLaneSection is used to automate the computation of relation of neighboring lanes. Please make sure every lane under MapLaneSection has at least 3 waypoints. Example of single lane splitting into a right-turn only lane and a straight lane Make Lanes top # Select the Lane/Line option under Create Mode A large yellow TARGET_WAYPOINT will appear in the center of the scene. This is where the TEMP_WAYPOINT objects will be placed. The TARGET_WAYPOINT will not appear if your roads had no mesh collider attached. Drag in the appropriate MapLaneSection to be the Parent Object Click Waypoint button to create a new TEMP_WAYPOINT . This is where the lane will begin. Move the scene so that the TARGET_WAYPOINT is in the desired location of the next TEMP_WAYPOINT With 2 waypoints, the Create Straight function will connect the waypoints and use the number in Waypoint Count to determine how many waypoints to add in between More than 2 waypoints can be used with the Create Straight function and they will be connected in the order they were created, and the Waypoint Count value will not be used With 3 waypoints, the Create Curve function can be used to create a Bezier curve between the 1st and 3rd waypoints and the Waypoint Count value will be used to determine how many waypoints to add in between. Verify Lane is the selected Map Object Type Verify the selected Lane Turn Type is correct Select the appropriate lane boundry types Enter the speed limit of the lane (in m/s) Enter the desired number of waypoints in the lane (minimum 2 for a straight lane and 3 for a curved lane) Click the appropriate Connect button to create the lane To adjust the positions of the waypoints, with the MapLane selected, in the Inspector check Display Handles . Individual waypoints can now have their position adjusted You can also show each waypoint's handle by clicking Toggle Handles button in HD Map Annotation window. NOTE Be sure to toggle off when finished editing. Below you can also find a group of helper buttons to manipulate the lane's waypoints. Append - Adds a waypoint after the last index Prepend - Adds a waypoint before the first index Remove First - Removes the first waypoint Remove Last - Remove the the last waypoint Reverse - Reverses the waypoint order Clear - Removes all waypoints Double - Doubles the waypoint count Half - Reduces the waypoint count be approximately half Make Boundary Lines top # Drag in the appropriate MapLaneSection to be the Parent Object The same process as for lanes can be used to create boundary lines, but the Map Object Type will be BoundaryLine It is better to have the direction of a boundary line match the direction of the lane if possible For every boundary line, you also need to drag it into the corresponding field in the lane object. Boundary lines can be combined for adjacent lanes - no need to make duplicates for the same line. It is best to name boundary lines left and right so it is easier to tell which line goes into the public references on the map lane data. Annotate Intersections top # Create Parent Object top # In the Map prefab, create a new object and name it \"Intersections\" In the Inspector of the Map , drag the new Intersections object into the Intersections holder In Intersections , create a new object Add the MapIntersection script to the new object and position it in the center of the intersection that will be annotated You can also create an MapIntersection object using the Intersection button in HD Map Annotation window You need to adjust the Trigger Bounds of the MapIntersection so that the box covers the space of the intersection The MapIntersection will contain all lanes, traffic signals, stop lines, and traffic signs in the intersection Create Intersection Lanes top # Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object Place the intersection object in the center of the intersection. The same process for creating normal MapLane is used here If the Lane involves changing directions, verify the correct Lane Turn Type is selected If you are annotating boundary lines for each lane, remember to set VIRTUAL as the Line Type for the line objects If there is a stop line in the intersection, the start of the intersection lanes should be after the stop line Be sure the trigger bounds size covers most of the intersection, in X,Y, and Z axes, but does not overlap with any stop lines. This trigger volume counts NPCs that enter the intersection and will cause errors if an NPC is counted before it reaches the intersection itself. For NPCs to properly navigate an intersection, the Yield To Lanes list of a lane (usually a turning lane) must be manually filled in. With View Selected toggled in the HD Map Annotation window, the lanes in the Yield To Lanes list will be highlighted in yellow to make it easier to verify that the correct lanes are in the list. Before an NPC enters an intersection lane, it will check that there are no NPCs on the lanes in the Yield To Lanes before continuing. Enter the number of lanes that the current lanes yields to as the size of Yield To Lanes For each element in the list, drag in a lane that takes priority over the selected lane For example, generally when turning left the NPC will yield to the oncoming traffic going straight so the straight lanes should be in the left turn lane's Yield To Lanes list Create Traffic Signals top # Select the Signal option under Create Mode Drag in the MapIntersection as the Parent Object Select the correct Signal Type Traffic Signals are created on top of existing meshes. The signal annotation directions will be the same as the directions of the mesh. Please make sure the directions of the mesh is correct. Select the mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Click Create Signal to annotate the traffic signal Make sure the bounds of the map signal match the mesh object bounds Create Traffic Signs top # Select the Sign option under Create Mode Drag in the MapIntersection as the Parent Object Select the Sign mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Change the Sign Type to the desired type Click Create Sign to create the annotation Find the stop line MapLine that is associated with the created MapSign Select the MapSign and drag the MapLine into the Stop Line box Verify the annotation is created in the correct orientation. The Z-axis (blue) should be facing the same way the sign faces. Move the Bounding Box so that is matches the sign Bound Offsets adjusts the location of the box Bound Scale adjusts the size of the box Create Stop Lines top # Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object A similar process to boundary lines and traffic lanes is used for stop lines Change the Map Object Type to StopLine Forward Right vs Forward Left depend on the direction of the lane related to this stop line and the order that the waypoints were created in. The \"Forward\" direction should match the direction of the lane Example: In a right-hand drive map (cars are on the right side of the road), if the waypoints are created from the outside of the road inwards, then Forward Right should be selected. To verify if the correct direction was selected, Toggle Tool Handle Rotation so that the tool handles are in the active object's rotation. The Z-axis of the selected Stop Line should point to the intersection. A StopLine needs to be with the lanes that approach it. The last waypoint of approaching lanes should be perpendicular and intersect with the stop line Stop lines should be checked IsStopSign for stop sign intersections and left false for signal light intersections For intersections that do not have signals, stop lines are not needed. Create Pole top # Poles are required for Autoware Vector maps. In an intersection with traffic lights, there is 1 MapPole on each corner, next to a stop line. The MapPole holds references to all traffic lights that signal the closest stop line. Select the Pole option under Create Mode Drag in the MapIntersection as the Parent Object A TARGET_WAYPOINT will appear in the center of the scene. This is where the pole annotation will be created. Position the TARGET_WAYPOINT on the corner of the intersection near the stop line Click Create Pole to create the annotation Find the traffic light MapSignal that are associated with the closest stop line Select the MapPole In the Inspector, change the Signal Lights size to be the number of MapSignal that signal the stop line Drag 1 MapSignal into each element of Signal Lights Annotate Self-Reversing Lanes top # These types of lanes are only supported on Apollo 5.0 Annotation Information top # There are TrafficLanes and Intersections objects. TrafficLanes object is for straight lane without any branch. Intersections object is for branching lanes, which has same predecessor lane. Annotate Self-Reversing Lanes under Traffic Lanes top # Add MapLaneSection game object under TrafficLanes. MapLaneSection has one pair of lanes, which are forward and reverse lanes. Annotate lane given name to MapLane_forward and move it under MapLaneSection. Use straight line connect menu with way point count and move each point to shape lane. Use Display Lane option to show width of lane. In order for parking to work, there should have some space between parking space and lane. Duplicate the MapLane_forward and rename it to MapLane_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane_forward, MapLane_reverse) and change properties like the following: Is Self Reverse Lane: True Lane Turn Type: NO_TURN Left Bound Type: SOLID_WHITE Right Bound Type: SOLID_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map) Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way. Annotate Self-Reversing Lanes under Intersections top # Add MapIntersection game object under Intersections. MapIntersection has several pair of lanes. Annotate lane given name to MapLane_forward move it under MapIntersection. Use curve connect menu with way point count, 5. Duplicate the MapLane_forward and rename it to MapLane1_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane_forward, MapLane_reverse) and change properties like the following: Is Self Reverse Lane: True Left Bound Type: DOTTED_WHITE Right Bound Type: DOTTED_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map). Choose each lane and decide its turn type considering its direction. Lane Turn Type: NO_TURN or LEFT_TURN or RIGHT_TURN Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way. Annotate Other Features top # Other annotation features may be included in the TrafficLanes or Intersections objects or they may be sorted into other parent objects. Create Pedestrian NavMesh top # To support pedestrians, the map must include a NavMesh created with the the Unity Editor. Be sure map scene is saved in the correct folder in Assets/External/Environments/YourMap Mark all walkable ground meshes as Navigation Static in the Inspector panel Open Window -> AI -> Navigation Select meshes and mark Navigation Area in the Object menu of the Navigation panel Road and Sidewalk MUST be separate meshes and layered for Pedestrian crosswalk to work (see second image below) Click Bake button in the Bake menu of the Navigation panel. NavMesh will be created in a folder named from the scene file. Create Pedestrian Path Sidewalk top # This annotation controls where pedestrians will pass on sidewalks. Pedestrians can walk anywhere but will stay on annotated areas if possible. Select the Pedestrian option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center for the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT on the desired path Click Waypoint to create a TEMP_WAYPOINT Repeat to create a trail of TEMP_WAYPOINT along the desired path Click Connect to create the MapPedestrian object Create Pedestrian Path Crosswalk top # This annotation controls where pedestrians will walk on crosswalks. Pedestrians can walk anywhere but will stay on annotated areas if possible. When annotated correctly, pedestrians will only walk when signal lights are red. Select the Pedestrian option under Create Mode Drag in the MapIntersection Parent Object . This MUST be the MapIntersection object for the pedestrian to know what signal states are. Again, MapIntersection holder MUST be the parent for crosswalk map pedestrian annotation. A TARGET_WAYPOINT will appear in the center for the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT on the desired path Click Waypoint to create a TEMP_WAYPOINT start Click Waypoint to create a TEMP_WAYPOINT end Select Crosswalk type and select Z forward orientation. MapPedestrian object MUST be Z-forward INTO intersection. Click Connect to create the MapPedestrian object Create Junction top # Junction annotations need to be annotated for Apollo 5.0 for Map Exporter to work correctly. It converts the boundary of the intersection. Select the Junction option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one vertex of the junction Click Waypoint to create a TEMP_WAYPOINT Create the desired number of TEMP_WAYPOINTS Click Connect to create the MapJunction Create Crosswalk top # Select the CrossWalk option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the crosswalk Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapCrossWalk Create Clear Area top # Select the ClearArea option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the clear area Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapClearArea Create Parking Space top # Select the ParkingSpace option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to the corner of the parking space closest to an approaching vehicle Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapParkingSpace NOTE: Apollo 5.0 requires some space between the edge of a lane and the parking space. To verify that there is space, select the MapLane that goes past the MapParkingSpace and click Display Lane . This will show the width of the lane facilitate adjustment of the lane to have a gap between the lane and parking space. Create Speed Bump top # Select the SpeedBump option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one end of the speed bump Click Waypoint to create a TEMP_WAYPOINT Create 2 TEMP_WAYPOINT in the order shown below The TEMP_WAYPOINT should be wide enough to be outside the lane Click Connect to create the MapSpeedBump To verify that the MapSpeedBump is wide enough, select the MapLane that the MapSpeedBump crosses. Enable Display Lane to visualize the width of the lane. If necesary, select the MapSpeedBump and enable Display Handles to adjust the positions of the individual waypoints. Export Map Annotations top # HD Map Annotations may be exported in a variety of formats. Current supported formats are: Apollo 5.0 HD Map Apollo 3.0 HD Map Autoware Vector Map Lanelet2 Map OpenDrive Map To export a map: Open the HD Map Export tool in Unity : Simulator -> Export HD Map Select the desired format from the dropdown Export Format Enter the desired save location of the exported map Click Export to create the exported map Note the exporter will fail if any lane has no boundary lines annotated - you need to either manually annotate them or use the Create lines button in HD Map Annotation window to create fake boundary lines. Import Map Annotations top # The simulator can import a variety of formats of annotated map. NOTE After importing an HD map, you will be required to check each annotation to make sure import is correct. Simulator will provide logs if there are issues that need resolved. If the importer cannot parse a HD map, it will make holder objects that will need manually edited. Current supported formats are: Lanelet2 Map Apollo 5.0 HD Map OpenDRIVE Map To import a map: Open the HD Map Export tool in Unity : Simulator -> Import HD Map Select the format of the input map from the dropdown Import Format Select the file or folder that will be imported Click Import to import the map annotations into the simulator Distance Threshold is used to down-sample straight lines/lanes and Delta Threshold is used to down-sample curved lines/lanes. Changing both of them to 0 will keep all points, but for large maps displaying all annotations may lead to Unity crash. Lanelet2 map importer Notes: Lanes will be automatically imported and grouped as MapLaneSection if possible Intersections with four-way traffic lights / stop signs or two-way stop signs can be imported and grouped under MapIntersection Left-turn lanes are automatically found and their corresponding lanes to yield are also obtained automatically to get NPCs working correctly For each MapIntersection , you need to manually adjust X and Z for the Trigger Bounds as explained in annotate intersection part . Remember to check objects under Intersections are grouped correctly. Map Formats top # For more information on the map formats, please see the links below: Apollo HD Autoware Vector Lanelet2 OpenDRIVE","title":"Map annotation"},{"location":"simulation-content/map-annotation/#creating-a-new-map","text":"Make sure your roads belong to the layer of Default since waypoints will be only created on this layer. Turn on Gizmos in Unity Editor Scene panel so annotation will be visible. Make sure your environment has Mesh Collider Unity components added. Open HD Map Annotation in Unity : Simulator -> Annotate HD Map By default, map annotation is not shown. Click View All under View Modes to show all existing map annotations. Or you can click Lock View Selected to only show the annotations you are selecting. NOTE Large maps with heavy annotation will slow the Unity Editor; be sure to only enable what you need to work with. You can also use the Unity Editor features to hide objects that are not needed in the Hierarchy panel. Before annotating, drag in the correct Parent Object , for example TrafficLanes . Then every new object you create will be under TrafficLanes object. The expected object hierarchy is as follows: Map - This prefab which will be the object containing all the HD Annotations. The MapHolder script should be added to this prefab TrafficLanes - This object will hold all of the MapLaneSection and MapLane which cannot be grouped under MapLaneSection MapLaneSection - This will hold all of the lanes in a section of road MapLane - A single annotated lane Intersections - This object will hold all of the MapIntersection MapIntersection - This object will hold all of the annotations for an intersection MapLane - A single annotated lane MapLine - A single annotated line (e.g. a stop line) MapSignal - A single annotated traffic signal MapSign - A single annotated traffic sign (e.g. a stop sign) BoundaryLines - This object will hold all of boundary lines More notes on HD Map Annotation window: Under Utilities , there are three Utility buttons Snap all which will snap all your annotations to ground layer. NOTE Colliders above the annotation will catch the raycast; be sure to turn off before using this tool. Remove lines which will remove all extra boundary lines in the map annotations. If two boundary lines' end points are both overlapping, one of the lines will be removed and the other will be shared. Create lines which will create fake boundary lines for all lanes which miss boundary lines using fixed width. To quick annotate a map, you can first annotate lanes, click this button to create boundary lines, and then adjust them. Under Create Intersection / Lane Section Holder , there are two buttons Intersection and Lane Section to create these two objects quickly and attach the corresponding scripts. To make Map a prefab, drag it from the scene Hierarchy into the Project folder After annotation is done, remember to save: select the Map prefab, in the Inspector click Overrides -> Apply All","title":"Creating a New Map"},{"location":"simulation-content/map-annotation/#annotate-lanes","text":"","title":"Annotate Lanes"},{"location":"simulation-content/map-annotation/#create-parent-object-lanes","text":"In the Map prefab (if you don't have one, you can create an empty game object, attaching MapHolder script and make it a prefab), create a new object and name it \"TrafficLanes\" In the Inspector of the Map , drag the new TrafficLanes object into the Traffic Lanes holder In TrafficLanes , create a new object Add the MapLaneSection script to the object and position it close the section of road that will be annotated You can also create a MapLaneSection object using the Lane Section button in HD Map Annotation window. MapLaneSection script is needed for sections that contain more than 1 lane. For sections with only 1 lane, they can be left as a child object of the TrafficLanes object or grouped with the boundary lines under a different parent object without the MapLaneSection script. Each MapLaneSection will contain parallel lanes with similar lengths 1 MapLane per lane of road as well as 1 MapLine per boundary line (optional, you can use the utility button Create lines to generate fake boundary lines) If the annotations will be broken up into multiple MapLane (e.g. a straight section and a curved section), multiple MapLaneSection are required A MapLane cannot begin or end in the middle of another MapLane (e.g. a lane splits or merges). This situation would require a 2nd MapLaneSection to be created where the split/merge begins. The other lanes on the road will also need to be broken up into multiple MapLane to be included in the multiple MapLaneSection MapLaneSection is used to automate the computation of relation of neighboring lanes. Please make sure every lane under MapLaneSection has at least 3 waypoints. Example of single lane splitting into a right-turn only lane and a straight lane","title":"Create Parent Object"},{"location":"simulation-content/map-annotation/#make-lanes","text":"Select the Lane/Line option under Create Mode A large yellow TARGET_WAYPOINT will appear in the center of the scene. This is where the TEMP_WAYPOINT objects will be placed. The TARGET_WAYPOINT will not appear if your roads had no mesh collider attached. Drag in the appropriate MapLaneSection to be the Parent Object Click Waypoint button to create a new TEMP_WAYPOINT . This is where the lane will begin. Move the scene so that the TARGET_WAYPOINT is in the desired location of the next TEMP_WAYPOINT With 2 waypoints, the Create Straight function will connect the waypoints and use the number in Waypoint Count to determine how many waypoints to add in between More than 2 waypoints can be used with the Create Straight function and they will be connected in the order they were created, and the Waypoint Count value will not be used With 3 waypoints, the Create Curve function can be used to create a Bezier curve between the 1st and 3rd waypoints and the Waypoint Count value will be used to determine how many waypoints to add in between. Verify Lane is the selected Map Object Type Verify the selected Lane Turn Type is correct Select the appropriate lane boundry types Enter the speed limit of the lane (in m/s) Enter the desired number of waypoints in the lane (minimum 2 for a straight lane and 3 for a curved lane) Click the appropriate Connect button to create the lane To adjust the positions of the waypoints, with the MapLane selected, in the Inspector check Display Handles . Individual waypoints can now have their position adjusted You can also show each waypoint's handle by clicking Toggle Handles button in HD Map Annotation window. NOTE Be sure to toggle off when finished editing. Below you can also find a group of helper buttons to manipulate the lane's waypoints. Append - Adds a waypoint after the last index Prepend - Adds a waypoint before the first index Remove First - Removes the first waypoint Remove Last - Remove the the last waypoint Reverse - Reverses the waypoint order Clear - Removes all waypoints Double - Doubles the waypoint count Half - Reduces the waypoint count be approximately half","title":"Make Lanes"},{"location":"simulation-content/map-annotation/#make-boundary-lines","text":"Drag in the appropriate MapLaneSection to be the Parent Object The same process as for lanes can be used to create boundary lines, but the Map Object Type will be BoundaryLine It is better to have the direction of a boundary line match the direction of the lane if possible For every boundary line, you also need to drag it into the corresponding field in the lane object. Boundary lines can be combined for adjacent lanes - no need to make duplicates for the same line. It is best to name boundary lines left and right so it is easier to tell which line goes into the public references on the map lane data.","title":"Make Boundary Lines"},{"location":"simulation-content/map-annotation/#annotate-intersections","text":"","title":"Annotate Intersections"},{"location":"simulation-content/map-annotation/#create-parent-object-intersection","text":"In the Map prefab, create a new object and name it \"Intersections\" In the Inspector of the Map , drag the new Intersections object into the Intersections holder In Intersections , create a new object Add the MapIntersection script to the new object and position it in the center of the intersection that will be annotated You can also create an MapIntersection object using the Intersection button in HD Map Annotation window You need to adjust the Trigger Bounds of the MapIntersection so that the box covers the space of the intersection The MapIntersection will contain all lanes, traffic signals, stop lines, and traffic signs in the intersection","title":"Create Parent Object"},{"location":"simulation-content/map-annotation/#create-intersection-lanes","text":"Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object Place the intersection object in the center of the intersection. The same process for creating normal MapLane is used here If the Lane involves changing directions, verify the correct Lane Turn Type is selected If you are annotating boundary lines for each lane, remember to set VIRTUAL as the Line Type for the line objects If there is a stop line in the intersection, the start of the intersection lanes should be after the stop line Be sure the trigger bounds size covers most of the intersection, in X,Y, and Z axes, but does not overlap with any stop lines. This trigger volume counts NPCs that enter the intersection and will cause errors if an NPC is counted before it reaches the intersection itself. For NPCs to properly navigate an intersection, the Yield To Lanes list of a lane (usually a turning lane) must be manually filled in. With View Selected toggled in the HD Map Annotation window, the lanes in the Yield To Lanes list will be highlighted in yellow to make it easier to verify that the correct lanes are in the list. Before an NPC enters an intersection lane, it will check that there are no NPCs on the lanes in the Yield To Lanes before continuing. Enter the number of lanes that the current lanes yields to as the size of Yield To Lanes For each element in the list, drag in a lane that takes priority over the selected lane For example, generally when turning left the NPC will yield to the oncoming traffic going straight so the straight lanes should be in the left turn lane's Yield To Lanes list","title":"Create Intersection Lanes"},{"location":"simulation-content/map-annotation/#create-traffic-signals","text":"Select the Signal option under Create Mode Drag in the MapIntersection as the Parent Object Select the correct Signal Type Traffic Signals are created on top of existing meshes. The signal annotation directions will be the same as the directions of the mesh. Please make sure the directions of the mesh is correct. Select the mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Click Create Signal to annotate the traffic signal Make sure the bounds of the map signal match the mesh object bounds","title":"Create Traffic Signals"},{"location":"simulation-content/map-annotation/#create-traffic-signs","text":"Select the Sign option under Create Mode Drag in the MapIntersection as the Parent Object Select the Sign mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Change the Sign Type to the desired type Click Create Sign to create the annotation Find the stop line MapLine that is associated with the created MapSign Select the MapSign and drag the MapLine into the Stop Line box Verify the annotation is created in the correct orientation. The Z-axis (blue) should be facing the same way the sign faces. Move the Bounding Box so that is matches the sign Bound Offsets adjusts the location of the box Bound Scale adjusts the size of the box","title":"Create Traffic Signs"},{"location":"simulation-content/map-annotation/#create-stop-lines","text":"Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object A similar process to boundary lines and traffic lanes is used for stop lines Change the Map Object Type to StopLine Forward Right vs Forward Left depend on the direction of the lane related to this stop line and the order that the waypoints were created in. The \"Forward\" direction should match the direction of the lane Example: In a right-hand drive map (cars are on the right side of the road), if the waypoints are created from the outside of the road inwards, then Forward Right should be selected. To verify if the correct direction was selected, Toggle Tool Handle Rotation so that the tool handles are in the active object's rotation. The Z-axis of the selected Stop Line should point to the intersection. A StopLine needs to be with the lanes that approach it. The last waypoint of approaching lanes should be perpendicular and intersect with the stop line Stop lines should be checked IsStopSign for stop sign intersections and left false for signal light intersections For intersections that do not have signals, stop lines are not needed.","title":"Create Stop Lines"},{"location":"simulation-content/map-annotation/#create-pole","text":"Poles are required for Autoware Vector maps. In an intersection with traffic lights, there is 1 MapPole on each corner, next to a stop line. The MapPole holds references to all traffic lights that signal the closest stop line. Select the Pole option under Create Mode Drag in the MapIntersection as the Parent Object A TARGET_WAYPOINT will appear in the center of the scene. This is where the pole annotation will be created. Position the TARGET_WAYPOINT on the corner of the intersection near the stop line Click Create Pole to create the annotation Find the traffic light MapSignal that are associated with the closest stop line Select the MapPole In the Inspector, change the Signal Lights size to be the number of MapSignal that signal the stop line Drag 1 MapSignal into each element of Signal Lights","title":"Create Pole"},{"location":"simulation-content/map-annotation/#annotate-self-reversing-lanes","text":"These types of lanes are only supported on Apollo 5.0","title":"Annotate Self-Reversing Lanes"},{"location":"simulation-content/map-annotation/#annotation-information","text":"There are TrafficLanes and Intersections objects. TrafficLanes object is for straight lane without any branch. Intersections object is for branching lanes, which has same predecessor lane.","title":"Annotation Information"},{"location":"simulation-content/map-annotation/#annotate-self-reversing-lanes-under-traffic-lanes","text":"Add MapLaneSection game object under TrafficLanes. MapLaneSection has one pair of lanes, which are forward and reverse lanes. Annotate lane given name to MapLane_forward and move it under MapLaneSection. Use straight line connect menu with way point count and move each point to shape lane. Use Display Lane option to show width of lane. In order for parking to work, there should have some space between parking space and lane. Duplicate the MapLane_forward and rename it to MapLane_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane_forward, MapLane_reverse) and change properties like the following: Is Self Reverse Lane: True Lane Turn Type: NO_TURN Left Bound Type: SOLID_WHITE Right Bound Type: SOLID_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map) Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way.","title":"Annotate Self-Reversing Lanes under Traffic Lanes"},{"location":"simulation-content/map-annotation/#annotate-self-reversing-lanes-under-intersections","text":"Add MapIntersection game object under Intersections. MapIntersection has several pair of lanes. Annotate lane given name to MapLane_forward move it under MapIntersection. Use curve connect menu with way point count, 5. Duplicate the MapLane_forward and rename it to MapLane1_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane_forward, MapLane_reverse) and change properties like the following: Is Self Reverse Lane: True Left Bound Type: DOTTED_WHITE Right Bound Type: DOTTED_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map). Choose each lane and decide its turn type considering its direction. Lane Turn Type: NO_TURN or LEFT_TURN or RIGHT_TURN Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way.","title":"Annotate Self-Reversing Lanes under Intersections"},{"location":"simulation-content/map-annotation/#annotate-other-features","text":"Other annotation features may be included in the TrafficLanes or Intersections objects or they may be sorted into other parent objects.","title":"Annotate Other Features"},{"location":"simulation-content/map-annotation/#create-pedestrian-navmesh","text":"To support pedestrians, the map must include a NavMesh created with the the Unity Editor. Be sure map scene is saved in the correct folder in Assets/External/Environments/YourMap Mark all walkable ground meshes as Navigation Static in the Inspector panel Open Window -> AI -> Navigation Select meshes and mark Navigation Area in the Object menu of the Navigation panel Road and Sidewalk MUST be separate meshes and layered for Pedestrian crosswalk to work (see second image below) Click Bake button in the Bake menu of the Navigation panel. NavMesh will be created in a folder named from the scene file.","title":"Create Pedestrian NavMesh"},{"location":"simulation-content/map-annotation/#create-pedestrian-path-sidewalk","text":"This annotation controls where pedestrians will pass on sidewalks. Pedestrians can walk anywhere but will stay on annotated areas if possible. Select the Pedestrian option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center for the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT on the desired path Click Waypoint to create a TEMP_WAYPOINT Repeat to create a trail of TEMP_WAYPOINT along the desired path Click Connect to create the MapPedestrian object","title":"Create Pedestrian Path Sidewalk"},{"location":"simulation-content/map-annotation/#create-pedestrian-path-crosswalk","text":"This annotation controls where pedestrians will walk on crosswalks. Pedestrians can walk anywhere but will stay on annotated areas if possible. When annotated correctly, pedestrians will only walk when signal lights are red. Select the Pedestrian option under Create Mode Drag in the MapIntersection Parent Object . This MUST be the MapIntersection object for the pedestrian to know what signal states are. Again, MapIntersection holder MUST be the parent for crosswalk map pedestrian annotation. A TARGET_WAYPOINT will appear in the center for the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT on the desired path Click Waypoint to create a TEMP_WAYPOINT start Click Waypoint to create a TEMP_WAYPOINT end Select Crosswalk type and select Z forward orientation. MapPedestrian object MUST be Z-forward INTO intersection. Click Connect to create the MapPedestrian object","title":"Create Pedestrian Path Crosswalk"},{"location":"simulation-content/map-annotation/#create-junction","text":"Junction annotations need to be annotated for Apollo 5.0 for Map Exporter to work correctly. It converts the boundary of the intersection. Select the Junction option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one vertex of the junction Click Waypoint to create a TEMP_WAYPOINT Create the desired number of TEMP_WAYPOINTS Click Connect to create the MapJunction","title":"Create Junction"},{"location":"simulation-content/map-annotation/#create-crosswalk","text":"Select the CrossWalk option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the crosswalk Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapCrossWalk","title":"Create Crosswalk"},{"location":"simulation-content/map-annotation/#create-clear-area","text":"Select the ClearArea option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the clear area Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapClearArea","title":"Create Clear Area"},{"location":"simulation-content/map-annotation/#create-parking-space","text":"Select the ParkingSpace option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to the corner of the parking space closest to an approaching vehicle Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapParkingSpace NOTE: Apollo 5.0 requires some space between the edge of a lane and the parking space. To verify that there is space, select the MapLane that goes past the MapParkingSpace and click Display Lane . This will show the width of the lane facilitate adjustment of the lane to have a gap between the lane and parking space.","title":"Create Parking Space"},{"location":"simulation-content/map-annotation/#create-speed-bump","text":"Select the SpeedBump option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one end of the speed bump Click Waypoint to create a TEMP_WAYPOINT Create 2 TEMP_WAYPOINT in the order shown below The TEMP_WAYPOINT should be wide enough to be outside the lane Click Connect to create the MapSpeedBump To verify that the MapSpeedBump is wide enough, select the MapLane that the MapSpeedBump crosses. Enable Display Lane to visualize the width of the lane. If necesary, select the MapSpeedBump and enable Display Handles to adjust the positions of the individual waypoints.","title":"Create Speed Bump"},{"location":"simulation-content/map-annotation/#export-map-annotations","text":"HD Map Annotations may be exported in a variety of formats. Current supported formats are: Apollo 5.0 HD Map Apollo 3.0 HD Map Autoware Vector Map Lanelet2 Map OpenDrive Map To export a map: Open the HD Map Export tool in Unity : Simulator -> Export HD Map Select the desired format from the dropdown Export Format Enter the desired save location of the exported map Click Export to create the exported map Note the exporter will fail if any lane has no boundary lines annotated - you need to either manually annotate them or use the Create lines button in HD Map Annotation window to create fake boundary lines.","title":"Export Map Annotations"},{"location":"simulation-content/map-annotation/#import-map-annotations","text":"The simulator can import a variety of formats of annotated map. NOTE After importing an HD map, you will be required to check each annotation to make sure import is correct. Simulator will provide logs if there are issues that need resolved. If the importer cannot parse a HD map, it will make holder objects that will need manually edited. Current supported formats are: Lanelet2 Map Apollo 5.0 HD Map OpenDRIVE Map To import a map: Open the HD Map Export tool in Unity : Simulator -> Import HD Map Select the format of the input map from the dropdown Import Format Select the file or folder that will be imported Click Import to import the map annotations into the simulator Distance Threshold is used to down-sample straight lines/lanes and Delta Threshold is used to down-sample curved lines/lanes. Changing both of them to 0 will keep all points, but for large maps displaying all annotations may lead to Unity crash. Lanelet2 map importer Notes: Lanes will be automatically imported and grouped as MapLaneSection if possible Intersections with four-way traffic lights / stop signs or two-way stop signs can be imported and grouped under MapIntersection Left-turn lanes are automatically found and their corresponding lanes to yield are also obtained automatically to get NPCs working correctly For each MapIntersection , you need to manually adjust X and Z for the Trigger Bounds as explained in annotate intersection part . Remember to check objects under Intersections are grouped correctly.","title":"Import Map Annotations"},{"location":"simulation-content/map-annotation/#map-formats","text":"For more information on the map formats, please see the links below: Apollo HD Autoware Vector Lanelet2 OpenDRIVE","title":"Map Formats"},{"location":"simulation-content/pointcloud-import/","text":"Point Cloud Import SVL Simulator uses its own format to store all of the point cloud data for rendering. To make any point cloud usable within the simulator, it has to be processed first. Built-in point cloud importer tool provides all the functionality required to convert most popular point cloud file formats (PCD, PLY, LAS, LAZ) into data usable for simulation. It is also able to create approximated mesh collider based on imported data. If you're using pre-built point cloud environments, you don't have to use this tool. Its main purpose is to create new environments compatible with Simulator from raw point cloud data. Accessing the importer top # To access point cloud importer window, open Simulator project in Unity editor, then navigate into Simulator/Import Point Cloud on the menu bar. Point cloud importer window will be opened. Import process overview top # Importer window requires you to at least specity input and output files, but also gives you access to additional settings. All of these are described in more detail in their respective sections. To start the import process, just click \"Import\" button at the bottom of the window. Depending on the point cloud size and chosen settings, this process might take a while. You'll see multiple progress bars during this time for preprocessing steps and the main import process itself. If you cancel the import process, partially processed data is not usable. Processing points can be performed on multiple threads, but first pass is always single-threaded. Progress bar reporting that only one thread is used during this time is normal. After the first pass, import process speeds up severely depending on amount of threads available. Importer will validate some of your import settings before the import process. There is a chance that \"Import\" button will be disabled. Error or warning message is always displayed in this case, with an information about the reason. Most convenient way of using the importer is with target environment scene open in the background - in this case the tool will offer to setup open scene with all the components required for rendering after the import process succeeds. You can also decide to skip this step and set the components yourself. Details about this are available on the point cloud rendering page. Importer settings top # Importer window is split into multiple sections, each with settings related to different subject. Input Files top # This tab is used to select point cloud files that should be converted into combatible format. Multiple files to import can be added at once. All of the files listed in this tab will be merged into single point cloud after importing. If you prefer to keep them separated, import each one of them individually (rendering multiple point clouds is supported, but decreases performance). All of the input files must be of one of following formats: PCD , PLY , LAS , LAZ . Mixing multiple formats is supported. Add Folder button will add all of the compatible files from selected folder into the list. This does not include subdirectories. Output Files top # This tab expects you to specify a directory that will be used to store all of the generated files. This directory does not have to be inside of the project's folder and can be located in any accessible place on the drive. If you decide to use directory inside the project, path relative to Assets folder fill be used instead of absolute path to help with cross-machine migration. Using directory with processed point cloud data already present will overwrite all of previous data. Warning about this is displayed in such case. Please note that output directory will contain a lot of files after import process. If this directory is located under Assets folder, Unity will generate metadata for each of the files. This process takes a long time and the metadata is completely unused, so it's recommended to skip meta file generation altogether. This can be done by using Unity's special folder names - just make sure that folder name ends with ~ . Tree Settings top # This tab describes data structure and layout of the imported point cloud. Default settings are working well with most of the standard data sets, so changing them is usually not required. Parameter Name Description Tree Type Tree data structure used to subdivide point cloud. Octree and Quadtree variants are available. Sampling Sampling used on each level of the tree structure, determining distribution of points in the final cloud. Cell Center uses voxel-based, uniform sampling. Poisson Disk offers more natural-looking distribution, but is slower and does not support mesh generation. Root Node Subdivision Determines amount of per-axis samples for each node. Higher values will create larger nodes, while lower values will create deeper tree structures. Node Branch Threshold Minimum amount of points in the node required for splitting it further. Prevents creating mostly empty nodes and overly deep tree structure. Max Tree Depth Maximum amount of levels in the tree structure. Levels deeper than the specified value will be discarded. Min Point Distance Minimum distance between points in a node. Points below this threshold will be discarded. Mesh Settings top # This tab contains options related to collider mesh generation. Note that mesh generation will only work for Cell Center sampling. Parameter Name Description Generate Mesh If this option is enabled, mesh collider will be generated and additional settings will be available. Road Only Mesh If this option is enabled, mesh generation will attempt to only create mesh for the road, ignoring other structures. This option is not suitable for all data sets - if you experience problems, try again with this feature disabled. Mesh Detail Level Level of the tree that should be used for mesh generation. This should corelate with represented area size - for single intersection level 2 is usually enough, while larger areas with multiple roads might require level 5 or higher. Erosion Passes Number of erosion passes that will be performed after mesh generation. Erosion will remove and flatten steep surfaces. See erosion for details. Erosion Angle Threshold Surfaces angled steeper than this threshold will be processed during erosion passes. Remove Small Surfaces If this option is enabled, small, secluded surfaces that are not attached to larger clusters will be removed after mesh generation. Erosion top # Depending on the mesh detail and point cloud quality, you might get some unwanted obstacles included in the mesh generation process. If you prefer a flat surface that's easier to drive on, you can include one or more erosion passes during import. This will remove steep sections of the mesh and flatten the underlying geometry. You can see how the erosion process affects the final mesh in the image below. Build Settings top # This tab contains options related to import process and initial point cloud transformations. Some options under this tab will affect amount of system memory required during the import process. You will see the warning if you're exceeding your system's limits - lower one of related settings values in this case. Parameter Name Description Thread Count Amount of threads that the import process should be split across. It's not guaranteed that all of the threads will be utilized fully, but higher thread count reduces import time. Affects memory requirements. Chunk Size Maximum amount of points loaded into memory at once, per thread. This only affects import process and has no effect on output data. Affects memory requirements. Center If this option is enabled, local space origin of the output point cloud will be moved to its bounds center. Normalize If this option is enabled, all points positions will be rescaled to fit normalized coordinates. LAS RGB 8bit Workaround If this option is enabled, additional 8 bit offset is used when reading color and intensity from LAS files. Use this if your data imported from LAS files seems to have corrupted color and/or intensity values. Axes Coordinate system axes convention used in original files. Setting this to correct value ensures that output point cloud will be compatible with Unity's coordinate system.","title":"Point cloud import"},{"location":"simulation-content/pointcloud-import/#accessing-the-importer","text":"To access point cloud importer window, open Simulator project in Unity editor, then navigate into Simulator/Import Point Cloud on the menu bar. Point cloud importer window will be opened.","title":"Accessing the importer"},{"location":"simulation-content/pointcloud-import/#import-process-overview","text":"Importer window requires you to at least specity input and output files, but also gives you access to additional settings. All of these are described in more detail in their respective sections. To start the import process, just click \"Import\" button at the bottom of the window. Depending on the point cloud size and chosen settings, this process might take a while. You'll see multiple progress bars during this time for preprocessing steps and the main import process itself. If you cancel the import process, partially processed data is not usable. Processing points can be performed on multiple threads, but first pass is always single-threaded. Progress bar reporting that only one thread is used during this time is normal. After the first pass, import process speeds up severely depending on amount of threads available. Importer will validate some of your import settings before the import process. There is a chance that \"Import\" button will be disabled. Error or warning message is always displayed in this case, with an information about the reason. Most convenient way of using the importer is with target environment scene open in the background - in this case the tool will offer to setup open scene with all the components required for rendering after the import process succeeds. You can also decide to skip this step and set the components yourself. Details about this are available on the point cloud rendering page.","title":"Import process overview"},{"location":"simulation-content/pointcloud-import/#importer-settings","text":"Importer window is split into multiple sections, each with settings related to different subject.","title":"Importer settings"},{"location":"simulation-content/pointcloud-import/#input-files","text":"This tab is used to select point cloud files that should be converted into combatible format. Multiple files to import can be added at once. All of the files listed in this tab will be merged into single point cloud after importing. If you prefer to keep them separated, import each one of them individually (rendering multiple point clouds is supported, but decreases performance). All of the input files must be of one of following formats: PCD , PLY , LAS , LAZ . Mixing multiple formats is supported. Add Folder button will add all of the compatible files from selected folder into the list. This does not include subdirectories.","title":"Input Files"},{"location":"simulation-content/pointcloud-import/#output-files","text":"This tab expects you to specify a directory that will be used to store all of the generated files. This directory does not have to be inside of the project's folder and can be located in any accessible place on the drive. If you decide to use directory inside the project, path relative to Assets folder fill be used instead of absolute path to help with cross-machine migration. Using directory with processed point cloud data already present will overwrite all of previous data. Warning about this is displayed in such case. Please note that output directory will contain a lot of files after import process. If this directory is located under Assets folder, Unity will generate metadata for each of the files. This process takes a long time and the metadata is completely unused, so it's recommended to skip meta file generation altogether. This can be done by using Unity's special folder names - just make sure that folder name ends with ~ .","title":"Output Files"},{"location":"simulation-content/pointcloud-import/#tree-settings","text":"This tab describes data structure and layout of the imported point cloud. Default settings are working well with most of the standard data sets, so changing them is usually not required. Parameter Name Description Tree Type Tree data structure used to subdivide point cloud. Octree and Quadtree variants are available. Sampling Sampling used on each level of the tree structure, determining distribution of points in the final cloud. Cell Center uses voxel-based, uniform sampling. Poisson Disk offers more natural-looking distribution, but is slower and does not support mesh generation. Root Node Subdivision Determines amount of per-axis samples for each node. Higher values will create larger nodes, while lower values will create deeper tree structures. Node Branch Threshold Minimum amount of points in the node required for splitting it further. Prevents creating mostly empty nodes and overly deep tree structure. Max Tree Depth Maximum amount of levels in the tree structure. Levels deeper than the specified value will be discarded. Min Point Distance Minimum distance between points in a node. Points below this threshold will be discarded.","title":"Tree Settings"},{"location":"simulation-content/pointcloud-import/#mesh-settings","text":"This tab contains options related to collider mesh generation. Note that mesh generation will only work for Cell Center sampling. Parameter Name Description Generate Mesh If this option is enabled, mesh collider will be generated and additional settings will be available. Road Only Mesh If this option is enabled, mesh generation will attempt to only create mesh for the road, ignoring other structures. This option is not suitable for all data sets - if you experience problems, try again with this feature disabled. Mesh Detail Level Level of the tree that should be used for mesh generation. This should corelate with represented area size - for single intersection level 2 is usually enough, while larger areas with multiple roads might require level 5 or higher. Erosion Passes Number of erosion passes that will be performed after mesh generation. Erosion will remove and flatten steep surfaces. See erosion for details. Erosion Angle Threshold Surfaces angled steeper than this threshold will be processed during erosion passes. Remove Small Surfaces If this option is enabled, small, secluded surfaces that are not attached to larger clusters will be removed after mesh generation.","title":"Mesh Settings"},{"location":"simulation-content/pointcloud-import/#erosion","text":"Depending on the mesh detail and point cloud quality, you might get some unwanted obstacles included in the mesh generation process. If you prefer a flat surface that's easier to drive on, you can include one or more erosion passes during import. This will remove steep sections of the mesh and flatten the underlying geometry. You can see how the erosion process affects the final mesh in the image below.","title":"Erosion"},{"location":"simulation-content/pointcloud-import/#build-settings","text":"This tab contains options related to import process and initial point cloud transformations. Some options under this tab will affect amount of system memory required during the import process. You will see the warning if you're exceeding your system's limits - lower one of related settings values in this case. Parameter Name Description Thread Count Amount of threads that the import process should be split across. It's not guaranteed that all of the threads will be utilized fully, but higher thread count reduces import time. Affects memory requirements. Chunk Size Maximum amount of points loaded into memory at once, per thread. This only affects import process and has no effect on output data. Affects memory requirements. Center If this option is enabled, local space origin of the output point cloud will be moved to its bounds center. Normalize If this option is enabled, all points positions will be rescaled to fit normalized coordinates. LAS RGB 8bit Workaround If this option is enabled, additional 8 bit offset is used when reading color and intensity from LAS files. Use this if your data imported from LAS files seems to have corrupted color and/or intensity values. Axes Coordinate system axes convention used in original files. Setting this to correct value ensures that output point cloud will be compatible with Unity's coordinate system.","title":"Build Settings"},{"location":"simulation-content/pointcloud-rendering/","text":"Point Cloud Rendering SVL Simulator supports scenes containing point cloud data. Point clouds can be rendered alongside other geometry like meshes or particles, are combatible with most of the features available in high definition render pipeline and can be detected by multiple sensors, including LiDAR. Point cloud rendering in Simulator can handle large point clouds (hundreds of millions of points) thanks to octree-based structure and selective, frustum-based culling. If you want to create your own point cloud based environment, you should start with importing your point cloud data into simulator. This step is not necessary for pre-built point cloud environments - they do not require any additional setup. Rendering Features top # Most of the features available for point cloud renderers are optional and can be toggled based on individual use cases. Lighting, for example, can be turned off if point cloud has light data already baked in, or calculated fully based on scene settings like time of day or weather. Most notable features include: Full deferred lighting compatible with HDRP's lighting system Filling holes between points Rendering multiple point clouds at once Normals estimation Shadow casting/receiving Point streaming Point budget (for memory and render buffer) Camera frustum or distance-based culling Mesh collider generation Support for color/depth camera sensors and LiDAR Setup top # The easiest way to set up point cloud rendering is to perform an import with the target scene open in the background. When importing is done, you will be prompted with a choice to automatically add all of the required components to the open scene. If accepted, point cloud with default rendering settings will immediately show up in scene view. Alternatively, a prefab located in Assets/Prefabs/PointCloudRenderer.prefab can be added to any scene for a quick setup. This requires to specify point cloud data path in NodeTreeLoader component. More complex setups with multiple point clouds or masked rendering require setting up components manually. Details can be found in Components Overview section. Components Overview top # In the most common scenario with a single point cloud, one of each of the components described below will be present on a scene. For more complex cases (e.g. multiple point clouds or separate color and intensity data) it's possible to add multiple instances of NodeTreeLoader and/or NodeTreeRenderer to one scene. Only one instance of PointCloudManager should ever be present. PointCloudManager # This component is responsible for keeping track of shared point cloud resources and orchestrating rendering for one or more point cloud renderers. It will also automatically set up HDRP passes and enable or disable them depending on all renderers' settings. There are no settings available for this component. Only one instance should be active at a time. Disabling this component will disable rendering of all point cloud data. NodeTreeLoader # This component is responsible for managing single point cloud's data in memory. It will load chunks of point cloud data from mass storage based on each renderer's requests (performed on a separate thread) and will keep track of total amount of points loaded into memory. When defined point budget is exceeded, older points will be unloaded. Single instance of this component can only operate on one point cloud, but multiple renderers can use data stored in a single NodeTreeLoader . This component can also optionally load mesh colliders for a point cloud if they were generated during import process. NodeTreeRenderer # This component represents a single point cloud to render and stores all related settings. Transform to which this component is tied affects translation, rotation and scale of the point cloud. By default, point cloud rendered by a single renderer will be visible in all cameras and compatible sensors, but not in shadow caster pass. This can be controlled on a per-renderer basis. Note that this component will not actively trigger any rendering. It's used mostly as a container for data that will be used by PointCloudManager . NodeTreeRenderer is default implementation that includes camera-based culling, which will make load requests to NodeTreeLoader . Adding other culling methods can be done by extending the base class, PointCloudRenderer . Rendering Modes Overview top # Depending on the requirements, there are multiple combinations of rendering modes available. Sensors supported in point cloud rendering are compatible with all of them. The most simple one ( Points ) will render points with size dependent on distance from the camera. This image is stable, but does not support lighting and will show holes if point cloud is not dense enough. Improved version of this mode ( Cones ) will use 3D cones instead of 2D circles, which creates better transitions between very close points, but has a slight performance cost. More advanced rendering mode, Solid , will remove points that should be invisible from observer's position and interpolate color and depth data between leftover points. It can also reconstruct normals data from depth, and subsequently calculate lighting for point cloud. Filters used here are executed in screen space, which means that camera position and rotation affect rendering output. This might introduce some artifacts around edges, especially around large depth differences. It's suggested to test available modes and choose the one best suited for given point cloud and use case. If lighting is required, solid rendering mode is the only one currently supporting it. Settings top # Loader Settings top # Parameter Name Type Description Data Path string Directory in which imported point cloud data is stored. Paths starting with .../ will be treated as relative to Assets folder in the project, others are treated as absolute. Point Limit int Maximum amount of points that can be loaded into memory at once. These points are treated as cache between render buffer and hard drive. When point limit is exceeded, older points will be unloaded. Load Meshes bool If this option is enabled, collider meshes will loaded alongside the point cloud data. This requires Generate Mesh option in point cloud import settings to be enabled. Renderer Settings top # Settings visible for renderer depend on currently selected rendering mode. Irrelevant settings will be hidden in inspector. Enabling additional features will sometimes show new settings related to them. Shared Rendering Settings top # Parameter Name Type Description Colorize enum Colorization mode to use for point cloud. If selected data is not present in source point cloud, result will be black. Available modes: Colors , Intensity , Rainbow Intensity and Rainbow Height . Render Mode enum Describes how points should be represented. Available modes: Points (most simple one), Cones (similar to points, with interpolation) and Solid (fills holes, required for lighting). Mask enum Enum flags used to enable or disable renderer's visibility. Point cloud will be visible only in selected passes. Available options: Camera (all color and depth cameras), Shadowcaster (enables shadow casting by point cloud), LiDAR (makes point cloud detectable by LiDAR sensor). Shadow Point Size float Only visible if Shadowcaster is enabled under Mask . Determines size of a point for shadow casting purposes. Shadow Bias float Only visible if Shadowcaster is enabled under Mask . Determines shadow bias for each point. Value is relative to Shadow Point Size . Point/Cones Rendering Settings top # Parameter Name Type Description Constant Size bool If this option is enabled, point size will not depend on distance from camera. Pixel Size float Only visible with Constant Size enabled. Determines size of a point. Absolute Size float Only visible with Constant Size disabled. Determines size of a point at a near plane. Points further away will be smaller. Min Pixel Size float Only visible with Constant Size disabled. Determines minimum size of a point, irrelevant from distance. Solid Rendering Mode top # Cascades options for Remove Hidden and Smooth Normals stages affect filter window size for both these effects and its scaling with distance for camera. They can be visualized by toggling Show Preview option on. Cascade options for Remove Hidden stage will only be visible if Hidden Point Removal option is set to Screen Space . For Depth Prepass , they will be replaced with point rendering settings . Parameter Name Type Description Lighting Mode enum Determines how light will affect the point cloud. Available modes: Unlit (no lighting, no shadows), Shadow Receiver (no lighting, receives shadows), Full Deferred (receives lighting and shadows) Hidden Point Removal enum Method used for removing obscured points. Screen Space is the default, faster option. Depth Prepass reduces flickering on edges, but requires more precise setup and can have impact on performance. Calculate Normals bool If enabled, normals approximation stage will be performed and world-space normals will be available. Required for lighting. Smooth Normals bool If enabled, additional normals smoothing stage will be performed to reduce noise. Results will replace normals data from Calculate Normals option. FoV Reprojection bool If enabled, wider image of point cloud will be rendered internally and it will be reprojected to desired camera FOV. This option reduces artifacts near the edges of the image. Does not affect final FOV. Reprojection Ratio float Multiplier determining ratio between FOV used internally for FoV Reprojection option and original FOV of the camera. Solid rendering also has some debug options under separate tab. These are mostly directly affecting separate rendering stages, but some of them can me modified to affect final image. Parameter Name Type Description Linear depth bool If enabled, linear depth will be used throughout all compute shaders instead of depth buffer values. Force fill bool If enabled, holes below horizon line will be filled regardless of their size. Can create blurred artifacts for large holes, but removes all holes in road for low density point clouds. Fill threshold float Sets height threshold under which Force fill option will be used if it's enabled. Blend sky bool If enabled, point cloud color and intensity will be blended to sky color instead of black. Removes black color around edges, but requires additional sky pre-render for lit mode. Culling Settings top # Parameter Name Type Description Node Tree Loader NodeTreeLoader Reference to loader that will be queried for point cache updates. Cull Camera Camera Reference to camera used for culling. If this field is left empty, main simulator camera will be used after simulation is started. Cull Mode enum Determines how points are evaluated for visibility. Available modes: Camera Frustum (both view frustum and distance from camera are considered), Distance (only distance from camera is considered - recommended if LiDAR is used). Point Limit int Number of points that should be pushed to render buffer. This value determines maximum amount of points that are visible at once and directly affects buffer size. Min Projection float Minimum screen projection size along single axis (in pixels) that octree node has to occupy to be traversed further. Nodes below this threshold will not be visible. Rebuild Steps int Number of frames to spend for render buffer rebuild. Previous fully completed buffer will be used until new one is ready. Values higher than 1 reduce per-frame cost of buffer rebuild, but can introduce popping.","title":"Point cloud rendering"},{"location":"simulation-content/pointcloud-rendering/#rendering-features","text":"Most of the features available for point cloud renderers are optional and can be toggled based on individual use cases. Lighting, for example, can be turned off if point cloud has light data already baked in, or calculated fully based on scene settings like time of day or weather. Most notable features include: Full deferred lighting compatible with HDRP's lighting system Filling holes between points Rendering multiple point clouds at once Normals estimation Shadow casting/receiving Point streaming Point budget (for memory and render buffer) Camera frustum or distance-based culling Mesh collider generation Support for color/depth camera sensors and LiDAR","title":"Rendering Features"},{"location":"simulation-content/pointcloud-rendering/#setup","text":"The easiest way to set up point cloud rendering is to perform an import with the target scene open in the background. When importing is done, you will be prompted with a choice to automatically add all of the required components to the open scene. If accepted, point cloud with default rendering settings will immediately show up in scene view. Alternatively, a prefab located in Assets/Prefabs/PointCloudRenderer.prefab can be added to any scene for a quick setup. This requires to specify point cloud data path in NodeTreeLoader component. More complex setups with multiple point clouds or masked rendering require setting up components manually. Details can be found in Components Overview section.","title":"Setup"},{"location":"simulation-content/pointcloud-rendering/#components-overview","text":"In the most common scenario with a single point cloud, one of each of the components described below will be present on a scene. For more complex cases (e.g. multiple point clouds or separate color and intensity data) it's possible to add multiple instances of NodeTreeLoader and/or NodeTreeRenderer to one scene. Only one instance of PointCloudManager should ever be present.","title":"Components Overview"},{"location":"simulation-content/pointcloud-rendering/#pointcloudmanager","text":"This component is responsible for keeping track of shared point cloud resources and orchestrating rendering for one or more point cloud renderers. It will also automatically set up HDRP passes and enable or disable them depending on all renderers' settings. There are no settings available for this component. Only one instance should be active at a time. Disabling this component will disable rendering of all point cloud data.","title":"PointCloudManager"},{"location":"simulation-content/pointcloud-rendering/#nodetreeloader","text":"This component is responsible for managing single point cloud's data in memory. It will load chunks of point cloud data from mass storage based on each renderer's requests (performed on a separate thread) and will keep track of total amount of points loaded into memory. When defined point budget is exceeded, older points will be unloaded. Single instance of this component can only operate on one point cloud, but multiple renderers can use data stored in a single NodeTreeLoader . This component can also optionally load mesh colliders for a point cloud if they were generated during import process.","title":"NodeTreeLoader"},{"location":"simulation-content/pointcloud-rendering/#nodetreerenderer","text":"This component represents a single point cloud to render and stores all related settings. Transform to which this component is tied affects translation, rotation and scale of the point cloud. By default, point cloud rendered by a single renderer will be visible in all cameras and compatible sensors, but not in shadow caster pass. This can be controlled on a per-renderer basis. Note that this component will not actively trigger any rendering. It's used mostly as a container for data that will be used by PointCloudManager . NodeTreeRenderer is default implementation that includes camera-based culling, which will make load requests to NodeTreeLoader . Adding other culling methods can be done by extending the base class, PointCloudRenderer .","title":"NodeTreeRenderer"},{"location":"simulation-content/pointcloud-rendering/#rendering-modes-overview","text":"Depending on the requirements, there are multiple combinations of rendering modes available. Sensors supported in point cloud rendering are compatible with all of them. The most simple one ( Points ) will render points with size dependent on distance from the camera. This image is stable, but does not support lighting and will show holes if point cloud is not dense enough. Improved version of this mode ( Cones ) will use 3D cones instead of 2D circles, which creates better transitions between very close points, but has a slight performance cost. More advanced rendering mode, Solid , will remove points that should be invisible from observer's position and interpolate color and depth data between leftover points. It can also reconstruct normals data from depth, and subsequently calculate lighting for point cloud. Filters used here are executed in screen space, which means that camera position and rotation affect rendering output. This might introduce some artifacts around edges, especially around large depth differences. It's suggested to test available modes and choose the one best suited for given point cloud and use case. If lighting is required, solid rendering mode is the only one currently supporting it.","title":"Rendering Modes Overview"},{"location":"simulation-content/pointcloud-rendering/#settings","text":"","title":"Settings"},{"location":"simulation-content/pointcloud-rendering/#loader-settings","text":"Parameter Name Type Description Data Path string Directory in which imported point cloud data is stored. Paths starting with .../ will be treated as relative to Assets folder in the project, others are treated as absolute. Point Limit int Maximum amount of points that can be loaded into memory at once. These points are treated as cache between render buffer and hard drive. When point limit is exceeded, older points will be unloaded. Load Meshes bool If this option is enabled, collider meshes will loaded alongside the point cloud data. This requires Generate Mesh option in point cloud import settings to be enabled.","title":"Loader Settings"},{"location":"simulation-content/pointcloud-rendering/#renderer-settings","text":"Settings visible for renderer depend on currently selected rendering mode. Irrelevant settings will be hidden in inspector. Enabling additional features will sometimes show new settings related to them.","title":"Renderer Settings"},{"location":"simulation-content/pointcloud-rendering/#shared-rendering-settings","text":"Parameter Name Type Description Colorize enum Colorization mode to use for point cloud. If selected data is not present in source point cloud, result will be black. Available modes: Colors , Intensity , Rainbow Intensity and Rainbow Height . Render Mode enum Describes how points should be represented. Available modes: Points (most simple one), Cones (similar to points, with interpolation) and Solid (fills holes, required for lighting). Mask enum Enum flags used to enable or disable renderer's visibility. Point cloud will be visible only in selected passes. Available options: Camera (all color and depth cameras), Shadowcaster (enables shadow casting by point cloud), LiDAR (makes point cloud detectable by LiDAR sensor). Shadow Point Size float Only visible if Shadowcaster is enabled under Mask . Determines size of a point for shadow casting purposes. Shadow Bias float Only visible if Shadowcaster is enabled under Mask . Determines shadow bias for each point. Value is relative to Shadow Point Size .","title":"Shared Rendering Settings"},{"location":"simulation-content/pointcloud-rendering/#point/cones-rendering-settings","text":"Parameter Name Type Description Constant Size bool If this option is enabled, point size will not depend on distance from camera. Pixel Size float Only visible with Constant Size enabled. Determines size of a point. Absolute Size float Only visible with Constant Size disabled. Determines size of a point at a near plane. Points further away will be smaller. Min Pixel Size float Only visible with Constant Size disabled. Determines minimum size of a point, irrelevant from distance.","title":"Point/Cones Rendering Settings"},{"location":"simulation-content/pointcloud-rendering/#solid-rendering-mode","text":"Cascades options for Remove Hidden and Smooth Normals stages affect filter window size for both these effects and its scaling with distance for camera. They can be visualized by toggling Show Preview option on. Cascade options for Remove Hidden stage will only be visible if Hidden Point Removal option is set to Screen Space . For Depth Prepass , they will be replaced with point rendering settings . Parameter Name Type Description Lighting Mode enum Determines how light will affect the point cloud. Available modes: Unlit (no lighting, no shadows), Shadow Receiver (no lighting, receives shadows), Full Deferred (receives lighting and shadows) Hidden Point Removal enum Method used for removing obscured points. Screen Space is the default, faster option. Depth Prepass reduces flickering on edges, but requires more precise setup and can have impact on performance. Calculate Normals bool If enabled, normals approximation stage will be performed and world-space normals will be available. Required for lighting. Smooth Normals bool If enabled, additional normals smoothing stage will be performed to reduce noise. Results will replace normals data from Calculate Normals option. FoV Reprojection bool If enabled, wider image of point cloud will be rendered internally and it will be reprojected to desired camera FOV. This option reduces artifacts near the edges of the image. Does not affect final FOV. Reprojection Ratio float Multiplier determining ratio between FOV used internally for FoV Reprojection option and original FOV of the camera. Solid rendering also has some debug options under separate tab. These are mostly directly affecting separate rendering stages, but some of them can me modified to affect final image. Parameter Name Type Description Linear depth bool If enabled, linear depth will be used throughout all compute shaders instead of depth buffer values. Force fill bool If enabled, holes below horizon line will be filled regardless of their size. Can create blurred artifacts for large holes, but removes all holes in road for low density point clouds. Fill threshold float Sets height threshold under which Force fill option will be used if it's enabled. Blend sky bool If enabled, point cloud color and intensity will be blended to sky color instead of black. Removes black color around edges, but requires additional sky pre-render for lit mode.","title":"Solid Rendering Mode"},{"location":"simulation-content/pointcloud-rendering/#culling-settings","text":"Parameter Name Type Description Node Tree Loader NodeTreeLoader Reference to loader that will be queried for point cache updates. Cull Camera Camera Reference to camera used for culling. If this field is left empty, main simulator camera will be used after simulation is started. Cull Mode enum Determines how points are evaluated for visibility. Available modes: Camera Frustum (both view frustum and distance from camera are considered), Distance (only distance from camera is considered - recommended if LiDAR is used). Point Limit int Number of points that should be pushed to render buffer. This value determines maximum amount of points that are visible at once and directly affects buffer size. Min Projection float Minimum screen projection size along single axis (in pixels) that octree node has to occupy to be traversed further. Nodes below this threshold will not be visible. Rebuild Steps int Number of frames to spend for render buffer rebuild. Previous fully completed buffer will be used until new one is ready. Values higher than 1 reduce per-frame cost of buffer rebuild, but can introduce popping.","title":"Culling Settings"},{"location":"simulation-content/sensors-list/","text":"Sensors List This page details the different available sensors and the configuration options possible. For more on how to build sensor plugins, please see Sensor Plugins . Table of Contents Examples How to Specify a Sensor Clock Color Camera Sensor Effects Depth Camera Segmentation Camera LiDAR 3D Ground Truth Apollo Perception Visualizer 3D CAN-Bus GPS Device GPS Odometry GPS-INS Status LGSVL Control Apollo Control Keyboard Control Wheel Control Cruise Control IMU 2D Ground Truth Radar Ultrasonic Control Calibration Transform Sensor Signal Sensor Video Recording Sensor Comfort Sensor Stop Line Sensor Vehicle Odometry Vehicle State HUD Keyboard Control AutowareAI Control Lane-line Sensor Lane Following Examples top # Example JSON configurations are available here: Apollo 5.0 JSON Apollo 3.0 JSON Autoware.AI Autoware.Auto Some LiDAR sensor JSONs: Velodyne VLP-16 Velodyne VLP-32C Velodyne VLS-128 How to Specify a Sensor top # A vehicle configuration is in the following format: [ SENSOR, SENSOR, SENSOR ] A SENSOR is defined in the JSON configuration in the following format: { \"type\": STRING, \"name\": STRING, \"params\": {PARAMS}, \"parent\": STRING, \"transform\": { \"x\": FLOAT, \"y\": FLOAT, \"z\": FLOAT, \"pitch\": FLOAT, \"yaw\": FLOAT, \"roll\": FLOAT, } } type is the type of sensor. name is the name of the sensor. This is how the sensor will be identified. params are the explicitly specified parameters. If a parameter is not set, the Default Value in the sensor definition will be used. ex. {\"Width\": 1920, \"Height\": 1080} There are 2 parameters that all sensors have Parameter Description Default Value Topic defines the topic that the sensor will subscribe/publish to null Frame defines the frame_id if the sensor publishes a ROS message. See ROS Header Message for more information null parent (OPTIONAL) a sensor's transform can be relative to another sensor. STRING is the name of the base sensor transform to which this sensor is relative. If omitted, the transform is relative to the origin of the vehicle. transform is the location and rotation of the sensor relative to the local position of the vehicle. The Unity left-hand coordinate system is used (+x right, +y up, +z forward, +pitch tilts the front down, +yaw rotates clockwise when viewed from above, +roll tilts the left side down). x is the position of the sensor along the x-axis y is the position of the sensor along the y-axis z is the position of the sensor along the z-axis pitch is the rotation around the x-axis yaw is the rotation around the y-axis roll is the rotation around the z-axis Clock top # AssetBundle This sensor outputs simulated time to ROS as rosgraph_msgs/Clock message, or to CyberRT as clock message. The only parameter to use is topic/channel name. For ROS, you can add <param name=\"/use_sim_time\" value=\"true\"> to their ROS launch file, or use rosparam set /use_sim_time true in command line, to have a ROS node use simulation time according to the /clock topic. For more details please refer to this page . For CyberRT, you can set clock_mode in cyber.pb.conf as MODE_MOCK to have CyberRT use simulation time according to the /clock topic. { \"type\": \"ClockSensor\", \"name\": \"Clock Sensor\", \"params\": { \"Topic\": \"/clock\" } } Color Camera top # AssetBundle This is the type of sensor that would be used for the Main Camera in Apollo or other AD stacks. (For questions about camera matrix, please refer to this FAQ .) Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 JpegQuality defines the quality if the image output % Int 75 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion * List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 CubemapSize size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 * If Distorted is true , DistortionParameters must be an empty list or a list of FOUR floats. The values in this list should come from calibration result of real camera. Setting arbitrary values may cause undefined result. If Distorted is false , DistortionParameters , Fisheye and Xi are ignored. ** If Fisheye is true , Xi should be a value from calibration result of real camera. Setting arbitrary value may cause undefined result. If Fisheye is false , Xi is ignored. *** CubemapSize should only be 512, 1024, or 2048. { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ], \"Postprocessing\": [ ] }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Sensor Effects top # Color Camera has multiple post processing sensor effects that can be added to the Postprocessing field in params . Effects can be combined with an array of Postprocessing fields but order is hard coded. SunFlare - creates a sun flare effect Parameter Description Type Default Value Min Max type postprocessing sensor effect name String sunIntensity defines the intensity of the sun flare Float 1 0 10 haloIntensity defines the intensity of the sun flare halo Float 1 0 10 ghostingIntensity defines the intensity of the mirror effect Float 1 0 10 \"Postprocessing\": [ { \"type\": \"SunFlare\", \"sunIntensity\": 1, \"haloIntensity\": 1, \"ghostingIntensity\": 1 } ] Rain - creates rain drops on the lens Parameter Description Type Default Value Min Max type postprocessing sensor effect name String size defines the size of the drops Float 1 0 1 \"Postprocessing\": [ { \"type\": \"Rain\", \"size\": 1 } ] GreyScale - converts color to grey scale Parameter Description Type Default Value Min Max type postprocessing sensor effect name String intensity defines the intensity of the grey scale Float 1 0 1 \"Postprocessing\": [ { \"type\": \"GreyScale\", \"intensity\": 1 } ] VideoArtifacts - creates jpeg compression artifacts Parameter Description Type Default Value Min Max type postprocessing sensor effect name String intensity defines the intensity of the effect Float 0.25 0 1 blockSize defines the size of the affected block Int 32 1 128 \"Postprocessing\": [ { \"type\": \"VideoArtifacts\", \"intensity\": 0.25, \"blockSize\": 32 } ] Depth Camera top # AssetBundle This sensor returns an image where the shades on the grey-scale correspond to the depth of objects. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 5 1 100 JpegQuality defines the quality if the image output % Int 100 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion* List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 CubemapSize size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 * See notes on DistortionParameters for Color Camera. ** See notes on Xi for Color Camera. *** See notes on CubemapSize for Color Camera. { \"type\": \"DepthCameraSensor\", \"name\": \"Depth Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/depth_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Segmentation Camera top # AssetBundle This sensor returns an image where objects are colored corresponding to their tag: Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F If a tag is included in \"InstanceSegmentationTags\", each instance of objects with that tag is colored differently, but all will have same hue (e.g. all cars will be bluish and all pedestrians will be reddish). Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion* List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 CubemapSize size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 InstanceSegmentationTags define tags with instance segmentation List of String empty list * See notes on DistortionParameters for Color Camera. ** See notes on Xi for Color Camera. *** See notes on CubemapSize for Color Camera. { \"type\": \"SegmentationCameraSensor\", \"name\": \"Segmentation Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"InstanceSegmentationTags\": [ ], \"Topic\": \"/simulator/segmentation_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Example of Instance Segmentation Tags. Be aware that this effects performance greatly. \"InstanceSegmentationTags\": [ \"Car\", \"Pedestrian\", \"Obstacle\", \"Building\" ], LiDAR top # AssetBundle This sensor returns a point cloud after 1 revolution. Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF * If VerticalRayAngles is not empty, LaserCount will be automatically set to the length of VerticalRayAngles , and FieldOfView and CenterAngle will be ignored. A sample of uniformly distributed angles: { \"type\": \"LidarSensor\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } A sample of non-uniformly distributed angles: { \"type\": \"LidarSensor\", \"name\": \"Lidar-NonUniform\", \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } 3D Ground Truth top # AssetBundle This sensor returns 3D ground truth data for training and creates bounding boxes around the detected objects. The color of the object corresponds to the object's type: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines the how close an object must be to the sensor to be detected meters Float 100 1 1000 { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Apollo Perception Visualizer 3D top # AssetBundle This sensor will visualize 3D bounding boxes on objects as detected by Apollo. It does not publish any data and instead subscribes to the perception topic from Apollo. The color of the boxes are: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta { \"type\": \"ApolloPerceptionVisualizer3D\", \"name\": \"Apollo Perception Visualizer 3D Sensor\", \"params\": { \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } CAN-Bus top # AssetBundle This sensor sends data about the vehicle chassis. The data includes: Speed [m/s] Throttle [%] Braking [%] Steering [+/- %] Parking Brake Status [bool] High Beam Status [bool] Low Beam Status [bool] Hazard Light Status [bool] Fog Light Status [bool] Left Turn Signal Status [bool] Right Turn Signal Status [bool] Wiper Status [bool] Reverse Gear Status [bool] Selected Gear [Int] Engine Status [bool] Engine RPM [RPM] GPS Latitude [Latitude] GPS Longitude [Longitude] Altitude [m] Orientation [3D Vector of Euler angles] Velocity [3D Vector of m/s] Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/canbus\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } GPS Device top # AssetBundle This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } GPS Odometry top # AssetBundle This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates and the vehicle velocity. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 ChildFrame used by Autoware IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_odometry\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } GPS-INS Status top # AssetBundle This sensor outputs the status of the GPS correction due to INS. The Simulator is an ideal environment in which GPS is always corrected. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published [Hertz] Float 12.5 1 100 { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } LGSVL Control top # AssetBundle This sensor is required for a vehicle to subscribe to a control topic published in ROS or ROS2 with message type lgsvl_msgs/VehicleControlData . { \"type\": \"LGSVLControlSensor\", \"name\": \"AD Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } Apollo Control top # AssetBundle This sensor is required for a vehicle to subscribe to the control topic from Apollo using the CyberRT bridge. { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } The sensor also demonstrates how a user can check when SVL Simulator receives the first control message from the AD stack like Apollo. Based on this time, you can start the rest of the simulation or do some other interesting things. To wait for control message from apollo in a Python script, you can call on_custom() of the ego agent. For example, the following code set isControlReceived of agent self.ego to be True when the simulator first receives a message of the /apollo/control topic. def on_control_received(agent, kind, context): if kind == \"checkControl\": agent.isControlReceived = True log.info(\"Control message recieved\") self.ego.on_custom(on_control_received) Keyboard Control top # AssetBundle This sensor is required for a vehicle to accept keyboard control commands. Parameters are not required. { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" } Wheel Control top # AssetBundle This sensor is required for a vehicle to accept Logitech G920 wheel control commands. Parameters are not required. { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" } Cruise Control top # AssetBundle This sensor causes the vehicle to accelerate to the desired speed and then maintain the desired speed. Parameter Description Unit Type Default Value Minimum Maximum CruiseSpeed defines the desired speed meters/second Float 0 0 200 { \"type\": \"CruiseControlSensor\", \"name\": \"AD Car Control\", \"params\": { \"CruiseSpeed\": 10 } } IMU top # AssetBundle This sensor output at a fixed rate of 100 Hz. IMU publishes data on topics where the 2nd topic has corrected IMU data. Parameter Description CorectedTopic defines the 2nd topic that the data is published to CorrectedFrame defines the 2nd frame for the ROS header { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } 2D Ground Truth top # AssetBundle This sensor outputs an image where objects are encased in a box. The color of the box depends on the type of object. Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 Width defines the width of the image pixels Int 1920 1 1920 Height defines the height of the iamge pixels Int 1080 1 1080 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor to be in the image meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor to be in the image meters Float 2000 0.01 2000 DetectionRange defines how close an object must be to be given a bounding box meters Float 100 0.01 2000 { \"type\": \"PerceptionSensor2D\", \"name\": \"2D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Radar top # AssetBundle This sensor outputs the objects detected by the radar. Detected objects are visualized with a box colored by their type: Type Color Car Green Agent Magenta Bicycle Cyan Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 13.4 1 100 { \"type\": \"RadarSensor\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Ultrasonic top # AssetBundle This sensor outputs the distance (to the center of the sensor) of the closest point within the sensor's FOV. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 400 1 1920 Height defines the height of the image output pixels Int 160 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 JpegQuality defines the quality if the image output % Int 75 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 40 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.3 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2.0 0.01 2000 { \"type\": \"UltrasonicSensor\", \"name\": \"Ultrasonic Sensor\", \"params\": { \"Width\": 400, \"Height\": 160, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.3, \"MaxDistance\": 2, \"Topic\": \"/simulator/ultrasonic\", \"Frame\": \"ultrasonic\" }, \"transform\": { \"x\": 0, \"y\": 0.5, \"z\": 2.5, \"pitch\": -13, \"yaw\": 0, \"roll\": 0 } } Control Calibration top # AssetBundle This sensor outputs control calibration criteria collected by AD Stacks (Apollo, Autoware). It generates steering, throttle or brakes with gear commands between minimum and maximum of velocity during duration. Parameter Description Unit Type Minimum Maximum min_velocity defines the minimum velocity when criterion is executed meters/second Float 0 50.0 max_velocity defines the maximum velocity when criterion is executed meters/second Float 0 50.0 throttle defines the throttle which makes acceleration Percent Float 0 100.0 brakes defines the brakes which make deceleration Percent Float 0 100.0 steering defines ego vehicle's steering Percent Float -100.0 100.0 gear defines ego vehicle's direction (forward or reverse) String duration defines criterion's execution time second Float 0 { \"type\": \"ControlCalibrationSensor\", \"name\": \"Control Calibration\", \"params\": { \"states\": [{ \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 } ] } } Total Control Calibration Criteria: Transform Sensor top # AssetBundle This sensor is specifically used to parent other sensors. For example, if there is a cluster of sensors a Transform sensor can be added at the location of the cluster and then the individual sensors can have a transform that is relative to the location of the Transform sensor`. Example usage { \"type\": \"TransformSensor\", \"name\": \"Cluster Reference\", \"transform\": { \"x\": 0.75, \"y\": 1.7, \"z\": 1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ] }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0.1, \"y\": 0, \"z\": -0.1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0, \"y\": 0.2, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Signal Sensor top # AssetBundle This sensor returns ground truth data for traffic light signals connected to the current lane of ego vehicle and creates bounding boxes around the detected signals. The color of the bounding box corresponds to the signal's type: Bounding Box Signal Green Green Yellow Yellow Red Red Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines how close a traffic light must be to the sensor to be detected meters Float 100 1 1000 { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/simulator/ground_truth/signals\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Video Recording Sensor top # AssetBundle This sensor records a video for test cases. For local simulations, the path to the recorded video will be shown in the Video Recording Sensor section of the Test Results for each simulation. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the video pixels Int 1920 1 1920 Height defines the width of the video pixels Int 1080 1 1080 Framerate defines the number of frames per second of the video fps Int 15 1 15 Bitrate defines the average number of bits per second Kbps Int 3000 1000 6000 MaxBitrate defines the maximum number of bits per second Kbps Int 6000 1000 6000 Quality defines the target constant quality level for VBR rate control (0 to 51, 0 means automatic) Int 22 0 51 { \"type\": \"VideoRecordingSensor\", \"name\": \"Video Recording Sensor\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Framerate\": 15, \"Bitrate\": 3000, \"MaxBitrate\": 6000, \"Quality\": 22 }, \"transform\": { \"x\": 0, \"y\": 10.0, \"z\": -10.0, \"pitch\": 30, \"yaw\": 0, \"roll\": 0 } } Comfort Sensor top # AssetBundle Comfort Sensor will detect whether a vehicle's acceleration, rotation or other values are out of acceptable ranges. For more details, check github . Parameter Description Unit Type Default Value Minimum Maximum maxAccelAllowed Maximum acceleration allowed m/s^2 Int maxJerkAllowed Maximum jerk allowed m/s^3 Int maxAngularVelocityAllowed Maximum angular velocity allowed deg/s Int maxAngularAccelerationAllowed Maximum angular acceleration allowed deg/s^2 Int rollTolerance Maximum deg rotation on the x axis deg Int slipTolerance Maximum deg difference between vehicle's velocity and vehicle's forward deg Int { \"type\" : \"Comfort\", \"name\" : \"Comfort Sensor\", \"params\": { \"maxAccelAllowed\": 8, \"maxJerkAllowed\": 4, \"maxAngularVelocityAllowed\": 200, \"maxAngularAccelerationAllowed\": 100, \"rollTolerance\": 10, \"slipTolerance\": 15 } } Stop Line Sensor top # AssetBundle The Stop Line sensor is a sensor used purely for analyzing the results of a simulation when the Create test report option is enabled for the simulation (See here for more information on test reports). The sensor will allow the simulator to detect and report stop line violations. { \"name\": \"Stop Line Sensor\", \"type\": \"StopLineSensor\" } Vehicle Odometry top # AssetBundle The Vehicle Odometry sensor publishes information on the vehicle velocity and front and rear angles in ROS and ROS2 using the lgsvl_msgs/VehicleOdometry message type. { \"type\": \"VehicleOdometrySensor\", \"name\": \"Vehicle Odometry Sensor\", \"params\": { \"Topic\": \"lgsvl/vehicle_odom\", }, } Vehicle State top # AssetBundle The Vehicle State sensor publishes information on the state of the vehicle that is not captured in the Can Bus sensor, such as the state of the headlights, blinkers, and wipers. The sensor publishes in ROS and ROS2 using the lgsvl_msgs/VehicleStateData message type. See here for all the fields published by the sensor. { \"type\": \"VehicleStateSensor\", \"name\": \"Vehicle State Sensor\", \"params\": { \"Topic\": \"lgsvl/vehicle_state\", }, } HUD Keyboard Control top # AssetBundle The HUD Keyboard Control sensor is a Keyboard Control sensor with an additional Heads Up Display (HUD) that displays the following information: Speed (mph) Current gear Engine speed (rpm) Ignition status indicator Parking brake indicator The HUD is shown by enabling the sensor visualization for the HUD Keyboard Sensor. { \"type\": \"HUDKeyboardControlSensor\", \"name\": \"HUD Keyboard Control Sensor\" } AutowareAI Control top # AssetBundle The AutowareAI Control sensor subscribes to the vehicle control topic from Autoware AI in ROS and allows the ego vehicle to react to control commands. The sensor subscribes to autoware_msgs/VehicleCmd message type. { \"type\": \"VehicleStateSensor\", \"name\": \"Vehicle State Sensor\", \"params\": { \"Topic\": \"lgsvl/vehicle_state\", }, } Lane-line Sensor top # AssetBundle This sensor outputs lines data for the current lane in third-degree polynomial format, along with line type and color. More details are available on the lane-line sensor page. Parameter Description Unit Type Default Value Minimum Maximum Width The width of the image output * pixels Int 1920 1 1920 Height The height of the image output * pixels Int 1080 1 1080 Frequency The maximum rate that messages will be published Hertz Int 15 1 100 FieldOfView The vertical angle that the camera sees degrees Float 60 1 90 MinDistance The near plane of the preview camera * meters Float 0.1 0.01 1000 MaxDistance The far plane of the preview camera * meters Float 2000 0.01 2000 DetectionRange Defines how far from the sensor line will be sampled meters Float 50 10 200 SampleDelta The distance between discrete line samples meters Float 0.5 0.05 1 * These parameters only affect the preview, not the output data itself. { \"type\": \"LaneLineSensor\", \"name\": \"LaneLineSensor\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 10, \"FieldOfView\": 60, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"DetectionRange\": 50, \"SampleDelta\": 0.5, \"Topic\": \"/simulator/lane_line\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": 3.0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Lane Following top # AssetBundle The Lane Following sensor is used to steer an ego vehicle using a deep learning model. The sensor subscribes to steering commands from the model which are sent as lgsvl_msgs/VehicleControlData messages in ROS2 and applies them to the ego vehicle. Read this tutorial to learn more. { \"type\": \"LaneFollowingSensor\", \"name\": \"Lane Following Sensor\", \"params\": { \"Topic\": \"/lanefollowing/steering_cmd\", } }","title":"List of sensors"},{"location":"simulation-content/sensors-list/#examples","text":"Example JSON configurations are available here: Apollo 5.0 JSON Apollo 3.0 JSON Autoware.AI Autoware.Auto Some LiDAR sensor JSONs: Velodyne VLP-16 Velodyne VLP-32C Velodyne VLS-128","title":"Examples"},{"location":"simulation-content/sensors-list/#how-to-specify-a-sensor","text":"A vehicle configuration is in the following format: [ SENSOR, SENSOR, SENSOR ] A SENSOR is defined in the JSON configuration in the following format: { \"type\": STRING, \"name\": STRING, \"params\": {PARAMS}, \"parent\": STRING, \"transform\": { \"x\": FLOAT, \"y\": FLOAT, \"z\": FLOAT, \"pitch\": FLOAT, \"yaw\": FLOAT, \"roll\": FLOAT, } } type is the type of sensor. name is the name of the sensor. This is how the sensor will be identified. params are the explicitly specified parameters. If a parameter is not set, the Default Value in the sensor definition will be used. ex. {\"Width\": 1920, \"Height\": 1080} There are 2 parameters that all sensors have Parameter Description Default Value Topic defines the topic that the sensor will subscribe/publish to null Frame defines the frame_id if the sensor publishes a ROS message. See ROS Header Message for more information null parent (OPTIONAL) a sensor's transform can be relative to another sensor. STRING is the name of the base sensor transform to which this sensor is relative. If omitted, the transform is relative to the origin of the vehicle. transform is the location and rotation of the sensor relative to the local position of the vehicle. The Unity left-hand coordinate system is used (+x right, +y up, +z forward, +pitch tilts the front down, +yaw rotates clockwise when viewed from above, +roll tilts the left side down). x is the position of the sensor along the x-axis y is the position of the sensor along the y-axis z is the position of the sensor along the z-axis pitch is the rotation around the x-axis yaw is the rotation around the y-axis roll is the rotation around the z-axis","title":"How to Specify a Sensor"},{"location":"simulation-content/sensors-list/#clock","text":"AssetBundle This sensor outputs simulated time to ROS as rosgraph_msgs/Clock message, or to CyberRT as clock message. The only parameter to use is topic/channel name. For ROS, you can add <param name=\"/use_sim_time\" value=\"true\"> to their ROS launch file, or use rosparam set /use_sim_time true in command line, to have a ROS node use simulation time according to the /clock topic. For more details please refer to this page . For CyberRT, you can set clock_mode in cyber.pb.conf as MODE_MOCK to have CyberRT use simulation time according to the /clock topic. { \"type\": \"ClockSensor\", \"name\": \"Clock Sensor\", \"params\": { \"Topic\": \"/clock\" } }","title":"Clock"},{"location":"simulation-content/sensors-list/#color-camera","text":"AssetBundle This is the type of sensor that would be used for the Main Camera in Apollo or other AD stacks. (For questions about camera matrix, please refer to this FAQ .) Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 JpegQuality defines the quality if the image output % Int 75 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion * List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 CubemapSize size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 * If Distorted is true , DistortionParameters must be an empty list or a list of FOUR floats. The values in this list should come from calibration result of real camera. Setting arbitrary values may cause undefined result. If Distorted is false , DistortionParameters , Fisheye and Xi are ignored. ** If Fisheye is true , Xi should be a value from calibration result of real camera. Setting arbitrary value may cause undefined result. If Fisheye is false , Xi is ignored. *** CubemapSize should only be 512, 1024, or 2048. { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ], \"Postprocessing\": [ ] }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Color Camera"},{"location":"simulation-content/sensors-list/#sensor-effects","text":"Color Camera has multiple post processing sensor effects that can be added to the Postprocessing field in params . Effects can be combined with an array of Postprocessing fields but order is hard coded. SunFlare - creates a sun flare effect Parameter Description Type Default Value Min Max type postprocessing sensor effect name String sunIntensity defines the intensity of the sun flare Float 1 0 10 haloIntensity defines the intensity of the sun flare halo Float 1 0 10 ghostingIntensity defines the intensity of the mirror effect Float 1 0 10 \"Postprocessing\": [ { \"type\": \"SunFlare\", \"sunIntensity\": 1, \"haloIntensity\": 1, \"ghostingIntensity\": 1 } ] Rain - creates rain drops on the lens Parameter Description Type Default Value Min Max type postprocessing sensor effect name String size defines the size of the drops Float 1 0 1 \"Postprocessing\": [ { \"type\": \"Rain\", \"size\": 1 } ] GreyScale - converts color to grey scale Parameter Description Type Default Value Min Max type postprocessing sensor effect name String intensity defines the intensity of the grey scale Float 1 0 1 \"Postprocessing\": [ { \"type\": \"GreyScale\", \"intensity\": 1 } ] VideoArtifacts - creates jpeg compression artifacts Parameter Description Type Default Value Min Max type postprocessing sensor effect name String intensity defines the intensity of the effect Float 0.25 0 1 blockSize defines the size of the affected block Int 32 1 128 \"Postprocessing\": [ { \"type\": \"VideoArtifacts\", \"intensity\": 0.25, \"blockSize\": 32 } ]","title":"Sensor Effects"},{"location":"simulation-content/sensors-list/#depth-camera","text":"AssetBundle This sensor returns an image where the shades on the grey-scale correspond to the depth of objects. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 5 1 100 JpegQuality defines the quality if the image output % Int 100 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion* List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 CubemapSize size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 * See notes on DistortionParameters for Color Camera. ** See notes on Xi for Color Camera. *** See notes on CubemapSize for Color Camera. { \"type\": \"DepthCameraSensor\", \"name\": \"Depth Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/depth_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Depth Camera"},{"location":"simulation-content/sensors-list/#segmentation-camera","text":"AssetBundle This sensor returns an image where objects are colored corresponding to their tag: Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F If a tag is included in \"InstanceSegmentationTags\", each instance of objects with that tag is colored differently, but all will have same hue (e.g. all cars will be bluish and all pedestrians will be reddish). Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion* List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 CubemapSize size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 InstanceSegmentationTags define tags with instance segmentation List of String empty list * See notes on DistortionParameters for Color Camera. ** See notes on Xi for Color Camera. *** See notes on CubemapSize for Color Camera. { \"type\": \"SegmentationCameraSensor\", \"name\": \"Segmentation Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"InstanceSegmentationTags\": [ ], \"Topic\": \"/simulator/segmentation_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Example of Instance Segmentation Tags. Be aware that this effects performance greatly. \"InstanceSegmentationTags\": [ \"Car\", \"Pedestrian\", \"Obstacle\", \"Building\" ],","title":"Segmentation Camera"},{"location":"simulation-content/sensors-list/#lidar","text":"AssetBundle This sensor returns a point cloud after 1 revolution. Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF * If VerticalRayAngles is not empty, LaserCount will be automatically set to the length of VerticalRayAngles , and FieldOfView and CenterAngle will be ignored. A sample of uniformly distributed angles: { \"type\": \"LidarSensor\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } A sample of non-uniformly distributed angles: { \"type\": \"LidarSensor\", \"name\": \"Lidar-NonUniform\", \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"LiDAR"},{"location":"simulation-content/sensors-list/#3d-ground-truth","text":"AssetBundle This sensor returns 3D ground truth data for training and creates bounding boxes around the detected objects. The color of the object corresponds to the object's type: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines the how close an object must be to the sensor to be detected meters Float 100 1 1000 { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"3D Ground Truth"},{"location":"simulation-content/sensors-list/#apollo-perception-visualizer-3d","text":"AssetBundle This sensor will visualize 3D bounding boxes on objects as detected by Apollo. It does not publish any data and instead subscribes to the perception topic from Apollo. The color of the boxes are: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta { \"type\": \"ApolloPerceptionVisualizer3D\", \"name\": \"Apollo Perception Visualizer 3D Sensor\", \"params\": { \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Apollo Perception Visualizer 3D"},{"location":"simulation-content/sensors-list/#can-bus","text":"AssetBundle This sensor sends data about the vehicle chassis. The data includes: Speed [m/s] Throttle [%] Braking [%] Steering [+/- %] Parking Brake Status [bool] High Beam Status [bool] Low Beam Status [bool] Hazard Light Status [bool] Fog Light Status [bool] Left Turn Signal Status [bool] Right Turn Signal Status [bool] Wiper Status [bool] Reverse Gear Status [bool] Selected Gear [Int] Engine Status [bool] Engine RPM [RPM] GPS Latitude [Latitude] GPS Longitude [Longitude] Altitude [m] Orientation [3D Vector of Euler angles] Velocity [3D Vector of m/s] Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/canbus\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"CAN-Bus"},{"location":"simulation-content/sensors-list/#gps-device","text":"AssetBundle This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"GPS Device"},{"location":"simulation-content/sensors-list/#gps-odometry","text":"AssetBundle This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates and the vehicle velocity. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 ChildFrame used by Autoware IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_odometry\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"GPS Odometry"},{"location":"simulation-content/sensors-list/#gps-ins-status","text":"AssetBundle This sensor outputs the status of the GPS correction due to INS. The Simulator is an ideal environment in which GPS is always corrected. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published [Hertz] Float 12.5 1 100 { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"GPS-INS Status"},{"location":"simulation-content/sensors-list/#vehicle-control","text":"AssetBundle This sensor is required for a vehicle to subscribe to a control topic published in ROS or ROS2 with message type lgsvl_msgs/VehicleControlData . { \"type\": \"LGSVLControlSensor\", \"name\": \"AD Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } }","title":"LGSVL Control"},{"location":"simulation-content/sensors-list/#apollo-control","text":"AssetBundle This sensor is required for a vehicle to subscribe to the control topic from Apollo using the CyberRT bridge. { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } The sensor also demonstrates how a user can check when SVL Simulator receives the first control message from the AD stack like Apollo. Based on this time, you can start the rest of the simulation or do some other interesting things. To wait for control message from apollo in a Python script, you can call on_custom() of the ego agent. For example, the following code set isControlReceived of agent self.ego to be True when the simulator first receives a message of the /apollo/control topic. def on_control_received(agent, kind, context): if kind == \"checkControl\": agent.isControlReceived = True log.info(\"Control message recieved\") self.ego.on_custom(on_control_received)","title":"Apollo Control"},{"location":"simulation-content/sensors-list/#keyboard-control","text":"AssetBundle This sensor is required for a vehicle to accept keyboard control commands. Parameters are not required. { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }","title":"Keyboard Control"},{"location":"simulation-content/sensors-list/#wheel-control","text":"AssetBundle This sensor is required for a vehicle to accept Logitech G920 wheel control commands. Parameters are not required. { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }","title":"Wheel Control"},{"location":"simulation-content/sensors-list/#cruise-control","text":"AssetBundle This sensor causes the vehicle to accelerate to the desired speed and then maintain the desired speed. Parameter Description Unit Type Default Value Minimum Maximum CruiseSpeed defines the desired speed meters/second Float 0 0 200 { \"type\": \"CruiseControlSensor\", \"name\": \"AD Car Control\", \"params\": { \"CruiseSpeed\": 10 } }","title":"Cruise Control"},{"location":"simulation-content/sensors-list/#imu","text":"AssetBundle This sensor output at a fixed rate of 100 Hz. IMU publishes data on topics where the 2nd topic has corrected IMU data. Parameter Description CorectedTopic defines the 2nd topic that the data is published to CorrectedFrame defines the 2nd frame for the ROS header { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"IMU"},{"location":"simulation-content/sensors-list/#2d-ground-truth","text":"AssetBundle This sensor outputs an image where objects are encased in a box. The color of the box depends on the type of object. Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 Width defines the width of the image pixels Int 1920 1 1920 Height defines the height of the iamge pixels Int 1080 1 1080 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor to be in the image meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor to be in the image meters Float 2000 0.01 2000 DetectionRange defines how close an object must be to be given a bounding box meters Float 100 0.01 2000 { \"type\": \"PerceptionSensor2D\", \"name\": \"2D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"2D Ground Truth"},{"location":"simulation-content/sensors-list/#radar","text":"AssetBundle This sensor outputs the objects detected by the radar. Detected objects are visualized with a box colored by their type: Type Color Car Green Agent Magenta Bicycle Cyan Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 13.4 1 100 { \"type\": \"RadarSensor\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Radar"},{"location":"simulation-content/sensors-list/#ultrasonic","text":"AssetBundle This sensor outputs the distance (to the center of the sensor) of the closest point within the sensor's FOV. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 400 1 1920 Height defines the height of the image output pixels Int 160 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 JpegQuality defines the quality if the image output % Int 75 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 40 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.3 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 2.0 0.01 2000 { \"type\": \"UltrasonicSensor\", \"name\": \"Ultrasonic Sensor\", \"params\": { \"Width\": 400, \"Height\": 160, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.3, \"MaxDistance\": 2, \"Topic\": \"/simulator/ultrasonic\", \"Frame\": \"ultrasonic\" }, \"transform\": { \"x\": 0, \"y\": 0.5, \"z\": 2.5, \"pitch\": -13, \"yaw\": 0, \"roll\": 0 } }","title":"Ultrasonic"},{"location":"simulation-content/sensors-list/#control-calibration","text":"AssetBundle This sensor outputs control calibration criteria collected by AD Stacks (Apollo, Autoware). It generates steering, throttle or brakes with gear commands between minimum and maximum of velocity during duration. Parameter Description Unit Type Minimum Maximum min_velocity defines the minimum velocity when criterion is executed meters/second Float 0 50.0 max_velocity defines the maximum velocity when criterion is executed meters/second Float 0 50.0 throttle defines the throttle which makes acceleration Percent Float 0 100.0 brakes defines the brakes which make deceleration Percent Float 0 100.0 steering defines ego vehicle's steering Percent Float -100.0 100.0 gear defines ego vehicle's direction (forward or reverse) String duration defines criterion's execution time second Float 0 { \"type\": \"ControlCalibrationSensor\", \"name\": \"Control Calibration\", \"params\": { \"states\": [{ \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 } ] } } Total Control Calibration Criteria:","title":"Control Calibration"},{"location":"simulation-content/sensors-list/#transform-sensor","text":"AssetBundle This sensor is specifically used to parent other sensors. For example, if there is a cluster of sensors a Transform sensor can be added at the location of the cluster and then the individual sensors can have a transform that is relative to the location of the Transform sensor`. Example usage { \"type\": \"TransformSensor\", \"name\": \"Cluster Reference\", \"transform\": { \"x\": 0.75, \"y\": 1.7, \"z\": 1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ] }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0.1, \"y\": 0, \"z\": -0.1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0, \"y\": 0.2, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Transform Sensor"},{"location":"simulation-content/sensors-list/#signal-sensor","text":"AssetBundle This sensor returns ground truth data for traffic light signals connected to the current lane of ego vehicle and creates bounding boxes around the detected signals. The color of the bounding box corresponds to the signal's type: Bounding Box Signal Green Green Yellow Yellow Red Red Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines how close a traffic light must be to the sensor to be detected meters Float 100 1 1000 { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/simulator/ground_truth/signals\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Signal Sensor"},{"location":"simulation-content/sensors-list/#video-recording-sensor","text":"AssetBundle This sensor records a video for test cases. For local simulations, the path to the recorded video will be shown in the Video Recording Sensor section of the Test Results for each simulation. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the video pixels Int 1920 1 1920 Height defines the width of the video pixels Int 1080 1 1080 Framerate defines the number of frames per second of the video fps Int 15 1 15 Bitrate defines the average number of bits per second Kbps Int 3000 1000 6000 MaxBitrate defines the maximum number of bits per second Kbps Int 6000 1000 6000 Quality defines the target constant quality level for VBR rate control (0 to 51, 0 means automatic) Int 22 0 51 { \"type\": \"VideoRecordingSensor\", \"name\": \"Video Recording Sensor\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Framerate\": 15, \"Bitrate\": 3000, \"MaxBitrate\": 6000, \"Quality\": 22 }, \"transform\": { \"x\": 0, \"y\": 10.0, \"z\": -10.0, \"pitch\": 30, \"yaw\": 0, \"roll\": 0 } }","title":"Video Recording Sensor"},{"location":"simulation-content/sensors-list/#comfort-sensor","text":"AssetBundle Comfort Sensor will detect whether a vehicle's acceleration, rotation or other values are out of acceptable ranges. For more details, check github . Parameter Description Unit Type Default Value Minimum Maximum maxAccelAllowed Maximum acceleration allowed m/s^2 Int maxJerkAllowed Maximum jerk allowed m/s^3 Int maxAngularVelocityAllowed Maximum angular velocity allowed deg/s Int maxAngularAccelerationAllowed Maximum angular acceleration allowed deg/s^2 Int rollTolerance Maximum deg rotation on the x axis deg Int slipTolerance Maximum deg difference between vehicle's velocity and vehicle's forward deg Int { \"type\" : \"Comfort\", \"name\" : \"Comfort Sensor\", \"params\": { \"maxAccelAllowed\": 8, \"maxJerkAllowed\": 4, \"maxAngularVelocityAllowed\": 200, \"maxAngularAccelerationAllowed\": 100, \"rollTolerance\": 10, \"slipTolerance\": 15 } }","title":"Comfort Sensor"},{"location":"simulation-content/sensors-list/#stopline-sensor","text":"AssetBundle The Stop Line sensor is a sensor used purely for analyzing the results of a simulation when the Create test report option is enabled for the simulation (See here for more information on test reports). The sensor will allow the simulator to detect and report stop line violations. { \"name\": \"Stop Line Sensor\", \"type\": \"StopLineSensor\" }","title":"Stop Line Sensor"},{"location":"simulation-content/sensors-list/#vehicle-odometry","text":"AssetBundle The Vehicle Odometry sensor publishes information on the vehicle velocity and front and rear angles in ROS and ROS2 using the lgsvl_msgs/VehicleOdometry message type. { \"type\": \"VehicleOdometrySensor\", \"name\": \"Vehicle Odometry Sensor\", \"params\": { \"Topic\": \"lgsvl/vehicle_odom\", }, }","title":"Vehicle Odometry"},{"location":"simulation-content/sensors-list/#vehicle-state","text":"AssetBundle The Vehicle State sensor publishes information on the state of the vehicle that is not captured in the Can Bus sensor, such as the state of the headlights, blinkers, and wipers. The sensor publishes in ROS and ROS2 using the lgsvl_msgs/VehicleStateData message type. See here for all the fields published by the sensor. { \"type\": \"VehicleStateSensor\", \"name\": \"Vehicle State Sensor\", \"params\": { \"Topic\": \"lgsvl/vehicle_state\", }, }","title":"Vehicle State"},{"location":"simulation-content/sensors-list/#hud-keyboard-control","text":"AssetBundle The HUD Keyboard Control sensor is a Keyboard Control sensor with an additional Heads Up Display (HUD) that displays the following information: Speed (mph) Current gear Engine speed (rpm) Ignition status indicator Parking brake indicator The HUD is shown by enabling the sensor visualization for the HUD Keyboard Sensor. { \"type\": \"HUDKeyboardControlSensor\", \"name\": \"HUD Keyboard Control Sensor\" }","title":"HUD Keyboard Control"},{"location":"simulation-content/sensors-list/#autowareai-control","text":"AssetBundle The AutowareAI Control sensor subscribes to the vehicle control topic from Autoware AI in ROS and allows the ego vehicle to react to control commands. The sensor subscribes to autoware_msgs/VehicleCmd message type. { \"type\": \"VehicleStateSensor\", \"name\": \"Vehicle State Sensor\", \"params\": { \"Topic\": \"lgsvl/vehicle_state\", }, }","title":"AutowareAI Control"},{"location":"simulation-content/sensors-list/#lane-line-sensor","text":"AssetBundle This sensor outputs lines data for the current lane in third-degree polynomial format, along with line type and color. More details are available on the lane-line sensor page. Parameter Description Unit Type Default Value Minimum Maximum Width The width of the image output * pixels Int 1920 1 1920 Height The height of the image output * pixels Int 1080 1 1080 Frequency The maximum rate that messages will be published Hertz Int 15 1 100 FieldOfView The vertical angle that the camera sees degrees Float 60 1 90 MinDistance The near plane of the preview camera * meters Float 0.1 0.01 1000 MaxDistance The far plane of the preview camera * meters Float 2000 0.01 2000 DetectionRange Defines how far from the sensor line will be sampled meters Float 50 10 200 SampleDelta The distance between discrete line samples meters Float 0.5 0.05 1 * These parameters only affect the preview, not the output data itself. { \"type\": \"LaneLineSensor\", \"name\": \"LaneLineSensor\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 10, \"FieldOfView\": 60, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"DetectionRange\": 50, \"SampleDelta\": 0.5, \"Topic\": \"/simulator/lane_line\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": 3.0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Lane-line Sensor"},{"location":"simulation-content/sensors-list/#lane-following","text":"AssetBundle The Lane Following sensor is used to steer an ego vehicle using a deep learning model. The sensor subscribes to steering commands from the model which are sent as lgsvl_msgs/VehicleControlData messages in ROS2 and applies them to the ego vehicle. Read this tutorial to learn more. { \"type\": \"LaneFollowingSensor\", \"name\": \"Lane Following Sensor\", \"params\": { \"Topic\": \"/lanefollowing/steering_cmd\", } }","title":"Lane Following"},{"location":"simulation-content/sharing/","text":"Sharing Assets The sharing feature allows you to share any private library assets and simulations with new and existing users in a controlled manner, without having to publish them publicly. Sharing an asset top # Sharing assets is very straightforward. At the top right of every private owned library asset and simulation there is a \"Share\" button. This will bring up a popup, which you can enter one or more email addresses to share the item. Email addresses for registered users will automatically add the item to the recipient's library. Conversely, unregistered users will be sent an email inviting them to sign up and accept the shared item. Viewing shared assets top # Once a private asset has been shared with you, it will automatically be added to your library. These assets will have a yellow circle indicating it was shared. The \"Shared with Me\" filter can also be used to easily view all shared assets. Unsharing top # Owners of shared assets and simulations can revoke access at any time. By bringing up the \"Share\" popup once more, a list of all shared users will be visible. By pressing the trash can next to a user's name, the item will be revoked from that user.","title":"Sharing assets"},{"location":"simulation-content/sharing/#sharing-an-asset","text":"Sharing assets is very straightforward. At the top right of every private owned library asset and simulation there is a \"Share\" button. This will bring up a popup, which you can enter one or more email addresses to share the item. Email addresses for registered users will automatically add the item to the recipient's library. Conversely, unregistered users will be sent an email inviting them to sign up and accept the shared item.","title":"Sharing an asset"},{"location":"simulation-content/sharing/#viewing-shared-assets","text":"Once a private asset has been shared with you, it will automatically be added to your library. These assets will have a yellow circle indicating it was shared. The \"Shared with Me\" filter can also be used to easily view all shared assets.","title":"Viewing shared assets"},{"location":"simulation-content/sharing/#unsharing","text":"Owners of shared assets and simulations can revoke access at any time. By bringing up the \"Share\" popup once more, a list of all shared users will be visible. By pressing the trash can next to a user's name, the item will be revoked from that user.","title":"Unsharing"},{"location":"simulation-content/total-control-calibration-criteria/","text":"Total Control Calibration Criteria back This page has control calibration criteria JSON. { \"type\": \"ControlCalibrationSensor\", \"name\": \"Control Calibration\", \"params\": { \"states\": [{ \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 20, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 24, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 20, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 24, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 20, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 27, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 18, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 10, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 18, \"brakes\": 0, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": -10, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 22, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 45, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": -45, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 15, \"steering\": -65, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 } ] } }","title":"Total Control Calibration Criteria"},{"location":"simulation-content/velodyne-json-examples/","text":"Velodyne VLP-16 # [ { \"type\": \"LidarSensor\", \"name\": \"Velodyne VLP-16\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"FieldOfView\": 30, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ] Velodyne VLP-32C # [ { \"type\": \"LidarSensor\", \"name\": \"Velodyne VLP-32C\", \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ] Velodyne VLS-128 # [ { \"type\": \"LidarSensor\", \"name\": \"Velodyne VLS-128\", \"params\": { \"VerticalRayAngles\": [ -11.742, -1.99, 3.4, -5.29, -0.78, 4.61, -4.08, 1.31, -6.5, -1.11, 4.28, -4.41, 0.1, 6.48, -3.2, 2.19, -3.86, 1.53, -9.244, -1.77, 2.74, -5.95, -0.56, 4.83, -2.98, 2.41, -6.28, -0.89, 3.62, -5.07, 0.32, 7.58, -0.34, 5.18, -3.64, 1.75, -25, -2.43, 2.96, -5.73, 0.54, 9.7, -2.76, 2.63, -7.65, -1.55, 3.84, -4.85, 3.188, -5.51, -0.12, 5.73, -4.3, 1.09, -16.042, -2.21, 4.06, -4.63, 0.76, 15, -3.42, 1.97, -6.85, -1.33, -5.62, -0.23, 5.43, -3.53, 0.98, -19.582, -2.32, 3.07, -4.74, 0.65, 11.75, -2.65, 1.86, -7.15, -1.44, 3.95, -2.1, 3.29, -5.4, -0.01, 4.5, -4.19, 1.2, -13.565, -1.22, 4.17, -4.52, 0.87, 6.08, -3.31, 2.08, -6.65, 1.42, -10.346, -1.88, 3.51, -6.06, -0.67, 4.72, -3.97, 2.3, -6.39, -1, 4.39, -5.18, 0.21, 6.98, -3.09, 4.98, -3.75, 1.64, -8.352, -2.54, 2.85, -5.84, -0.45, 8.43, -2.87, 2.52, -6.17, -1.66, 3.73, -4.96, 0.43 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ]","title":"Velodyne json examples"},{"location":"simulation-content/velodyne-json-examples/#velodyne-vlp-16","text":"[ { \"type\": \"LidarSensor\", \"name\": \"Velodyne VLP-16\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"FieldOfView\": 30, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ]","title":"Velodyne VLP-16"},{"location":"simulation-content/velodyne-json-examples/#velodyne-vlp-32c","text":"[ { \"type\": \"LidarSensor\", \"name\": \"Velodyne VLP-32C\", \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ]","title":"Velodyne VLP-32C"},{"location":"simulation-content/velodyne-json-examples/#velodyne-vls-128","text":"[ { \"type\": \"LidarSensor\", \"name\": \"Velodyne VLS-128\", \"params\": { \"VerticalRayAngles\": [ -11.742, -1.99, 3.4, -5.29, -0.78, 4.61, -4.08, 1.31, -6.5, -1.11, 4.28, -4.41, 0.1, 6.48, -3.2, 2.19, -3.86, 1.53, -9.244, -1.77, 2.74, -5.95, -0.56, 4.83, -2.98, 2.41, -6.28, -0.89, 3.62, -5.07, 0.32, 7.58, -0.34, 5.18, -3.64, 1.75, -25, -2.43, 2.96, -5.73, 0.54, 9.7, -2.76, 2.63, -7.65, -1.55, 3.84, -4.85, 3.188, -5.51, -0.12, 5.73, -4.3, 1.09, -16.042, -2.21, 4.06, -4.63, 0.76, 15, -3.42, 1.97, -6.85, -1.33, -5.62, -0.23, 5.43, -3.53, 0.98, -19.582, -2.32, 3.07, -4.74, 0.65, 11.75, -2.65, 1.86, -7.15, -1.44, 3.95, -2.1, 3.29, -5.4, -0.01, 4.5, -4.19, 1.2, -13.565, -1.22, 4.17, -4.52, 0.87, 6.08, -3.31, 2.08, -6.65, 1.42, -10.346, -1.88, 3.51, -6.06, -0.67, 4.72, -3.97, 2.3, -6.39, -1, 4.39, -5.18, 0.21, 6.98, -3.09, 4.98, -3.75, 1.64, -8.352, -2.54, 2.85, -5.84, -0.45, 8.43, -2.87, 2.52, -6.17, -1.66, 3.73, -4.96, 0.43 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ]","title":"Velodyne VLS-128"},{"location":"support/contributing/","text":"Contributing to SVL Simulator As an open project and community, we welcome and highly encourage contributions back to SVL Simulator. Through feedback, questions, bug reports/issues, new features, documentation, or demonstrations showing your use case of SVL Simulator, you can help contribute to the SVL Simulator project in several different ways. Feedback, questions, bug reports, and issues top # The best way to give feedback, raise an issue, or ask a question about SVL Simulator is to submit a Github issue to our main Github repository: https://www.github.com/lgsvl/simulator . Please also check first if your question has already been answered in an existing issue or on our Frequently Asked Questions page. We will try to respond to open issues as quickly as possible. If you would like to get in touch about partnerships, licensing, or larger collaborations, please email us at contact@svlsimulator.com . Submitting a Pull Request top # We welcome pull requests for new features or bug fixes to SVL Simulator. You can submit a pull request here . After submitting the initial pull request, you will be prompted to agree to our Contributor License Agreement . Your pull request will then be ready for review and merging. Below are a few notes/guidelines on submitting pull requests and working on our project. While we are an open project, please be aware that we have a custom license . You must agree to the terms of our license to use and contribute back to our project. Please make sure that any preexisting licenses/terms of your contributions are compatible with our terms. Under the terms of the Unity Asset Store Terms of Service and EULA , you cannot contribute back to our project anything which you have obtained from the Unity Asset Store (unless you have separate agreements in place with Unity). Our team synchronizes the lgsvl/simulator repository internally first, then pushes to Github approximately once a week. When we merge pull requests, it will first be merged into our internal source, then pushed to Github, so there may be a slight delay for your merged PR to show up in the public repository. We always rebase and do not merge branches - when submitting a pull request, please make sure your code is rebased on the latest version of the master branch. If you are working on a large feature or a big change to how the simulator works, please submit a Github issue first or reach out to us to discuss the idea before you start working on it or submit a pull request. SVL Simulator is cross platform. Please do not write platform-specific code, or if you do, please make sure you do so while supporting all of our supported platforms. Documentation top # We currently use mkdocs version 1.0.4 to build our documentation website . To submit a piece of documentation, please submit a PR to the lgsvl/simulator repository, placing your materials in the Docs directory. After merging, our team will build and upload to the public website. Demonstrations top # If you are using the SVL Simulator for your development, we would like to hear from you. We are also always excited to see any demonstration videos, blog posts, or articles showing use cases for the SVL Simulator. Please reach out to us at contact@svlsimulator.com to let us know about your application.","title":"Contributing"},{"location":"support/contributing/#feedback,-questions,-bug-reports,-and-issues","text":"The best way to give feedback, raise an issue, or ask a question about SVL Simulator is to submit a Github issue to our main Github repository: https://www.github.com/lgsvl/simulator . Please also check first if your question has already been answered in an existing issue or on our Frequently Asked Questions page. We will try to respond to open issues as quickly as possible. If you would like to get in touch about partnerships, licensing, or larger collaborations, please email us at contact@svlsimulator.com .","title":"Feedback, questions, bug reports, and issues"},{"location":"support/contributing/#submitting-a-pull-request","text":"We welcome pull requests for new features or bug fixes to SVL Simulator. You can submit a pull request here . After submitting the initial pull request, you will be prompted to agree to our Contributor License Agreement . Your pull request will then be ready for review and merging. Below are a few notes/guidelines on submitting pull requests and working on our project. While we are an open project, please be aware that we have a custom license . You must agree to the terms of our license to use and contribute back to our project. Please make sure that any preexisting licenses/terms of your contributions are compatible with our terms. Under the terms of the Unity Asset Store Terms of Service and EULA , you cannot contribute back to our project anything which you have obtained from the Unity Asset Store (unless you have separate agreements in place with Unity). Our team synchronizes the lgsvl/simulator repository internally first, then pushes to Github approximately once a week. When we merge pull requests, it will first be merged into our internal source, then pushed to Github, so there may be a slight delay for your merged PR to show up in the public repository. We always rebase and do not merge branches - when submitting a pull request, please make sure your code is rebased on the latest version of the master branch. If you are working on a large feature or a big change to how the simulator works, please submit a Github issue first or reach out to us to discuss the idea before you start working on it or submit a pull request. SVL Simulator is cross platform. Please do not write platform-specific code, or if you do, please make sure you do so while supporting all of our supported platforms.","title":"Submitting a Pull Request"},{"location":"support/contributing/#documentation","text":"We currently use mkdocs version 1.0.4 to build our documentation website . To submit a piece of documentation, please submit a PR to the lgsvl/simulator repository, placing your materials in the Docs directory. After merging, our team will build and upload to the public website.","title":"Documentation"},{"location":"support/contributing/#demonstrations","text":"If you are using the SVL Simulator for your development, we would like to hear from you. We are also always excited to see any demonstration videos, blog posts, or articles showing use cases for the SVL Simulator. Please reach out to us at contact@svlsimulator.com to let us know about your application.","title":"Demonstrations"},{"location":"support/faq/","text":"SVL Simulator FAQ What are the recommended system specs? What are the minimum REQUIRED system specs? Does the simulator run on Windows/Mac/Linux? Why does the simulator not open on Linux? Which Unity version is required and how do I get it? Why does my Simulator say \"Invalid: Out of date Assetbundle\"? How do I setup development environment for Unity on Ubuntu? Where are Unity log files located Why are assets/scenes missing/empty after cloning from git? Why do I get an error saying some files (e.g. rosbridge_websocket.launch) are missing in Apollo? ROS Bridge won How do I control the ego vehicle (my vehicle) spawn position? How can I add a custom ego vehicle to SVL Simulator? How can I add extra sensors to vehicles in SVL Simulator? How do I get parameters in camera matrix? How can I add a custom map to SVL Simulator? How can I create or edit map annotations? Why are pedestrians not spawning when annotated correctly? Why can't I find catkin_make command when building Apollo? Why is Apollo perception module turning on and off all the time? Why does the Apollo vehicle stop at stop line and not cross intersections? Dreamview in Apollo shows \"Hardware GPS triggers safety mode. No GNSS status message.\" Why does Rviz not load the Autoware vector map? Why are there no maps when I make a local build? Why is the TARGET_WAYPOINT missing when using the Map Annotation Tool Why does the simulator start and then say the simulation is \"Invalid\"? Why are there no assets when building the simulator from Unity Editor? Why are there libraries missing when running a PythonAPI script? How to fix the \"RuntimeError: The current Numpy installation\" error? Other questions? What are the recommended system specs? What are the minimum REQUIRED system specs? top # For optimal performance, we recommend that you run the simulator on a system with at least a 4 GHz Quad core CPU, NVIDIA GTX 1080 graphics card (8GB memory), and 16GB memory or higher, running on Windows 10. While you can run on a lower-spec system, the performance of the simulator will be impacted and you will probably see much lower frame rates. The minimum specification to run is a 3GHz dual core CPU, NVIDIA graphics card, and 8 GB memory system. Note that simulator runs better on Windows due to fact that Unity and NVIDIA drivers provide better performance on Windows than on Linux. If Apollo or Autoware will be running on the same system, upgrading to a GPU with at least 10GB memory is recommended. Does the simulator run on Windows/Mac/Linux? top # Officially, you can run SVL Simulator on Windows 10 and Ubuntu 18.04 (or later). We do not support macOS at this time. Why does the simulator not open on Linux? top # The Simulator requires the vulkan libraries to be installed on Linux: sudo apt install libvulkan1 Which Unity version is required and how do I get it? top # SVL Simulator is currently on Unity version 2019.1.10f1, and can be downloaded from the Unity Download Archive. You can download the Windows version here: https://unity3d.com/get-unity/download/archive You can download the Linux version (20191.10f1) here: https://beta.unity3d.com/download/f007ed779b7a/UnitySetup-2019.1.10f1 We are constantly working to ensure that SVL Simulator runs on the latest version of Unity which supports all of our required functionality. Why does my Simulator say \"Invalid: Out of date Assetbundle\"? top # Assetbundle versions change as we add new features. To get the latest assetbundles, add assets uploaded by SVL Admin on content asset store to your Library and restart your local simulator. How do I setup development environment for Unity on Ubuntu? top # Install Unity Editor dependencies: sudo apt install \\ gconf-service lib32gcc1 lib32stdc++6 libasound2 libc6 libc6-i386 libcairo2 libcap2 libcups2 \\ libdbus-1-3 libexpat1 libfontconfig1 libfreetype6 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 \\ libgl1 libglib2.0-0 libglu1 libgtk2.0-0 libgtk-3-0 libnspr4 libnss3 libpango1.0-0 libstdc++6 \\ libx11-6 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 \\ libxrender1 libxtst6 zlib1g debconf libgtk2.0-0 libsoup2.4-1 libarchive13 libpng16-16 Download and install Unity 2019.1.10f1: curl -fLo UnitySetup https://beta.unity3d.com/download/f007ed779b7a/UnitySetup-2019.1.10f1 chmod +x UnitySetup ./UnitySetup --unattended --install-location=/opt/Unity --components=Unity Install .NET Core SDK, available from https://dotnet.microsoft.com/download On Ubuntu 18.04 run following commands: wget -q https://packages.microsoft.com/config/ubuntu/18.04/packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get install apt-transport-https sudo apt-get update sudo apt-get install dotnet-sdk-2.2 Install Mono, available from https://www.mono-project.com/download/stable/#download-lin On Ubuntu 18.04 run following commands: sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF sudo apt install apt-transport-https ca-certificates echo \"deb https://download.mono-project.com/repo/ubuntu stable-xenial main\" | sudo tee /etc/apt/sources.list.d/mono-official-stable.list sudo apt update sudo apt install mono-devel Install Visual Studio Code, available in Ubuntu Software or from https://code.visualstudio.com/docs/setup/linux Open VS Code and install C# extension Press Ctrl+Shift+X Search for C# Install extension C# for Visual Studio Code (powered by OmniSharp) Install Unity Debug Extension, available here: https://marketplace.visualstudio.com/items?itemName=Unity.unity-debug Set Unity preferences to use VS Code. See instructions here: https://code.visualstudio.com/docs/other/unity#_setup-vs-code-as-unity-script-editor To find out where Code is installed use which code Where are Unity log files located? top # Version Location Windows Binary %APPDATA%\\..\\LocalLow\\LGElectronics\\SVLSimulator\\Player.log Windows Editor %APPDATA%\\..\\Local\\Unity\\Editor\\Editor.log Linux Binary ~/.config/unity3d/LGElectronics/SVLSimulator/Player.log Linux Editor ~/.config/unity3d/Editor.log Why are assets/scenes missing/empty after cloning from git? top # We use Git LFS for large file storage to improve performance of cloning. Before cloning, install and run git lfs install . Then repeat the git clone process. You can find the Git LFS installation instructions here: https://github.com/git-lfs/git-lfs/wiki/Installation Typically if you do not have Git LFS installed or configured then you will see the following error when opening Unity project: error CS026: The type or namespace name \"WebSocketSharp\" could not be found. Are you missing a using directive or an assembly reference? Why do I get an error saying some files (e.g. rosbridge_websocket.launch) are missing in Apollo? top # If you see that some files are missing from ros_pkgs folder in Apollo repository, you need to make sure that you are cloning all submodules: git clone --recurse-submodules https://github.com/lgsvl/apollo.git ROS Bridge won't connect? top # First make sure you are running rosbridge. If using our Apollo docker image, run: rosbridge.sh For standalone ROS environments run: roslaunch rosbridge_server rosbridge_websocket.launch If you are running ROS bridge on different machine, verify that simulator can connect to it and you do not have firewall blocking ports. How do I control the ego vehicle (my vehicle) spawn position? top # Find the \"spawn_transform\" game objects in scene and adjust their transform position. If you are creating new maps make sure you add \"SpawnInfo\" component to empty game object. The Simulator will use location of first game object that has SpawnInfo component. How can I add a custom ego vehicle to SVL Simulator? top # Please see our tutorial on how to add a new ego vehicle to SVL Simulator here . How can I add extra sensors to vehicles in SVL Simulator? top # Adding sensors to a vehicle is done by editing the configuration JSON in the WebUI. See Sensor JSON Options for details on all the available sensors. How do I get parameters in camera matrix like the following? top # [fx 0 cx 0 fy cy 0 0 1] Our reference camera sensors use the pinhole camera model, where all of these parameters can be calculated from other parameters. For focal lengths, i.e. fx and fy , the pinhole camera has same focal lengths in both horizontal and vertical directions as: fx = fy = Height / 2 / Mathf.Tan(FieldOfView / 2.0f * Mathf.Deg2Rad) . Note that since FieldOfView is the vertical FOV, we use half of Height in the calculation. For optical center, i.e. cx and cy , since Unity uses symmetric view frustum by default, the optical center is always at the center of the image. So we have cx = Width / 2.0f and cy = Height / 2.0f How can I add a custom map to SVL Simulator? top # See Maps for details. How can I create or edit map annotations? top # Please see our tutorial on how to add map annotations in SVL Simulator here . Why are pedestrians not spawning when annotated correctly? top # SVL Simulator uses Unity's NavMesh API to work correctly. In Unity Editor, select Window -> AI -> Navigation and bake the NavMesh. Why can't I find catkin_make command when building Apollo? top # Make sure you are not running Apollo dev_start/into.sh scripts as root. The will not work as root. You need to run them as non-root user, without sudo. Why is Apollo perception module turning on and off all the time? top # This means that Apollo perception process is exiting with error. Check apollo/data/log/perception.ERROR file for error messages. Typically you will see following error: Check failed: error == cudaSuccess (8 vs. 0) invalid device function This means one of two things: 1) GPU you are using is not supported by Apollo. Apollo requires CUDA 8 compatible hardware, it won't work if GPU is too old or too new. Apollo officially supports only GTX 1080. RTX 2080 will not work. 2) Other option is that CUDA driver is broken. To fix this you will need to restart your computer. Check that CUDA works on your host system by running one of CUDA examples before running Apollo Why does the Apollo vehicle stop at stop line and not cross intersections? top # Apollo vehicle continues over intersection only when traffic light is green. If perception module does not see traffic light, the vehicle won't move. Check previous question to verify that perception module is running and Apollo is seeing traffic light (top left of dreamview should say GREEN or RED). Dreamview in Apollo shows \"Hardware GPS triggers safety mode. No GNSS status message.\" top # This is expected behavior. SVL Simulator does simulation on software level. It sends only ROS messages to Apollo. Dreamview in Apollo has extra checks that tries to verify if hardware devices are working correctly and are not disconnected. This error message means that Apollo does not see GPS hardware working (as it is not present). It it safe to ignore it. Why does Rviz not load the Autoware vector map? top # Loading SanFrancisco map in Rviz for Autoware is a very slow process, because SanFrancisco map has many annotations and Rviz cannot handle them efficiently. It will either crash or will take many minutes if not hours. You can checkout older commit of autoware-data repository that has annotations only for smaller part of SanFrancisco. git checkout e3cfe709e4af32ad2ea8ea4de85579b9916fe516 Why are there no maps when I make a local build? top # See Build Instructions . It is not required to build the whole simulator using this tool. Why is the TARGET_WAYPOINT missing when using the Map Annotation Tool ? top # Make sure the meshes that make up the road have the Default layer assigned to them and they have a Mesh Collider added. Why does the simulator start and then say the simulation is \"Invalid\"? top # If the vehicle(s) selected for the simulation have a bridge, then a Bridge Connection String is required. The format of the string is IP:port (e.g. localhost:9090 ). The simulator does not assume a port so it must be specified. Why are there no assets when building the simulator from Unity Editor? top # Assets (environments and vehicles) are not included in the main simulator repository to reduce it's size. Maps and vehicles can get large because of 3D assets and textures. See Adding Assets for instructions on how to add assets to the project. Why are there libraries missing when running a PythonAPI quickstart script? top # PythonAPI quickstart scripts use Python libraries that are available publicly. To install all the required libraries, execute the command below inside the PythonAPI directory. pip3 install --user -e . How to fix the \"RuntimeError: The current Numpy installation\" error? top # There is a known issue with Numpy on Windows with the newest updates. See this issue for more information: https://tinyurl.com/y3dm3h86 . To fix this issue, execute the command below. pip install numpy==1.19.3 Other questions? top # See our Github issues page, or email us at contact@svlsimulator.com .","title":"Frequently asked questions"},{"location":"support/faq/#what-are-the-recommended-system-specs-what-are-the-minimum-required-system-specs","text":"For optimal performance, we recommend that you run the simulator on a system with at least a 4 GHz Quad core CPU, NVIDIA GTX 1080 graphics card (8GB memory), and 16GB memory or higher, running on Windows 10. While you can run on a lower-spec system, the performance of the simulator will be impacted and you will probably see much lower frame rates. The minimum specification to run is a 3GHz dual core CPU, NVIDIA graphics card, and 8 GB memory system. Note that simulator runs better on Windows due to fact that Unity and NVIDIA drivers provide better performance on Windows than on Linux. If Apollo or Autoware will be running on the same system, upgrading to a GPU with at least 10GB memory is recommended.","title":"What are the recommended system specs? What are the minimum REQUIRED system specs?"},{"location":"support/faq/#does-the-simulator-run-on-windows-mac-linux","text":"Officially, you can run SVL Simulator on Windows 10 and Ubuntu 18.04 (or later). We do not support macOS at this time.","title":"Does the simulator run on Windows/Mac/Linux?"},{"location":"support/faq/#why-does-the-simulator-not-open-on-linux","text":"The Simulator requires the vulkan libraries to be installed on Linux: sudo apt install libvulkan1","title":"Why does the simulator not open on Linux?"},{"location":"support/faq/#which-unity-version-is-required-and-how-do-i-get-it","text":"SVL Simulator is currently on Unity version 2019.1.10f1, and can be downloaded from the Unity Download Archive. You can download the Windows version here: https://unity3d.com/get-unity/download/archive You can download the Linux version (20191.10f1) here: https://beta.unity3d.com/download/f007ed779b7a/UnitySetup-2019.1.10f1 We are constantly working to ensure that SVL Simulator runs on the latest version of Unity which supports all of our required functionality.","title":"Which Unity version is required and how do I get it?"},{"location":"support/faq/#why-does-my-simulator-say-invalid-out-of-date-assetbundle","text":"Assetbundle versions change as we add new features. To get the latest assetbundles, add assets uploaded by SVL Admin on content asset store to your Library and restart your local simulator.","title":"Why does my Simulator say \"Invalid: Out of date Assetbundle\"?"},{"location":"support/faq/#how-do-i-setup-development-environment-for-unity-on-ubuntu","text":"Install Unity Editor dependencies: sudo apt install \\ gconf-service lib32gcc1 lib32stdc++6 libasound2 libc6 libc6-i386 libcairo2 libcap2 libcups2 \\ libdbus-1-3 libexpat1 libfontconfig1 libfreetype6 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 \\ libgl1 libglib2.0-0 libglu1 libgtk2.0-0 libgtk-3-0 libnspr4 libnss3 libpango1.0-0 libstdc++6 \\ libx11-6 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 \\ libxrender1 libxtst6 zlib1g debconf libgtk2.0-0 libsoup2.4-1 libarchive13 libpng16-16 Download and install Unity 2019.1.10f1: curl -fLo UnitySetup https://beta.unity3d.com/download/f007ed779b7a/UnitySetup-2019.1.10f1 chmod +x UnitySetup ./UnitySetup --unattended --install-location=/opt/Unity --components=Unity Install .NET Core SDK, available from https://dotnet.microsoft.com/download On Ubuntu 18.04 run following commands: wget -q https://packages.microsoft.com/config/ubuntu/18.04/packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get install apt-transport-https sudo apt-get update sudo apt-get install dotnet-sdk-2.2 Install Mono, available from https://www.mono-project.com/download/stable/#download-lin On Ubuntu 18.04 run following commands: sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF sudo apt install apt-transport-https ca-certificates echo \"deb https://download.mono-project.com/repo/ubuntu stable-xenial main\" | sudo tee /etc/apt/sources.list.d/mono-official-stable.list sudo apt update sudo apt install mono-devel Install Visual Studio Code, available in Ubuntu Software or from https://code.visualstudio.com/docs/setup/linux Open VS Code and install C# extension Press Ctrl+Shift+X Search for C# Install extension C# for Visual Studio Code (powered by OmniSharp) Install Unity Debug Extension, available here: https://marketplace.visualstudio.com/items?itemName=Unity.unity-debug Set Unity preferences to use VS Code. See instructions here: https://code.visualstudio.com/docs/other/unity#_setup-vs-code-as-unity-script-editor To find out where Code is installed use which code","title":"How do I setup development environment for Unity on Ubuntu?"},{"location":"support/faq/#where-are-unity-log-files-located","text":"Version Location Windows Binary %APPDATA%\\..\\LocalLow\\LGElectronics\\SVLSimulator\\Player.log Windows Editor %APPDATA%\\..\\Local\\Unity\\Editor\\Editor.log Linux Binary ~/.config/unity3d/LGElectronics/SVLSimulator/Player.log Linux Editor ~/.config/unity3d/Editor.log","title":"Where are Unity log files located"},{"location":"support/faq/#why-are-assets-scenes-missing-empty-after-cloning-from-git","text":"We use Git LFS for large file storage to improve performance of cloning. Before cloning, install and run git lfs install . Then repeat the git clone process. You can find the Git LFS installation instructions here: https://github.com/git-lfs/git-lfs/wiki/Installation Typically if you do not have Git LFS installed or configured then you will see the following error when opening Unity project: error CS026: The type or namespace name \"WebSocketSharp\" could not be found. Are you missing a using directive or an assembly reference?","title":"Why are assets/scenes missing/empty after cloning from git?"},{"location":"support/faq/#why-do-i-get-an-error-saying-som-files-e-g-rosbridge-websocket-launch-are-missing-in-apollo","text":"If you see that some files are missing from ros_pkgs folder in Apollo repository, you need to make sure that you are cloning all submodules: git clone --recurse-submodules https://github.com/lgsvl/apollo.git","title":"Why do I get an error saying some files (e.g. rosbridge_websocket.launch) are missing in Apollo?"},{"location":"support/faq/#ros-bridge-won-t-connect","text":"First make sure you are running rosbridge. If using our Apollo docker image, run: rosbridge.sh For standalone ROS environments run: roslaunch rosbridge_server rosbridge_websocket.launch If you are running ROS bridge on different machine, verify that simulator can connect to it and you do not have firewall blocking ports.","title":"ROS Bridge won"},{"location":"support/faq/#how-do-i-control-the-ego-vehicle-my-vehicle-spawn-position","text":"Find the \"spawn_transform\" game objects in scene and adjust their transform position. If you are creating new maps make sure you add \"SpawnInfo\" component to empty game object. The Simulator will use location of first game object that has SpawnInfo component.","title":"How do I control the ego vehicle (my vehicle) spawn position?"},{"location":"support/faq/#how-can-i-add-a-custom-ego-vehicle-to-lgsvl-simulator","text":"Please see our tutorial on how to add a new ego vehicle to SVL Simulator here .","title":"How can I add a custom ego vehicle to SVL Simulator?"},{"location":"support/faq/#how-can-i-add-extra-sensors-to-vehicles-in-lgsvl-simulator","text":"Adding sensors to a vehicle is done by editing the configuration JSON in the WebUI. See Sensor JSON Options for details on all the available sensors.","title":"How can I add extra sensors to vehicles in SVL Simulator?"},{"location":"support/faq/#how-do-i-get-parameters-in-camera-matrix","text":"[fx 0 cx 0 fy cy 0 0 1] Our reference camera sensors use the pinhole camera model, where all of these parameters can be calculated from other parameters. For focal lengths, i.e. fx and fy , the pinhole camera has same focal lengths in both horizontal and vertical directions as: fx = fy = Height / 2 / Mathf.Tan(FieldOfView / 2.0f * Mathf.Deg2Rad) . Note that since FieldOfView is the vertical FOV, we use half of Height in the calculation. For optical center, i.e. cx and cy , since Unity uses symmetric view frustum by default, the optical center is always at the center of the image. So we have cx = Width / 2.0f and cy = Height / 2.0f","title":"How do I get parameters in camera matrix?"},{"location":"support/faq/#how-can-i-add-a-custom-map-to-lgsvl-simulator","text":"See Maps for details.","title":"How can I add a custom map to SVL Simulator?"},{"location":"support/faq/#how-can-i-create-or-edit-map-annotations","text":"Please see our tutorial on how to add map annotations in SVL Simulator here .","title":"How can I create or edit map annotations?"},{"location":"support/faq/#why-are-pedestrians-not-spawning-when-annotated-correctly","text":"SVL Simulator uses Unity's NavMesh API to work correctly. In Unity Editor, select Window -> AI -> Navigation and bake the NavMesh.","title":"Why are pedestrians not spawning when annotated correctly?"},{"location":"support/faq/#why-can-t-i-find-catkin-make-command-when-building-apollo","text":"Make sure you are not running Apollo dev_start/into.sh scripts as root. The will not work as root. You need to run them as non-root user, without sudo.","title":"Why can't I find catkin_make command when building Apollo?"},{"location":"support/faq/#why-is-apollo-perception-module-turning-on-and-off-all-the-time","text":"This means that Apollo perception process is exiting with error. Check apollo/data/log/perception.ERROR file for error messages. Typically you will see following error: Check failed: error == cudaSuccess (8 vs. 0) invalid device function This means one of two things: 1) GPU you are using is not supported by Apollo. Apollo requires CUDA 8 compatible hardware, it won't work if GPU is too old or too new. Apollo officially supports only GTX 1080. RTX 2080 will not work. 2) Other option is that CUDA driver is broken. To fix this you will need to restart your computer. Check that CUDA works on your host system by running one of CUDA examples before running Apollo","title":"Why is Apollo perception module turning on and off all the time?"},{"location":"support/faq/#why-does-the-apollo-vehicle-stop-at-stop-line-and-not-cross-intersections","text":"Apollo vehicle continues over intersection only when traffic light is green. If perception module does not see traffic light, the vehicle won't move. Check previous question to verify that perception module is running and Apollo is seeing traffic light (top left of dreamview should say GREEN or RED).","title":"Why does the Apollo vehicle stop at stop line and not cross intersections?"},{"location":"support/faq/#dreamview-in-apollo-shows-hardware-gps-triggers-safety-mode-no-gnss-status-message","text":"This is expected behavior. SVL Simulator does simulation on software level. It sends only ROS messages to Apollo. Dreamview in Apollo has extra checks that tries to verify if hardware devices are working correctly and are not disconnected. This error message means that Apollo does not see GPS hardware working (as it is not present). It it safe to ignore it.","title":"Dreamview in Apollo shows \"Hardware GPS triggers safety mode. No GNSS status message.\""},{"location":"support/faq/#why-does-rviz-not-load-the-autoware-vector-map","text":"Loading SanFrancisco map in Rviz for Autoware is a very slow process, because SanFrancisco map has many annotations and Rviz cannot handle them efficiently. It will either crash or will take many minutes if not hours. You can checkout older commit of autoware-data repository that has annotations only for smaller part of SanFrancisco. git checkout e3cfe709e4af32ad2ea8ea4de85579b9916fe516","title":"Why does Rviz not load the Autoware vector map?"},{"location":"support/faq/#why-are-there-no-maps-when-i-make-a-local-build","text":"See Build Instructions . It is not required to build the whole simulator using this tool.","title":"Why are there no maps when I make a local build?"},{"location":"support/faq/#what-is-the-target-waypoint-missing-when-using-the-map-annotation-tool","text":"Make sure the meshes that make up the road have the Default layer assigned to them and they have a Mesh Collider added.","title":"Why is the TARGET_WAYPOINT missing when using the Map Annotation Tool"},{"location":"support/faq/#why-does-the-simulator-start-and-then-say-the-simulation-is-invalid","text":"If the vehicle(s) selected for the simulation have a bridge, then a Bridge Connection String is required. The format of the string is IP:port (e.g. localhost:9090 ). The simulator does not assume a port so it must be specified.","title":"Why does the simulator start and then say the simulation is \"Invalid\"?"},{"location":"support/faq/#why-are-there-no-assets-when-building-the-simulator-from-unity-editor","text":"Assets (environments and vehicles) are not included in the main simulator repository to reduce it's size. Maps and vehicles can get large because of 3D assets and textures. See Adding Assets for instructions on how to add assets to the project.","title":"Why are there no assets when building the simulator from Unity Editor?"},{"location":"support/faq/#why-are-there-libraries-missing-when-running-a-pythonapi-script","text":"PythonAPI quickstart scripts use Python libraries that are available publicly. To install all the required libraries, execute the command below inside the PythonAPI directory. pip3 install --user -e .","title":"Why are there libraries missing when running a PythonAPI script?"},{"location":"support/faq/#how-to-fix-the-runtime-the-current-numpy-installation-error","text":"There is a known issue with Numpy on Windows with the newest updates. See this issue for more information: https://tinyurl.com/y3dm3h86 . To fix this issue, execute the command below. pip install numpy==1.19.3","title":"How to fix the \"RuntimeError: The current Numpy installation\" error?"},{"location":"support/faq/#other-questions","text":"See our Github issues page, or email us at contact@svlsimulator.com .","title":"Other questions?"},{"location":"support/support-requests/","text":"Support Requests and Reporting Problems","title":"Support Requests and Reporting Problems"},{"location":"support/troubleshooting/","text":"Troubleshooting Important: if you have run a previous version (2020.06 or older) of SVL Simulator on your machine, then your assets like maps, vehicles, clusters, simulations will not be automatically migrated to the new version 2021.1 or above. To solve this issue, you will have to create a new account on the new cloud-based web-ui , build your assets in editor using SVL Simulator 2021.1 or greater, upload the newly built asset bundles to your library and create new simulations . If you have any other issues with downloaded data, that need all local data to be wiped, you can delete the folder \u201cLGElectronics\u201d, in the location below: Platform Filepath Windows %APPDATA%..\\LocalLow\\LGElectronics Linux ~/.config/unity3d/LGElectronics NOTE: This will delete all existing simulation configurations and log files related to SVL Simulator. You can then restart the SVL Simulator binary executable, and the folder will be created again and you will have to link to a new cluster . Logs top # Logs can be found at the following locations: Version Location Windows Binary %APPDATA%..\\LocalLow\\LGElectronics\\SVLSimulator\\Player.log Windows Editor %APPDATA%..\\Local\\Unity\\Editor\\Editor.log Linux Binary ~/.config/unity3d/LGElectronics/SVLSimulator/Player.log Linux Editor ~/.config/unity3d/Unity/Editor/Editor.log Unity Help top # Please see our Unity Help document for additional tips and troubleshooting guides when working with Unity Editor in Developer Mode. Frequently Asked Questions (FAQ) top # You can find our FAQ here , which may be helpful in answering common questions or issues that may arise.","title":"Troubleshooting"},{"location":"support/troubleshooting/#logs","text":"Logs can be found at the following locations: Version Location Windows Binary %APPDATA%..\\LocalLow\\LGElectronics\\SVLSimulator\\Player.log Windows Editor %APPDATA%..\\Local\\Unity\\Editor\\Editor.log Linux Binary ~/.config/unity3d/LGElectronics/SVLSimulator/Player.log Linux Editor ~/.config/unity3d/Unity/Editor/Editor.log","title":"Logs"},{"location":"support/troubleshooting/#unity-help","text":"Please see our Unity Help document for additional tips and troubleshooting guides when working with Unity Editor in Developer Mode.","title":"Unity Help"},{"location":"support/troubleshooting/#frequently-asked-questions-(faq)","text":"You can find our FAQ here , which may be helpful in answering common questions or issues that may arise.","title":"Frequently Asked Questions (FAQ)"},{"location":"support/unity-help/","text":"Unity Help SVL Simulator was created with Unity Real-Time Development Platform and its High-Definition Render Pipeline (HDRP). HDRP is Unity Technologies' scriptable pipeline that is used to create software applications that prioritize graphics quality for realism in demanding scenarios utilizing GPU hardware. Since this technology is relatively new, there can be some issues and solutions that are not documented. This document goes over some of our own tips, guides, and recommended best practices for working inside the Unity Editor. HDRP Requirements top # HDRP targets high-end PCs and prioritizes high definition visuals but has specific system requirements. Target supported APIs are D3D11, D3D12, Metal, and Vulkan Architecture x86, x86_64 We highly recommend GPU's with at least 8GB of memory - HDRP and SVL Simulator are very GPU-intensive. HDRP Asset Conversion top # If you are converting non-HDRP assets to HDRP, there are a few things to watch out for. Before importing assets into HDRP, be sure to remove all custom shaders Change materials to use Unity's standard shaders Use Unity's tools and documentation to properly convert assets HDRP shader issues top # After pulling major changes to SVL Simulator, or a crash, HDRP may get corrupted. This is a known issue but it can be fixed easily: Close Unity Navigate to the root folder of SVL Simulator that you cloned Find Library folder Delete Library folder Open Unity. Wait while Unity creates the Library folder Close Unity and open again Unity Editor Tips top # There are many ways to fix, prevent issues, and improve workflows with Unity Editor. Turn off scene lighting in Scene View Panel. This adds lighting to the scene view camera because SVL Simulator adds lights at runtime When opening a prefab or a environment scene you may see a cyan texture on meshes; this is fine. This is Unity compiling a shader - DO NOT close Unity or try to do anything until it is finished. Before making a bundle build, be sure to open assets in the editor so Unity can do this first. Set Script Changes While Playing to Stop Playing and Recompile in Edit -> Preferences -> General window. This prevents bad states after changing code and leaving the Editor in play mode. It is also recommended to toggle off Auto Refresh and use Ctrl+R to recompile but it is up to your preferences. Change the default Unity Editor Layout by moving the Console View Panel next to the Project View Panel. This allows you to see any errors easily and make better screen captures for any issues you report. If you have issues with an asset, you can right click and choose Reimport. This helps solve many issues with individual assets. SVL Simulator uses many Editor Gizmos to visualize tools. Be sure to enable this button in the Scene View Panel. Be sure to install support for both Windows and Linux. This is needed to make any asset bundle because SVL Simulator supports both of these targets. This image is an example of a Windows target but with Linux support.","title":"Unity help"},{"location":"support/unity-help/#hdrp-requirements","text":"HDRP targets high-end PCs and prioritizes high definition visuals but has specific system requirements. Target supported APIs are D3D11, D3D12, Metal, and Vulkan Architecture x86, x86_64 We highly recommend GPU's with at least 8GB of memory - HDRP and SVL Simulator are very GPU-intensive.","title":"HDRP Requirements"},{"location":"support/unity-help/#hdrp-asset-conversion","text":"If you are converting non-HDRP assets to HDRP, there are a few things to watch out for. Before importing assets into HDRP, be sure to remove all custom shaders Change materials to use Unity's standard shaders Use Unity's tools and documentation to properly convert assets","title":"HDRP Asset Conversion"},{"location":"support/unity-help/#hdrp-shader-issues","text":"After pulling major changes to SVL Simulator, or a crash, HDRP may get corrupted. This is a known issue but it can be fixed easily: Close Unity Navigate to the root folder of SVL Simulator that you cloned Find Library folder Delete Library folder Open Unity. Wait while Unity creates the Library folder Close Unity and open again","title":"HDRP shader issues"},{"location":"support/unity-help/#unity-editor-tips","text":"There are many ways to fix, prevent issues, and improve workflows with Unity Editor. Turn off scene lighting in Scene View Panel. This adds lighting to the scene view camera because SVL Simulator adds lights at runtime When opening a prefab or a environment scene you may see a cyan texture on meshes; this is fine. This is Unity compiling a shader - DO NOT close Unity or try to do anything until it is finished. Before making a bundle build, be sure to open assets in the editor so Unity can do this first. Set Script Changes While Playing to Stop Playing and Recompile in Edit -> Preferences -> General window. This prevents bad states after changing code and leaving the Editor in play mode. It is also recommended to toggle off Auto Refresh and use Ctrl+R to recompile but it is up to your preferences. Change the default Unity Editor Layout by moving the Console View Panel next to the Project View Panel. This allows you to see any errors easily and make better screen captures for any issues you report. If you have issues with an asset, you can right click and choose Reimport. This helps solve many issues with individual assets. SVL Simulator uses many Editor Gizmos to visualize tools. Be sure to enable this button in the Scene View Panel. Be sure to install support for both Windows and Linux. This is needed to make any asset bundle because SVL Simulator supports both of these targets. This image is an example of a Windows target but with Linux support.","title":"Unity Editor Tips"},{"location":"system-under-test/apollo-json-example/","text":"Example JSON Configuration for an Apollo 3.0 Vehicle Bridge Type top # ROS Apollo Published Topics top # Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/velodyne64/compensator/PointCloud2 LiDAR /apollo/sensor/camera/traffic/image_short/compressed Main Camera /apollo/sensor/camera/traffic/image_long/compressed Telephoto Camera Subscribed Topics top # Topic Sensor Name /apollo/control Apollo Car Control Complete JSON Configuration top # [ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"RadarSensor\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.337, \"z\": 3.691, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/velodyne64/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/traffic/image_short/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/traffic/image_long/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Example JSON Configuration for an Apollo 3.0 Vehicle [](#top)"},{"location":"system-under-test/apollo-json-example/#bridge-type","text":"ROS Apollo","title":"Bridge Type"},{"location":"system-under-test/apollo-json-example/#published-topics","text":"Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/velodyne64/compensator/PointCloud2 LiDAR /apollo/sensor/camera/traffic/image_short/compressed Main Camera /apollo/sensor/camera/traffic/image_long/compressed Telephoto Camera","title":"Published Topics"},{"location":"system-under-test/apollo-json-example/#subscribed-topcs","text":"Topic Sensor Name /apollo/control Apollo Car Control","title":"Subscribed Topics"},{"location":"system-under-test/apollo-json-example/#complete-json-configuration","text":"[ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"RadarSensor\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.337, \"z\": 3.691, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/velodyne64/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/traffic/image_short/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/traffic/image_long/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Complete JSON Configuration"},{"location":"system-under-test/apollo-master-instructions/","text":"Running latest Apollo with SVL Simulator These instructions are tested after the last commit enabling SVL Simulator with latest Apollo master. Commits after that are assumed to work as well, but not guaranteed. Big changes have recently been introduced in Apollo master; camera perception may not be working yet, and LiDAR perception is recently able to build but still unstable (and uses a lot of GPU memory making it challenging for Apollo to share an 8GB GPU with the SVL Simulator). Because of this we will be using the modular testing features of the SVL Simulator which allows the simulator to directly publish perception and traffic light messages. For those who used our fork of Apollo 5.0 before: please note the new step to select the correct setup mode in Dreamview . Table of Contents Getting Started Prerequisites Setup Docker Installing Docker CE Installing the NVIDIA Container Toolkit Cloning the Repository Building Apollo and bridge Adding a Vehicle Adding an HD Map Setting Clock Mode in Apollo Launching Apollo alongisde the Simulator Adding a Vehicle Adding an HD Map Getting Started top # This guide outlines the steps required to setup Apollo for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator . Prerequisites top # Ubuntu 18.04 or later NVIDIA graphics card (required for Perception) NVIDIA proprietary driver (>=410.48) must be installed Setup top # Docker top # Apollo is designed to run out of Docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made. Installing Docker CE top # To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps . Installing the NVIDIA Container Toolkit top # Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.39 Driver Version: 460.39 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:02:00.0 On | N/A | | 35% 54C P0 44W / 180W | 3048MiB / 8116MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available here . Cloning the Repository top # Clone latest Apollo using the following command: git clone https://github.com/ApolloAuto/apollo Building Apollo and bridge top # Now everything should be in place to build Apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE The Apollo build may fail on machines with less than 1GB of RAM per CPU core due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, either re-start it until it succeeds, but if it continues to fail (especially when running the linker) then you'll need to address the low memory situation by either adding more memory to your build machine or enabling or increasing available swap space . If your Apollo build is crashing on a 16GB machine with little or no swap, try setting it to 16GB. Adding a Vehicle to Apollo top # In order to run Apollo with the simulator, the calibration files for the vehicle you use in the simulator must be available in Apollo for perception module to work correctly. Currently, only calibration files for Lincoln 2017 MKZ are included in the latest Apollo. More calibration files may be added later. To add a new vehicle to Apollo: Create a new folder with the new vehicle's name under APOLLO_ROOT/modules/calibration/data . Copy all calibration files of Lincoln 2017 MKZ to the folder. Update vehicle parameters in vehicle_param.pb.txt if your new vehicle is different from Lincoln 2017 MKZ. Update LiDAR calibration files in velodyne_params folder. If you have different configuration for LiDAR with respect to novatel, you need to update LiDAR's height and extrinsics. Update camera calibration files in camera_params folder. You need to update camera intrinsics and extrinsics for your own camera and sensor configurations. You may also need to update other sensor calibration files if you are using them on your vehicle. After you add the new vehicle, you need to restart dreamview to refresh the vehicle list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh Adding an HD Map to Apollo top # An HD map in Apollo format corresponding to the environment you use in the simulator must be available in Apollo for modules like routing , planning , etc. to work. Currently, only HD map files for Borregas Ave are included in the the latest Apollo at this time. More HD maps may be added later. You can also download HD Maps for any map available in the Web UI Store ( for example ), and manually add them to /apollo/modules/map/data/ . Or you can export your own map file following Export Map Annotations , then generate routing_map and sim_map following these instructions . In our Apollo 5.0 fork, we have a script generate_map.sh to help users generate map files. You can copy that script to APOLLO_ROOT/scripts/ folder to use it. For details, please follow here . After you add the new map, you need to restart Dreamview to refresh the map list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh Alternatively, you can analyze top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 . Setting Clock Mode in Apollo (Optional) top # SVL Simulator has a clock sensor which outputs a simulated time that can be used as the reference time for Apollo. When used, message lag which may result from transporting large amounts of data between the Simulator and Apollo will not affect Apollo performance. In addition, the clock sensor allows Apollo to run with \"simulator time\" when used in \"stepped simulation\" mode as shown in Python API Quickstart Script 33 . To be able to use the clock sensor (which is part of the Apollo 6.0 (modular testing) sensor configuration), clock_mode must be set to MODE_MOCK in cyber.pb.conf . Launching Apollo alongside the Simulator top # Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch Apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about Dreamview not being build if you do not run the script from the /apollo directory. ./scripts/bootstrap_lgsvl.sh Launch bridge (inside docker container): ./scripts/bridge.sh Run the SVL Simulator outside of docker. See instructions for Running the Simulator Create a Simulation using the Random Traffic runtime template and selecting the BorregasAve map and Lincoln2017MKZ vehicle with the Apollo 6.0 (modular testing) sensor configuration Enter localhost:9090 as the Bridge Connection String (if apollo and the simulator are running on separate machines you will need to enter the IPv4 address of the machine running the bridge instead of localhost ) (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Run Simulation\" Open Apollo Dreamview in a browser by navigating to: localhost:8888 NEW for Apollo Master : Select the Mkz Lgsvl setup mode, from the menu to the left of the vehicle menu. Select the Lincoln2017MKZ LGSVL vehicle and Borregas Ave map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Planning , Prediction , Routing , and Control (there is no need to enable Perception or Traffic Light because the modular testing sensors directly publish perception and traffic light results). Navigate to the Route Editing tab. Select a destination by clicking on a lane-line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). ./dev_start.sh stop If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090 Adding a Vehicle top # Only calibration files for Lincoln 2017 MKZ is included in the latest Apollo at this time. More calibration files map be added later. Adding an HD Map top # Only HD map files for Borregas Ave is included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map from our map annotation tool , name it base_map.bin and then generate routing_map and sim_map following these Apollo map instructions .","title":"Latest Apollo"},{"location":"system-under-test/apollo-master-instructions/#getting-started","text":"This guide outlines the steps required to setup Apollo for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator .","title":"Getting Started"},{"location":"system-under-test/apollo-master-instructions/#prerequisites","text":"Ubuntu 18.04 or later NVIDIA graphics card (required for Perception) NVIDIA proprietary driver (>=410.48) must be installed","title":"Prerequisites"},{"location":"system-under-test/apollo-master-instructions/#setup","text":"","title":"Setup"},{"location":"system-under-test/apollo-master-instructions/#docker","text":"Apollo is designed to run out of Docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made.","title":"Docker"},{"location":"system-under-test/apollo-master-instructions/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"system-under-test/apollo-master-instructions/#installing-nvidia-docker","text":"Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.39 Driver Version: 460.39 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:02:00.0 On | N/A | | 35% 54C P0 44W / 180W | 3048MiB / 8116MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available here .","title":"Installing the NVIDIA Container Toolkit"},{"location":"system-under-test/apollo-master-instructions/#cloning-the-repository","text":"Clone latest Apollo using the following command: git clone https://github.com/ApolloAuto/apollo","title":"Cloning the Repository"},{"location":"system-under-test/apollo-master-instructions/#building-apollo-and-bridge","text":"Now everything should be in place to build Apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE The Apollo build may fail on machines with less than 1GB of RAM per CPU core due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, either re-start it until it succeeds, but if it continues to fail (especially when running the linker) then you'll need to address the low memory situation by either adding more memory to your build machine or enabling or increasing available swap space . If your Apollo build is crashing on a 16GB machine with little or no swap, try setting it to 16GB.","title":"Building Apollo and bridge"},{"location":"system-under-test/apollo-master-instructions/#adding-a-vehicle","text":"In order to run Apollo with the simulator, the calibration files for the vehicle you use in the simulator must be available in Apollo for perception module to work correctly. Currently, only calibration files for Lincoln 2017 MKZ are included in the latest Apollo. More calibration files may be added later. To add a new vehicle to Apollo: Create a new folder with the new vehicle's name under APOLLO_ROOT/modules/calibration/data . Copy all calibration files of Lincoln 2017 MKZ to the folder. Update vehicle parameters in vehicle_param.pb.txt if your new vehicle is different from Lincoln 2017 MKZ. Update LiDAR calibration files in velodyne_params folder. If you have different configuration for LiDAR with respect to novatel, you need to update LiDAR's height and extrinsics. Update camera calibration files in camera_params folder. You need to update camera intrinsics and extrinsics for your own camera and sensor configurations. You may also need to update other sensor calibration files if you are using them on your vehicle. After you add the new vehicle, you need to restart dreamview to refresh the vehicle list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh","title":"Adding a Vehicle"},{"location":"system-under-test/apollo-master-instructions/#adding-an-hd-map","text":"An HD map in Apollo format corresponding to the environment you use in the simulator must be available in Apollo for modules like routing , planning , etc. to work. Currently, only HD map files for Borregas Ave are included in the the latest Apollo at this time. More HD maps may be added later. You can also download HD Maps for any map available in the Web UI Store ( for example ), and manually add them to /apollo/modules/map/data/ . Or you can export your own map file following Export Map Annotations , then generate routing_map and sim_map following these instructions . In our Apollo 5.0 fork, we have a script generate_map.sh to help users generate map files. You can copy that script to APOLLO_ROOT/scripts/ folder to use it. For details, please follow here . After you add the new map, you need to restart Dreamview to refresh the map list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh Alternatively, you can analyze top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 .","title":"Adding an HD Map"},{"location":"system-under-test/apollo-master-instructions/#setting-clock-mode","text":"SVL Simulator has a clock sensor which outputs a simulated time that can be used as the reference time for Apollo. When used, message lag which may result from transporting large amounts of data between the Simulator and Apollo will not affect Apollo performance. In addition, the clock sensor allows Apollo to run with \"simulator time\" when used in \"stepped simulation\" mode as shown in Python API Quickstart Script 33 . To be able to use the clock sensor (which is part of the Apollo 6.0 (modular testing) sensor configuration), clock_mode must be set to MODE_MOCK in cyber.pb.conf .","title":"Setting Clock Mode in Apollo"},{"location":"system-under-test/apollo-master-instructions/#launching-apollo-alongide-the-simulator","text":"Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch Apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about Dreamview not being build if you do not run the script from the /apollo directory. ./scripts/bootstrap_lgsvl.sh Launch bridge (inside docker container): ./scripts/bridge.sh Run the SVL Simulator outside of docker. See instructions for Running the Simulator Create a Simulation using the Random Traffic runtime template and selecting the BorregasAve map and Lincoln2017MKZ vehicle with the Apollo 6.0 (modular testing) sensor configuration Enter localhost:9090 as the Bridge Connection String (if apollo and the simulator are running on separate machines you will need to enter the IPv4 address of the machine running the bridge instead of localhost ) (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Run Simulation\" Open Apollo Dreamview in a browser by navigating to: localhost:8888 NEW for Apollo Master : Select the Mkz Lgsvl setup mode, from the menu to the left of the vehicle menu. Select the Lincoln2017MKZ LGSVL vehicle and Borregas Ave map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Planning , Prediction , Routing , and Control (there is no need to enable Perception or Traffic Light because the modular testing sensors directly publish perception and traffic light results). Navigate to the Route Editing tab. Select a destination by clicking on a lane-line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). ./dev_start.sh stop If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090","title":"Launching Apollo alongisde the Simulator"},{"location":"system-under-test/apollo-master-instructions/#adding-a-vehicle","text":"Only calibration files for Lincoln 2017 MKZ is included in the latest Apollo at this time. More calibration files map be added later.","title":"Adding a Vehicle"},{"location":"system-under-test/apollo-master-instructions/#adding-an-hd-map","text":"Only HD map files for Borregas Ave is included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map from our map annotation tool , name it base_map.bin and then generate routing_map and sim_map following these Apollo map instructions .","title":"Adding an HD Map"},{"location":"system-under-test/apollo5-0-instructions/","text":"Running Apollo 5.0 with SVL Simulator Table of Contents Getting Started Prerequisites Setup Docker Installing Docker CE Installing NVIDIA Container Toolkit Pulling LGSVL Docker image Cloning the Repository Building Apollo and bridge Launching Apollo alongisde the Simulator Supported Vehicles Adding an HD Map Getting Started top # This guide outlines the steps required to setup Apollo 5.0 for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator . Prerequisites top # Linux operating system (preferably Ubuntu 18.04 or later) NVIDIA graphics card (required for Perception) NVIDIA proprietary drivers must be installed. Apollo 5.0 does not support Volta, Turing, or Ampere architectures (this includes Titan V, GTX 16xx, and RTX GPUs). Setup top # Docker top # Apollo 5.0 is designed to be run inside of a Docker container. The working directory of this repository will be mounted as a volume when starting the container so the image will not need to be rebuilt each time a modification is made. Installing Docker CE top # To install Docker CE please refer to the official documentation . NOTE: Apollo 5.0 does not work if the Docker is started with sudo . We suggest following through with the post installation steps . Installing NVIDIA Container Toolkit top # Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed, enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.102.04 Driver Version: 450.102.04 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:02:00.0 On | N/A | | 41% 31C P8 21W / 260W | 270MiB / 11016MiB | 3% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1404 G /usr/lib/xorg/Xorg 20MiB | | 0 N/A N/A 1467 G /usr/bin/gnome-shell 57MiB | | 0 N/A N/A 2491 G /usr/lib/xorg/Xorg 131MiB | | 0 N/A N/A 2630 G /usr/bin/gnome-shell 44MiB | | 0 N/A N/A 3423 G ...AAAAAAAA== --shared-files 12MiB | +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available from the official documentation . Pulling LGSVL Docker image top # LGSVL maintains a Docker image to be used alongside this repository. The Docker image is available from Docker Hub . To pull the image use the following command: docker pull lgsvl/apollo-5.0 Cloning the Repository top # This repository includes the protobuf branch of lgsvl_msgs as a submodule. To make sure that the submodule is also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo-5.0.git Building Apollo and bridge top # Now everything should be in place to build Apollo. Apollo must be built from inside the container. To launch the container, navigate to the directory where the repository was cloned and enter: docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. Enter the container: docker/scripts/dev_into.sh Build Apollo 5.0 (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE: The Apollo 5.0 build may run out of memory and crash on some machines due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, either re-start it until it succeeds, or try changing line ~150 of function build() in apollo.sh as follows: Replace this: JOB_ARG=\"--jobs=$(nproc) --ram_utilization_factor 80\" With this, which will use all but two cores for the build: JOB_ARG=\"--jobs=$(expr $(nproc) - 2) --ram_utilization_factor 70\" Alternatively, you can analyze with top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 . If it continues to fail (especially when running the linker) then you'll need to address the low memory situation by either adding more memory to your build machine or enabling or increasing available swap space . If your Apollo 5.0 build is crashing on a 16GB machine with little or no swap, try setting it to 16GB. Launching Apollo alongside the Simulator top # Here we only describe only a simple case of driving from point A to point B using Apollo 5.0 and the SVL Simulator. Launch Apollo 5.0 Enter the container: docker/scripts/dev_into.sh Start Apollo 5.0: bootstrap.sh Note: You may receive errors about Dreamview not being built if you do not run the script from the /apollo directory. Start the bridge: bridge.sh Run the SVL Simulator (outside of Docker). See these instructions . Create a Simulation from the Random Traffic runtime template selecting the BorregasAve Map, the Lincoln2017MKZ Vehicle, and the Apollo 5.0 Sensor Configuration. Enter localhost:9090 as the Bridge IP . (Optional) Enable Traffic and Pedestrians. (Optional) Set the Time of Day and weather settings. Publish the simulation. Select the created simulation and click Run Simulation . Open Apollo 5.0 Dreamview in a browser by navigating to: localhost:8888 Select the Lincoln2017MKZ vehicle and BorregasAve map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Traffic Light , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane-line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the Docker container, run this in a new terminal (not inside the Docker container): docker/scripts/dev_start.sh stop If you are using ufw , it is easiest to completely disable the firewall to allow connections: sudo ufw disable If that is not possible, add the following rules: sudo ufw allow 8888 sudo ufw allow 9090 These are required even if running the simulator and Apollo on the same machine. Supported Vehicles top # Only the calibration files for Lincoln 2017 MKZ, Jaguar 2015 XE, and Hyundai 2018 Nexo are included in the the simulator branch of the LGSVL fork of Apollo 5.0 at this time. More calibration files may be added later. Adding an HD Map top # Some default maps in your Maps library have their HD map files included in the simulator branch of the LGSVL fork of Apollo 5.0. You can find other maps are in the Maps store . If you want to add a new HD map to Apollo 5.0, follow these steps: Either select a map in your Maps library and download the apollo50 HD map or export your own map from our map annotation tool as base_map.bin . Create a new map folder under APOLLO_ROOT/modules/map/data/ and put base_map.bin in the folder. Inside Apollo 5.0 Docker container: Generate map files required by Apollo 5.0: cd /apollo generate_map.sh YOUR_MAP_FOLDER_NAME You need to restart Dreamview to refresh the map list: bootstrap.sh stop && bootstrap.sh","title":"Apollo 5.0"},{"location":"system-under-test/apollo5-0-instructions/#getting-started","text":"This guide outlines the steps required to setup Apollo 5.0 for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator .","title":"Getting Started"},{"location":"system-under-test/apollo5-0-instructions/#prerequisites","text":"Linux operating system (preferably Ubuntu 18.04 or later) NVIDIA graphics card (required for Perception) NVIDIA proprietary drivers must be installed. Apollo 5.0 does not support Volta, Turing, or Ampere architectures (this includes Titan V, GTX 16xx, and RTX GPUs).","title":"Prerequisites"},{"location":"system-under-test/apollo5-0-instructions/#setup","text":"","title":"Setup"},{"location":"system-under-test/apollo5-0-instructions/#docker","text":"Apollo 5.0 is designed to be run inside of a Docker container. The working directory of this repository will be mounted as a volume when starting the container so the image will not need to be rebuilt each time a modification is made.","title":"Docker"},{"location":"system-under-test/apollo5-0-instructions/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . NOTE: Apollo 5.0 does not work if the Docker is started with sudo . We suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"system-under-test/apollo5-0-instructions/#installing-nvidia-container-toolkit","text":"Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed, enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.102.04 Driver Version: 450.102.04 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:02:00.0 On | N/A | | 41% 31C P8 21W / 260W | 270MiB / 11016MiB | 3% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1404 G /usr/lib/xorg/Xorg 20MiB | | 0 N/A N/A 1467 G /usr/bin/gnome-shell 57MiB | | 0 N/A N/A 2491 G /usr/lib/xorg/Xorg 131MiB | | 0 N/A N/A 2630 G /usr/bin/gnome-shell 44MiB | | 0 N/A N/A 3423 G ...AAAAAAAA== --shared-files 12MiB | +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available from the official documentation .","title":"Installing NVIDIA Container Toolkit"},{"location":"system-under-test/apollo5-0-instructions/#pulling-lgsvl-docker-image","text":"LGSVL maintains a Docker image to be used alongside this repository. The Docker image is available from Docker Hub . To pull the image use the following command: docker pull lgsvl/apollo-5.0","title":"Pulling LGSVL Docker image"},{"location":"system-under-test/apollo5-0-instructions/#cloning-the-repository","text":"This repository includes the protobuf branch of lgsvl_msgs as a submodule. To make sure that the submodule is also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo-5.0.git","title":"Cloning the Repository"},{"location":"system-under-test/apollo5-0-instructions/#building-apollo-and-bridge","text":"Now everything should be in place to build Apollo. Apollo must be built from inside the container. To launch the container, navigate to the directory where the repository was cloned and enter: docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. Enter the container: docker/scripts/dev_into.sh Build Apollo 5.0 (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE: The Apollo 5.0 build may run out of memory and crash on some machines due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, either re-start it until it succeeds, or try changing line ~150 of function build() in apollo.sh as follows: Replace this: JOB_ARG=\"--jobs=$(nproc) --ram_utilization_factor 80\" With this, which will use all but two cores for the build: JOB_ARG=\"--jobs=$(expr $(nproc) - 2) --ram_utilization_factor 70\" Alternatively, you can analyze with top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 . If it continues to fail (especially when running the linker) then you'll need to address the low memory situation by either adding more memory to your build machine or enabling or increasing available swap space . If your Apollo 5.0 build is crashing on a 16GB machine with little or no swap, try setting it to 16GB.","title":"Building Apollo and bridge"},{"location":"system-under-test/apollo5-0-instructions/#launching-apollo-alongide-the-simulator","text":"Here we only describe only a simple case of driving from point A to point B using Apollo 5.0 and the SVL Simulator. Launch Apollo 5.0 Enter the container: docker/scripts/dev_into.sh Start Apollo 5.0: bootstrap.sh Note: You may receive errors about Dreamview not being built if you do not run the script from the /apollo directory. Start the bridge: bridge.sh Run the SVL Simulator (outside of Docker). See these instructions . Create a Simulation from the Random Traffic runtime template selecting the BorregasAve Map, the Lincoln2017MKZ Vehicle, and the Apollo 5.0 Sensor Configuration. Enter localhost:9090 as the Bridge IP . (Optional) Enable Traffic and Pedestrians. (Optional) Set the Time of Day and weather settings. Publish the simulation. Select the created simulation and click Run Simulation . Open Apollo 5.0 Dreamview in a browser by navigating to: localhost:8888 Select the Lincoln2017MKZ vehicle and BorregasAve map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Traffic Light , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane-line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the Docker container, run this in a new terminal (not inside the Docker container): docker/scripts/dev_start.sh stop If you are using ufw , it is easiest to completely disable the firewall to allow connections: sudo ufw disable If that is not possible, add the following rules: sudo ufw allow 8888 sudo ufw allow 9090 These are required even if running the simulator and Apollo on the same machine.","title":"Launching Apollo alongisde the Simulator"},{"location":"system-under-test/apollo5-0-instructions/#supported-vehicles","text":"Only the calibration files for Lincoln 2017 MKZ, Jaguar 2015 XE, and Hyundai 2018 Nexo are included in the the simulator branch of the LGSVL fork of Apollo 5.0 at this time. More calibration files may be added later.","title":"Supported Vehicles"},{"location":"system-under-test/apollo5-0-instructions/#adding-an-hd-map","text":"Some default maps in your Maps library have their HD map files included in the simulator branch of the LGSVL fork of Apollo 5.0. You can find other maps are in the Maps store . If you want to add a new HD map to Apollo 5.0, follow these steps: Either select a map in your Maps library and download the apollo50 HD map or export your own map from our map annotation tool as base_map.bin . Create a new map folder under APOLLO_ROOT/modules/map/data/ and put base_map.bin in the folder. Inside Apollo 5.0 Docker container: Generate map files required by Apollo 5.0: cd /apollo generate_map.sh YOUR_MAP_FOLDER_NAME You need to restart Dreamview to refresh the map list: bootstrap.sh stop && bootstrap.sh","title":"Adding an HD Map"},{"location":"system-under-test/apollo5-0-json-example/","text":"Example JSON Configuration for an Apollo 5.0 Vehicle Bridge Type top # CyberRT Published Topics top # Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/ins_stat GPS INS Status /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/lidar128/compensator/PointCloud2 Lidar /apollo/sensor/camera/front_6mm/image/compressed Main Camera /apollo/sensor/camera/front_12mm/image/compressed Telephoto Camera Subscribed Topics top # Topic Sensor Name /apollo/control Apollo Car Control Complete JSON Configuration top # [ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"RadarSensor\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.337, \"z\": 3.691, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/lidar128/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_12mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Example JSON Configuration for an Apollo 5.0 Vehicle [](#top)"},{"location":"system-under-test/apollo5-0-json-example/#bridge-type","text":"CyberRT","title":"Bridge Type"},{"location":"system-under-test/apollo5-0-json-example/#published-topics","text":"Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/ins_stat GPS INS Status /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/lidar128/compensator/PointCloud2 Lidar /apollo/sensor/camera/front_6mm/image/compressed Main Camera /apollo/sensor/camera/front_12mm/image/compressed Telephoto Camera","title":"Published Topics"},{"location":"system-under-test/apollo5-0-json-example/#subscribed-topics","text":"Topic Sensor Name /apollo/control Apollo Car Control","title":"Subscribed Topics"},{"location":"system-under-test/apollo5-0-json-example/#complete-json-configuration","text":"[ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"RadarSensor\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.337, \"z\": 3.691, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/lidar128/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_12mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Complete JSON Configuration"},{"location":"system-under-test/apollo6-0-instructions/","text":"Running Apollo 6.0 with SVL Simulator These instructions have been tested with the r6.0.0 branch from the Apollo repository ( latest commit at the time of writing). Apollo 6.0 introduced major changes to the perception stack and as a result camera perception is not yet working, and LiDAR perception is unstable. Because of this, this guide will focus on using the modular testing feature of the SVL Simulator to simulate perception instead of using Apollo's perception module. For those who used our fork of Apollo 5.0 before: please note the new step to select the correct setup mode in Dreamview . Table of Contents Getting Started Prerequisites Setup Docker Installing Docker CE Installing NVIDIA Docker Cloning the Repository Building Apollo and the bridge Adding a Vehicle Adding an HD Map Launching Apollo alongisde the Simulator Adding a Vehicle Adding an HD Map Getting Started top # This guide outlines the steps required to setup Apollo for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator . Prerequisites top # Ubuntu 18.04 or later NVIDIA graphics card (required for perception and prediction) NVIDIA proprietary driver (>=410.48) must be installed Setup top # Docker top # Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made. Installing Docker CE top # To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps . Installing NVIDIA Docker top # Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.64 Driver Version: 440.64 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:01:00.0 On | N/A | | 27% 29C P8 7W / 180W | 579MiB / 8117MiB | 1% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1745 G /usr/lib/xorg/Xorg 40MiB | | 0 1862 G /usr/bin/gnome-shell 49MiB | | 0 4409 G /usr/lib/xorg/Xorg 223MiB | | 0 4545 G /usr/bin/gnome-shell 140MiB | | 0 4962 G ...uest-channel-token=10798087356903621100 27MiB | | 0 9570 G /proc/self/exe 50MiB | | 0 17619 G ...uest-channel-token=14399957398263092148 40MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Cloning the Repository top # Clone the Apollo using the following command: git clone https://github.com/ApolloAuto/apollo Checkout the r6.0.0 branch: cd apollo git checkout r6.0.0 Do not checkout the v6.0.0 tag as it does not include the latest changes. Building Apollo and the bridge top # Now everything should be in place to build Apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE The Apollo build may fail on machines with less than 1GB of RAM per CPU core due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, you can try re-start it until it succeeds, but if it continues to fail (especially when running the linker) then you'll need to address the low memory situation by either adding more memory to your build machine or enabling or increasing available swap space . If your Apollo build is crashing on a 16GB machine with little or no swap, try setting it to 16GB. Adding a Vehicle to Apollo top # In order to run Apollo with the simulator, the calibration files for the vehicle you use in the simulator must be available in Apollo for perception module to work correctly. Currently, only calibration files for Lincoln 2017 MKZ are included in the latest Apollo. More calibration files may be added later. To add a new vehicle to Apollo: Create a new folder with the new vehicle's name under APOLLO_ROOT/modules/calibration/data . Copy all calibration files of Lincoln 2017 MKZ to the folder. Update vehicle parameters in vehicle_param.pb.txt if your new vehicle is different from Lincoln 2017 MKZ. Update LiDAR calibration files in velodyne_params folder. If you have different configuration for LiDAR with respect to novatel, you need to update LiDAR's height and extrinsics. Update camera calibration files in camera_params folder. You need to update camera intrinsics and extrinsics for your own camera and sensor configurations. You may also need to update other sensor calibration files if you are using them on your vehicle. After you add the new vehicle, you need to restart Dreamview to refresh the vehicle list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh Adding an HD Map to Apollo top # An HD map in Apollo format corresponding to the environment you use in the simulator must be available in Apollo for modules like routing , planning , etc. to work. Currently, only HD map files for Borregas Ave are included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map file following Export Map Annotations , then generate routing_map and sim_map following these instructions . In our Apollo 5.0 fork, we have a script generate_map.sh ( link ) to help users generate map files. You can copy that script to APOLLO_ROOT/scripts/ folder to use it. For details, please follow here . After you add the new map, you need to restart Dreamview to refresh the map list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh Alternatively, you can analyze top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 . Launching Apollo alongside the Simulator top # Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch Apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about Dreamview not being built if you do not run the script from the /apollo directory. ./scripts/bootstrap_lgsvl.sh Launch bridge (inside docker container): ./scripts/bridge.sh Run the SVL Simulator outside of docker. See instructions for Running the Simulator Create a Simulation the BorregasAve map and Lincoln2017MKZ (Apollo 5.0) vehicle Enter localhost:9090 as the Bridge Connection String (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Play\" Open Apollo Dreamview in a browser by navigating to: localhost:8888 NEW for Apollo Master : Select the Mkz Lgsvl setup mode, from the menu to the left of the vehicle menu. Select the Lincoln2017MKZ LGSVL vehicle and Borregas Ave map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane-line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090 Adding a Vehicle top # Only calibration files for Lincoln 2017 MKZ is included in the latest Apollo at this time. More calibration files map be added later. Adding an HD Map top # Only HD map files for Borregas Ave is included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map from our map annotation tool , name it base_map.bin and then generate routing_map and sim_map following these instructions .","title":"Running Apollo 6.0 with SVL Simulator [](#top)"},{"location":"system-under-test/apollo6-0-instructions/#getting-started","text":"This guide outlines the steps required to setup Apollo for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator .","title":"Getting Started"},{"location":"system-under-test/apollo6-0-instructions/#prerequisites","text":"Ubuntu 18.04 or later NVIDIA graphics card (required for perception and prediction) NVIDIA proprietary driver (>=410.48) must be installed","title":"Prerequisites"},{"location":"system-under-test/apollo6-0-instructions/#setup","text":"","title":"Setup"},{"location":"system-under-test/apollo6-0-instructions/#docker","text":"Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made.","title":"Docker"},{"location":"system-under-test/apollo6-0-instructions/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"system-under-test/apollo6-0-instructions/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.64 Driver Version: 440.64 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:01:00.0 On | N/A | | 27% 29C P8 7W / 180W | 579MiB / 8117MiB | 1% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1745 G /usr/lib/xorg/Xorg 40MiB | | 0 1862 G /usr/bin/gnome-shell 49MiB | | 0 4409 G /usr/lib/xorg/Xorg 223MiB | | 0 4545 G /usr/bin/gnome-shell 140MiB | | 0 4962 G ...uest-channel-token=10798087356903621100 27MiB | | 0 9570 G /proc/self/exe 50MiB | | 0 17619 G ...uest-channel-token=14399957398263092148 40MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing NVIDIA Docker"},{"location":"system-under-test/apollo6-0-instructions/#cloning-the-repository","text":"Clone the Apollo using the following command: git clone https://github.com/ApolloAuto/apollo Checkout the r6.0.0 branch: cd apollo git checkout r6.0.0 Do not checkout the v6.0.0 tag as it does not include the latest changes.","title":"Cloning the Repository"},{"location":"system-under-test/apollo6-0-instructions/#building-apollo-and-bridge","text":"Now everything should be in place to build Apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE The Apollo build may fail on machines with less than 1GB of RAM per CPU core due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, you can try re-start it until it succeeds, but if it continues to fail (especially when running the linker) then you'll need to address the low memory situation by either adding more memory to your build machine or enabling or increasing available swap space . If your Apollo build is crashing on a 16GB machine with little or no swap, try setting it to 16GB.","title":"Building Apollo and the bridge"},{"location":"system-under-test/apollo6-0-instructions/#adding-a-vehicle","text":"In order to run Apollo with the simulator, the calibration files for the vehicle you use in the simulator must be available in Apollo for perception module to work correctly. Currently, only calibration files for Lincoln 2017 MKZ are included in the latest Apollo. More calibration files may be added later. To add a new vehicle to Apollo: Create a new folder with the new vehicle's name under APOLLO_ROOT/modules/calibration/data . Copy all calibration files of Lincoln 2017 MKZ to the folder. Update vehicle parameters in vehicle_param.pb.txt if your new vehicle is different from Lincoln 2017 MKZ. Update LiDAR calibration files in velodyne_params folder. If you have different configuration for LiDAR with respect to novatel, you need to update LiDAR's height and extrinsics. Update camera calibration files in camera_params folder. You need to update camera intrinsics and extrinsics for your own camera and sensor configurations. You may also need to update other sensor calibration files if you are using them on your vehicle. After you add the new vehicle, you need to restart Dreamview to refresh the vehicle list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh","title":"Adding a Vehicle"},{"location":"system-under-test/apollo6-0-instructions/#adding-an-hd-map","text":"An HD map in Apollo format corresponding to the environment you use in the simulator must be available in Apollo for modules like routing , planning , etc. to work. Currently, only HD map files for Borregas Ave are included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map file following Export Map Annotations , then generate routing_map and sim_map following these instructions . In our Apollo 5.0 fork, we have a script generate_map.sh ( link ) to help users generate map files. You can copy that script to APOLLO_ROOT/scripts/ folder to use it. For details, please follow here . After you add the new map, you need to restart Dreamview to refresh the map list. Inside Apollo docker: bootstrap_lgsvl.sh stop && bootstrap_lgsvl.sh Alternatively, you can analyze top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 .","title":"Adding an HD Map"},{"location":"system-under-test/apollo6-0-instructions/#launching-apollo-alongide-the-simulator","text":"Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch Apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about Dreamview not being built if you do not run the script from the /apollo directory. ./scripts/bootstrap_lgsvl.sh Launch bridge (inside docker container): ./scripts/bridge.sh Run the SVL Simulator outside of docker. See instructions for Running the Simulator Create a Simulation the BorregasAve map and Lincoln2017MKZ (Apollo 5.0) vehicle Enter localhost:9090 as the Bridge Connection String (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Play\" Open Apollo Dreamview in a browser by navigating to: localhost:8888 NEW for Apollo Master : Select the Mkz Lgsvl setup mode, from the menu to the left of the vehicle menu. Select the Lincoln2017MKZ LGSVL vehicle and Borregas Ave map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane-line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090","title":"Launching Apollo alongisde the Simulator"},{"location":"system-under-test/apollo6-0-instructions/#adding-a-vehicle","text":"Only calibration files for Lincoln 2017 MKZ is included in the latest Apollo at this time. More calibration files map be added later.","title":"Adding a Vehicle"},{"location":"system-under-test/apollo6-0-instructions/#adding-an-hd-map","text":"Only HD map files for Borregas Ave is included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map from our map annotation tool , name it base_map.bin and then generate routing_map and sim_map following these instructions .","title":"Adding an HD Map"},{"location":"system-under-test/apollo6-0-json-example/","text":"Example JSON Configuration for an Apollo 6.0 Vehicle Bridge Type top # CyberRT Published Topics top # Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/ins_stat GPS INS Status /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/perception/obstacles 3D Ground Truth Sensor /apollo/perception/traffic_light Signal Sensor /apollo/sensor/conti_radar Radar (optional) /apollo/sensor/lidar128/compensator/PointCloud2 Lidar (optional) /apollo/sensor/camera/front_6mm/image/compressed Main Camera (optional) /apollo/sensor/camera/front_12mm/image/compressed Telephoto Camera (optional) Note: The Radar , LiDAR , and Cameras are not needed for modular testing since the perception module in Apollo will not be running. We suggest against including them unless their data is needed for other reasons. Subscribed Topics top # Topic Sensor Name /apollo/control Apollo Car Control Complete JSON Configuration excluding optional sensors top # [ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/traffic_light\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ] Complete JSON Configuration including optional sensors top # [ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/traffic_light\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"RadarSensor\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.337, \"z\": 3.691, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/lidar128/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_12mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Example JSON Configuration for an Apollo 6.0 Vehicle [](#top)"},{"location":"system-under-test/apollo6-0-json-example/#bridge-type","text":"CyberRT","title":"Bridge Type"},{"location":"system-under-test/apollo6-0-json-example/#published-topics","text":"Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/ins_stat GPS INS Status /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/perception/obstacles 3D Ground Truth Sensor /apollo/perception/traffic_light Signal Sensor /apollo/sensor/conti_radar Radar (optional) /apollo/sensor/lidar128/compensator/PointCloud2 Lidar (optional) /apollo/sensor/camera/front_6mm/image/compressed Main Camera (optional) /apollo/sensor/camera/front_12mm/image/compressed Telephoto Camera (optional) Note: The Radar , LiDAR , and Cameras are not needed for modular testing since the perception module in Apollo will not be running. We suggest against including them unless their data is needed for other reasons.","title":"Published Topics"},{"location":"system-under-test/apollo6-0-json-example/#subscribed-topics","text":"Topic Sensor Name /apollo/control Apollo Car Control","title":"Subscribed Topics"},{"location":"system-under-test/apollo6-0-json-example/#complete-json-configuration","text":"[ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/traffic_light\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Complete JSON Configuration excluding optional sensors"},{"location":"system-under-test/apollo6-0-json-example/#complete-json-configuration","text":"[ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsInsSensor\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0.070351, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/traffic_light\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"RadarSensor\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.337, \"z\": 3.691, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/lidar128/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/apollo/sensor/camera/front_12mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"WheelControlSensor\", \"name\": \"Wheel Car Control\" }, { \"type\": \"ApolloControlSensor\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Complete JSON Configuration excluding optional sensors"},{"location":"system-under-test/autoware-auto-instructions/","text":"Autoware.Auto with SVL Simulator Table of Contents Overview Setup Requirements Install Docker CE Install NVIDIA Container Toolkit Simulator Installation Install Autoware.auto Install ADE Download Autoware.Auto top Install ROS2 LGSVL Bridge 1. Using the Package Manager 2. Building source code. Install ROS2 LGSVL Messages Run Simulator alongside Autoware.Auto Start the Autoware.Auto containers without NVIDIA setup: Start the Autoware.Auto containers with NVIDIA setup: Overview top # This guide describes setting up and using Autoware.Auto with the SVL Simulator. As Autoware.Auto is still under-development, full self-driving is not yet possible. This guide will focus on running individual modules which have been implemented. Setup top # Requirements top # Linux operating system NVIDIA graphics card Install Docker CE top # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps to run docker as a non-root user. Install NVIDIA Container Toolkit top # Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed, enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.59 Driver Version: 440.59 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available from the official documentation . Simulator installation top # Download and extract the latest simulator release under the ~/adehome folder. (Optional) Download the latest PythonAPI release (make sure the release version matches the simulator) and install it using pip: cd PythonAPI pip3 install --user . Install Autoware.auto top # Install ADE top # # https://ade-cli.readthedocs.io/en/latest/install.html cd ~/.local/bin wget https://gitlab.com/ApexAI/ade-cli/uploads/f6c47dc34cffbe90ca197e00098bdd3f/ade+x86_64 $ mv ade+x86_64 ade $ chmod +x ade $ ./ade --version 4.0.0 $ ./ade update-cli $ ./ade --version <latest-version> PATH=$PATH:~/.local/bin mkdir -p ~/adehome cd ~/adehome touch .adehome Download Autoware.Auto top # Download Autoware.Auto under the ~/adehome folder. cd ~/adehome git clone https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto.git Installation and Development: Setup guide guide for Autoware.auto. Install ROS2 LGSVL Bridge top # There are two ways of installing ros2-lgsvl-bridge: Using the Package Manager Building source code 1. Using the Package Manager # # In the ade container sudo apt update sudo apt install ros-foxy-lgsvl-bridge 2. Building source code. # Downloading cd ~/adehome/AutowareAuto/src/external git clone https://github.com/lgsvl/ros2-lgsvl-bridge.git cd ros2-lgsvl-bridge git pull origin foxy-devel git checkout foxy-devel Building Refer to README.md in the repo. # In the ade container cd ~/AutowareAuto colcon build --packages-select lgsvl_bridge --cmake-args '-DCMAKE_BUILD_TYPE=Release' Running Refer to README.md in the repo. source ~/AutowareAuto/src/external/ros2-lgsvl-bridge/install/setup.bash lgsvl_bridge Install ROS2 LGSVL Messages top # Downloading mkdir -p ~/adehome/AutowareAuto/src/external/ cd ~/adehome/AutowareAuto/src/external/ git clone https://github.com/lgsvl/lgsvl_msgs.git Building # In the ade container cd ~/AutowareAuto colcon build --cmake-args '-DCMAKE_BUILD_TYPE=Release' # You may want to build only lgsvl_msgs package with the following command. colcon build --packages-select lgsvl_msgs --cmake-args '-DCMAKE_BUILD_TYPE=Release' Testing cd ~/AutowareAuto source install/setup.bash ros2 msg list |grep lgsvl_msgs # If you can see the list of lgsvl_msgs, they're ready to be used. Run Simulator alongside Autoware.Auto top # The ROS2 web bridge allows the simulator and Autoware.auto to communicate. To test this connection we can visualize sensor data from the simulator in rviz2 (running in the Autoware.auto container). Start the Autoware.Auto containers without NVIDIA setup: # cd ~/adehome/AutowareAuto source .aderc-amd64-foxy-lgsvl ade start Start the Autoware.Auto containers with NVIDIA setup: # Create a aderc file which has nvidia setup: vim ~/adehome/AutowareAuto/.aderc-amd64-foxy-lgsvl-nvidia .aderc-amd64-foxy-lgsvl-nvidia: export ADE_DOCKER_RUN_ARGS=\"--cap-add=SYS_PTRACE --net=host --privileged --add-host ade:127.0.0.1 -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --runtime=nvidia -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e NVIDIA_VISIBLE_DEVICES=all \\ -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,display\" export ADE_GITLAB=gitlab.com export ADE_REGISTRY=registry.gitlab.com export ADE_DISABLE_NVIDIA_DOCKER=false export ADE_IMAGES=\" registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/ade-foxy:master registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/binary-foxy:master registry.gitlab.com/autowarefoundation/autoware.auto/ade-lgsvl/foxy:2020.06 nvidia/cuda:11.0-base \" Start ADE container: cd ~/adehome/AutowareAuto source .aderc-amd64-foxy-lgsvl-nvidia ade start Enter the container and start rviz2: ade enter cd ~/AutowareAuto colcon build source ~/AutowareAuto/install/setup.bash rviz2 -d /home/\"${USER}\"/AutowareAuto/install/autoware_auto_examples/share/autoware_auto_examples/rviz2/autoware.rviz Start the SVL Simulator outside ADE container by launching the executable and click on the OPEN BROWSER button to open the web UI. $ (path\\to\\downloaded\\simulator)/svlsimulator-linux64-2021.1/simulator In the Vehicles tab under Library look for Lexus2016RXHybrid . If not available, see the Library page to add it. Make sure that Autoware.Auto sensor configuration has the ROS2 bridge and all of sensors are added. Click Vehicles under Library in the left side and click Lexus2016RXHybrid and click Autoware.Auto in Sensor Configurations. If you can see i mark next to sensor name, click Add to Library button to add sensor plugins into library. Switch to the Simulations tab and click the Add new button: Enter a Simulation Name and click Next. Select Random Traffic in Runtime Template. Select a map from the drop down menu. If none are available follow this guide to get a map. Select the Lexus2016RXHybrid from the drop down menu of Vehicle. Select the Autoware.Auto in Sensor Configuration and click Next. Select Autoware.Auto (Apex.AI) in Autopilot and enter the bridge address (default: localhost:9090 ) in Bridge IP box and click Next. Click Publish. Press Run Simulation button. Launch ROS2 LGSVL bridge in a new terminal: # In the ADE container $ lgsvl_bridge NOTE ROS2 LGSVL Bridge needs to be running. You should now be able to see the LiDAR point cloud in rviz (see image below). If the pointcloud is not visible make sure the Fixed Frame (under Global Options) is set to lidar_front and that a PointCloud2 message is added which listens on the /lidar_front/points_raw topic.","title":"Autoware.Auto"},{"location":"system-under-test/autoware-auto-instructions/#general","text":"This guide describes setting up and using Autoware.Auto with the SVL Simulator. As Autoware.Auto is still under-development, full self-driving is not yet possible. This guide will focus on running individual modules which have been implemented.","title":"Overview"},{"location":"system-under-test/autoware-auto-instructions/#setup","text":"","title":"Setup"},{"location":"system-under-test/autoware-auto-instructions/#requirements","text":"Linux operating system NVIDIA graphics card","title":"Requirements"},{"location":"system-under-test/autoware-auto-instructions/#install-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps to run docker as a non-root user.","title":"Install Docker CE"},{"location":"system-under-test/autoware-auto-instructions/#install-nvidia-container-toolkit","text":"Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed, enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.59 Driver Version: 440.59 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available from the official documentation .","title":"Install NVIDIA Container Toolkit"},{"location":"system-under-test/autoware-auto-instructions/#simulator-installation","text":"Download and extract the latest simulator release under the ~/adehome folder. (Optional) Download the latest PythonAPI release (make sure the release version matches the simulator) and install it using pip: cd PythonAPI pip3 install --user .","title":"Simulator Installation"},{"location":"system-under-test/autoware-auto-instructions/#install-autoware-auto","text":"","title":"Install Autoware.auto"},{"location":"system-under-test/autoware-auto-instructions/#Install-ADE","text":"# https://ade-cli.readthedocs.io/en/latest/install.html cd ~/.local/bin wget https://gitlab.com/ApexAI/ade-cli/uploads/f6c47dc34cffbe90ca197e00098bdd3f/ade+x86_64 $ mv ade+x86_64 ade $ chmod +x ade $ ./ade --version 4.0.0 $ ./ade update-cli $ ./ade --version <latest-version> PATH=$PATH:~/.local/bin mkdir -p ~/adehome cd ~/adehome touch .adehome","title":"Install ADE"},{"location":"system-under-test/autoware-auto-instructions/#download-autowareauto-top","text":"Download Autoware.Auto under the ~/adehome folder. cd ~/adehome git clone https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto.git Installation and Development: Setup guide guide for Autoware.auto.","title":"Download Autoware.Auto top"},{"location":"system-under-test/autoware-auto-instructions/#install-ros2-lgsvl-bridge","text":"There are two ways of installing ros2-lgsvl-bridge: Using the Package Manager Building source code","title":"Install ROS2 LGSVL Bridge"},{"location":"system-under-test/autoware-auto-instructions/#1-using-the-package-manager","text":"# In the ade container sudo apt update sudo apt install ros-foxy-lgsvl-bridge","title":"1. Using the Package Manager"},{"location":"system-under-test/autoware-auto-instructions/#2-building-source-code","text":"","title":"2. Building source code."},{"location":"system-under-test/autoware-auto-instructions/#install-ros2-lgsvl-messages","text":"","title":"Install ROS2 LGSVL Messages"},{"location":"system-under-test/autoware-auto-instructions/#run-simulator-alongside-autoware-auto","text":"The ROS2 web bridge allows the simulator and Autoware.auto to communicate. To test this connection we can visualize sensor data from the simulator in rviz2 (running in the Autoware.auto container).","title":"Run Simulator alongside Autoware.Auto"},{"location":"system-under-test/autoware-auto-instructions/#start-the-autowareauto-containers-without-nvidia-setup","text":"cd ~/adehome/AutowareAuto source .aderc-amd64-foxy-lgsvl ade start","title":"Start the Autoware.Auto containers without NVIDIA setup:"},{"location":"system-under-test/autoware-auto-instructions/#start-the-autowareauto-containers-with-nvidia-setup","text":"Create a aderc file which has nvidia setup: vim ~/adehome/AutowareAuto/.aderc-amd64-foxy-lgsvl-nvidia .aderc-amd64-foxy-lgsvl-nvidia: export ADE_DOCKER_RUN_ARGS=\"--cap-add=SYS_PTRACE --net=host --privileged --add-host ade:127.0.0.1 -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --runtime=nvidia -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e NVIDIA_VISIBLE_DEVICES=all \\ -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,display\" export ADE_GITLAB=gitlab.com export ADE_REGISTRY=registry.gitlab.com export ADE_DISABLE_NVIDIA_DOCKER=false export ADE_IMAGES=\" registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/ade-foxy:master registry.gitlab.com/autowarefoundation/autoware.auto/autowareauto/amd64/binary-foxy:master registry.gitlab.com/autowarefoundation/autoware.auto/ade-lgsvl/foxy:2020.06 nvidia/cuda:11.0-base \" Start ADE container: cd ~/adehome/AutowareAuto source .aderc-amd64-foxy-lgsvl-nvidia ade start Enter the container and start rviz2: ade enter cd ~/AutowareAuto colcon build source ~/AutowareAuto/install/setup.bash rviz2 -d /home/\"${USER}\"/AutowareAuto/install/autoware_auto_examples/share/autoware_auto_examples/rviz2/autoware.rviz Start the SVL Simulator outside ADE container by launching the executable and click on the OPEN BROWSER button to open the web UI. $ (path\\to\\downloaded\\simulator)/svlsimulator-linux64-2021.1/simulator In the Vehicles tab under Library look for Lexus2016RXHybrid . If not available, see the Library page to add it. Make sure that Autoware.Auto sensor configuration has the ROS2 bridge and all of sensors are added. Click Vehicles under Library in the left side and click Lexus2016RXHybrid and click Autoware.Auto in Sensor Configurations. If you can see i mark next to sensor name, click Add to Library button to add sensor plugins into library. Switch to the Simulations tab and click the Add new button: Enter a Simulation Name and click Next. Select Random Traffic in Runtime Template. Select a map from the drop down menu. If none are available follow this guide to get a map. Select the Lexus2016RXHybrid from the drop down menu of Vehicle. Select the Autoware.Auto in Sensor Configuration and click Next. Select Autoware.Auto (Apex.AI) in Autopilot and enter the bridge address (default: localhost:9090 ) in Bridge IP box and click Next. Click Publish. Press Run Simulation button. Launch ROS2 LGSVL bridge in a new terminal: # In the ADE container $ lgsvl_bridge NOTE ROS2 LGSVL Bridge needs to be running. You should now be able to see the LiDAR point cloud in rviz (see image below). If the pointcloud is not visible make sure the Fixed Frame (under Global Options) is set to lidar_front and that a PointCloud2 message is added which listens on the /lidar_front/points_raw topic.","title":"Start the Autoware.Auto containers with NVIDIA setup:"},{"location":"system-under-test/autoware-auto-json-example/","text":"Example JSON Configuration for an Autoware Auto Vehicle Bridge Type top # ROS2 Published Topics top # Topic Sensor Name /lgsvl/state_report CAN Bus /gnss/fix GPS /lgsvl/gnss_odom GPS Odometry /imu/imu_raw IMU /lidar_front/points_raw LidarFront /lidar_rear/points_raw LidarRear /lgsvl/clock Simulation Clock Subscribed Topics top # Topic Sensor Name /lgsvl/vehicle_control_cmd Autoware Car Control /lgsvl/vehicle_state_cmd Autoware Auto Vehicle State Complete JSON Configuration top # [ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/lgsvl/state_report\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gnss/fix\", \"Frame\": \"gnss\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 30.0, \"Topic\": \"/lgsvl/gnss_odom\", \"Frame\": \"odom\", \"ChildFrame\": \"base_link\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"LidarFront\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/lidar_front/points_raw\", \"Frame\": \"lidar_front\" }, \"transform\": { \"x\": 0.022, \"y\": 1.49, \"z\": 1.498, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"LidarRear\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/lidar_rear/points_raw\", \"Frame\": \"lidar_rear\" }, \"transform\": { \"x\": 0.022, \"y\": 1.49, \"z\": 0.308, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LGSVLControlSensor\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/lgsvl/vehicle_control_cmd\" } }, { \"type\": \"VehicleStateSensor\", \"name\": \"Autoware Auto Vehicle State\", \"params\": { \"Topic\": \"/lgsvl/vehicle_state_cmd\" } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"ClockSensor\", \"name\": \"Simulation Clock\", \"params\": { \"Topic\": \"/lgsvl/clock\" } } ]","title":"Example JSON Configuration for an Autoware Auto Vehicle [](#top)"},{"location":"system-under-test/autoware-auto-json-example/#bridge-type","text":"ROS2","title":"Bridge Type"},{"location":"system-under-test/autoware-auto-json-example/#published-topics","text":"Topic Sensor Name /lgsvl/state_report CAN Bus /gnss/fix GPS /lgsvl/gnss_odom GPS Odometry /imu/imu_raw IMU /lidar_front/points_raw LidarFront /lidar_rear/points_raw LidarRear /lgsvl/clock Simulation Clock","title":"Published Topics"},{"location":"system-under-test/autoware-auto-json-example/#subscribed-topics","text":"Topic Sensor Name /lgsvl/vehicle_control_cmd Autoware Car Control /lgsvl/vehicle_state_cmd Autoware Auto Vehicle State","title":"Subscribed Topics"},{"location":"system-under-test/autoware-auto-json-example/#complete-json-configuration","text":"[ { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/lgsvl/state_report\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gnss/fix\", \"Frame\": \"gnss\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 30.0, \"Topic\": \"/lgsvl/gnss_odom\", \"Frame\": \"odom\", \"ChildFrame\": \"base_link\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"LidarFront\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/lidar_front/points_raw\", \"Frame\": \"lidar_front\" }, \"transform\": { \"x\": 0.022, \"y\": 1.49, \"z\": 1.498, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"LidarRear\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/lidar_rear/points_raw\", \"Frame\": \"lidar_rear\" }, \"transform\": { \"x\": 0.022, \"y\": 1.49, \"z\": 0.308, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LGSVLControlSensor\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/lgsvl/vehicle_control_cmd\" } }, { \"type\": \"VehicleStateSensor\", \"name\": \"Autoware Auto Vehicle State\", \"params\": { \"Topic\": \"/lgsvl/vehicle_state_cmd\" } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"ClockSensor\", \"name\": \"Simulation Clock\", \"params\": { \"Topic\": \"/lgsvl/clock\" } } ]","title":"Complete JSON Configuration"},{"location":"system-under-test/autoware-instructions/","text":"Autoware.AI 1.14.0 with SVL Simulator The software and source code in this repository are intended only for use with the SVL Simulator and should not be used in a real vehicle. Table of Contents General Setup Requirements Install Docker CE Install NVIDIA Container Toolkit Install SVL Simulator Install Autoware Launch Autoware Alongside SVL Simulator Driving by following vector map: Adding a Vehicle Adding an HD Map General top # This guide goes through how to run Autoware.AI with the SVL Simulator. In order to run Autoware with the SVL Simulator, it is easiest to pull the official Autoware Docker image (see the official guide, Case 1 for more details), but it is also possible to build Autoware from source . Autoware communicates with the SVL Simulator using the rosbridge_suite, which provides JSON interfacing with ROS publishers/subscribers. The official Autoware Docker containers have rosbridge_suite included. Setup top # Requirements top # Linux operating system NVIDIA graphics card Install Docker CE top # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps to run docker as a non-root user. Install NVIDIA Container Toolkit top # Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed, enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.59 Driver Version: 440.59 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available from the official documentation . Install SVL Simulator top # This guide outlines the steps required to setup Autoware.AI for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator . Install Autoware top # Make sure you have Git Large File Storage (LFS) installed before cloning the repository in the next step. If git lfs outputs git: 'lfs' is not a git command. , then you need to install it: Instructions for installation are here . Verify the installation: $ git lfs install Git LFS initialized. Create a directory called shared_dir in your home directory to hold HD maps and launch files for the simulator. The Autoware Docker container will mount this folder. mkdir ~/shared_dir cd ~/shared_dir git clone https://github.com/lgsvl/autoware-data.git If there wasn't a line beginning with Filtering content: output, then Git LFS hasn't been installed. Remove the autoware-data directory, install Git LFS with git lfs install , and then re-issue the git clone . Clone the docker repository from autoware.ai into a working directory: cd $WORKING_DIRECTORY git clone https://github.com/Autoware-AI/docker.git Launch Autoware Alongside SVL Simulator top # Run the Autoware 1.14.0 container and enter into it: cd $WORKING_DIRECTORY/docker/generic ./run.sh -t 1.14.0 If you get the usermod error as follows: usermod: user autoware is currently used by process 1 Check if the $UID is 1000 $ echo $UID If your $UID is 1000, you would not have usermod error. Otherwise, it's better to build container locally to avoid usermod error as follows: $ ./build.sh --version 1.14.0 $ ./run.sh -t local Once inside the container, install a missing ROS package: sudo apt update && sudo apt install ros-$ROS_DISTRO-image-transport-plugins -y If you need to check which $ROS_DISTRO you have installed run the following: ls /opt/ros/ Launch the runtime manager: roslaunch runtime_manager runtime_manager.launch A few terminals will open, as well as a GUI for the runtime manager. In the runtime manager, click on the 'Quick Start' tab and load the following launch files from ~/shared_dir/autoware-data/BorregasAve/ by clicking \"Ref\" to the right of each text box: my_map.launch my_sensing_simulator.launch my_localization.launch my_detection.launch my_mission_planning.launch my_motion_planning.launch Click \"Map\" to load the launch file pertaining to the HD maps. An \"Ok\" should appear to the right of the \"Ref\" button when successfully loaded. Then click \"Sensing\" which also launches rosbridge. Run the SVL Simulator Create a Simulation choosing BorregasAve map and Jaguar2015XE (Autoware) or another Autoware compatible vehicle. Select Autoware.AI under Autopilot selection Enter localhost:9090 for the Bridge Connection String. Run the created Simulation A vehicle should appear in Borregas Ave in Sunnyvale, CA. In the Autoware Runtime Manager, continue loading the other launch files - click \"Localization\" and wait for the time to display to the right of \"Ref\". Then click \"Rviz\" to launch Rviz - the vector map and location of the vehicle in the map should show. The vehicle may be mis-localized as the initial pose is important for NDT matching. To fix this, click \"2D Pose Estimate\" in Rviz, then click an approximate position for the vehicle on the map and drag in the direction it is facing before releasing the mouse button. This should allow NDT matching to find the vehicle pose (it may take a few tries). Note that the point cloud will not show up in rviz until ndt matching starts publishing a pose. An alternative would be to use GNSS for an initial pose or for localization. Open my_localization.launch file in map folder of autoware-data repository and uncomment nmea2tfpose part and comment ndt_matching instead. Driving by following vector map: # To drive following the HD map follow these steps: Start Mission Planning launch file in Autoware Runtime Manager. In rviz, mark a destination by clicking '2D Nav Goal' and clicking at the destination and dragging along the road direction. Make sure to only choose a route that looks valid along the lane centerlines that are marked with orange lines in rviz. If an invalid destination is selected nothing will change in rviz, and you will need to relaunch the Mission Planning launch file in the Quick Launch tab to try another destination. After choosing a valid destination the route will be highlighted in blue in rviz. Lastly, start Motion Planning launch file. then ego vehicle starts driving autonomously. Adding a Vehicle top # The default vehicles have the calibration files included in the LGSVL Autoware Data Github repository. Adding an HD Map top # The default maps have the Vector map files included in the LGSVL Autoware Data Github repository.","title":"ROS (Autoware.AI)"},{"location":"system-under-test/autoware-instructions/#general","text":"This guide goes through how to run Autoware.AI with the SVL Simulator. In order to run Autoware with the SVL Simulator, it is easiest to pull the official Autoware Docker image (see the official guide, Case 1 for more details), but it is also possible to build Autoware from source . Autoware communicates with the SVL Simulator using the rosbridge_suite, which provides JSON interfacing with ROS publishers/subscribers. The official Autoware Docker containers have rosbridge_suite included.","title":"General"},{"location":"system-under-test/autoware-instructions/#setup","text":"","title":"Setup"},{"location":"system-under-test/autoware-instructions/#requirements","text":"Linux operating system NVIDIA graphics card","title":"Requirements"},{"location":"system-under-test/autoware-instructions/#install-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps to run docker as a non-root user.","title":"Install Docker CE"},{"location":"system-under-test/autoware-instructions/#install-nvidia-container-toolkit","text":"Before installing the NVIDIA Container Toolkit, make sure that you have the appropriate NVIDIA drivers installed. To test if the NVIDIA drivers are properly installed, enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.59 Driver Version: 440.59 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for the NVIDIA Container Toolkit are available from the official documentation .","title":"Install NVIDIA Container Toolkit"},{"location":"system-under-test/autoware-instructions/#install-lgsvl-simulator","text":"This guide outlines the steps required to setup Autoware.AI for use with the SVL Simulator. If you have not already set up the simulator, please do so first by following the instructions in Installing the SVL Simulator .","title":"Install SVL Simulator"},{"location":"system-under-test/autoware-instructions/#install-autoware","text":"Make sure you have Git Large File Storage (LFS) installed before cloning the repository in the next step. If git lfs outputs git: 'lfs' is not a git command. , then you need to install it: Instructions for installation are here . Verify the installation: $ git lfs install Git LFS initialized. Create a directory called shared_dir in your home directory to hold HD maps and launch files for the simulator. The Autoware Docker container will mount this folder. mkdir ~/shared_dir cd ~/shared_dir git clone https://github.com/lgsvl/autoware-data.git If there wasn't a line beginning with Filtering content: output, then Git LFS hasn't been installed. Remove the autoware-data directory, install Git LFS with git lfs install , and then re-issue the git clone . Clone the docker repository from autoware.ai into a working directory: cd $WORKING_DIRECTORY git clone https://github.com/Autoware-AI/docker.git","title":"Install Autoware"},{"location":"system-under-test/autoware-instructions/#launch-autoware-alongside-lgsvl-simulator","text":"Run the Autoware 1.14.0 container and enter into it: cd $WORKING_DIRECTORY/docker/generic ./run.sh -t 1.14.0 If you get the usermod error as follows: usermod: user autoware is currently used by process 1 Check if the $UID is 1000 $ echo $UID If your $UID is 1000, you would not have usermod error. Otherwise, it's better to build container locally to avoid usermod error as follows: $ ./build.sh --version 1.14.0 $ ./run.sh -t local Once inside the container, install a missing ROS package: sudo apt update && sudo apt install ros-$ROS_DISTRO-image-transport-plugins -y If you need to check which $ROS_DISTRO you have installed run the following: ls /opt/ros/ Launch the runtime manager: roslaunch runtime_manager runtime_manager.launch A few terminals will open, as well as a GUI for the runtime manager. In the runtime manager, click on the 'Quick Start' tab and load the following launch files from ~/shared_dir/autoware-data/BorregasAve/ by clicking \"Ref\" to the right of each text box: my_map.launch my_sensing_simulator.launch my_localization.launch my_detection.launch my_mission_planning.launch my_motion_planning.launch Click \"Map\" to load the launch file pertaining to the HD maps. An \"Ok\" should appear to the right of the \"Ref\" button when successfully loaded. Then click \"Sensing\" which also launches rosbridge. Run the SVL Simulator Create a Simulation choosing BorregasAve map and Jaguar2015XE (Autoware) or another Autoware compatible vehicle. Select Autoware.AI under Autopilot selection Enter localhost:9090 for the Bridge Connection String. Run the created Simulation A vehicle should appear in Borregas Ave in Sunnyvale, CA. In the Autoware Runtime Manager, continue loading the other launch files - click \"Localization\" and wait for the time to display to the right of \"Ref\". Then click \"Rviz\" to launch Rviz - the vector map and location of the vehicle in the map should show. The vehicle may be mis-localized as the initial pose is important for NDT matching. To fix this, click \"2D Pose Estimate\" in Rviz, then click an approximate position for the vehicle on the map and drag in the direction it is facing before releasing the mouse button. This should allow NDT matching to find the vehicle pose (it may take a few tries). Note that the point cloud will not show up in rviz until ndt matching starts publishing a pose. An alternative would be to use GNSS for an initial pose or for localization. Open my_localization.launch file in map folder of autoware-data repository and uncomment nmea2tfpose part and comment ndt_matching instead.","title":"Launch Autoware Alongside SVL Simulator"},{"location":"system-under-test/autoware-instructions/#driving-by-following-vector-map","text":"To drive following the HD map follow these steps: Start Mission Planning launch file in Autoware Runtime Manager. In rviz, mark a destination by clicking '2D Nav Goal' and clicking at the destination and dragging along the road direction. Make sure to only choose a route that looks valid along the lane centerlines that are marked with orange lines in rviz. If an invalid destination is selected nothing will change in rviz, and you will need to relaunch the Mission Planning launch file in the Quick Launch tab to try another destination. After choosing a valid destination the route will be highlighted in blue in rviz. Lastly, start Motion Planning launch file. then ego vehicle starts driving autonomously.","title":"Driving by following vector map:"},{"location":"system-under-test/autoware-instructions/#adding-a-vehicle","text":"The default vehicles have the calibration files included in the LGSVL Autoware Data Github repository.","title":"Adding a Vehicle"},{"location":"system-under-test/autoware-instructions/#adding-an-hd-map","text":"The default maps have the Vector map files included in the LGSVL Autoware Data Github repository.","title":"Adding an HD Map"},{"location":"system-under-test/autoware-json-example/","text":"Example JSON Configuration for an Autoware Vehicle Bridge Type top # ROS Published Topics top # Topic Sensor Name /nmea_sentence GPS /odom GPS Odometry /imu_raw IMU /points_raw Lidar /simulator/camera_node/image/compressed Main Camera Subscribed Topics top # Topic Sensor Name /vehicle_cmd Autoware Car Control Complete JSON Configuration top # [ { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/odom\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/camera_node/image/compressed\", \"Frame\": \"camera\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"AutowareAiControlSensor\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Example JSON Configuration for an Autoware Vehicle [](#top)"},{"location":"system-under-test/autoware-json-example/#bridge-type","text":"ROS","title":"Bridge Type"},{"location":"system-under-test/autoware-json-example/#published-topics","text":"Topic Sensor Name /nmea_sentence GPS /odom GPS Odometry /imu_raw IMU /points_raw Lidar /simulator/camera_node/image/compressed Main Camera","title":"Published Topics"},{"location":"system-under-test/autoware-json-example/#subscribed-topics","text":"Topic Sensor Name /vehicle_cmd Autoware Car Control","title":"Subscribed Topics"},{"location":"system-under-test/autoware-json-example/#complete-json-configuration","text":"[ { \"type\": \"GpsSensor\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GpsOdometrySensor\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/odom\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ImuSensor\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"LidarSensor\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/camera_node/image/compressed\", \"Frame\": \"camera\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"AutowareAiControlSensor\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Complete JSON Configuration"},{"location":"system-under-test/lgsvl-msgs/","text":"lgsvl_msgs lgsvl_msgs is a ROS / ROS2 hybrid package that provides AD stack agnostic message definitions for interfacing with the SVL Simulator. The package contains the following definitions: - Detection3DArray.msg # A list of 3D detections - Detection3D.msg # 3D detection including id, label, score, and 3D bounding box - BoundingBox3D.msg # A 3D bounding box definition - Detection2DArray.msg # A list of 2D detections - Detection2D.msg # 2D detection including id, label, score, and 2D bounding box - BoundingBox2D.msg # A 2D bounding box definition - SignalArray.msg # A list of traffic light detections - Signal.msg # 3D detection of a traffic light including id, label, score, and 3D bounding box - CanBusData.msg # Can bus data for an ego vehicle published by the simulator - VehicleControlData.msg # Vehicle control commands that the simulator subscribes to - VehicleStateData.msg # Description of the full state of an ego vehicle - Ultrasonic.msg # Minimum detected distance by an ultrasonic sensor - VehicleOdometry.msg # Odometry for an ego vehicle Installation lgsvl_msgs is built by the ROS build farm. Binaries can be installed on Ubuntu as follows: sudo apt update sudo apt install ros-$ROS_DISTRO-lgsvl_msgs Note: It may take up to a week for the builds to become available after a release. Building from source ROS Create a catkin workspace for the package and clone the repository into the source folder: mkdir -p catkin_ws/src cd catkin_ws/src git clone https://github.com/lgsvl/lgsvl_msgs.git Build the package from workspace root: cd .. catkin_make Source in terminal where rosnodes who publish / subscribe to the messages are running: source devel/setup.bash ROS 2 Clone repository and build with colcon: git clone https://github.com/lgsvl/lgsvl_msgs.git cd lgsvl_msgs colcon build Source in terminal where rosnodes who publish / subscribe to the messages are running: source install/setup.bash","title":"The lgsvl_msgs package"},{"location":"system-under-test/ros2-bridge/","text":"SVL Simulator ROS 2 Bridge The SVL Simulator can publish and subscribe to ROS 2 messages by connecting to the ROS2 LGSVL Bridge . This custom, native ROS2 bridge yields higher performance than the previously used ros2-web-bridge . This document describes the installation process. Installing Dependencies top # The ROS2 LGSVL Bridge requires the following packages to be built: - ROS2 (rcutils and rcl) - colcon - boost Installing ROS 2 top # See the official ROS 2 installation guide to install ROS 2. The choice of the distribution will somewhat depend on the Ubuntu version installed. Ubuntu 18.04 supports Dashing Diademata ( dashing ) and Eloquent Elusor ( eloquent ), with dashing being the LTS distro. Ubuntu 20.04 only supports Foxy Fitzroy ( foxy ), which is also an LTS distro. The ROS 2 LGSVL Bridge supports Dashing Diademata and newer distributions. For Ubuntu versions older than 18.04, which would require older ROS 2 distributions, we suggest using ROS 2 docker images . Installing Colcon top # Colcon is a command line tool that facilitates building ROS packages. To install: sudo apt update sudo apt install python3-colcon-common-extensions Installing Boost top # To install boost run: sudo apt install libboost-all-dev Installing the ROS 2 LGSVL Bridge top # There are two ways of installing ros2-lgsvl-bridge: Using the Package Manager Building source code 1. Using the Package Manager top # source /opt/ros/(name\\of\\ros2\\distro)/setup.bash sudo apt update sudo apt install ros-$ROS_DISTRO-lgsvl-bridge 2. Building source code. top # Follow these steps to install the ROS2 LGSVL Bridge: Clone the ros2-lgsvl-bridge repository from github: git clone https://github.com/lgsvl/ros2-lgsvl-bridge.git Source the installed ROS2 distribution in the terminal window: source /opt/ros/(name\\of\\ros2\\distro)/setup.bash Switch to the correct devel branch for the installed ROS 2 distribution: cd ros2-lgsvl-bridge git checkout ${ROS_DISTRO}-devel Build using colcon: colcon build --cmake-args '-DCMAKE_BUILD_TYPE=Release' Running the ROS2 LGSVL Bridge top # If the Bridge is already built, to run ROS2 bridge: source (path\\to\\bridge\\repository)/install/setup.bash lgsvl_bridge","title":"Setting up ROS 2 bridge"},{"location":"system-under-test/ros2-bridge/#installing-dependencies","text":"The ROS2 LGSVL Bridge requires the following packages to be built: - ROS2 (rcutils and rcl) - colcon - boost","title":"Installing Dependencies"},{"location":"system-under-test/ros2-bridge/#installing-ros-2","text":"See the official ROS 2 installation guide to install ROS 2. The choice of the distribution will somewhat depend on the Ubuntu version installed. Ubuntu 18.04 supports Dashing Diademata ( dashing ) and Eloquent Elusor ( eloquent ), with dashing being the LTS distro. Ubuntu 20.04 only supports Foxy Fitzroy ( foxy ), which is also an LTS distro. The ROS 2 LGSVL Bridge supports Dashing Diademata and newer distributions. For Ubuntu versions older than 18.04, which would require older ROS 2 distributions, we suggest using ROS 2 docker images .","title":"Installing ROS 2"},{"location":"system-under-test/ros2-bridge/#installing-colcon","text":"Colcon is a command line tool that facilitates building ROS packages. To install: sudo apt update sudo apt install python3-colcon-common-extensions","title":"Installing Colcon"},{"location":"system-under-test/ros2-bridge/#installing-boost","text":"To install boost run: sudo apt install libboost-all-dev","title":"Installing Boost"},{"location":"system-under-test/ros2-bridge/#installing-the-ros-2-lgsvl-bridge","text":"There are two ways of installing ros2-lgsvl-bridge: Using the Package Manager Building source code","title":"Installing the ROS 2 LGSVL Bridge"},{"location":"system-under-test/ros2-bridge/#1.-using-the-package-manager","text":"source /opt/ros/(name\\of\\ros2\\distro)/setup.bash sudo apt update sudo apt install ros-$ROS_DISTRO-lgsvl-bridge","title":"1. Using the Package Manager"},{"location":"system-under-test/ros2-bridge/#2.-building-source-code.","text":"Follow these steps to install the ROS2 LGSVL Bridge: Clone the ros2-lgsvl-bridge repository from github: git clone https://github.com/lgsvl/ros2-lgsvl-bridge.git Source the installed ROS2 distribution in the terminal window: source /opt/ros/(name\\of\\ros2\\distro)/setup.bash Switch to the correct devel branch for the installed ROS 2 distribution: cd ros2-lgsvl-bridge git checkout ${ROS_DISTRO}-devel Build using colcon: colcon build --cmake-args '-DCMAKE_BUILD_TYPE=Release'","title":"2. Building source code."},{"location":"system-under-test/ros2-bridge/#running-the-ros2-lgsvl-bridge","text":"If the Bridge is already built, to run ROS2 bridge: source (path\\to\\bridge\\repository)/install/setup.bash lgsvl_bridge","title":"Running the ROS2 LGSVL Bridge"},{"location":"system-under-test/simulator-messages/","text":"Simulator Messages This page details the messages available by default in the simulator. Each bridge type (ROS, ROS2, CyberRT) will have its own message types. Table of Contents ROS Message Types ROS 2 Message Types CyberRT Message Types ROS Message Types top # The simulator supports many of the common standard ROS messages. Additionally, the simulator supports custom ROS messages defined for Autoware AI as well as the simulator's template messages for Autonomous Driving which are included in lgsvl_msgs. Below is a list of these messages. Message Type Namespace Associated Sensor Default Topic Name Notes std_msgs/Header Ros - - Standard header used for ROS messages std_msgs/Time Ros - - Usually used for timestamps in std_msgs/Header rosgraph_msg/Clock Ros Clock Sensor /clock Required when /use_sim_time param is set to true std_msgs/ColorRGBA Ros - - - sensor_msgs/CompressedImage Ros Camera Sensor - JPEG encoded images - usually subscribed to by AD stacks for camera images sensor_msgs/Image Ros Camera Sensor - Uncompressed images - usually subscribed to by AD stacks for camera images sensor_msgs/Imu Ros Imu Sensor - Linear acceleration, angular velocity, and (optional) orientation as a quaternion sensor_msgs/Joy Ros - - Reports the state of a joystick / wheel axes and buttons sensor_msgs/LaserScan Ros - - Single beam LiDAR points reported as an array of range measurements sensor_msgs/PointCloud2 Ros LiDAR Sensor - Point cloud data as typically output from a 3D LiDAR's driver sensor_msgs/PointField Ros - - Included in sensor_msgs/PointCloud2 to provide type information on the point sensor_msgs/NavSatStatus Ros GPS Sensor - Status report for GNSS sensor_msgs/NavSatFix Ros GPS Sensor - GNSS position fix reported llh format nav_msgs/Odometry Ros GPS Odometry Sensor - An odometry message containing a pose and a twist message nmea_msgs/Sentence Ros GPS Sensor - GNSS fix represented as an NMEA sentence - used in Autoware AI geometry_msgs/Point Ros - - XYZ coordinates geometry_msgs/Pose Ros - - 3D position and orientation (quaternion) geometry_msgs/PoseWithCovariance Ros - - geometry_msgs/Pose with an additional covariance field geometry_msgs/Quaternion Ros - - - geometry_msgs/Twist Ros - - Linear and angular velocity geometry_msgs/TwistStamped Ros - lgsvl_msgs/DetectedRadarObject - Contains s geometry_msgs/Twist and a std_msgs/Header geometry_msgs/TwistWithCovariance Ros - - Contains a geometry_msgs/Twist with an additional covariance field geometry_msgs/Vector3 Ros - - 3D vector with x, y, z coordinates std_srvs/Empty Ros - - Empty service std_srvs/SetBool Ros - - Service for setting a boolean parameter std_srvs/Trigger Ros - - Service for triggering an event autoware_msgs/steer_cmd Autoware - - Steer command message autoware_msgs/accel_cmd Autoware - - Acceleration command message autoware_msgs/brake_cmd Autoware - - Brake command message autoware_msgs/lamp_cmd Autoware - - Message for controlling lights on vehicle autoware_msgs/ControlCommand Autoware - - Vehicle control command providing linear velocity, linear acceleration, and a steering angle autoware_msgs/VehicleCmd Autoware AutowareAI Control Sensor /vehicle_cmd Complete vehicle control command containing the other 'command' message - simulator subscribes to this autoware_msgs/DetectedObject Autoware - - Attributes of objects detected by perception autoware_msgs/DetectedObjectArray Autoware - - Array of autoware_msgs/DetectedObject lgsvl_msgs/BoundingBox2D Lgsvl - - 2D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/BoundingBox3D Lgsvl - - 3D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/Detection2D Lgsvl - - Ground truth detection data for obstacles in 2D lgsvl_msgs/Detection2DArray Lgsvl Ground Truth 2D Sensor - Array of ground truth values for 2D detections lgsvl_msgs/Detection3D Lgsvl - - Ground truth detection data for obstacles in 3D lgsvl_msgs/Detection3DArray Lgsvl Ground Truth 3D Sensor - Array of ground truth values for 3D detections lgsvl_msgs/Signal Lgsvl - - Ground truth traffic light information lgsvl_msgs/SignalArray Lgsvl Signal Sensor - Array of lgsvl_msgs/Signal for all traffic lights in view of ego lgsvl_msgs/CanBusData Lgsvl CanBus Sensor - Collection of information available through CAN bus and published by simulator lgsvl_msgs/VehicleControlData Lgsvl Vehicle Control Sensor - Vehicle control command simulator can subscribe to lgsvl_msgs/VehicleStateData Lgsvl Vehicle State Sensor - Feedback from vehicle control command published by simulator which can be used for implementing a feedback controller lgsvl_msgs/VehicleOdometry Lgsvl Vehicle Odometry Sensor - Velocity and front and rear wheel angle measurements lgsvl_msgs/DetectedRadarObject Lgsvl - - Representation of an obstacle detected by radar lgsvl_msgs/DetectedRadarObjectArray Lgsvl Radar Sensor - Array of lgsvl_msgs/DetectedRadarObject published by the simulator lgsvl_srvs/Int Lgsvl - - Service that sends an integer value lgsvl_srvs/String Lgsvl - - Service that sends a string ROS 2 Message Types top # There is a lot of overlap between the message types supported for ROS and ROS 2 with the main difference being Autoware AI. Message Type Namespace Associated Sensor Default Topic Name Notes std_msgs/Header Ros2 - - Standard header used for ROS 2 messages builtin_interfaces/Time Ros2 - - Usually used for timestamps in std_msgs/Header rosgraph_msg/Clock Ros2 Clock Sensor /clock Required when /use_sim_time param is set to true std_msgs/ColorRGBA Ros2 - - - sensor_msgs/CompressedImage Ros2 Camera Sensor - JPEG encoded images - usually subscribed to by AD stacks for camera images sensor_msgs/Image Ros2 Camera Sensor - Uncompressed images - usually subscribed to by AD stacks for camera images sensor_msgs/Imu Ros2 Imu Sensor - Linear acceleration, angular velocity, and (optional) orientation as a quaternion nav_msgs/Odometry Ros2 GPS Odometry Sensor - An odometry message containing a pose and a twist message sensor_msgs/LaserScan Ros2 - - Single beam LiDAR points reported as an array of range measurements sensor_msgs/PointCloud2 Ros2 LiDAR Sensor - Point cloud data as typically output from a 3D LiDAR's driver sensor_msgs/PointField Ros2 - - Included in sensor_msgs/PointCloud2 to provide type information on the point sensor_msgs/NavSatStatus Ros2 GPS Sensor - Status report for GNSS sensor_msgs/NavSatFix Ros2 GPS Sensor - GNSS position fix reported llh format nmea_msgs/Sentence Ros2 GPS Sensor - GNSS fix represented as an NMEA sentence geometry_msgs/Point Ros2 - - XYZ coordinates geometry_msgs/Pose Ros2 - - 3D position and orientation (quaternion) geometry_msgs/PoseWithCovariance Ros2 - - geometry_msgs/Pose with an additional covariance field geometry_msgs/Quaternion Ros2 - - - geometry_msgs/Twist Ros2 - - Linear and angular velocity geometry_msgs/TwistStamped Ros2 - lgsvl_msgs/DetectedRadarObject - Contains s geometry_msgs/Twist and a std_msgs/Header geometry_msgs/TwistWithCovariance Ros2 - - Contains a geometry_msgs/Twist with an additional covariance field geometry_msgs/Vector3 Ros2 - - 3D vector with x, y, z coordinates lgsvl_msgs/BoundingBox2D Lgsvl - - 2D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/BoundingBox3D Lgsvl - - 3D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/Detection2D Lgsvl - - Ground truth detection data for obstacles in 2D lgsvl_msgs/Detection2DArray Lgsvl Ground Truth 2D Sensor - Array of ground truth values for 2D detections lgsvl_msgs/Detection3D Lgsvl - - Ground truth detection data for obstacles in 3D lgsvl_msgs/Detection3DArray Lgsvl Ground Truth 3D Sensor - Array of ground truth values for 3D detections lgsvl_msgs/Signal Lgsvl - - Ground truth traffic light information lgsvl_msgs/SignalArray Lgsvl Signal Sensor - Array of lgsvl_msgs/Signal for all traffic lights in view of ego lgsvl_msgs/CanBusData Lgsvl CanBus Sensor - Collection of information available through CAN bus and published by simulator lgsvl_msgs/VehicleControlData Lgsvl Vehicle Control Sensor - Vehicle control command simulator can subscribe to lgsvl_msgs/VehicleStateData Lgsvl Vehicle State Sensor - Feedback from vehicle control command published by simulator which can be used for implementing a feedback controller lgsvl_msgs/VehicleOdometry Lgsvl Vehicle Odometry Sensor - Velocity and front and rear wheel angle measurements lgsvl_msgs/DetectedRadarObject Lgsvl - - Representation of an obstacle detected by radar lgsvl_msgs/DetectedRadarObjectArray Lgsvl Radar Sensor - Array of lgsvl_msgs/DetectedRadarObject published by the simulator CyberRT Message Types top # Latest versions of Apollo (>=3.5) use a middleware called CyberRT. The SVL Simulator provides a bridge for communication with Apollo using CyberRT messages. The supported messages are listed below: Message Type Namespace Associated Sensor Default Channel Name Notes apollo.canbus.Chassis Cyber CanBus Sensor /apollo/canbus/chassis CAN bus message published by the simulator apollo.control.ControlCommand Cyber CanBus Sensor /apollo/control Vehicle control message the simulator subscribes to apollo.localization.Gps Cyber GPS Odometry Sensor /apollo/sensor/gnss/odometry Vehicle world coordinates in UTM as well as orientation and velocity apollo.drivers.gnss.GnssBestPose Cyber GPS Sensor /apollo/sensor/gnss/best_pose World coordinates in LLH apollo.drivers.gnss.Imu Cyber IMU Sensor /apollo/sensor/gnss/imu Raw IMU sensor measurements apollo.drivers.gnss.CorrectedImu Cyber IMU Sensor /apollo/sensor/gnss/corrected_imu Corrected IMU measurements along with orientation apollo.drivers.gnss.InsStat Cyber GPS-INS Status /apollo/sensor/gnss/ins_stat INS Status apollo.drivers.ContiRadar Cyber Radar Sensor /apollo/sensor/conti_radar Continental radar messages apollo.drivers.PointCloud Cyber LiDAR Sensor /apollo/sensor/lidar128/compensator/PointCloud2 Compensated LiDAR pointcloud message apollo.drivers.Image Cyber Camera Sensor /apollo/sensor/camera/front_12mm/image images from RGB camera apollo.drivers.CompressedImage Cyber Camera Sensor /apollo/sensor/camera/front_12mm/image/compressed Compressed (jpg encoded) images from RGB camera apollo.perception.PerceptionObstacles Cyber Ground Truth 3D Sensor /apollo/perception/obstacles List of detected obstacles used in modular testing to replace the perception module apollo.perception.TrafficLightDetection Cyber Signal Sensor /apollo/perception/traffic_light List of detected traffic lights and their status used in modular testing to replace the perception module","title":"Simulator messages"},{"location":"system-under-test/simulator-messages/#ros-message-types","text":"The simulator supports many of the common standard ROS messages. Additionally, the simulator supports custom ROS messages defined for Autoware AI as well as the simulator's template messages for Autonomous Driving which are included in lgsvl_msgs. Below is a list of these messages. Message Type Namespace Associated Sensor Default Topic Name Notes std_msgs/Header Ros - - Standard header used for ROS messages std_msgs/Time Ros - - Usually used for timestamps in std_msgs/Header rosgraph_msg/Clock Ros Clock Sensor /clock Required when /use_sim_time param is set to true std_msgs/ColorRGBA Ros - - - sensor_msgs/CompressedImage Ros Camera Sensor - JPEG encoded images - usually subscribed to by AD stacks for camera images sensor_msgs/Image Ros Camera Sensor - Uncompressed images - usually subscribed to by AD stacks for camera images sensor_msgs/Imu Ros Imu Sensor - Linear acceleration, angular velocity, and (optional) orientation as a quaternion sensor_msgs/Joy Ros - - Reports the state of a joystick / wheel axes and buttons sensor_msgs/LaserScan Ros - - Single beam LiDAR points reported as an array of range measurements sensor_msgs/PointCloud2 Ros LiDAR Sensor - Point cloud data as typically output from a 3D LiDAR's driver sensor_msgs/PointField Ros - - Included in sensor_msgs/PointCloud2 to provide type information on the point sensor_msgs/NavSatStatus Ros GPS Sensor - Status report for GNSS sensor_msgs/NavSatFix Ros GPS Sensor - GNSS position fix reported llh format nav_msgs/Odometry Ros GPS Odometry Sensor - An odometry message containing a pose and a twist message nmea_msgs/Sentence Ros GPS Sensor - GNSS fix represented as an NMEA sentence - used in Autoware AI geometry_msgs/Point Ros - - XYZ coordinates geometry_msgs/Pose Ros - - 3D position and orientation (quaternion) geometry_msgs/PoseWithCovariance Ros - - geometry_msgs/Pose with an additional covariance field geometry_msgs/Quaternion Ros - - - geometry_msgs/Twist Ros - - Linear and angular velocity geometry_msgs/TwistStamped Ros - lgsvl_msgs/DetectedRadarObject - Contains s geometry_msgs/Twist and a std_msgs/Header geometry_msgs/TwistWithCovariance Ros - - Contains a geometry_msgs/Twist with an additional covariance field geometry_msgs/Vector3 Ros - - 3D vector with x, y, z coordinates std_srvs/Empty Ros - - Empty service std_srvs/SetBool Ros - - Service for setting a boolean parameter std_srvs/Trigger Ros - - Service for triggering an event autoware_msgs/steer_cmd Autoware - - Steer command message autoware_msgs/accel_cmd Autoware - - Acceleration command message autoware_msgs/brake_cmd Autoware - - Brake command message autoware_msgs/lamp_cmd Autoware - - Message for controlling lights on vehicle autoware_msgs/ControlCommand Autoware - - Vehicle control command providing linear velocity, linear acceleration, and a steering angle autoware_msgs/VehicleCmd Autoware AutowareAI Control Sensor /vehicle_cmd Complete vehicle control command containing the other 'command' message - simulator subscribes to this autoware_msgs/DetectedObject Autoware - - Attributes of objects detected by perception autoware_msgs/DetectedObjectArray Autoware - - Array of autoware_msgs/DetectedObject lgsvl_msgs/BoundingBox2D Lgsvl - - 2D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/BoundingBox3D Lgsvl - - 3D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/Detection2D Lgsvl - - Ground truth detection data for obstacles in 2D lgsvl_msgs/Detection2DArray Lgsvl Ground Truth 2D Sensor - Array of ground truth values for 2D detections lgsvl_msgs/Detection3D Lgsvl - - Ground truth detection data for obstacles in 3D lgsvl_msgs/Detection3DArray Lgsvl Ground Truth 3D Sensor - Array of ground truth values for 3D detections lgsvl_msgs/Signal Lgsvl - - Ground truth traffic light information lgsvl_msgs/SignalArray Lgsvl Signal Sensor - Array of lgsvl_msgs/Signal for all traffic lights in view of ego lgsvl_msgs/CanBusData Lgsvl CanBus Sensor - Collection of information available through CAN bus and published by simulator lgsvl_msgs/VehicleControlData Lgsvl Vehicle Control Sensor - Vehicle control command simulator can subscribe to lgsvl_msgs/VehicleStateData Lgsvl Vehicle State Sensor - Feedback from vehicle control command published by simulator which can be used for implementing a feedback controller lgsvl_msgs/VehicleOdometry Lgsvl Vehicle Odometry Sensor - Velocity and front and rear wheel angle measurements lgsvl_msgs/DetectedRadarObject Lgsvl - - Representation of an obstacle detected by radar lgsvl_msgs/DetectedRadarObjectArray Lgsvl Radar Sensor - Array of lgsvl_msgs/DetectedRadarObject published by the simulator lgsvl_srvs/Int Lgsvl - - Service that sends an integer value lgsvl_srvs/String Lgsvl - - Service that sends a string","title":"ROS Message Types"},{"location":"system-under-test/simulator-messages/#ros2-message-types","text":"There is a lot of overlap between the message types supported for ROS and ROS 2 with the main difference being Autoware AI. Message Type Namespace Associated Sensor Default Topic Name Notes std_msgs/Header Ros2 - - Standard header used for ROS 2 messages builtin_interfaces/Time Ros2 - - Usually used for timestamps in std_msgs/Header rosgraph_msg/Clock Ros2 Clock Sensor /clock Required when /use_sim_time param is set to true std_msgs/ColorRGBA Ros2 - - - sensor_msgs/CompressedImage Ros2 Camera Sensor - JPEG encoded images - usually subscribed to by AD stacks for camera images sensor_msgs/Image Ros2 Camera Sensor - Uncompressed images - usually subscribed to by AD stacks for camera images sensor_msgs/Imu Ros2 Imu Sensor - Linear acceleration, angular velocity, and (optional) orientation as a quaternion nav_msgs/Odometry Ros2 GPS Odometry Sensor - An odometry message containing a pose and a twist message sensor_msgs/LaserScan Ros2 - - Single beam LiDAR points reported as an array of range measurements sensor_msgs/PointCloud2 Ros2 LiDAR Sensor - Point cloud data as typically output from a 3D LiDAR's driver sensor_msgs/PointField Ros2 - - Included in sensor_msgs/PointCloud2 to provide type information on the point sensor_msgs/NavSatStatus Ros2 GPS Sensor - Status report for GNSS sensor_msgs/NavSatFix Ros2 GPS Sensor - GNSS position fix reported llh format nmea_msgs/Sentence Ros2 GPS Sensor - GNSS fix represented as an NMEA sentence geometry_msgs/Point Ros2 - - XYZ coordinates geometry_msgs/Pose Ros2 - - 3D position and orientation (quaternion) geometry_msgs/PoseWithCovariance Ros2 - - geometry_msgs/Pose with an additional covariance field geometry_msgs/Quaternion Ros2 - - - geometry_msgs/Twist Ros2 - - Linear and angular velocity geometry_msgs/TwistStamped Ros2 - lgsvl_msgs/DetectedRadarObject - Contains s geometry_msgs/Twist and a std_msgs/Header geometry_msgs/TwistWithCovariance Ros2 - - Contains a geometry_msgs/Twist with an additional covariance field geometry_msgs/Vector3 Ros2 - - 3D vector with x, y, z coordinates lgsvl_msgs/BoundingBox2D Lgsvl - - 2D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/BoundingBox3D Lgsvl - - 3D ground truth bounding box for objects in view of the vehicle lgsvl_msgs/Detection2D Lgsvl - - Ground truth detection data for obstacles in 2D lgsvl_msgs/Detection2DArray Lgsvl Ground Truth 2D Sensor - Array of ground truth values for 2D detections lgsvl_msgs/Detection3D Lgsvl - - Ground truth detection data for obstacles in 3D lgsvl_msgs/Detection3DArray Lgsvl Ground Truth 3D Sensor - Array of ground truth values for 3D detections lgsvl_msgs/Signal Lgsvl - - Ground truth traffic light information lgsvl_msgs/SignalArray Lgsvl Signal Sensor - Array of lgsvl_msgs/Signal for all traffic lights in view of ego lgsvl_msgs/CanBusData Lgsvl CanBus Sensor - Collection of information available through CAN bus and published by simulator lgsvl_msgs/VehicleControlData Lgsvl Vehicle Control Sensor - Vehicle control command simulator can subscribe to lgsvl_msgs/VehicleStateData Lgsvl Vehicle State Sensor - Feedback from vehicle control command published by simulator which can be used for implementing a feedback controller lgsvl_msgs/VehicleOdometry Lgsvl Vehicle Odometry Sensor - Velocity and front and rear wheel angle measurements lgsvl_msgs/DetectedRadarObject Lgsvl - - Representation of an obstacle detected by radar lgsvl_msgs/DetectedRadarObjectArray Lgsvl Radar Sensor - Array of lgsvl_msgs/DetectedRadarObject published by the simulator","title":"ROS 2 Message Types"},{"location":"system-under-test/simulator-messages/#cyberrt-message-types","text":"Latest versions of Apollo (>=3.5) use a middleware called CyberRT. The SVL Simulator provides a bridge for communication with Apollo using CyberRT messages. The supported messages are listed below: Message Type Namespace Associated Sensor Default Channel Name Notes apollo.canbus.Chassis Cyber CanBus Sensor /apollo/canbus/chassis CAN bus message published by the simulator apollo.control.ControlCommand Cyber CanBus Sensor /apollo/control Vehicle control message the simulator subscribes to apollo.localization.Gps Cyber GPS Odometry Sensor /apollo/sensor/gnss/odometry Vehicle world coordinates in UTM as well as orientation and velocity apollo.drivers.gnss.GnssBestPose Cyber GPS Sensor /apollo/sensor/gnss/best_pose World coordinates in LLH apollo.drivers.gnss.Imu Cyber IMU Sensor /apollo/sensor/gnss/imu Raw IMU sensor measurements apollo.drivers.gnss.CorrectedImu Cyber IMU Sensor /apollo/sensor/gnss/corrected_imu Corrected IMU measurements along with orientation apollo.drivers.gnss.InsStat Cyber GPS-INS Status /apollo/sensor/gnss/ins_stat INS Status apollo.drivers.ContiRadar Cyber Radar Sensor /apollo/sensor/conti_radar Continental radar messages apollo.drivers.PointCloud Cyber LiDAR Sensor /apollo/sensor/lidar128/compensator/PointCloud2 Compensated LiDAR pointcloud message apollo.drivers.Image Cyber Camera Sensor /apollo/sensor/camera/front_12mm/image images from RGB camera apollo.drivers.CompressedImage Cyber Camera Sensor /apollo/sensor/camera/front_12mm/image/compressed Compressed (jpg encoded) images from RGB camera apollo.perception.PerceptionObstacles Cyber Ground Truth 3D Sensor /apollo/perception/obstacles List of detected obstacles used in modular testing to replace the perception module apollo.perception.TrafficLightDetection Cyber Signal Sensor /apollo/perception/traffic_light List of detected traffic lights and their status used in modular testing to replace the perception module","title":"CyberRT Message Types"},{"location":"system-under-test/sut-introduction/","text":"System Under Test Introduction System Under Test refers to any module or system that you connect to SVL Simulator to test or verify. This can be a specific software module and its algorithms, an end-to-end autonomous system including target computing hardware, or a reference robotics software stack running on a desktop PC. By connecting the System Under Test to SVL Simulator, you can virtually simulate the real-world conditions and environmental input required by the systems you are developing. This includes the physics state of the virtual 3D environment, including all dynamic actors like traffic, and sensor data that interprets this environment. Interfacing with SVL Simulator # The simulator provides several default communication interface mechanisms that are common within robotics, including ROS and ROS2-based communication. You can see all of the supported message types for ROS, ROS2, and CyberRT by our default bridges here . If you use a custom or proprietary communication protocol and interface for your System Under Test, SVL Simulator bridge plugins allow you to build the proper interface to handle the specific format and protocol of your communication. Reference Systems Under Test # SVL Simulator itself does not contain any autonomous vehicle software or logic that runs on a self-driving vehicle or robot - this system (the System Under Test) is provided and tested by the user together with the simulator. We instead provide guides and tutorials on open source reference platforms that can be easily connected to SVL Simulator. This provides a jumping off point for additional development of particular algorithms, replacement of various modules, higher-level testing for applications like Future Mobility. See the links below for guides on the running a reference autonomous driving system: ROS-based AD system Autoware.Auto (ROS 2) Apollo (latest) Apollo 5.0","title":"Introduction"},{"location":"system-under-test/sut-introduction/#interfacing-with-svl-simulator","text":"The simulator provides several default communication interface mechanisms that are common within robotics, including ROS and ROS2-based communication. You can see all of the supported message types for ROS, ROS2, and CyberRT by our default bridges here . If you use a custom or proprietary communication protocol and interface for your System Under Test, SVL Simulator bridge plugins allow you to build the proper interface to handle the specific format and protocol of your communication.","title":"Interfacing with SVL Simulator"},{"location":"system-under-test/sut-introduction/#reference-systems-under-test","text":"SVL Simulator itself does not contain any autonomous vehicle software or logic that runs on a self-driving vehicle or robot - this system (the System Under Test) is provided and tested by the user together with the simulator. We instead provide guides and tutorials on open source reference platforms that can be easily connected to SVL Simulator. This provides a jumping off point for additional development of particular algorithms, replacement of various modules, higher-level testing for applications like Future Mobility. See the links below for guides on the running a reference autonomous driving system: ROS-based AD system Autoware.Auto (ROS 2) Apollo (latest) Apollo 5.0","title":"Reference Systems Under Test"},{"location":"third-party-integration/openai-gym/","text":"Reinforcement Learning with OpenAI Gym OpenAI Gym is a toolkit for developing reinforcement learning algorithms. Gym provides a collection of test problems called environments which can be used to train an agent using a reinforcement learning. Each environment defines the reinforcement learnign problem the agent will try to solve. To facilitate developing reinforcement learning algorithms with the SVL Simulator , we have developed gym-lgsvl , a custom environment that using the openai gym interface. gym-lgsvl can be used with general reinforcement learning algorithms implementations that are compatible with openai gym. Developers can modify the environment to define the specific reinforcement learning problem they are trying to solve. Requirements top # Python >= 3.5 Pip lgsvl simulator API openai gym numpy opencv Setup top # Install SVL Simulator using this guide : Install Python API using this guide . Install openai gym, numpy, and opencv: pip install --user gym, numpy, opencv-python Clone the gym-lgsvl repository: git clone https://github.com/lgsvl/gym-lgsvl.git Install gym-lgsvl using pip: cd gym-lgsvl/ pip install --user -e . Getting Started top # The simulator must be running on the menu scene to be used with the gym-lgsvl . The scene can be loaded either in the Unity Editor or the simulator binary build (download latest release ). The binary build will have superior performance. The script named random_agent.py will launch a random agent which will sample acceleration and steering values for the ego vehicle for every step of the episode. Run the agent to test your setup: ./random_agent.py After a few seconds the BorregasAve scene should load and spawn the ego vehicle along with some NPC vehicles. The ego vehicle will drive randomly. gym_lgsvl can be used with RL libraries that support openai gym environments. Below is an example of training using the A2C implementation from baselines : python -m baselines.run --alg=a2c --env=gym_lgsvl:lgsvl-v0 --num_timesteps=1e5 Customizing the environment top # The specifics of the environment you will need will depend on the reinforcement learning problem you are trying to solve. By default, the gym-lgsvl environment has a simple setup intended to be a starting point for building more advanced problems. Training an agent with the default environment would be difficult without modiification. In the default configuration, the vehicle uses a single front facing camera as observation and uses continuous control parameters for driving the vehicle. For more advanced state representations, modifications will be needed. The entire environment is defined in lgsvl_env.py . CONFIG top # Some of the basic configuration are passed to the environment through CONFIG . action_space and observation_space definitions are required and are defined using gym.spaces . The default action_space is: \"action_space\" : spaces.Box( np.array([-1,-1]), np.array([+1,+1,]), dtype=np.float32, ), which defines a continuous action space defined as a 2-D array. The first element is the steering value and the second is braking/throttle (negative values are braking and positive are throttle). The observation space is defined as a single camera image from the front camera using the Box space from gym: \"observation_space\" : spaces.Box( low=0, high=255, shape=(297, 528, 3), dtype=np.uint8 ) # RGB image from front camera The shape tuple specifies the image size. The simulator API will always send 1920x1080 images. If any other size is used to define the observation space, the camera images will be resized to the specified size before being passed on as an observation. Reward calculation top # The environment also calculates a reward function based on the distance travelled by the ego vehicle and collisions. The reward is calculated in the _calculate_reward() function based on distance travelled. The collision penalty is added on at the end (if applicable) when the collision callback is invoked ( _on_collision ). The collision callback will also terminate the episode and start a new episode. Sensors top # By default the ego vehicle will only use the front facing camera. The ego vehicle is setup using _setup_ego() . Sensors are also defined here. To define more sensors, grab the sensor from the sensors list and invoke its specific methods to save data. To collect observations, _get_observations() is called. NPC Behavior top # The NPCs are defined in _setup_npc . The NPCs are spawned randomly around the ego vehicle spawn point and will follow the lanes and traffic rules to move around.","title":"OpenAI Gym"},{"location":"third-party-integration/openai-gym/#requirements","text":"Python >= 3.5 Pip lgsvl simulator API openai gym numpy opencv","title":"Requirements"},{"location":"third-party-integration/openai-gym/#setup","text":"Install SVL Simulator using this guide : Install Python API using this guide . Install openai gym, numpy, and opencv: pip install --user gym, numpy, opencv-python Clone the gym-lgsvl repository: git clone https://github.com/lgsvl/gym-lgsvl.git Install gym-lgsvl using pip: cd gym-lgsvl/ pip install --user -e .","title":"Setup"},{"location":"third-party-integration/openai-gym/#getting-started","text":"The simulator must be running on the menu scene to be used with the gym-lgsvl . The scene can be loaded either in the Unity Editor or the simulator binary build (download latest release ). The binary build will have superior performance. The script named random_agent.py will launch a random agent which will sample acceleration and steering values for the ego vehicle for every step of the episode. Run the agent to test your setup: ./random_agent.py After a few seconds the BorregasAve scene should load and spawn the ego vehicle along with some NPC vehicles. The ego vehicle will drive randomly. gym_lgsvl can be used with RL libraries that support openai gym environments. Below is an example of training using the A2C implementation from baselines : python -m baselines.run --alg=a2c --env=gym_lgsvl:lgsvl-v0 --num_timesteps=1e5","title":"Getting Started"},{"location":"third-party-integration/openai-gym/#customizing-the-environment","text":"The specifics of the environment you will need will depend on the reinforcement learning problem you are trying to solve. By default, the gym-lgsvl environment has a simple setup intended to be a starting point for building more advanced problems. Training an agent with the default environment would be difficult without modiification. In the default configuration, the vehicle uses a single front facing camera as observation and uses continuous control parameters for driving the vehicle. For more advanced state representations, modifications will be needed. The entire environment is defined in lgsvl_env.py .","title":"Customizing the environment"},{"location":"third-party-integration/openai-gym/#config","text":"Some of the basic configuration are passed to the environment through CONFIG . action_space and observation_space definitions are required and are defined using gym.spaces . The default action_space is: \"action_space\" : spaces.Box( np.array([-1,-1]), np.array([+1,+1,]), dtype=np.float32, ), which defines a continuous action space defined as a 2-D array. The first element is the steering value and the second is braking/throttle (negative values are braking and positive are throttle). The observation space is defined as a single camera image from the front camera using the Box space from gym: \"observation_space\" : spaces.Box( low=0, high=255, shape=(297, 528, 3), dtype=np.uint8 ) # RGB image from front camera The shape tuple specifies the image size. The simulator API will always send 1920x1080 images. If any other size is used to define the observation space, the camera images will be resized to the specified size before being passed on as an observation.","title":"CONFIG"},{"location":"third-party-integration/openai-gym/#reward-calculation","text":"The environment also calculates a reward function based on the distance travelled by the ego vehicle and collisions. The reward is calculated in the _calculate_reward() function based on distance travelled. The collision penalty is added on at the end (if applicable) when the collision callback is invoked ( _on_collision ). The collision callback will also terminate the episode and start a new episode.","title":"Reward calculation"},{"location":"third-party-integration/openai-gym/#sensors","text":"By default the ego vehicle will only use the front facing camera. The ego vehicle is setup using _setup_ego() . Sensors are also defined here. To define more sensors, grab the sensor from the sensors list and invoke its specific methods to save data. To collect observations, _get_observations() is called.","title":"Sensors"},{"location":"third-party-integration/openai-gym/#npc-behavior","text":"The NPCs are defined in _setup_npc . The NPCs are spawned randomly around the ego vehicle spawn point and will follow the lanes and traffic rules to move around.","title":"NPC Behavior"},{"location":"tutorials/create-ros2-ad-stack/","text":"How to create a ROS2-based AD stack with SVL Simulator This documentation describes how to develop ROS2 nodes to receive sensor data from SVL Simulator and send control commands to drive a car. The Lane Following model is a ROS2 -based Autonomous Driving stack developed with SVL Simulator . In high-level overview, the model is composed of three modules: a sensor module, a perception module, and a control module. The sensor module receives raw sensor data such as camera images from the simulator and preprocess the data before feeding into the perception module. Then, the perception module takes in the preprocessed data, extracts lane information, and predicts steering wheel commands. Finally, the control module sends a predicted control command back to the simulator, which would drive a car autonomously. Table of Contents Requirements Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Creating a ROS2 Package setup.py package.xml Building a ROS2 Package Running ROS2 LGSVL Bridge Writing ROS2 Subscriber Node Subscribe to a single topic Subscribe to multiple topics simultaneously Writing ROS2 Publisher Node Publish command back to SVL Simulator Running ROS2 Node References Requirements top # Docker Python3 ROS2 TensorFlow, Keras SVL Simulator Setup top # Our AD stack implementation is based on ROS2 and uses rosbridge to communicate with SVL Simulator. To do that, we need to prepare Ubuntu machine with ROS2 installed. We provide a Docker image containing Ubuntu 18.04 and ROS2 installed so you can just pull the Docker image and start writing code right away. Installing Docker CE top # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing NVIDIA Docker top # Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling Docker Image top # docker pull lgsvl/lanefollowing:latest What's inside Docker Image # Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS1 melodic + rosbridge ROS2 dashing + rosbridge Jupyter Notebook Creating a ROS2 Package top # A ROS2 package is simply a directory and should contain files named package.xml and setup.py . Create folders as below and create setup.py and package.xml . Please note that the package name must match with the folder name of your ROS package. ros2_ws/ src/ lane_following/ setup.py package.xml setup.py # from setuptools import setup package_name = 'lane_following' setup( name=package_name, version='0.0.1', packages=[ 'train', ], py_modules=[ 'collect', 'drive', ], install_requires=['setuptools'], zip_safe=True, author='David Uhm', author_email='david.uhm@lge.com', maintainer='David Uhm', maintainer_email='david.uhm@lge.com', keywords=[ 'ROS', 'ROS2', 'deep learning', 'lane following', 'end to end', 'SVL Simulator', 'Autonomous Driving' ], classifiers=[ 'Intended Audience :: Developers', 'Programming Language :: Python', 'Topic :: Software Development', ], description='ROS2-based End-to-End Lane Following model', license='BSD', tests_require=['pytest'], entry_points={ 'console_scripts': [ 'collect = collect:main', 'drive = drive:main', ], }, ) package.xml # <?xml version=\"1.0\"?> <package format=\"2\"> <name>lane_following</name> <version>0.0.1</version> <description>ROS2-based End-to-End Lane Following model</description> <maintainer email=\"david.uhm@lge.com\">David Uhm</maintainer> <license>BSD</license> <exec_depend>rclpy</exec_depend> <exec_depend>std_msgs</exec_depend> <exec_depend>sensor_msgs</exec_depend> <test_depend>ament_copyright</test_depend> <test_depend>ament_flake8</test_depend> <test_depend>ament_pep257</test_depend> <test_depend>python3-pytest</test_depend> <export> <build_type>ament_python</build_type> </export> </package> Building a ROS2 Package top # Now, you can build your package as below: source /opt/ros/dashing/setup.bash cd ~/ros2_ws colcon build --symlink-install Running ROS2 LGSVL Bridge top # ROS2 LGSVL Bridge provides a JSON API to ROS functionality for non-ROS programs such as SVL Simulator. It is not provided as part of a default ROS2 Dashing installation, but has been installed separately for you in the Docker image already. You can run ROS2 LGSVL Bridge to connect your ROS node with SVL Simulator as below: source /path/to/ros2-lgsvl-bridge/install/setup.bash lgsvl_bridge Writing ROS2 Subscriber Node top # ROS nodes communicate with each other by passing messages. Messages are routed via a topic with publish/subscribe concepts. A node sends out a message by publishing it to a given topic. Then, a node that is interested in a certain kind of data will subscribe to the appropriate topic. In our cases, SVL Simulator publishes sensor data such as camera images or LiDAR point clouds via rosbridge, and then the Lane Following model subscribes to that topic to receive sensor data, preprocesses the data, feeds them into the pretrained model, and finally computes a control command based on perceived sensor data. Below is an example of how to subscribe to sensor data topics from a ROS node. You can subscribe to a single topic only or multiple topics simultaneously. Subscribe to a single topic top # import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage class Drive(Node): def __init__(self): super().__init__('drive') self.sub_image = self.create_subscription(CompressedImage, '/simulator/sensor/camera/center/compressed', self.callback) def callback(self, msg): self.get_logger().info('Subscribed: {}'.format(msg.data)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main() Subscribe to multiple topics simultaneously top # In order to subscribe to ROS messages of different types from multiple sources, we need to take the timestamps of those messages into account. ROS2 Message Filters is the ROS package that synchronizes incoming messages by the timestamps contained in their headers and outputs them in the form of a single callback. Install this package in your ROS2 workspace and build it. cd ~/ros2_ws/src git clone https://github.com/intel/ros2_message_filters.git cd .. colcon build --symlink-install Then, you can import it in your Python script by import message_filters and use it as below: import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage from geometry_msgs.msg import TwistStamped import message_filters class Collect(Node): def __init__(self): super().__init__('collect') sub_center_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/center/compressed') sub_left_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/left/compressed') sub_right_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/right/compressed') sub_control = message_filters.Subscriber(self, TwistStamped, '/simulator/control/command') ts = message_filters.ApproximateTimeSynchronizer([sub_center_camera, sub_left_camera, sub_right_camera, sub_control], 1, 0.1) ts.registerCallback(self.callback) def callback(self, center_camera, left_camera, right_camera, control): self.get_logger().info('Subscribed: {}'.format(control.twist.angular.x)) def main(args=None): rclpy.init(args=args) collect = Collect() rclpy.spin(collect) if __name__ == '__main__': main() Writing ROS2 Publisher Node top # The publisher sends data to a topic. When you create a publisher you have to tell ROS of which type the data will be. In order to drive a car autonomously, the Lane Following model publishes a predicted control command back to the simulator via rosbridge. Publish command back to SVL Simulator top # import rclpy from rclpy.node import Node from geometry_msgs.msg import TwistStamped class Drive(Node): def __init__(self): super().__init__('drive') self.control_pub = self.create_publisher(TwistStamped, '/lanefollowing/steering_cmd') self.timer_period = 0.02 # seconds self.timer = self.create_timer(self.timer_period, self.callback) self.steering = 0. def callback(self): message = TwistStamped() message.twist.angular.x = float(self.steering) self.control_pub.publish(message) self.get_logger().info('Publishing: {}'.format(message.twist.angular.x)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main() Running ROS2 Node top # Once you have setup the rosbridge connection to SVL Simulator, you can launch your ROS node as follows: source /opt/ros/dashing/setup.bash source /path/to/ros2_ws/install/local_setup.bash ros2 run {your_package} {your_node} References top # Lane Following Github Repository SVL Simulator ROS2 Documentation ROS2 Message Filters","title":"Creating a simple ROS2-based AD stack"},{"location":"tutorials/create-ros2-ad-stack/#requirements","text":"Docker Python3 ROS2 TensorFlow, Keras SVL Simulator","title":"Requirements"},{"location":"tutorials/create-ros2-ad-stack/#setup","text":"Our AD stack implementation is based on ROS2 and uses rosbridge to communicate with SVL Simulator. To do that, we need to prepare Ubuntu machine with ROS2 installed. We provide a Docker image containing Ubuntu 18.04 and ROS2 installed so you can just pull the Docker image and start writing code right away.","title":"Setup"},{"location":"tutorials/create-ros2-ad-stack/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"tutorials/create-ros2-ad-stack/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing NVIDIA Docker"},{"location":"tutorials/create-ros2-ad-stack/#pulling-docker-image","text":"docker pull lgsvl/lanefollowing:latest","title":"Pulling Docker Image"},{"location":"tutorials/create-ros2-ad-stack/#whats-inside-docker-image","text":"Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS1 melodic + rosbridge ROS2 dashing + rosbridge Jupyter Notebook","title":"What's inside Docker Image"},{"location":"tutorials/create-ros2-ad-stack/#creating-a-ros2-package","text":"A ROS2 package is simply a directory and should contain files named package.xml and setup.py . Create folders as below and create setup.py and package.xml . Please note that the package name must match with the folder name of your ROS package. ros2_ws/ src/ lane_following/ setup.py package.xml","title":"Creating a ROS2 Package"},{"location":"tutorials/create-ros2-ad-stack/#setuppy","text":"from setuptools import setup package_name = 'lane_following' setup( name=package_name, version='0.0.1', packages=[ 'train', ], py_modules=[ 'collect', 'drive', ], install_requires=['setuptools'], zip_safe=True, author='David Uhm', author_email='david.uhm@lge.com', maintainer='David Uhm', maintainer_email='david.uhm@lge.com', keywords=[ 'ROS', 'ROS2', 'deep learning', 'lane following', 'end to end', 'SVL Simulator', 'Autonomous Driving' ], classifiers=[ 'Intended Audience :: Developers', 'Programming Language :: Python', 'Topic :: Software Development', ], description='ROS2-based End-to-End Lane Following model', license='BSD', tests_require=['pytest'], entry_points={ 'console_scripts': [ 'collect = collect:main', 'drive = drive:main', ], }, )","title":"setup.py"},{"location":"tutorials/create-ros2-ad-stack/#packagexml","text":"<?xml version=\"1.0\"?> <package format=\"2\"> <name>lane_following</name> <version>0.0.1</version> <description>ROS2-based End-to-End Lane Following model</description> <maintainer email=\"david.uhm@lge.com\">David Uhm</maintainer> <license>BSD</license> <exec_depend>rclpy</exec_depend> <exec_depend>std_msgs</exec_depend> <exec_depend>sensor_msgs</exec_depend> <test_depend>ament_copyright</test_depend> <test_depend>ament_flake8</test_depend> <test_depend>ament_pep257</test_depend> <test_depend>python3-pytest</test_depend> <export> <build_type>ament_python</build_type> </export> </package>","title":"package.xml"},{"location":"tutorials/create-ros2-ad-stack/#building-a-ros2-package","text":"Now, you can build your package as below: source /opt/ros/dashing/setup.bash cd ~/ros2_ws colcon build --symlink-install","title":"Building a ROS2 Package"},{"location":"tutorials/create-ros2-ad-stack/#running-ros2-lgsvl-bridge","text":"ROS2 LGSVL Bridge provides a JSON API to ROS functionality for non-ROS programs such as SVL Simulator. It is not provided as part of a default ROS2 Dashing installation, but has been installed separately for you in the Docker image already. You can run ROS2 LGSVL Bridge to connect your ROS node with SVL Simulator as below: source /path/to/ros2-lgsvl-bridge/install/setup.bash lgsvl_bridge","title":"Running ROS2 LGSVL Bridge"},{"location":"tutorials/create-ros2-ad-stack/#writing-ros2-subscriber-node","text":"ROS nodes communicate with each other by passing messages. Messages are routed via a topic with publish/subscribe concepts. A node sends out a message by publishing it to a given topic. Then, a node that is interested in a certain kind of data will subscribe to the appropriate topic. In our cases, SVL Simulator publishes sensor data such as camera images or LiDAR point clouds via rosbridge, and then the Lane Following model subscribes to that topic to receive sensor data, preprocesses the data, feeds them into the pretrained model, and finally computes a control command based on perceived sensor data. Below is an example of how to subscribe to sensor data topics from a ROS node. You can subscribe to a single topic only or multiple topics simultaneously.","title":"Writing ROS2 Subscriber Node"},{"location":"tutorials/create-ros2-ad-stack/#subscribe-to-a-single-topic","text":"import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage class Drive(Node): def __init__(self): super().__init__('drive') self.sub_image = self.create_subscription(CompressedImage, '/simulator/sensor/camera/center/compressed', self.callback) def callback(self, msg): self.get_logger().info('Subscribed: {}'.format(msg.data)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main()","title":"Subscribe to a single topic"},{"location":"tutorials/create-ros2-ad-stack/#subscribe-to-multiple-topics-simultaneously","text":"In order to subscribe to ROS messages of different types from multiple sources, we need to take the timestamps of those messages into account. ROS2 Message Filters is the ROS package that synchronizes incoming messages by the timestamps contained in their headers and outputs them in the form of a single callback. Install this package in your ROS2 workspace and build it. cd ~/ros2_ws/src git clone https://github.com/intel/ros2_message_filters.git cd .. colcon build --symlink-install Then, you can import it in your Python script by import message_filters and use it as below: import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage from geometry_msgs.msg import TwistStamped import message_filters class Collect(Node): def __init__(self): super().__init__('collect') sub_center_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/center/compressed') sub_left_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/left/compressed') sub_right_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/right/compressed') sub_control = message_filters.Subscriber(self, TwistStamped, '/simulator/control/command') ts = message_filters.ApproximateTimeSynchronizer([sub_center_camera, sub_left_camera, sub_right_camera, sub_control], 1, 0.1) ts.registerCallback(self.callback) def callback(self, center_camera, left_camera, right_camera, control): self.get_logger().info('Subscribed: {}'.format(control.twist.angular.x)) def main(args=None): rclpy.init(args=args) collect = Collect() rclpy.spin(collect) if __name__ == '__main__': main()","title":"Subscribe to multiple topics simultaneously"},{"location":"tutorials/create-ros2-ad-stack/#writing-ros2-publisher-node","text":"The publisher sends data to a topic. When you create a publisher you have to tell ROS of which type the data will be. In order to drive a car autonomously, the Lane Following model publishes a predicted control command back to the simulator via rosbridge.","title":"Writing ROS2 Publisher Node"},{"location":"tutorials/create-ros2-ad-stack/#publish-command-back-to-svl-simulator","text":"import rclpy from rclpy.node import Node from geometry_msgs.msg import TwistStamped class Drive(Node): def __init__(self): super().__init__('drive') self.control_pub = self.create_publisher(TwistStamped, '/lanefollowing/steering_cmd') self.timer_period = 0.02 # seconds self.timer = self.create_timer(self.timer_period, self.callback) self.steering = 0. def callback(self): message = TwistStamped() message.twist.angular.x = float(self.steering) self.control_pub.publish(message) self.get_logger().info('Publishing: {}'.format(message.twist.angular.x)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main()","title":"Publish command back to SVL Simulator"},{"location":"tutorials/create-ros2-ad-stack/#running-ros2-node","text":"Once you have setup the rosbridge connection to SVL Simulator, you can launch your ROS node as follows: source /opt/ros/dashing/setup.bash source /path/to/ros2_ws/install/local_setup.bash ros2 run {your_package} {your_node}","title":"Running ROS2 Node"},{"location":"tutorials/create-ros2-ad-stack/#references","text":"Lane Following Github Repository SVL Simulator ROS2 Documentation ROS2 Message Filters","title":"References"},{"location":"tutorials/ground-truth-obstacles/","text":"Ground Truth Obstacles You can use the SVL Simulator to view, publish, and compare ground truth obstacle information. The simulator allows visualization of 2D or 3D bounding boxes of vehicles and pedestrians, and publishes detailed information about the ground truth obstacles. Sensor Configuration for Ground Truth Sensors top # [ { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth Sensor\", \"params\": { \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"roll\": 0, \"yaw\": 0 } }, { \"type\": \"PerceptionSensor2D\", \"name\": \"2D Ground Truth Sensor\", \"params\": { \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"roll\": 0, \"yaw\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Control Sensor\" } ] View Ground Truth Obstacles in Simulator top # Ground truth obstacles for vehicles and pedestrians can be visualized in the simulator with both 3D bounding boxes and 2D bounding boxes. To view 3D Bounding boxes in the simulator: Launch SVL simulator and click the Open Browser button Create a sensor configuration with ROS bridge and the sensors listed above on your desired vehicle Create a simulation: Set Random Traffic mode Set your desired map (e.g., BorregasAve ) Set your desried vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration created above Enable Random Traffic and Random Pedestrians Run simulation Enable visualization for 3D Ground Truth Sensor inside the simulator You should see 3D bounding boxes highlighting NPC vehicles and pedestrians in the simulator main view. To view 2D bounding boxes: Follow the steps listed above Enable visualization for 2D Ground Truth Sensor inside the simulator You should see 2D bounding boxes highlighting NPC vehicles and pedestrians in the camera view. Bounding Box Colors top # Green: Vehicles Yellow: Pedestrians Subscribe to Ground Truth ROS Messages top # SVL Simulator also publishes custom ROS messages describing the ground truth data of non-ego vehicles. In order to subscribe to the ground truth messages, you will need the ROS package lgsvl_msgs . It contains custom ROS message types for 2D and 3D bounding boxes. You will also need to be running rosbridge. Install the lgsvl_msgs ROS Package # Clone lgsvl_msgs to your ROS workspace or msgs directory: $ git clone https://github.com/lgsvl/lgsvl_msgs {MY_ROS_WS} Build the ROS workspace: $ catkin_make ROS Topics and Message Types for Ground Truth Messages top # Ground Truth 2D messages Topic: /simulator/ground_truth/2d_detections Message type: lgsvl_msgs/Detection2DArray: Link Ground Truth 3D messages Topic: /simulator/ground_truth/3d_detections Message type: lgsvl_msgs/Detection3DArray: Link View Object Detections from Apollo in SVL Simulator top # If you are running Apollo with SVL Simulator, you can also visualize object detection outputs from Apollo in the simulator using the Apollo Perception Visualizer 3D Sensor . Make sure that the Apollo perception module is running and detecting obstacles in Dreamview as below: To view object detections from Apollo: Launch SVL simulator and click the Open Browser button Add the Apollo Perception Visualizer 3D Sensor into your Apollo sensor configuration with the CyberRT bridge Set sensor topic as /apollo/perception/obstacles Create a simulation: Set Random Traffic mode Set your desired map (e.g., BorregasAve ) Set your desried vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration created above Enable Random Traffic and Random Pedestrians Run simulation Enable visualization for the Apollo Perception Visualizer 3D Sensor inside the simulator You should see 3D bounding boxes highlighting Apollo LiDAR detections in the simulator main view.","title":"Viewing and subscribing to ground truth obstacles"},{"location":"tutorials/ground-truth-obstacles/#sensor-configuration-for-ground-truth-sensors","text":"[ { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth Sensor\", \"params\": { \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.96, \"z\": 1.0510799, \"pitch\": 0, \"roll\": 0, \"yaw\": 0 } }, { \"type\": \"PerceptionSensor2D\", \"name\": \"2D Ground Truth Sensor\", \"params\": { \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.348, \"z\": 1.219, \"pitch\": 0, \"roll\": 0, \"yaw\": 0 } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Control Sensor\" } ]","title":"Sensor Configuration for Ground Truth Sensors"},{"location":"tutorials/ground-truth-obstacles/#view-ground-truth-obstacles-in-simulator","text":"Ground truth obstacles for vehicles and pedestrians can be visualized in the simulator with both 3D bounding boxes and 2D bounding boxes. To view 3D Bounding boxes in the simulator: Launch SVL simulator and click the Open Browser button Create a sensor configuration with ROS bridge and the sensors listed above on your desired vehicle Create a simulation: Set Random Traffic mode Set your desired map (e.g., BorregasAve ) Set your desried vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration created above Enable Random Traffic and Random Pedestrians Run simulation Enable visualization for 3D Ground Truth Sensor inside the simulator You should see 3D bounding boxes highlighting NPC vehicles and pedestrians in the simulator main view. To view 2D bounding boxes: Follow the steps listed above Enable visualization for 2D Ground Truth Sensor inside the simulator You should see 2D bounding boxes highlighting NPC vehicles and pedestrians in the camera view.","title":"View Ground Truth Obstacles in Simulator"},{"location":"tutorials/ground-truth-obstacles/#bounding-box-colors","text":"Green: Vehicles Yellow: Pedestrians","title":"Bounding Box Colors"},{"location":"tutorials/ground-truth-obstacles/#subscribe-to-ground-truth-ros-messages","text":"SVL Simulator also publishes custom ROS messages describing the ground truth data of non-ego vehicles. In order to subscribe to the ground truth messages, you will need the ROS package lgsvl_msgs . It contains custom ROS message types for 2D and 3D bounding boxes. You will also need to be running rosbridge.","title":"Subscribe to Ground Truth ROS Messages"},{"location":"tutorials/ground-truth-obstacles/#install-the-lgsvl_msgs-ros-package","text":"Clone lgsvl_msgs to your ROS workspace or msgs directory: $ git clone https://github.com/lgsvl/lgsvl_msgs {MY_ROS_WS} Build the ROS workspace: $ catkin_make","title":"Install the lgsvl_msgs ROS Package"},{"location":"tutorials/ground-truth-obstacles/#ros-topics-and-message-types-for-ground-truth-messages","text":"Ground Truth 2D messages Topic: /simulator/ground_truth/2d_detections Message type: lgsvl_msgs/Detection2DArray: Link Ground Truth 3D messages Topic: /simulator/ground_truth/3d_detections Message type: lgsvl_msgs/Detection3DArray: Link","title":"ROS Topics and Message Types for Ground Truth Messages"},{"location":"tutorials/ground-truth-obstacles/#view-object-detections-from-apollo-in-svl-simulator","text":"If you are running Apollo with SVL Simulator, you can also visualize object detection outputs from Apollo in the simulator using the Apollo Perception Visualizer 3D Sensor . Make sure that the Apollo perception module is running and detecting obstacles in Dreamview as below: To view object detections from Apollo: Launch SVL simulator and click the Open Browser button Add the Apollo Perception Visualizer 3D Sensor into your Apollo sensor configuration with the CyberRT bridge Set sensor topic as /apollo/perception/obstacles Create a simulation: Set Random Traffic mode Set your desired map (e.g., BorregasAve ) Set your desried vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration created above Enable Random Traffic and Random Pedestrians Run simulation Enable visualization for the Apollo Perception Visualizer 3D Sensor inside the simulator You should see 3D bounding boxes highlighting Apollo LiDAR detections in the simulator main view.","title":"View Object Detections from Apollo in SVL Simulator"},{"location":"tutorials/lane-following/","text":"ROS2 End-to-End Lane Following Model with SVL Simulator This documentation describes applying a deep learning neural network for lane following in SVL Simulator . In this project, we use SVL Simulator for customizing sensors (one main camera and two side cameras) for a car, collect data for training, and deploying and testing a trained model. This project was inspired by NVIDIA's End-to-End Deep Learning Model for Self-Driving Cars Video top # Table of Contents top # Getting Started Prerequisites Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Features Training Details Network Architecture Hyperparameters Dataset How to Collect Data and Train Your Own Model with SVL Simulator Collect data from SVL Simulator Data preprocessing Train a model Drive with your trained model in SVL Simulator Future Works and Contributing References Getting Started top # First, clone this repository: git clone --recurse-submodules https://github.com/lgsvl/lanefollowing.git Next, pull the latest Docker image: docker pull lgsvl/lanefollowing:latest To build ROS2 packages: docker-compose up build_ros Now, launch the lane following model: docker-compose up drive (Optional) If you want visualizations, run drive_visual instead of drive: docker-compose up drive_visual That's it! Now, the lane following ROS2 node and the rosbridge should be up and running, waiting for SVL Simulator to connect. Prerequisites top # Docker CE NVIDIA Docker NVIDIA graphics card (required for training/inference with GPU) Setup top # Installing Docker CE top # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing NVIDIA Docker top # Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling Docker Image top # docker pull lgsvl/lanefollowing:latest What's inside Docker Image # Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS2 Dashing + rosbridge Jupyter Notebook Features top # Training mode: Manually drive the vehicle and collect data Autonomous Mode: The vehicle drives itself based on Lane Following model trained from the collected data ROS2-based Time synchronous data collection node deploying a trained model in a node Data preprocessing for training Data normalization Data augmentation Splitting data into training set and test set Writing/Reading data in HDF5 format Deep Learning model training: Train a model using Keras with TensorFlow backend Training Details top # Network Architecture top # The network has 559,419 parameters and consists of 9 layers, including 5 convolutional layers, 3 fully connected layers, and an output layer. Layer (type) Output Shape Param # lambda_1 (Lambda) (None, 70, 320, 3) 0 conv2d_1 (Conv2D) (None, 33, 158, 24) 1824 conv2d_2 (Conv2D) (None, 15, 77, 36) 21636 conv2d_3 (Conv2D) (None, 6, 37, 48) 43248 conv2d_4 (Conv2D) (None, 4, 35, 64) 27712 conv2d_5 (Conv2D) (None, 2, 33, 64) 36928 dropout_1 (Dropout) (None, 2, 33, 64) 0 flatten_1 (Flatten) (None, 4224) 0 dense_1 (Dense) (None, 100) 422500 dense_2 (Dense) (None, 50) 5050 dense_3 (Dense) (None, 10) 510 dense_4 (Dense) (None, 1) 11 Hyperparameters top # Learning rate: 1e-04 Learning rate decay: None Dropout rate: 0.5 Mini-batch size: 128 Epochs: 30 Optimization algorithm: Adam Loss function: Mean squared error Training/Test set ratio: 8:2 Dataset top # Number of training data: 48,624 labeled images Number of validation data: 12,156 labeled images Center Image top # Left Image top # Right Image top # Original Image top # Cropped Image top # Data Distribution top # How to Collect Data and Train Your Own Model with SVL Simulator top # Collect data from SVL Simulator top # To collect camera images as well as corresponding steering commands for training, we provide a ROS2 collect node which subscribes to three camera image topics and a control command topic, approximately synchronizes time stamps of those messages, and then saves them as csv and jpg files. The topic names and types are as below: - Center camera: /simulator/sensor/camera/center/compressed (sensor_msgs/CompressedImage) - Left camera: /simulator/sensor/camera/left/compressed (sensor_msgs/CompressedImage) - Right camera: /simulator/sensor/camera/right/compressed (sensor_msgs/CompressedImage) - Control command: /simulator/control/command (lgsvl_msgs/CanBusData) Complete sensor JSON configuration top # [ { \"type\": \"LaneFollowingSensor\", \"name\": \"Lane Following Sensor\", \"params\": { \"Topic\": \"/lanefollowing/steering_cmd\" } }, { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 15, \"Topic\": \"/simulator/control/command\" } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"ColorCameraSensor\", \"name\": \"Center Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/sensor/camera/center/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Left Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/sensor/camera/left/compressed\" }, \"transform\": { \"x\": -0.8, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Right Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/sensor/camera/right/compressed\" }, \"transform\": { \"x\": 0.8, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ] To drive a car and publish messages over rosbridge in training mode: - Launch SVL Simulator - Create a simulation in Random Traffic mode - Select your preferred map (e.g., San Francisco ) for training - Select your preferred vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration above - Make sure your vehicle has ROS2 bridge in the sensor setup - Run your simulation Finally, to launch rosbridge and collect ROS2 node in a terminal: docker-compose up collect The node will start collecting data as you drive the car around. You should be able to check log messages in the terminal where the collect node is running. The final data is saved in lanefollowing/ros2_ws/src/lane_following/train/data/ as csv and jpg files. Data preprocessing top # Before start training your model with the data you collected, data preprocessing is required. This task includes: - Resize image resolutions from 1920 x 1080 to 200 x 112 - Crop top portion of images as we are mostly interested in road part of an image - Data augmentation by adding artificial bias to side camera images or flipping images - Data normalization to help the model converge faster - Split data into training and testing dataset To run data preprocessing and obtain datasets for training: docker-compose up preprocess This will preprocess your data and write outputs in lanefollowing/ros2_ws/src/lane_following/train/data/hdf5/ into HDF5 format for better I/O performance for training. Train a model top # We use Keras with TensorFlow backend for training our model as an example. The hyperparameters such as learning rate, batch size, or number of epochs were chosen empirically. You can train a model as is but you are also welcome to modify the model architecture or any hyperparameters as you like in the code. To start training: docker-compose up train After training is done, your final trained model will be in lanefollowing/ros2_ws/src/lane_following/train/model/{current-date-and-time-in-utc}.h5 and your model is ready to drive autonomously. Drive with your trained model in SVL Simulator top # Now, it's time to deploy your trained model and test drive with it using SVL Simulator. You can replace your trained model with an existing one in lanefollowing/ros2_ws/src/lane_following/model/model.h5 as this is the path for deployment. To drive a car in autonomous mode: - Launch SVL Simulator - Create a simulation in Random Traffic mode - Select your preferred map (e.g., SingleLaneRoad ) for driving - Select your preferred vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration above - You are free to remove both side cameras from the sensor JSON as the model only uses the center camera in driving mode - Make sure your vehicle has ROS2 bridge in the sensor setup - Run your simulation To launch rosbridge and drive ROS2 node in a terminal: docker-compose up drive Or, if you want visualizations as well, run drive_visual instead: docker-compose up drive_visual Your car will start driving autonomously and try to mimic your driving behavior when training the model. Note that the model only controls steering inputs as you drive your vehicle forward. Future Works and Contributing top # Though the network can successfully drive and follow lanes on the bridge, there's still a lot of room for future improvements (i.e., biased to drive straight, afraid of shadows, few training data, and etc). - To improve model robustness collect more training data by driving in a wide variety of environments - Changing weather and lighting effects (rain, fog, road wetness, time of day) - Adding more road layouts and road textures - Adding more shadows on roads - Adding NPC cars around the ego vehicle - Predict the car throttle along with the steering angle - Take into accounts time series analysis using RNN (Recurrent Neural Network) References top # Lane Following Github Repository Lane Following Sensor SVL Simulator NVIDIA's End-to-End Deep Learning Model for Self-Driving Cars","title":"Deep learning lane following model"},{"location":"tutorials/lane-following/#video","text":"","title":"Video"},{"location":"tutorials/lane-following/#table-of-contents","text":"Getting Started Prerequisites Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Features Training Details Network Architecture Hyperparameters Dataset How to Collect Data and Train Your Own Model with SVL Simulator Collect data from SVL Simulator Data preprocessing Train a model Drive with your trained model in SVL Simulator Future Works and Contributing References","title":"Table of Contents"},{"location":"tutorials/lane-following/#getting-started","text":"First, clone this repository: git clone --recurse-submodules https://github.com/lgsvl/lanefollowing.git Next, pull the latest Docker image: docker pull lgsvl/lanefollowing:latest To build ROS2 packages: docker-compose up build_ros Now, launch the lane following model: docker-compose up drive (Optional) If you want visualizations, run drive_visual instead of drive: docker-compose up drive_visual That's it! Now, the lane following ROS2 node and the rosbridge should be up and running, waiting for SVL Simulator to connect.","title":"Getting Started"},{"location":"tutorials/lane-following/#prerequisites","text":"Docker CE NVIDIA Docker NVIDIA graphics card (required for training/inference with GPU)","title":"Prerequisites"},{"location":"tutorials/lane-following/#setup","text":"","title":"Setup"},{"location":"tutorials/lane-following/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"tutorials/lane-following/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing NVIDIA Docker"},{"location":"tutorials/lane-following/#pulling-docker-image","text":"docker pull lgsvl/lanefollowing:latest","title":"Pulling Docker Image"},{"location":"tutorials/lane-following/#whats-inside-docker-image","text":"Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS2 Dashing + rosbridge Jupyter Notebook","title":"What's inside Docker Image"},{"location":"tutorials/lane-following/#features","text":"Training mode: Manually drive the vehicle and collect data Autonomous Mode: The vehicle drives itself based on Lane Following model trained from the collected data ROS2-based Time synchronous data collection node deploying a trained model in a node Data preprocessing for training Data normalization Data augmentation Splitting data into training set and test set Writing/Reading data in HDF5 format Deep Learning model training: Train a model using Keras with TensorFlow backend","title":"Features"},{"location":"tutorials/lane-following/#training-details","text":"","title":"Training Details"},{"location":"tutorials/lane-following/#network-architecture","text":"The network has 559,419 parameters and consists of 9 layers, including 5 convolutional layers, 3 fully connected layers, and an output layer. Layer (type) Output Shape Param # lambda_1 (Lambda) (None, 70, 320, 3) 0 conv2d_1 (Conv2D) (None, 33, 158, 24) 1824 conv2d_2 (Conv2D) (None, 15, 77, 36) 21636 conv2d_3 (Conv2D) (None, 6, 37, 48) 43248 conv2d_4 (Conv2D) (None, 4, 35, 64) 27712 conv2d_5 (Conv2D) (None, 2, 33, 64) 36928 dropout_1 (Dropout) (None, 2, 33, 64) 0 flatten_1 (Flatten) (None, 4224) 0 dense_1 (Dense) (None, 100) 422500 dense_2 (Dense) (None, 50) 5050 dense_3 (Dense) (None, 10) 510 dense_4 (Dense) (None, 1) 11","title":"Network Architecture"},{"location":"tutorials/lane-following/#hyperparameters","text":"Learning rate: 1e-04 Learning rate decay: None Dropout rate: 0.5 Mini-batch size: 128 Epochs: 30 Optimization algorithm: Adam Loss function: Mean squared error Training/Test set ratio: 8:2","title":"Hyperparameters"},{"location":"tutorials/lane-following/#dataset","text":"Number of training data: 48,624 labeled images Number of validation data: 12,156 labeled images","title":"Dataset"},{"location":"tutorials/lane-following/#center-image","text":"","title":"Center Image"},{"location":"tutorials/lane-following/#left-image","text":"","title":"Left Image"},{"location":"tutorials/lane-following/#right-image","text":"","title":"Right Image"},{"location":"tutorials/lane-following/#original-image","text":"","title":"Original Image"},{"location":"tutorials/lane-following/#cropped-image","text":"","title":"Cropped Image"},{"location":"tutorials/lane-following/#data-distribution","text":"","title":"Data Distribution"},{"location":"tutorials/lane-following/#how-to-collect-data-and-train-your-own-model-with-svl-simulator","text":"","title":"How to Collect Data and Train Your Own Model with SVL Simulator"},{"location":"tutorials/lane-following/#collect-data-from-svl-simulator","text":"To collect camera images as well as corresponding steering commands for training, we provide a ROS2 collect node which subscribes to three camera image topics and a control command topic, approximately synchronizes time stamps of those messages, and then saves them as csv and jpg files. The topic names and types are as below: - Center camera: /simulator/sensor/camera/center/compressed (sensor_msgs/CompressedImage) - Left camera: /simulator/sensor/camera/left/compressed (sensor_msgs/CompressedImage) - Right camera: /simulator/sensor/camera/right/compressed (sensor_msgs/CompressedImage) - Control command: /simulator/control/command (lgsvl_msgs/CanBusData)","title":"Collect data from SVL Simulator"},{"location":"tutorials/lane-following/#complete-sensor-json-configuration","text":"[ { \"type\": \"LaneFollowingSensor\", \"name\": \"Lane Following Sensor\", \"params\": { \"Topic\": \"/lanefollowing/steering_cmd\" } }, { \"type\": \"CanBusSensor\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 15, \"Topic\": \"/simulator/control/command\" } }, { \"type\": \"KeyboardControlSensor\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"ColorCameraSensor\", \"name\": \"Center Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/sensor/camera/center/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Left Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/sensor/camera/left/compressed\" }, \"transform\": { \"x\": -0.8, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"ColorCameraSensor\", \"name\": \"Right Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 2000, \"Topic\": \"/simulator/sensor/camera/right/compressed\" }, \"transform\": { \"x\": 0.8, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ] To drive a car and publish messages over rosbridge in training mode: - Launch SVL Simulator - Create a simulation in Random Traffic mode - Select your preferred map (e.g., San Francisco ) for training - Select your preferred vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration above - Make sure your vehicle has ROS2 bridge in the sensor setup - Run your simulation Finally, to launch rosbridge and collect ROS2 node in a terminal: docker-compose up collect The node will start collecting data as you drive the car around. You should be able to check log messages in the terminal where the collect node is running. The final data is saved in lanefollowing/ros2_ws/src/lane_following/train/data/ as csv and jpg files.","title":"Complete sensor JSON configuration"},{"location":"tutorials/lane-following/#data-preprocessing","text":"Before start training your model with the data you collected, data preprocessing is required. This task includes: - Resize image resolutions from 1920 x 1080 to 200 x 112 - Crop top portion of images as we are mostly interested in road part of an image - Data augmentation by adding artificial bias to side camera images or flipping images - Data normalization to help the model converge faster - Split data into training and testing dataset To run data preprocessing and obtain datasets for training: docker-compose up preprocess This will preprocess your data and write outputs in lanefollowing/ros2_ws/src/lane_following/train/data/hdf5/ into HDF5 format for better I/O performance for training.","title":"Data preprocessing"},{"location":"tutorials/lane-following/#train-a-model","text":"We use Keras with TensorFlow backend for training our model as an example. The hyperparameters such as learning rate, batch size, or number of epochs were chosen empirically. You can train a model as is but you are also welcome to modify the model architecture or any hyperparameters as you like in the code. To start training: docker-compose up train After training is done, your final trained model will be in lanefollowing/ros2_ws/src/lane_following/train/model/{current-date-and-time-in-utc}.h5 and your model is ready to drive autonomously.","title":"Train a model"},{"location":"tutorials/lane-following/#drive-with-your-trained-model-in-svl-simulator","text":"Now, it's time to deploy your trained model and test drive with it using SVL Simulator. You can replace your trained model with an existing one in lanefollowing/ros2_ws/src/lane_following/model/model.h5 as this is the path for deployment. To drive a car in autonomous mode: - Launch SVL Simulator - Create a simulation in Random Traffic mode - Select your preferred map (e.g., SingleLaneRoad ) for driving - Select your preferred vehicle (e.g., Lincoln2017MKZ ) with the sensor configuration above - You are free to remove both side cameras from the sensor JSON as the model only uses the center camera in driving mode - Make sure your vehicle has ROS2 bridge in the sensor setup - Run your simulation To launch rosbridge and drive ROS2 node in a terminal: docker-compose up drive Or, if you want visualizations as well, run drive_visual instead: docker-compose up drive_visual Your car will start driving autonomously and try to mimic your driving behavior when training the model. Note that the model only controls steering inputs as you drive your vehicle forward.","title":"Drive with your trained model in SVL Simulator"},{"location":"tutorials/lane-following/#future-works-and-contributing","text":"Though the network can successfully drive and follow lanes on the bridge, there's still a lot of room for future improvements (i.e., biased to drive straight, afraid of shadows, few training data, and etc). - To improve model robustness collect more training data by driving in a wide variety of environments - Changing weather and lighting effects (rain, fog, road wetness, time of day) - Adding more road layouts and road textures - Adding more shadows on roads - Adding NPC cars around the ego vehicle - Predict the car throttle along with the steering angle - Take into accounts time series analysis using RNN (Recurrent Neural Network)","title":"Future Works and Contributing"},{"location":"tutorials/lane-following/#references","text":"Lane Following Github Repository Lane Following Sensor SVL Simulator NVIDIA's End-to-End Deep Learning Model for Self-Driving Cars","title":"References"},{"location":"tutorials/modular-testing/","text":"Modular Testing Our 3D Ground Truth sensor and Signal sensor now publish ground truth perception data to Apollo stack via CyberRT bridge or ROS Apollo bridge. The modular testing feature is useful for testing planning module of Apollo stack based on the assumption that the perception output is 100% accurate without any errors. In other words, we can completely bypass Apollo's perception modules (i.e., object detection and traffic light detection) and use ground truth labels for perception published by our simulator instead. Video: Modular Testing Introduction # Video: How to run modular testing random traffic simulation from cloud-based web UI # Sensor Specifications top # 3D Ground Truth Sensor (more details) # 3D Ground Truth sensor replaces Apollo's object detection module. It detects every NPC including vehicles and pedestrians around the EGO vehicle within a distance, which can be specified using a sensor parameter MaxDistance , and publishes ground truth labels such as 3D bounding boxes for the detected objects. Major message fields: ID: integer ID for an object Timestamp: time in seconds since Unix epoch Velocity: object velocity vector in m/s Acceleration: object acceleration vector in m/s Width: object width in meters Length: object length in meters Height: object height in meters Type: vehicle or pedestrian Position: easting, northing, altitude in GPS position Theta: object heading in radians Tracking time: duration of detection in seconds Polygon point: corner points for an object Sensor parameters: Frequency: 10 MaxDistance: 100 Topic: /apollo/perception/obstacles Signal Sensor (more details) # Signal sensor replaces Apollo's traffic light detection module. It detects every signal that is connected to the current lane of the EGO vehicle within a distance, which can also be specified using a sensor parameter MaxDistance , and then publishes the signal IDs as well as the current signal color information. It is important to note that the signal ID coming from the map annotation in the simulator must match with the ID in the Apollo HD map you're currently using in Apollo, because Apollo uses this ID to locate the detected traffic light in the map. Major message fields: ID: traffic light string ID must match with the signal ID in Apollo HD map Timestamp: time in seconds since Unix epoch Color: red, yellow, green, or black Blink: is traffic light blinking Sensor parameters: Frequency: 10 MaxDistance: 100 Topic: /apollo/perception/traffic_light Sensor JSON top # The following sensor parameters can be added to a vehicle sensor configuration (and can replace the LiDAR and camera sensors which will improve simulation performance). Refer to Vehicles in My Library for more information on vehicle sensor configuration. { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/traffic_light\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } How to Run Apollo Modular Testing on GoMentumDTL Map top {: #how-to-run-apollo-modular-testing-on- gomentumdtl -map data-toc-label='How to Run Apollo Modular Testing on GoMentumDTL Map'} # Configure and run simulator Map Choose GoMentumDTL map Vehicle Choose Lincoln2017MKZ vehicle for Apollo Attach 3D Ground Truth sensor and Signal sensor with the sensor parameters above Optionally, you can remove Color Camera and LiDAR sensors from the sensor configuration as these sensors are not required for modular testing Run simulation with Random Traffic enabled Launch Apollo following these instructions Make sure that you're receiving ground truth messages via the perception topics on Apollo side Topic for object detection: /apollo/perception/obstacles Topic for traffic light detection: /apollo/perception/traffic_light Run cyber_monitor for CyberRT bridge or rostopic echo for ROS Apollo bridge inside Apollo container In Dreamview, enable Localization module and verify if you can visually see that: Your EGO vehicle is placed on the right position on the Apollo HD map for GoMentumDTL Bounding boxes for vehicles and pedestrians around the EGO vehicle (if any) including heading and velocity vectors Traffic light signals that are connected to the current lane of EGO vehicle are detected Enable Transform , Prediction , Planning , Routing , and Control Note that we are not running Perception and Traffic Light modules for modular testing Apollo should now be able to drive to your destination without running Perception modules","title":"Modular testing with the Apollo AD stack"},{"location":"tutorials/modular-testing/#video-modular-testing-introduction","text":"","title":"Video: Modular Testing Introduction"},{"location":"tutorials/modular-testing/#video-how-to-run-modular-testing-random-traffic-simulation-from-cloud-based-web-ui","text":"","title":"Video: How to run modular testing random traffic simulation from cloud-based web UI"},{"location":"tutorials/modular-testing/#sensor-specifications","text":"","title":"Sensor Specifications"},{"location":"tutorials/modular-testing/#3d-ground-truth-sensor-more-details","text":"3D Ground Truth sensor replaces Apollo's object detection module. It detects every NPC including vehicles and pedestrians around the EGO vehicle within a distance, which can be specified using a sensor parameter MaxDistance , and publishes ground truth labels such as 3D bounding boxes for the detected objects. Major message fields: ID: integer ID for an object Timestamp: time in seconds since Unix epoch Velocity: object velocity vector in m/s Acceleration: object acceleration vector in m/s Width: object width in meters Length: object length in meters Height: object height in meters Type: vehicle or pedestrian Position: easting, northing, altitude in GPS position Theta: object heading in radians Tracking time: duration of detection in seconds Polygon point: corner points for an object Sensor parameters: Frequency: 10 MaxDistance: 100 Topic: /apollo/perception/obstacles","title":"3D Ground Truth Sensor (more details)"},{"location":"tutorials/modular-testing/#signal-sensor-more-details","text":"Signal sensor replaces Apollo's traffic light detection module. It detects every signal that is connected to the current lane of the EGO vehicle within a distance, which can also be specified using a sensor parameter MaxDistance , and then publishes the signal IDs as well as the current signal color information. It is important to note that the signal ID coming from the map annotation in the simulator must match with the ID in the Apollo HD map you're currently using in Apollo, because Apollo uses this ID to locate the detected traffic light in the map. Major message fields: ID: traffic light string ID must match with the signal ID in Apollo HD map Timestamp: time in seconds since Unix epoch Color: red, yellow, green, or black Blink: is traffic light blinking Sensor parameters: Frequency: 10 MaxDistance: 100 Topic: /apollo/perception/traffic_light","title":"Signal Sensor (more details)"},{"location":"tutorials/modular-testing/#sensor-json","text":"The following sensor parameters can be added to a vehicle sensor configuration (and can replace the LiDAR and camera sensors which will improve simulation performance). Refer to Vehicles in My Library for more information on vehicle sensor configuration. { \"type\": \"PerceptionSensor3D\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/obstacles\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"SignalSensor\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/apollo/perception/traffic_light\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Sensor JSON"},{"location":"tutorials/modular-testing/#how-to-run-apollo-modular-testing-on-gomentumdtl-map-top-how-to-run-apollo-modular-testing-on-gomentumdtl-map-data-toc-labelhow-to-run-apollo-modular-testing-on-gomentumdtl-map","text":"Configure and run simulator Map Choose GoMentumDTL map Vehicle Choose Lincoln2017MKZ vehicle for Apollo Attach 3D Ground Truth sensor and Signal sensor with the sensor parameters above Optionally, you can remove Color Camera and LiDAR sensors from the sensor configuration as these sensors are not required for modular testing Run simulation with Random Traffic enabled Launch Apollo following these instructions Make sure that you're receiving ground truth messages via the perception topics on Apollo side Topic for object detection: /apollo/perception/obstacles Topic for traffic light detection: /apollo/perception/traffic_light Run cyber_monitor for CyberRT bridge or rostopic echo for ROS Apollo bridge inside Apollo container In Dreamview, enable Localization module and verify if you can visually see that: Your EGO vehicle is placed on the right position on the Apollo HD map for GoMentumDTL Bounding boxes for vehicles and pedestrians around the EGO vehicle (if any) including heading and velocity vectors Traffic light signals that are connected to the current lane of EGO vehicle are detected Enable Transform , Prediction , Planning , Routing , and Control Note that we are not running Perception and Traffic Light modules for modular testing Apollo should now be able to drive to your destination without running Perception modules","title":"How to Run Apollo Modular Testing on GoMentumDTL Map top {: #how-to-run-apollo-modular-testing-on-gomentumdtl-map data-toc-label='How to Run Apollo Modular Testing on GoMentumDTL Map'}"},{"location":"user-interface/bridge-connection-ui/","text":"Bridge Connection UI When in a non-Headless Simulation, a list of published and subscribed topics can be found in the Simulator menu (plug icon). At the top of the menu is the selected vehicle. The bridge status can be: Disconnected , Connecting , or Connected The bridge address is the same that was entered as the Bridge Connection String when creating the Simulation. Each topic is then listed in the following format: PUB or SUB : indicates if the Simulator publishes or subscribes to messages on this topic Topic : is the topic that the messages are published/subscribed to Type : is the message type on this topic Count : is the total number of messages published/received when the bridge was connected","title":"Bridge connection UI"},{"location":"user-interface/config-and-cmd-line-params/","text":"Configuration File and Command Line Parameters Configuration File top # The Simulator configuration file config.yml includes parameters shared between different users and allows administrators to setup deployment specific settings. The list of supported configuration parameters is below: Parameter Name Type Default Value Description api_hostname string localhost The IP address of the network interface (or the hostname assigned to it) on which Simulator should listen for Python API commands. Specify \"*\" to listen on all network interfaces. api_port integer 8181 Port number used by Python API to connect. cloud_url string https://wise.svlsimulator.com Address of the web user interface. cloud_proxy string - URL pointing to HTTP proxy server for connecting to the web user interface. data_path string OS dependent set by Unity Local database path headless bool false Whether or not simulator should work in headless mode only. read_only bool false Whether or not the user is allowed to change anything in the database. This mode is used to run Simulator in public demo mode. To use a custom config.yml file, include it in the same folder as Simulator.exe or modify the one in the root directory of the cloned project. The file is expected to be in YAML format. Command Line Parameters top # Simulator accepts command line parameters during start up. These command line parameters override the values from the configuration file. The list of supported command line parameters is below: Parameter Name Argument Type Default Value Description --apihost string localhost The IP address of the network interface (or the hostname assigned to it) on which Simulator should listen for Python API commands. Specify \"*\" to listen on all network interfaces. --apiport integer 8181 Port number used by Python API to connect. --cloudurl string https://wise.svlsimulator.com Address of the web user interface. --data or -d string OS dependent set by Unity Local database path - - retryForever (none) - If present, Simulator attempts to connect to the web user interface indefinitely. --simid string auto gen To overwrite simid in database or set on first start.","title":"Configuration file and command line parameters"},{"location":"user-interface/config-and-cmd-line-params/#configuration-file","text":"The Simulator configuration file config.yml includes parameters shared between different users and allows administrators to setup deployment specific settings. The list of supported configuration parameters is below: Parameter Name Type Default Value Description api_hostname string localhost The IP address of the network interface (or the hostname assigned to it) on which Simulator should listen for Python API commands. Specify \"*\" to listen on all network interfaces. api_port integer 8181 Port number used by Python API to connect. cloud_url string https://wise.svlsimulator.com Address of the web user interface. cloud_proxy string - URL pointing to HTTP proxy server for connecting to the web user interface. data_path string OS dependent set by Unity Local database path headless bool false Whether or not simulator should work in headless mode only. read_only bool false Whether or not the user is allowed to change anything in the database. This mode is used to run Simulator in public demo mode. To use a custom config.yml file, include it in the same folder as Simulator.exe or modify the one in the root directory of the cloned project. The file is expected to be in YAML format.","title":"Configuration File"},{"location":"user-interface/config-and-cmd-line-params/#command-line-parameters","text":"Simulator accepts command line parameters during start up. These command line parameters override the values from the configuration file. The list of supported command line parameters is below: Parameter Name Argument Type Default Value Description --apihost string localhost The IP address of the network interface (or the hostname assigned to it) on which Simulator should listen for Python API commands. Specify \"*\" to listen on all network interfaces. --apiport integer 8181 Port number used by Python API to connect. --cloudurl string https://wise.svlsimulator.com Address of the web user interface. --data or -d string OS dependent set by Unity Local database path - - retryForever (none) - If present, Simulator attempts to connect to the web user interface indefinitely. --simid string auto gen To overwrite simid in database or set on first start.","title":"Command Line Parameters"},{"location":"user-interface/keyboard-shortcuts/","text":"Simulator Controls Key Bindings top # Officially supported: # F1 - Help menu \u2190 \u2191 \u2193 \u2192 - Drive vehicle forward/brake, turn 1 - 0 - Agent select and follow cam Camera Controls top # Mouse Right-Click hold & drag - Look/Rotate ~ - Free Camera W S - Zoom A D - Strafe Q E - Up/Down For developer use: # N - Toggle Non-Player Character (NPC) vehicles P - Toggle Pedestrians End - Toggle ignition (must be on for vehicle to move) Page Up - Shift to forward drive gear Page Down - Shift to reverse gear F12 - Reset Active Vehicle location Miscellaneous top # H - Toggle headlights F - Toggle fog lights I - Toggle interior light Right Shift - Toggle parking brake (needs to be off for vehicle to move) M - Toggle hazard lights < - Toggle left blinker > - Toggle right blinker Logitech G920 Wheel Inputs top # Right Pedal RB - Accelerate Middle Pedal LB - Brake Rotate Wheel - Turn A - Cycle through Agents B - Toggle Non-Player Character (NPC) vehicles X - Toggle Sensor Visualizers Y - Cycle through Camera states (Follow camera, Cinematic camera, Free camera) Start - [Hamburger Menu/3 horizonal lines] Pause Simulation (Interactive Simulation Only) Select - [Overlapping Squares] Open Menu RSB - Cycle through Agent headlight states LSB - Toggle Reverse Center - [Xbox] Toggle Interior Light (if available)","title":"Keyboard shortcuts"},{"location":"user-interface/keyboard-shortcuts/#key-bindings","text":"","title":"Key Bindings"},{"location":"user-interface/keyboard-shortcuts/#officially-supported","text":"F1 - Help menu \u2190 \u2191 \u2193 \u2192 - Drive vehicle forward/brake, turn 1 - 0 - Agent select and follow cam","title":"Officially supported:"},{"location":"user-interface/keyboard-shortcuts/#camera-controls","text":"Mouse Right-Click hold & drag - Look/Rotate ~ - Free Camera W S - Zoom A D - Strafe Q E - Up/Down","title":"Camera Controls"},{"location":"user-interface/keyboard-shortcuts/#for-developer-use","text":"N - Toggle Non-Player Character (NPC) vehicles P - Toggle Pedestrians End - Toggle ignition (must be on for vehicle to move) Page Up - Shift to forward drive gear Page Down - Shift to reverse gear F12 - Reset Active Vehicle location","title":"For developer use:"},{"location":"user-interface/keyboard-shortcuts/#miscellaneous","text":"H - Toggle headlights F - Toggle fog lights I - Toggle interior light Right Shift - Toggle parking brake (needs to be off for vehicle to move) M - Toggle hazard lights < - Toggle left blinker > - Toggle right blinker","title":"Miscellaneous"},{"location":"user-interface/keyboard-shortcuts/#logitech-g920-wheel-inputs","text":"Right Pedal RB - Accelerate Middle Pedal LB - Brake Rotate Wheel - Turn A - Cycle through Agents B - Toggle Non-Player Character (NPC) vehicles X - Toggle Sensor Visualizers Y - Cycle through Camera states (Follow camera, Cinematic camera, Free camera) Start - [Hamburger Menu/3 horizonal lines] Pause Simulation (Interactive Simulation Only) Select - [Overlapping Squares] Open Menu RSB - Cycle through Agent headlight states LSB - Toggle Reverse Center - [Xbox] Toggle Interior Light (if available)","title":"Logitech G920 Wheel Inputs"},{"location":"user-interface/sensor-visualizers/","text":"Sensor Visualizers When in a non-Headless Simulation, sensor visualizers can be toggled from the menu. To visualize a sensor, click the \"eye\" next to the sensor name. For each sensor, you can click the \"coordinate\" icon next to the sensor name to turn on the visualization of the sensor's transform. This includes the sensor and parent name and transform position/rotation. A white line shows the parent/child relationship. Sensors are identified by the name parameter from the JSON configuration. For full details on the possible JSON parameters see Sensor Parameters Not all sensors have visualizations available, only sensors who have will show their visualizations. Table of Contents Cameras Color Camera Depth Camera Segmentation Camera 2D Ground Truth LiDAR Radar 3D Ground Truth Lane-line Sensor Cameras top # When a camera is visualized, the image the sensor see is visualized in a window. This window can be resized by clicking-and-dragging the icon in the bottom right corner and can be made full-screen with the box icon in the top right corner. The window can be moved by clicking-and-dragging the top bar. To close the window, either click the X or click the \"eye\" button again. Color Camera top # Visualized Color camera shows the same things that are visible from the normal follow and free cameras, defined in the sensor configuration. Depth Camera top # Visualized Depth camera shows objects colored on a grayscale based on the distance between the camera and the object. Segmentation Camera top # Visualized Segmentation camera shows objects colored according to the object tag. Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F If a tag is included in \"InstanceSegmentationTags\", each instance of objects with that tag is colored differently, but all will have same hue. 2D Ground Truth top # Visualized 2D Ground Truth shows the same things as a color camera except pedestrians are enclosed in a yellow wire box and NPCs are enclosed in a green wire box. LiDAR top # Visualized LiDAR shows the point cloud that is detected. Radar top # Visualized Radar shows the radar cones and creates wireframe boxes enclosing NPCs in a green box, bicycles in a cyan box, and other EGOs in a magenta box. 3D Ground Truth top # Visualized 3D Ground Truth creates wireframe boxes enclosing pedestrians in a yellow box and NPCs in a green box. Lane-line Sensor top # Visualized Lane-line Sensor shows lines for current lane as an overlay for color image, with a perspective defined in JSON parameters.","title":"Sensor visualizers"},{"location":"user-interface/sensor-visualizers/#cameras","text":"When a camera is visualized, the image the sensor see is visualized in a window. This window can be resized by clicking-and-dragging the icon in the bottom right corner and can be made full-screen with the box icon in the top right corner. The window can be moved by clicking-and-dragging the top bar. To close the window, either click the X or click the \"eye\" button again.","title":"Cameras"},{"location":"user-interface/sensor-visualizers/#color-camera","text":"Visualized Color camera shows the same things that are visible from the normal follow and free cameras, defined in the sensor configuration.","title":"Color Camera"},{"location":"user-interface/sensor-visualizers/#depth-camera","text":"Visualized Depth camera shows objects colored on a grayscale based on the distance between the camera and the object.","title":"Depth Camera"},{"location":"user-interface/sensor-visualizers/#segmentation-camera","text":"Visualized Segmentation camera shows objects colored according to the object tag. Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F If a tag is included in \"InstanceSegmentationTags\", each instance of objects with that tag is colored differently, but all will have same hue.","title":"Segmentation Camera"},{"location":"user-interface/sensor-visualizers/#2d-ground-truth","text":"Visualized 2D Ground Truth shows the same things as a color camera except pedestrians are enclosed in a yellow wire box and NPCs are enclosed in a green wire box.","title":"2D Ground Truth"},{"location":"user-interface/sensor-visualizers/#lidar","text":"Visualized LiDAR shows the point cloud that is detected.","title":"LiDAR"},{"location":"user-interface/sensor-visualizers/#radar","text":"Visualized Radar shows the radar cones and creates wireframe boxes enclosing NPCs in a green box, bicycles in a cyan box, and other EGOs in a magenta box.","title":"Radar"},{"location":"user-interface/sensor-visualizers/#3d-ground-truth","text":"Visualized 3D Ground Truth creates wireframe boxes enclosing pedestrians in a yellow box and NPCs in a green box.","title":"3D Ground Truth"},{"location":"user-interface/sensor-visualizers/#lane-line-sensor","text":"Visualized Lane-line Sensor shows lines for current lane as an overlay for color image, with a perspective defined in JSON parameters.","title":"Lane-line Sensor"},{"location":"user-interface/simulation-menu/","text":"Simulation Menu When in a non-Headless Simulation, a menu can be accessed by clicking on the \"hamburger\" menu icon in the bottom left. Simulation Menu Contents Toggle Menu Bar Pause/Play Button Stop Menu Sensor Menu Interactive Menu Bridge Menu Controls Menu Information Menu Simulation Time Camera Mode Vehicle Select Menu 1) Toggle Menu Bar top # This button opens or closes the simulator menu bar. 2) Pause/Play Button top # This button pauses or plays the simulation. 3) Stop Menu top # This menu is accessed from the Stop button. Confirm or cancel stopping the simulation. Confirmation exits simulator to the main screen. 4) Sensor Menu top # This menu is accessed from the \"eye\" button. It lists all sensors on the selected vehicle and allows for the sensors to be visualized, as well as their coordinate transform relative to the parent transform sensor or BaseLink transform. See Sensor Visualization for more details. 4a. Sensor Name 4b. Toggle buttons to visualize sensor transform or sensor data 4c. Full screen window or close window buttons 4d. Click and drag to manually resize window 4e. Example of sensor data visualization with GroundTruth3D sensor 4f. Example of sensor transform information 5) Interactive Menu top # This menu is only available in an Interactive Simulation and is accessed from the \"sliders\" button. It contains tools to change the environment of the simulation at run-time. 5a. Time of day slider with a toggle button for freezing time or incrementing time. 5b. Environment effect parameter sliders. 5c. Toggles for NPC vehicles or Pedestrians. 6) Bridge Menu top # This menu is accessed from the \"plug\" button. It lists information on the bridge status as well as all published and subscribed topics. See Bridge Topics for more details. 7) Controls Menu top # This menu is accessed from the \"controller\" button. It lists all the keyboard commands in the simulation. Keyboard control for vehicles requires the Keyboard sensor. See Keyboard Shortcuts for more details. 8) Information Menu top # This menu is accessed from the i button. It lists the build info as well as any errors, logs, or warnings that are created in the current simulation. To clear all of these messages, click the \"Trash can\" icon in the bottom right of the menu. 8a. Build info and console log data 8b. Trash button to delete console log data 8c. Frames per second display 9) Simulation Time top # This timer displays simulation time. {hh:mm:ss:ms} 10) Camera Mode top # The camera icon in the bottom right indicates if the camera mode is currently follow, cinematic or free-roam. Clicking the camera button cycles between the three camera modes. When switching back to the follow camera, the camera will automatically be positioned behind the active vehicle. Follow Mode: Camera remains centered on the selected vehicle. W and S zoom and Mouse RightClick rotates the view around the center of the car. If the view has not been manually rotated, the camera will stay behind the vehicle. Cinematic Mode: Camera rotates between static views and animated camera movement around the selected vehicle. Free-roam Mode: Camera can be moved freely around the map using camera controls. 11) Vehicle Select Menu top # The vehicle listed in the bottom right is the current active vehicle. This menu is affected by keyboard input 1-0. Selecting a different vehicle will change the view to the follow camera of the selected vehicle. The number preceding each vehicle corresponds to the number key to select the vehicle.","title":"Simulation menu"},{"location":"user-interface/simulation-menu/#toggle-menu-bar","text":"This button opens or closes the simulator menu bar.","title":"Toggle Menu Bar"},{"location":"user-interface/simulation-menu/#play-pause-button","text":"This button pauses or plays the simulation.","title":"Pause/Play Button"},{"location":"user-interface/simulation-menu/#stop-menu","text":"This menu is accessed from the Stop button. Confirm or cancel stopping the simulation. Confirmation exits simulator to the main screen.","title":"Stop Menu"},{"location":"user-interface/simulation-menu/#sensor-menu","text":"This menu is accessed from the \"eye\" button. It lists all sensors on the selected vehicle and allows for the sensors to be visualized, as well as their coordinate transform relative to the parent transform sensor or BaseLink transform. See Sensor Visualization for more details. 4a. Sensor Name 4b. Toggle buttons to visualize sensor transform or sensor data 4c. Full screen window or close window buttons 4d. Click and drag to manually resize window 4e. Example of sensor data visualization with GroundTruth3D sensor 4f. Example of sensor transform information","title":"Sensor Menu"},{"location":"user-interface/simulation-menu/#interactive-menu","text":"This menu is only available in an Interactive Simulation and is accessed from the \"sliders\" button. It contains tools to change the environment of the simulation at run-time. 5a. Time of day slider with a toggle button for freezing time or incrementing time. 5b. Environment effect parameter sliders. 5c. Toggles for NPC vehicles or Pedestrians.","title":"Interactive Menu"},{"location":"user-interface/simulation-menu/#bridge-menu","text":"This menu is accessed from the \"plug\" button. It lists information on the bridge status as well as all published and subscribed topics. See Bridge Topics for more details.","title":"Bridge Menu"},{"location":"user-interface/simulation-menu/#controls-menu","text":"This menu is accessed from the \"controller\" button. It lists all the keyboard commands in the simulation. Keyboard control for vehicles requires the Keyboard sensor. See Keyboard Shortcuts for more details.","title":"Controls Menu"},{"location":"user-interface/simulation-menu/#information-menu","text":"This menu is accessed from the i button. It lists the build info as well as any errors, logs, or warnings that are created in the current simulation. To clear all of these messages, click the \"Trash can\" icon in the bottom right of the menu. 8a. Build info and console log data 8b. Trash button to delete console log data 8c. Frames per second display","title":"Information Menu"},{"location":"user-interface/simulation-menu/#simulation-time","text":"This timer displays simulation time. {hh:mm:ss:ms}","title":"Simulation Time"},{"location":"user-interface/simulation-menu/#camera-mode","text":"The camera icon in the bottom right indicates if the camera mode is currently follow, cinematic or free-roam. Clicking the camera button cycles between the three camera modes. When switching back to the follow camera, the camera will automatically be positioned behind the active vehicle. Follow Mode: Camera remains centered on the selected vehicle. W and S zoom and Mouse RightClick rotates the view around the center of the car. If the view has not been manually rotated, the camera will stay behind the vehicle. Cinematic Mode: Camera rotates between static views and animated camera movement around the selected vehicle. Free-roam Mode: Camera can be moved freely around the map using camera controls.","title":"Camera Mode"},{"location":"user-interface/simulation-menu/#vehicle-select-menu","text":"The vehicle listed in the bottom right is the current active vehicle. This menu is affected by keyboard input 1-0. Selecting a different vehicle will change the view to the follow camera of the selected vehicle. The number preceding each vehicle corresponds to the number key to select the vehicle.","title":"Vehicle Select Menu"},{"location":"user-interface/simulator-main-menu/","text":"Simulator Main Menu Simulator has multiple options on the main screen. Users can link to the cloud, run online and offline simulations and open the Visual Scenario Editor. Linking top # This is the menu when the user is running the application for the first time. Users must link their hardware to the cloud as a cluster. Link To Cloud button will open the web browser to the sign up page. After signing in, users choose the name for the hardware cluster they are currently using and the simulator will be linked. NOTE If you have not signed up, do so first, then re-open the simulator and click the Link To Cloud. Online Mode top # Online mode is available after linking to the cloud is complete. Open Browser button with open the cloud user account page in the web browser. Visual Editor button will open the Visual Scenario Editor to create user scenarios. Online button will open a drop down menu. Go Offline will disconnect the user from the cloud and allow users to run previous run simulations without connecting. Unlink will disconnect the user from the cloud and unlink the cluster from the machine. Clear Cache will delete all local saved database information. Quit will close the application. Offline Mode top # When you select offline mode, only previously saved and run simulations can be run without triggering from the online web user interface. Make sure you have at least one simulation configuration set up on your account. Drop down menu for all currently cached simulations run on this machine. Play button for selected offline simulation. Offline button will open a drop down menu for returning to online mode.","title":"Simulator main menu"},{"location":"user-interface/simulator-main-menu/#linking","text":"This is the menu when the user is running the application for the first time. Users must link their hardware to the cloud as a cluster. Link To Cloud button will open the web browser to the sign up page. After signing in, users choose the name for the hardware cluster they are currently using and the simulator will be linked. NOTE If you have not signed up, do so first, then re-open the simulator and click the Link To Cloud.","title":"Linking"},{"location":"user-interface/simulator-main-menu/#online-mode","text":"Online mode is available after linking to the cloud is complete. Open Browser button with open the cloud user account page in the web browser. Visual Editor button will open the Visual Scenario Editor to create user scenarios. Online button will open a drop down menu. Go Offline will disconnect the user from the cloud and allow users to run previous run simulations without connecting. Unlink will disconnect the user from the cloud and unlink the cluster from the machine. Clear Cache will delete all local saved database information. Quit will close the application.","title":"Online Mode"},{"location":"user-interface/simulator-main-menu/#offline-mode","text":"When you select offline mode, only previously saved and run simulations can be run without triggering from the online web user interface. Make sure you have at least one simulation configuration set up on your account. Drop down menu for all currently cached simulations run on this machine. Play button for selected offline simulation. Offline button will open a drop down menu for returning to online mode.","title":"Offline Mode"},{"location":"user-interface/web/clusters/","text":"Clusters The Clusters page allows you to add a new cluster to your account and make it available for creating simulations and executing them. If you do not have at least one valid cluster created and linked to your account, you will not be able to create or run any simulations. A cluster is the physical computing setup used to run a single simulation. Clusters can either be \"local\", where they consist of one or more machines connected to the same LAN, or \"cloud\", where they are one or more machines in the cloud controlled by the \"SVL Web User Interface\". A simulation that you create must designate the specific cluster on which the SVL Simulator binary executable will run. For more details about the cluster and simulations refer to the Cluster Simulation Introduction . A tutorial video on local cluster creation can be found here . Adding a Local Cluster top # Open the SVL Simulator executable and click \"Link to Cloud\". If this button is not available, look in the top right to make sure the simulator is Online. If Offline, click the button to toggle. You may need to toggle off and on if a connection is not working. When the browser opens and you are taken to the SVL Simulator online interface, log in then navigate to \"Clusters\". Enter a name for your local cluster and click \"Create cluster\". Editing a Local Cluster top # To edit a local cluster, click the three-dotted menu at the top right corner of a cluster card, and then click \"Edit\". You can then rename your cluster, or if there are multiple instances attached to the cluster, delete an instance. Be sure to click \"Save\" after making any changes. Deleting a Local Cluster top # To delete a local cluster, click the three-dotted menu at the top right corner of a cluster card, and then click \"Delete\".","title":"Clusters"},{"location":"user-interface/web/clusters/#local-cluster","text":"Open the SVL Simulator executable and click \"Link to Cloud\". If this button is not available, look in the top right to make sure the simulator is Online. If Offline, click the button to toggle. You may need to toggle off and on if a connection is not working. When the browser opens and you are taken to the SVL Simulator online interface, log in then navigate to \"Clusters\". Enter a name for your local cluster and click \"Create cluster\".","title":"Local Cluster"},{"location":"user-interface/web/clusters/#editing-a-local-cluster","text":"To edit a local cluster, click the three-dotted menu at the top right corner of a cluster card, and then click \"Edit\". You can then rename your cluster, or if there are multiple instances attached to the cluster, delete an instance. Be sure to click \"Save\" after making any changes.","title":"Editing a Local Cluster"},{"location":"user-interface/web/clusters/#deleting-a-local-cluster","text":"To delete a local cluster, click the three-dotted menu at the top right corner of a cluster card, and then click \"Delete\".","title":"Deleting a Local Cluster"},{"location":"user-interface/web/library/","text":"Library The Library page in the SVL Simulator user interface displays the set of maps and vehicles that you have added to your account, and which are available for use in simulations. A tutorial video on adding an asset from the Store, as well as uploading your own asset, can be found here . Introduction top # Library is the central place for all of the content and data associated with your SVL Simulator account. You can access your maps and vehicles from anywhere once you log in to your account, allowing you to save your content in the cloud, including your own map environments, plugins, vehicle models, and sensor configurations. Maps top # When you log in for the first time, your Library page will show some widely used default assets that are automatically made available to your account. You can click: \"All\" to see all maps that you have added to your Library. \"My Content\" lists maps for which you are the owner (meaning they were built and uploaded by you). \"My Shares\" lists maps that are owned by you and were to other users (using the Sharing feature). \"Added from Store\" lists maps from other users that you have added to your library from store. \"Shared with Me\" lists maps that were shared with you by other users. Adding a map top # You can add maps to your Library from either the Store page or by clicking the \"Add new\" button at the top right. To upload your own map: From the Library, click the \"Add new\" button. Drag and drop or click to upload an AssetBundle which you have built for the version of SVL Simulator you are running. Once the upload has finished and the AssetBundle has been processed, fill in the details about your new map, including Map Name, Description, License, Copyright, and Tags. Click \"Publish\" to publish the map to your personal library, allowing it to be used in your simulation configurations. You can click \"Keep as draft\" to save and publish later. Viewing a map top # Once you have successfully added a map to your Library, you will be able to see the map and its details when you select it. There are several additional actions possible on a map page: Edit: You can edit the map's detail information. Share: You can share your private maps with other users to allow them to use the map in their simulations. Download the associated HD map in various supported formats that were built into the asset. Previewing the HD Map top # Select the eye icon in the HD Maps header to open a dialog with an interactive version of the HD map in the browser. The preview includes the lane lines of the roads and can be zoomed or rotated to inspect different parts of the map. Editing a map top # Editing a map allows you to update the details about your new map, including Map Name, Description, License, Copyright, and Tags. If the map is your own, then you can also update the asset bundle of the map and also upload new asset bundles for supporting various 'SVL Simulator' versions. There are several additional actions possible when editing: Add to Store: You can make the map public so that other users may add it to their library. Unpublish: You can \"unpublish\" the map and return it to the \"draft\" status, disabling the ability for you to use it in a valid simulation. Delete Map: Delete the map entirely from your account and library. Once you do this, you will need to go back to where you obtained the map (or re-upload it) in order to add it to your library again. NOTE: Any changes to the map such as Removing from the Store, or Unpublishing it, or Deleting will not be retroactively applied. That is the changes will have no impact on the users who already have the map in their library. For example, if a map is already in a user's library, then that user can continue to use the map in the simulations even if the map has been removed from the store or has been deleted or unpublished by the owner. Creating a map top # Please see the Assets document for more information on creating and building custom maps and vehicles for SVL Simulator in Developer Mode. You can read the Map Annotation page for instructions on annotating, importing, and exporting HD maps. Vehicles top # The Vehicle tab under Library contains the list of vehicles available for you to use in your simulations. Adding a vehicle top # You can add vehicles to your Library from either the Store page or by clicking the \"Add new\" button at the top right. To upload your own vehicle: From your Library, click the \"Add new\" button. Drag and drop or click to upload an AssetBundle which you have built for the version of 'SVL Simulator' you are running. You can see how to create and build your own vehicle here . Once your vehicle has successfully uploaded, you can fill in its information. Click \"Next\" to configure the sensors for this vehicle. Each vehicle can have multiple configurations and must have at least one configuration to use the vehicle in a simulation. Click \"Create New Configuration\" to create a new configuration or \"Next\" to continue without a configuration. You can create a configuration later by editing the vehicle . A tutorial video on creating a sensor configuration for a vehicle can be found here . To create the configuration, enter a name for the configuration, select a Bridge , and click \"Create\". Once you have created a configuration, you may add sensors using the \"+\" icon next to the \"Root (Base Link)\". More information about sensor configuration is available in Editing Vehicles Click \"Publish\" to publish the vehicle to your personal Library, allowing it to be used in your simulation configurations. You can click \"Keep as draft\" to save and publish later. Viewing a Vehicle top # Once you have successfully added a vehicle to your Library page, you will be able to see the vehicle and its details when you select it. On a vehicle page, there are several additional actions you can take: Share: You can share your private vehicles with other users to allow them to use the vehicle in their simulations. Edit: Change the vehicle's detailed information including Name, Description, License, Copyright, and Tags. Sensor Configurations : Add, Modify, Copy, or Delete a sensor configuration for the vehicle. Editing a Vehicle top # Editing a vehicle allows you to update the details about your new vehicle, including Name, Description, License, Copyright, and Tags. If the vehicle is your own, then you can also update the asset bundle of the vehicle and also upload new asset bundles for supporting various 'SVL Simulator' versions. There are several additional actions possible when editing: Add to Store: You can make the vehicle public so that other users may add it to their library. Unpublish: You can \"unpublish\" the vehicle and return it to the \"draft\" status, disabling the ability for you to use it in a valid simulation. Delete Vehicle: Delete the vehicle entirely from your account and library. Once you do this, you will need to go back to where you obtained the vehicle (or re-upload it) in order to add it to your library again. NOTE: Any changes to the vehicle such as Removing from the Store, or Unpublishing it, or Deleting will not be retroactively applied. That is the changes will have no impact on the users who already have the vehicle in their library. For example, if a vehicle is already in a user's library, then that user can continue to use the vehicle in the simulations even if the vehicle has been removed from the store or has been deleted or unpublished by the owner. Managing Sensor Configurations top # Each vehicle can have multiple sensor configurations. When you use a vehicle in a simulation, you will have an opportunity to select the sensor configuration to use in that simulation. The currently available sensor configurations are listed on the vehicle page. If a vehicle was added from the Store or if a vehicle is shared with you, you will also see the configurations created by the owner of the vehicle. To create or modify a new sensor configuration when viewing a vehicle , click the icon to the right of \"Sensor Configurations\". When editing a vehicle , click the \"Sensor Configurations\" tab at the top of the view. A tutorial video on creating a sensor configuration for a vehicle can be found here . The Sensor Configurations tab provides several actions: Create: You can create a new configuration using the \"Create New Configuration\" button. View / Edit: You can view any configuration or edit a configuration you own by selecting the configuration or the Edit icon for a configuration. Copy: You can create a copy of an existing configuration using the Copy icon. If you want to modify a configuration from a different users, you must first create your own copy of that configuration and then modify the copy. Delete: You can delete any configuration that you own using the Delete icon which will permanently remove it entirely from the vehicle. You will be prompted to confirm the deletion before continuing. Modifying a Sensor Configuration top # The sensor configuration editor allows you to visually update the position and properties of a sensor used in this configuration. Each sensor has a parent which is either the \"Root (Base Link)\" or is another sensor. To add new sensors, click the \"+\" icon next to the parent and select the sensors you wish to add. Each sensor can be customized by selecting the senor from the tree view and modifying the values in the form that loads below. The \"Sensor Type\" field is read-only and represents the plugin name. The \"Name\" field must be unique for this sensor configuration and helps to identify the sensor in the tree view and in the simulation. The transformation fields, \"X\", \"Y\", \"Z\", \"Pitch\", \"Yaw\", and \"Roll\" configure the position of the sensor relative to its parent position and orientation. Many sensors have additional configuration properties that can be customized. These fields are displayed below the transformation fields. Note: You do not need to \"Save\" after editing a sensor but must click \"Save\" to commit all of the changes you have made. Interacting with the Visual Editor top # The visual editor is integrated with the parameter editor and allows you to visually place sensors around the vehicle. It includes several tools for manipulating a sensor: Selection tool selects the active sensor. Move tool moves the sensor along the X, Y, or Z axis. Rotate tool rotates the sensor around the X, Y, or Z axis. Vehicle Transparency tool toggles the transparency of the vehicle model to help when placing sensors in or below the vehicle. Orientation tool illustrates the current orientation of the vehicle. Clicking one of the axis cones will snap the camera to look down that axis. The editor also includes buttons to Undo and Redo any changes you've made to the sensor configuration. These actions will undo any sensor configuration changes include parameter values are not restricted to changes made in the visual editor. Migrating Legacy Sensor Configurations top # If you have an older legacy sensor configuration that uses the type instead of the plugin field to identify the plugins, you can select the \"<>\" icon in the Preview header to paste this configuration into the JSON Editor. Clicking \"Save\" will validate and save the configuration and return an updated version compatible with the current schema. NOTE : Most of the sensor type names have changed for the 2021.1 release. You can find the latest type names from the sensors documentation. Working with Sensor Configurations created by other users top # When a vehicle is shared with you or when a vehicle is added to the library, you also gain access to the sensor configurations created by the owner of the vehicle. Sometimes, these sensor configurations may contain plugins that you do not have in your library. When that occurs, you will see warning messages with available actions for each type of problem. If a plugin is available in the store but not in your Library, you can add it by clicking \"Add to Library\" next to the warning. If a plugin is owned by another user but not in the Store, you can request access to it from the owner by clicking \"Request Access\" next to the warning. If a plugin has been deleted by the owner but still exists in the sensor configuration, it must be removed by the owner before you can use this sensor configuration in a simulation. Bridge Types # No bridge : This is the default. This is used when there is no need to connect to an AD Stack. This selection does not require any additional information while setting up the Simulation. ROS : This bridge allows connecting to ROS1 based AV stacks. (like Autoware). The ROS1 Bridge requires an IP address and port number while setting up the Simulation. ROSApollo : This bridge allows connecting to ROS1 based AV stacks which requires protobuf message format. (like Apollo 3.0). ROS1 Apollo Bridge requires an IP address and port number while setting up the Simulation. ROS2 : This bridge allows connecting to ROS2 based AV stacks. ROS2 Bridge requires an IP address and port number while setting up the Simulation. CyberRT : This bridge allows connections to Apollo 5.0. CyberRT Bridge. CyberRT requires an IP address and port number while setting up the Simulation. Example sensor configuration JSON # You can find example sensor configurations for several Autopilot systems on the following pages: Apollo 5.0 JSON Apollo 3.0 JSON Autoware.AI JSON Autoware.Auto JSON Below is a shortened version of the JSON configuration on the Jaguar2015XE (Autoware) default vehicle. It uses a ROS bridge type. The JSON includes: a GPS sensor in the center of the vehicle that publishes data on the \"/nmea_sentence\" topic a LiDAR sensor 2.312m above the center of the vehicle that publishes data on the \"/points_raw\" topic a Keyboard Control input which allows the keyboard input to control the car a Vehicle Control input which subscribes to the Autoware AD Stack control commands [ { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Keyboard Control\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ] Plugins top # When you log in for the first time, your Library page will contain the default Bridge plugins. You can click: \"All\" to see all plugins that you have added to your Library. \"Owned by me\" lists plugins for which you are the owner (meaning that these were built and uploaded by you). \"Shared by me\" lists plugins for which you are the owner were shared to other users by you. \"Shared with me\" lists plugins that were shared to you by other users. Adding a plugin top # You can add plugins to your Library from either the Store page or by clicking the \"Add new\" button at the top right. To upload your own plugin: From the Library, click the \"Add new\" button. Drag and drop, or click to upload an AssetBundle which you have built for the version of SVL Simulator you are running. Once the upload has finished and the AssetBundle has been processed, fill in the details about your new plugin, including Plugin Name, Description, License, Copyright, and Tags. Click \"Publish\" to publish the plugin to your personal library, allowing it to be used in your simulations. You can click \"Keep as draft\" to save and publish later. Viewing a plugin top # Once you have successfully added a plugin to your Library page, you will be able to see the plugin and its details. There are several additional actions possible on a plugin page: Edit: You can edit the plugin's detail information. Share: You can share your private plugins with other users to allow them to use the plugin in their simulations. Editing a plugin top # Editing a plugin allows you to update the details about your new plugin, including Plugin Name, Description, License, Copyright, and Tags. If the plugin is your own, then you can also update the asset bundle of the plugin and also upload new asset bundles for supporting various 'SVL Simulator' versions. There are several additional actions possible when editing: Add to Store: You can make the plugin public so that other users may add it to their library. Unpublish: You can \"unpublish\" the plugin and return it to \"draft\" status, disabling the ability for you to use it in a valid simulation. Delete Plugin: Delete the plugin entirely from your account and library. Once you do this, you will need to go back to where you obtained the plugin (or re-upload it) in order to add it to your library again. NOTE: Any changes to the plugin such as Removing from the Store, or Unpublishing it, or Deleting will not be retroactively applied. That is the changes will have no impact on the users who already have the plugin in their library. For example, if a plugin is already in a user's library, then that user can continue to use the plugin in sensor configurations and use them in simulations even if the plugin has been removed from the store or has been deleted or unpublished by the owner.","title":"Library"},{"location":"user-interface/web/library/#introduction","text":"Library is the central place for all of the content and data associated with your SVL Simulator account. You can access your maps and vehicles from anywhere once you log in to your account, allowing you to save your content in the cloud, including your own map environments, plugins, vehicle models, and sensor configurations.","title":"Introduction"},{"location":"user-interface/web/library/#maps","text":"When you log in for the first time, your Library page will show some widely used default assets that are automatically made available to your account. You can click: \"All\" to see all maps that you have added to your Library. \"My Content\" lists maps for which you are the owner (meaning they were built and uploaded by you). \"My Shares\" lists maps that are owned by you and were to other users (using the Sharing feature). \"Added from Store\" lists maps from other users that you have added to your library from store. \"Shared with Me\" lists maps that were shared with you by other users.","title":"Maps"},{"location":"user-interface/web/library/#adding-a-map","text":"You can add maps to your Library from either the Store page or by clicking the \"Add new\" button at the top right. To upload your own map: From the Library, click the \"Add new\" button. Drag and drop or click to upload an AssetBundle which you have built for the version of SVL Simulator you are running. Once the upload has finished and the AssetBundle has been processed, fill in the details about your new map, including Map Name, Description, License, Copyright, and Tags. Click \"Publish\" to publish the map to your personal library, allowing it to be used in your simulation configurations. You can click \"Keep as draft\" to save and publish later.","title":"Adding a map"},{"location":"user-interface/web/library/#viewing-a-map","text":"Once you have successfully added a map to your Library, you will be able to see the map and its details when you select it. There are several additional actions possible on a map page: Edit: You can edit the map's detail information. Share: You can share your private maps with other users to allow them to use the map in their simulations. Download the associated HD map in various supported formats that were built into the asset.","title":"Viewing a map"},{"location":"user-interface/web/library/#previewing-the-hd-map","text":"Select the eye icon in the HD Maps header to open a dialog with an interactive version of the HD map in the browser. The preview includes the lane lines of the roads and can be zoomed or rotated to inspect different parts of the map.","title":"Previewing the HD map"},{"location":"user-interface/web/library/#editing-a-map","text":"Editing a map allows you to update the details about your new map, including Map Name, Description, License, Copyright, and Tags. If the map is your own, then you can also update the asset bundle of the map and also upload new asset bundles for supporting various 'SVL Simulator' versions. There are several additional actions possible when editing: Add to Store: You can make the map public so that other users may add it to their library. Unpublish: You can \"unpublish\" the map and return it to the \"draft\" status, disabling the ability for you to use it in a valid simulation. Delete Map: Delete the map entirely from your account and library. Once you do this, you will need to go back to where you obtained the map (or re-upload it) in order to add it to your library again. NOTE: Any changes to the map such as Removing from the Store, or Unpublishing it, or Deleting will not be retroactively applied. That is the changes will have no impact on the users who already have the map in their library. For example, if a map is already in a user's library, then that user can continue to use the map in the simulations even if the map has been removed from the store or has been deleted or unpublished by the owner.","title":"Editing a map"},{"location":"user-interface/web/library/#creating-a-map","text":"Please see the Assets document for more information on creating and building custom maps and vehicles for SVL Simulator in Developer Mode. You can read the Map Annotation page for instructions on annotating, importing, and exporting HD maps.","title":"Creating a map"},{"location":"user-interface/web/library/#vehicles","text":"The Vehicle tab under Library contains the list of vehicles available for you to use in your simulations.","title":"Vehicles"},{"location":"user-interface/web/library/#adding-a-vehicle","text":"You can add vehicles to your Library from either the Store page or by clicking the \"Add new\" button at the top right. To upload your own vehicle: From your Library, click the \"Add new\" button. Drag and drop or click to upload an AssetBundle which you have built for the version of 'SVL Simulator' you are running. You can see how to create and build your own vehicle here . Once your vehicle has successfully uploaded, you can fill in its information. Click \"Next\" to configure the sensors for this vehicle. Each vehicle can have multiple configurations and must have at least one configuration to use the vehicle in a simulation. Click \"Create New Configuration\" to create a new configuration or \"Next\" to continue without a configuration. You can create a configuration later by editing the vehicle . A tutorial video on creating a sensor configuration for a vehicle can be found here . To create the configuration, enter a name for the configuration, select a Bridge , and click \"Create\". Once you have created a configuration, you may add sensors using the \"+\" icon next to the \"Root (Base Link)\". More information about sensor configuration is available in Editing Vehicles Click \"Publish\" to publish the vehicle to your personal Library, allowing it to be used in your simulation configurations. You can click \"Keep as draft\" to save and publish later.","title":"Adding a vehicle"},{"location":"user-interface/web/library/#viewing-a-vehicle","text":"Once you have successfully added a vehicle to your Library page, you will be able to see the vehicle and its details when you select it. On a vehicle page, there are several additional actions you can take: Share: You can share your private vehicles with other users to allow them to use the vehicle in their simulations. Edit: Change the vehicle's detailed information including Name, Description, License, Copyright, and Tags. Sensor Configurations : Add, Modify, Copy, or Delete a sensor configuration for the vehicle.","title":"Viewing a Vehicle"},{"location":"user-interface/web/library/#editing-a-vehicle","text":"Editing a vehicle allows you to update the details about your new vehicle, including Name, Description, License, Copyright, and Tags. If the vehicle is your own, then you can also update the asset bundle of the vehicle and also upload new asset bundles for supporting various 'SVL Simulator' versions. There are several additional actions possible when editing: Add to Store: You can make the vehicle public so that other users may add it to their library. Unpublish: You can \"unpublish\" the vehicle and return it to the \"draft\" status, disabling the ability for you to use it in a valid simulation. Delete Vehicle: Delete the vehicle entirely from your account and library. Once you do this, you will need to go back to where you obtained the vehicle (or re-upload it) in order to add it to your library again. NOTE: Any changes to the vehicle such as Removing from the Store, or Unpublishing it, or Deleting will not be retroactively applied. That is the changes will have no impact on the users who already have the vehicle in their library. For example, if a vehicle is already in a user's library, then that user can continue to use the vehicle in the simulations even if the vehicle has been removed from the store or has been deleted or unpublished by the owner.","title":"Editing a Vehicle"},{"location":"user-interface/web/library/#managing-sensor-configurations","text":"Each vehicle can have multiple sensor configurations. When you use a vehicle in a simulation, you will have an opportunity to select the sensor configuration to use in that simulation. The currently available sensor configurations are listed on the vehicle page. If a vehicle was added from the Store or if a vehicle is shared with you, you will also see the configurations created by the owner of the vehicle. To create or modify a new sensor configuration when viewing a vehicle , click the icon to the right of \"Sensor Configurations\". When editing a vehicle , click the \"Sensor Configurations\" tab at the top of the view. A tutorial video on creating a sensor configuration for a vehicle can be found here . The Sensor Configurations tab provides several actions: Create: You can create a new configuration using the \"Create New Configuration\" button. View / Edit: You can view any configuration or edit a configuration you own by selecting the configuration or the Edit icon for a configuration. Copy: You can create a copy of an existing configuration using the Copy icon. If you want to modify a configuration from a different users, you must first create your own copy of that configuration and then modify the copy. Delete: You can delete any configuration that you own using the Delete icon which will permanently remove it entirely from the vehicle. You will be prompted to confirm the deletion before continuing.","title":"Managing Sensor Configurations"},{"location":"user-interface/web/library/#modifying-a-sensor-configuration","text":"The sensor configuration editor allows you to visually update the position and properties of a sensor used in this configuration. Each sensor has a parent which is either the \"Root (Base Link)\" or is another sensor. To add new sensors, click the \"+\" icon next to the parent and select the sensors you wish to add. Each sensor can be customized by selecting the senor from the tree view and modifying the values in the form that loads below. The \"Sensor Type\" field is read-only and represents the plugin name. The \"Name\" field must be unique for this sensor configuration and helps to identify the sensor in the tree view and in the simulation. The transformation fields, \"X\", \"Y\", \"Z\", \"Pitch\", \"Yaw\", and \"Roll\" configure the position of the sensor relative to its parent position and orientation. Many sensors have additional configuration properties that can be customized. These fields are displayed below the transformation fields. Note: You do not need to \"Save\" after editing a sensor but must click \"Save\" to commit all of the changes you have made.","title":"Modifying a Sensor Configuration"},{"location":"user-interface/web/library/#interacting-with-the-visual-editor","text":"The visual editor is integrated with the parameter editor and allows you to visually place sensors around the vehicle. It includes several tools for manipulating a sensor: Selection tool selects the active sensor. Move tool moves the sensor along the X, Y, or Z axis. Rotate tool rotates the sensor around the X, Y, or Z axis. Vehicle Transparency tool toggles the transparency of the vehicle model to help when placing sensors in or below the vehicle. Orientation tool illustrates the current orientation of the vehicle. Clicking one of the axis cones will snap the camera to look down that axis. The editor also includes buttons to Undo and Redo any changes you've made to the sensor configuration. These actions will undo any sensor configuration changes include parameter values are not restricted to changes made in the visual editor.","title":"Interacting with the Visual Editor"},{"location":"user-interface/web/library/#migrating-legacy-sensor-configurations","text":"If you have an older legacy sensor configuration that uses the type instead of the plugin field to identify the plugins, you can select the \"<>\" icon in the Preview header to paste this configuration into the JSON Editor. Clicking \"Save\" will validate and save the configuration and return an updated version compatible with the current schema. NOTE : Most of the sensor type names have changed for the 2021.1 release. You can find the latest type names from the sensors documentation.","title":"Migrating Legacy Sensor Configurations"},{"location":"user-interface/web/library/#working-with-sensor-configurations-created-by-other-users","text":"When a vehicle is shared with you or when a vehicle is added to the library, you also gain access to the sensor configurations created by the owner of the vehicle. Sometimes, these sensor configurations may contain plugins that you do not have in your library. When that occurs, you will see warning messages with available actions for each type of problem. If a plugin is available in the store but not in your Library, you can add it by clicking \"Add to Library\" next to the warning. If a plugin is owned by another user but not in the Store, you can request access to it from the owner by clicking \"Request Access\" next to the warning. If a plugin has been deleted by the owner but still exists in the sensor configuration, it must be removed by the owner before you can use this sensor configuration in a simulation.","title":"Working with Sensor Configurations created by other users"},{"location":"user-interface/web/library/#bridge-types","text":"No bridge : This is the default. This is used when there is no need to connect to an AD Stack. This selection does not require any additional information while setting up the Simulation. ROS : This bridge allows connecting to ROS1 based AV stacks. (like Autoware). The ROS1 Bridge requires an IP address and port number while setting up the Simulation. ROSApollo : This bridge allows connecting to ROS1 based AV stacks which requires protobuf message format. (like Apollo 3.0). ROS1 Apollo Bridge requires an IP address and port number while setting up the Simulation. ROS2 : This bridge allows connecting to ROS2 based AV stacks. ROS2 Bridge requires an IP address and port number while setting up the Simulation. CyberRT : This bridge allows connections to Apollo 5.0. CyberRT Bridge. CyberRT requires an IP address and port number while setting up the Simulation.","title":"Bridge Types"},{"location":"user-interface/web/library/#example-json","text":"You can find example sensor configurations for several Autopilot systems on the following pages: Apollo 5.0 JSON Apollo 3.0 JSON Autoware.AI JSON Autoware.Auto JSON Below is a shortened version of the JSON configuration on the Jaguar2015XE (Autoware) default vehicle. It uses a ROS bridge type. The JSON includes: a GPS sensor in the center of the vehicle that publishes data on the \"/nmea_sentence\" topic a LiDAR sensor 2.312m above the center of the vehicle that publishes data on the \"/points_raw\" topic a Keyboard Control input which allows the keyboard input to control the car a Vehicle Control input which subscribes to the Autoware AD Stack control commands [ { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Keyboard Control\", \"name\": \"Keyboard Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Example sensor configuration JSON"},{"location":"user-interface/web/library/#plugins","text":"When you log in for the first time, your Library page will contain the default Bridge plugins. You can click: \"All\" to see all plugins that you have added to your Library. \"Owned by me\" lists plugins for which you are the owner (meaning that these were built and uploaded by you). \"Shared by me\" lists plugins for which you are the owner were shared to other users by you. \"Shared with me\" lists plugins that were shared to you by other users.","title":"Plugins"},{"location":"user-interface/web/library/#adding-a-plugin","text":"You can add plugins to your Library from either the Store page or by clicking the \"Add new\" button at the top right. To upload your own plugin: From the Library, click the \"Add new\" button. Drag and drop, or click to upload an AssetBundle which you have built for the version of SVL Simulator you are running. Once the upload has finished and the AssetBundle has been processed, fill in the details about your new plugin, including Plugin Name, Description, License, Copyright, and Tags. Click \"Publish\" to publish the plugin to your personal library, allowing it to be used in your simulations. You can click \"Keep as draft\" to save and publish later.","title":"Adding a plugin"},{"location":"user-interface/web/library/#viewing-a-plugin","text":"Once you have successfully added a plugin to your Library page, you will be able to see the plugin and its details. There are several additional actions possible on a plugin page: Edit: You can edit the plugin's detail information. Share: You can share your private plugins with other users to allow them to use the plugin in their simulations.","title":"Viewing a plugin"},{"location":"user-interface/web/library/#editing-a-plugin","text":"Editing a plugin allows you to update the details about your new plugin, including Plugin Name, Description, License, Copyright, and Tags. If the plugin is your own, then you can also update the asset bundle of the plugin and also upload new asset bundles for supporting various 'SVL Simulator' versions. There are several additional actions possible when editing: Add to Store: You can make the plugin public so that other users may add it to their library. Unpublish: You can \"unpublish\" the plugin and return it to \"draft\" status, disabling the ability for you to use it in a valid simulation. Delete Plugin: Delete the plugin entirely from your account and library. Once you do this, you will need to go back to where you obtained the plugin (or re-upload it) in order to add it to your library again. NOTE: Any changes to the plugin such as Removing from the Store, or Unpublishing it, or Deleting will not be retroactively applied. That is the changes will have no impact on the users who already have the plugin in their library. For example, if a plugin is already in a user's library, then that user can continue to use the plugin in sensor configurations and use them in simulations even if the plugin has been removed from the store or has been deleted or unpublished by the owner.","title":"Editing a plugin"},{"location":"user-interface/web/simulations/","text":"Simulations The Simulations page contains the list of test case simulations you can execute. It serves as the central place from which you can create new simulations, as well as start and stop your simulations. A tutorial video on simulations creation and management can be found here . Creating a new simulation top # Please refer to the Running Simulator document for detailed instructions on creating a new simulation. The following pages denote the pieces of a new simulation configuration. General top # Simulation Name - Enter a name for your simulation Description (optional) - Enter a description for your simulation Tags (optional) - Specify tags for your simulation, which can be used to search through or filter simulations later Select Cluster - Choose the simulation cluster which will actually run the simulator executable and execute the simulation. If you select a cloud cluster, the following three toggles can not be changed. Create test report - Enable to generate a test report after execution. Headless mode - Enable to run the simulator without rendering visually in the executable Interactive mode - Enable to run the simulator interactively, with the ability to visualize sensors, pause, etc. within the executable Test case top # Select a runtime template: Random Traffic: A random simulation where starting conditions are defined here in the user interface Visual scenario editor: Run a scenario created by the Visual Scenario Editor Python API: Directly input the Python API test case script to automatically execute from the user interface. If you have selected a local cluster, you will still need to bring up your Autopilot separately. API Only: The simulator is simply started in API Only mode, and you are expected to use a runner to execute test cases (Python API, Visual Scenario Editor) Curently, only the Python API runtime template is available if you have selected a cloud cluster. Runtime Template: Random Traffic # Map: Select the map for the simulation Vehicle: Select one or more ego vehicles Simulation Date: Set the date within simulation Time of Day: Set the time of day Rain: [0-1] set how much rain should fall Fog: [0-1] set how thick fog should be Wetness: [0-1] set how wet the road surfaces should be Cloudiness: [0-1] set how much cloud cover there should be Random Traffic: Enable for non-ego traffic vehicles Random Pedestrians: Enable for pedestrian agents Random Bicyclists: Enable for bicyclists Use Pre-defined Seed: Enable then input an integer to deterministically produce a random simulation that can be reproduced Find the example guide here . Runtime Template: Visual scenario editor # Scenario: Upload the JSON file created by the Visual Scenario Editor. Runtime Template: Python API # Python Script: Input the Python API test case script that you would like to automatically execute when starting the simulation from the web user interface. Note that this is only supported for clusters running the Linux version of SVL Simulator. Map: Select the map for the simulation. This must match the name of map passed to sim.load() . Vehicle: Select one or more ego vehicles. These must match any hardcoded vehicle names passed to sim.add_agent() . NOTE: If you have selected a cloud cluster, only a single vehicle can be selected. Please see here for additional information on Python API mode. Runtime Template: API Only # For API Only simulations , you do not need to set any starting parameters, as the runner and test case you are expected to execute should determine the starting configuration. This could be from Visual Scenario Editor, or a Python API test case script. See the below documents for additional information on running the various types of test cases using the API Only runtime template: Python API Scripts Autopilot top # Local cluster top # Autopilot: A list of compatible Autopilot systems are highlighted to choose from, based on the ego vehicle and the cluster type selected on the Test case pane. Bridge IP: the IP address and port number that your bridge is running at, e.g. localhost:9090 or 127.0.0.1:9000 . Publish top # Finally, click \"Publish\" to publish the new simulation to your library and to be able to execute it. You can now run your simulation. Make sure the cluster you specified for the simulation is online, then click \"Run Simulation\".","title":"Simulations"},{"location":"user-interface/web/simulations/#creating-a-new-simulation","text":"Please refer to the Running Simulator document for detailed instructions on creating a new simulation. The following pages denote the pieces of a new simulation configuration.","title":"Creating a new simulation"},{"location":"user-interface/web/simulations/#general","text":"Simulation Name - Enter a name for your simulation Description (optional) - Enter a description for your simulation Tags (optional) - Specify tags for your simulation, which can be used to search through or filter simulations later Select Cluster - Choose the simulation cluster which will actually run the simulator executable and execute the simulation. If you select a cloud cluster, the following three toggles can not be changed. Create test report - Enable to generate a test report after execution. Headless mode - Enable to run the simulator without rendering visually in the executable Interactive mode - Enable to run the simulator interactively, with the ability to visualize sensors, pause, etc. within the executable","title":"General"},{"location":"user-interface/web/simulations/#test-case","text":"Select a runtime template: Random Traffic: A random simulation where starting conditions are defined here in the user interface Visual scenario editor: Run a scenario created by the Visual Scenario Editor Python API: Directly input the Python API test case script to automatically execute from the user interface. If you have selected a local cluster, you will still need to bring up your Autopilot separately. API Only: The simulator is simply started in API Only mode, and you are expected to use a runner to execute test cases (Python API, Visual Scenario Editor) Curently, only the Python API runtime template is available if you have selected a cloud cluster.","title":"Test case"},{"location":"user-interface/web/simulations/#random-traffic","text":"Map: Select the map for the simulation Vehicle: Select one or more ego vehicles Simulation Date: Set the date within simulation Time of Day: Set the time of day Rain: [0-1] set how much rain should fall Fog: [0-1] set how thick fog should be Wetness: [0-1] set how wet the road surfaces should be Cloudiness: [0-1] set how much cloud cover there should be Random Traffic: Enable for non-ego traffic vehicles Random Pedestrians: Enable for pedestrian agents Random Bicyclists: Enable for bicyclists Use Pre-defined Seed: Enable then input an integer to deterministically produce a random simulation that can be reproduced Find the example guide here .","title":"Runtime Template: Random Traffic"},{"location":"user-interface/web/simulations/#vse","text":"Scenario: Upload the JSON file created by the Visual Scenario Editor.","title":"Runtime Template: Visual scenario editor"},{"location":"user-interface/web/simulations/#python-api","text":"Python Script: Input the Python API test case script that you would like to automatically execute when starting the simulation from the web user interface. Note that this is only supported for clusters running the Linux version of SVL Simulator. Map: Select the map for the simulation. This must match the name of map passed to sim.load() . Vehicle: Select one or more ego vehicles. These must match any hardcoded vehicle names passed to sim.add_agent() . NOTE: If you have selected a cloud cluster, only a single vehicle can be selected. Please see here for additional information on Python API mode.","title":"Runtime Template: Python API"},{"location":"user-interface/web/simulations/#api-only","text":"For API Only simulations , you do not need to set any starting parameters, as the runner and test case you are expected to execute should determine the starting configuration. This could be from Visual Scenario Editor, or a Python API test case script. See the below documents for additional information on running the various types of test cases using the API Only runtime template: Python API Scripts","title":"Runtime Template: API Only"},{"location":"user-interface/web/simulations/#autopilot","text":"","title":"Autopilot"},{"location":"user-interface/web/simulations/#local-cluster","text":"Autopilot: A list of compatible Autopilot systems are highlighted to choose from, based on the ego vehicle and the cluster type selected on the Test case pane. Bridge IP: the IP address and port number that your bridge is running at, e.g. localhost:9090 or 127.0.0.1:9000 .","title":"Local cluster"},{"location":"user-interface/web/simulations/#publish","text":"Finally, click \"Publish\" to publish the new simulation to your library and to be able to execute it. You can now run your simulation. Make sure the cluster you specified for the simulation is online, then click \"Run Simulation\".","title":"Publish"},{"location":"user-interface/web/store/","text":"Store This document describes the Store page in the SVL Simulator user interface. The online user interface of the SVL Simulator enables you to create, configure, and execute simulation test cases from within your logged in SVL Simulator account in the browser. This includes the ability to browse through, and choose from, a default set of maps, vehicles (with sensor configurations), and plugins. These can be viewed in the Store page. A tutorial video on uploading an asset and adding it to the Store can be found here . For a complete walkthrough from installation to creating and running your first simulation, see the Running SVL Simulator document. Introduction top # When you first log in to your SVL Simulator account, you will see the Store page which includes views for Maps, Vehicles, and Plugins. Each section contains the full list of available selections that you can add to your own account, which can then be used in your simulations. You can always navigate back to this page by clicking \"Store\" in the navigation menu on the left. The Store section contains all maps, vehicles, and plugins that can currently be used in the 'SVL Simulator'. These assets have been published to the Store by their owners to be used by anybody, in accordance to the License, by adding these assets to their library and then including them in their simulations. For your own custom maps, vehicles, plugins, or simulation configurations, you can add or upload them directly to your account from the \"Library\" page . Each type of asset (Map, Vehicle, and Plugin) can be browsed or searched using the tabs at the top of the view: All assets include those assets that can be used by any user in their simulations in accordance with the License for that asset. Trending assets include the most popular assets in the store and are sorted by their popularity. Shared with Me includes assets that have been shared with you by other users. If you have previously removed a shared asset from your library, you can add it again from this list.","title":"Store"},{"location":"user-interface/web/store/#introduction","text":"When you first log in to your SVL Simulator account, you will see the Store page which includes views for Maps, Vehicles, and Plugins. Each section contains the full list of available selections that you can add to your own account, which can then be used in your simulations. You can always navigate back to this page by clicking \"Store\" in the navigation menu on the left. The Store section contains all maps, vehicles, and plugins that can currently be used in the 'SVL Simulator'. These assets have been published to the Store by their owners to be used by anybody, in accordance to the License, by adding these assets to their library and then including them in their simulations. For your own custom maps, vehicles, plugins, or simulation configurations, you can add or upload them directly to your account from the \"Library\" page . Each type of asset (Map, Vehicle, and Plugin) can be browsed or searched using the tabs at the top of the view: All assets include those assets that can be used by any user in their simulations in accordance with the License for that asset. Trending assets include the most popular assets in the store and are sorted by their popularity. Shared with Me includes assets that have been shared with you by other users. If you have previously removed a shared asset from your library, you can add it again from this list.","title":"Introduction"},{"location":"user-interface/web/test-results-visualization/","text":"Visualizing Test Results top The visualization tab allows you to playback a simulation and see the current state of each sensor over time. The visualization tab is only available in test reports of completed cloud simulations . For local simulations, you can use an interactive visualizer tool on the autonomous driving system side such as RViz for ROS or cyber_visualizer for Apollo. To navigate to the visualizer from Test Results, Select View for a test result. If there are multiple vehicles, select a vehicle. Select the Visualization tab. Note: Only Apollo-based recordings are currently supported by cloud simulations and the visualizer. The visualization section breaks down into the following features: Viewer Sensor Menu Playback Control Widgets Callbacks Viewer top # The primary feature of the Visualization Tab is the Viewer which creates a rendering of available sensor data to aid inspecting the results of a simulation. LiDAR is rendered as dots ranging from white to red illustrating the altitude relative to the ground. Detected obstacles are rendered enclosed in a blue box with labels above them. Trajectories are rendered as a wide line at ground level indicating the planned route for the vehicle. Each sensor can be enabled or disabled using the Sensor Menu to aid in focusing on specific results. Full Screen Mode top # The Viewer can switch to full screen mode using the full screen button to gain more space to view the visualization and widgets. Pressing the Escape key or clicking the full screen button a second time will exit full screen. Camera Angles top # Perspective - The camera is positioned above and behind the vehicle. Pan, tilt, zoom, and rotate are all available. Driver - The camera is fixed at the front of the vehicle. Only rotate is available. Top down - The camera is fixed above the vehicle looking down. Pan, zoom, and rotate are available. Pan / Tilt / Zoom / Rotate top # The camera position can be further refined by panning, tilting, zooming, or rotating if supported by the current camera angle. To pan or tilt , click and drag within the viewer. To zoom , hold Control (Command on Mac) and scroll with the scroll wheel within the viewer. To rotate , hold Control (Command on Mac), click, and drag within the viewer. To recenter the viewer after panning or rotating, click the recenter button. Note that zoom and tilt values are maintained when re-centering. Sensor Menu top # The Sensor menu contains a list of all available sensors for the selected vehicle in this simulation. They are arranged in a tree based on the full name of the sensor from the simulation. Branches of the tree can be collapsed or expanded using the triangle icon. If the sensor can be visualized in the viewer, selecting the sensor will toggle it on and off in the viewer. When the sensor is visible, the eye icon will be green. To view the details of the sensor at the current time, click the message icon to display the Message Widget. When the widget is visible, the message icon will be green. If the sensor cannot be visualized in the view, selecting the sensor will toggle a widget for that sensor. When the widget is visible, the widget icon will be green. Hiding the sensor menu top # The Sensor menu can be hidden using the close button to reveal more of the viewer. When hidden, the menu is collapsed to a menu icon. To expand the menu, click the menu icon. Playback Control top # The playback control manages the playback of the simulation results. The control consists of the following components: A slider component which can be selected or dragged to move to a specific time of the simulation. Tick labels above the slider which indicate the time relative to the start of the simulation. The play/pause button which toggles automatic playback of the simulation at the current time. The simulation time which displays the current time according to the simulation's time. The lookahead slider allows selection of the amount of time, in seconds, to display lookahead predictions if supported by the simulation. Widgets top # Each sensor can be further visualized with floating widgets displayed on top of the viewer . By default, all widgets are hidden but may be shown by selecting the eye icon from the sensor menu . Once displayed, each widget may be moved, resized, and closed. To move a widget , click and drag anywhere where the cursor changes to the move icon. To resize a widget , click and drag from the lower right corner of the widget. To close a widget , click the close button in the upper right corner of the widget. To view a different sensor in an open widget, click the sensor name in the title bar which will open a dropdown menu. This menu will only include sensors that can be displayed in the current widget. Typing part of the desired sensor's name will filter the dropdown to only those that match the entered text. Camera Widget top # The camera widget displays the image captured by a camera sensor at the current playback time. Meter Widget top # The meter widget includes a gauge component that displays a numeric value within a predefined range. The range and unit of the value are pre-configured by the simulator. This widget is used for data streams like the current velocity or acceleration. Note: The meter widget does not support resizing. Message Widget top # The message widget displays the data for the selected sensor at the current playback time. The data is presented as a tree which can be expanded or collapsed using the triangle icon. The entire tree can be expanded or collapsed using the expand button in the title bar. The data, in JSON format, may also be copied to the clipboard using the clipboard icon in the title bar. Note: Because the content of the widget is selectable, you must click and drag the border around the message in order to move the widget. Callbacks top # Callback events are categorized as one-time events during a test case that either affect the ego vehicle, or that the ego vehicle causes in the environment. The following types of events are supported: Collision involving ego (EgoCollision) Speed limit violation (SpeedViolation) Sudden braking (SuddenBrake) Sudden steer (SuddenSteer) Low simulation performance (LowFPS) The occurrence of at least one of these events will cause a simulation to be evaluated as \"Failed\". For each callback event, the returned information is reported, including the simulation time at which the event occurred. Selecting the eye icon will cause the visualization to seek to the time of the event in order to view the state of each sensor at that time. EgoCollision top # A collision involving an ego vehicle. Field Description Units OtherType Type of agent with which ego collided NPC|Pedestrian|Obstacle EgoVelocity Velocity vector of ego at time at collision time x, y, z in m/s OtherVelocity Velocity vector of agent/object at collision time km/hr EgoCollisionTotal Number of ego collisions in the event SpeedViolation top # A speed limit violation by an ego vehicle, based on the lane speed limit in the map annotations. Metric Description Units Duration Duration of speed limit violation event time Max speed Maximum speed of ego vehicle during event. m/s SpeedLimit The ego lane's annotated speed limit of the HD map m/s EgoStuck top # An ego stuck event by the ego vehicle, in which the ego travels a threshold distance and then stops moving for a threshold time SuddenBrake top # A sudden braking event by the ego vehicle, in which it exceeds a threshold deceleration value. SuddenSteer top # A sudden steering event by the ego vehicle, in which absolute magnitude of steering angle exceeds a threshold value. LowFPS top # A low simulation performance to indicate that results may not be accurate due to sufficiently low framerate during simulation on the cluster.","title":"Visualizing Test Results [[top]] {: #how-to-visualize-test-results data-toc-label='Visualizing Test Results'}"},{"location":"user-interface/web/test-results-visualization/#viewer","text":"The primary feature of the Visualization Tab is the Viewer which creates a rendering of available sensor data to aid inspecting the results of a simulation. LiDAR is rendered as dots ranging from white to red illustrating the altitude relative to the ground. Detected obstacles are rendered enclosed in a blue box with labels above them. Trajectories are rendered as a wide line at ground level indicating the planned route for the vehicle. Each sensor can be enabled or disabled using the Sensor Menu to aid in focusing on specific results.","title":"Visualization: Viewer"},{"location":"user-interface/web/test-results-visualization/#viewer-full-screen-mode","text":"The Viewer can switch to full screen mode using the full screen button to gain more space to view the visualization and widgets. Pressing the Escape key or clicking the full screen button a second time will exit full screen.","title":"Full Screen Mode top"},{"location":"user-interface/web/test-results-visualization/#camera-angles-top","text":"Perspective - The camera is positioned above and behind the vehicle. Pan, tilt, zoom, and rotate are all available. Driver - The camera is fixed at the front of the vehicle. Only rotate is available. Top down - The camera is fixed above the vehicle looking down. Pan, zoom, and rotate are available.","title":"Camera Angles top"},{"location":"user-interface/web/test-results-visualization/#pan-tilt-zoom-rotate-top","text":"The camera position can be further refined by panning, tilting, zooming, or rotating if supported by the current camera angle. To pan or tilt , click and drag within the viewer. To zoom , hold Control (Command on Mac) and scroll with the scroll wheel within the viewer. To rotate , hold Control (Command on Mac), click, and drag within the viewer. To recenter the viewer after panning or rotating, click the recenter button. Note that zoom and tilt values are maintained when re-centering.","title":"Pan / Tilt / Zoom / Rotate top"},{"location":"user-interface/web/test-results-visualization/#sensor-menu","text":"The Sensor menu contains a list of all available sensors for the selected vehicle in this simulation. They are arranged in a tree based on the full name of the sensor from the simulation. Branches of the tree can be collapsed or expanded using the triangle icon. If the sensor can be visualized in the viewer, selecting the sensor will toggle it on and off in the viewer. When the sensor is visible, the eye icon will be green. To view the details of the sensor at the current time, click the message icon to display the Message Widget. When the widget is visible, the message icon will be green. If the sensor cannot be visualized in the view, selecting the sensor will toggle a widget for that sensor. When the widget is visible, the widget icon will be green.","title":"Visualization: Sensor Menu"},{"location":"user-interface/web/test-results-visualization/#hiding-the-sensor-menu-top","text":"The Sensor menu can be hidden using the close button to reveal more of the viewer. When hidden, the menu is collapsed to a menu icon. To expand the menu, click the menu icon.","title":"Hiding the sensor menu top"},{"location":"user-interface/web/test-results-visualization/#playback-control","text":"The playback control manages the playback of the simulation results. The control consists of the following components: A slider component which can be selected or dragged to move to a specific time of the simulation. Tick labels above the slider which indicate the time relative to the start of the simulation. The play/pause button which toggles automatic playback of the simulation at the current time. The simulation time which displays the current time according to the simulation's time. The lookahead slider allows selection of the amount of time, in seconds, to display lookahead predictions if supported by the simulation.","title":"Visualization: Playback Control"},{"location":"user-interface/web/test-results-visualization/#widgets","text":"Each sensor can be further visualized with floating widgets displayed on top of the viewer . By default, all widgets are hidden but may be shown by selecting the eye icon from the sensor menu . Once displayed, each widget may be moved, resized, and closed. To move a widget , click and drag anywhere where the cursor changes to the move icon. To resize a widget , click and drag from the lower right corner of the widget. To close a widget , click the close button in the upper right corner of the widget. To view a different sensor in an open widget, click the sensor name in the title bar which will open a dropdown menu. This menu will only include sensors that can be displayed in the current widget. Typing part of the desired sensor's name will filter the dropdown to only those that match the entered text.","title":"Visualization: Widgets"},{"location":"user-interface/web/test-results-visualization/#camera-widget-top","text":"The camera widget displays the image captured by a camera sensor at the current playback time.","title":"Camera Widget top"},{"location":"user-interface/web/test-results-visualization/#meter-widget-top","text":"The meter widget includes a gauge component that displays a numeric value within a predefined range. The range and unit of the value are pre-configured by the simulator. This widget is used for data streams like the current velocity or acceleration. Note: The meter widget does not support resizing.","title":"Meter Widget top"},{"location":"user-interface/web/test-results-visualization/#message-widget-top","text":"The message widget displays the data for the selected sensor at the current playback time. The data is presented as a tree which can be expanded or collapsed using the triangle icon. The entire tree can be expanded or collapsed using the expand button in the title bar. The data, in JSON format, may also be copied to the clipboard using the clipboard icon in the title bar. Note: Because the content of the widget is selectable, you must click and drag the border around the message in order to move the widget.","title":"Message Widget top"},{"location":"user-interface/web/test-results-visualization/#callbacks","text":"Callback events are categorized as one-time events during a test case that either affect the ego vehicle, or that the ego vehicle causes in the environment. The following types of events are supported: Collision involving ego (EgoCollision) Speed limit violation (SpeedViolation) Sudden braking (SuddenBrake) Sudden steer (SuddenSteer) Low simulation performance (LowFPS) The occurrence of at least one of these events will cause a simulation to be evaluated as \"Failed\". For each callback event, the returned information is reported, including the simulation time at which the event occurred. Selecting the eye icon will cause the visualization to seek to the time of the event in order to view the state of each sensor at that time.","title":"Visualization: Callbacks"},{"location":"user-interface/web/test-results-visualization/#egocollision","text":"A collision involving an ego vehicle. Field Description Units OtherType Type of agent with which ego collided NPC|Pedestrian|Obstacle EgoVelocity Velocity vector of ego at time at collision time x, y, z in m/s OtherVelocity Velocity vector of agent/object at collision time km/hr EgoCollisionTotal Number of ego collisions in the event","title":"EgoCollision"},{"location":"user-interface/web/test-results-visualization/#speedviolation","text":"A speed limit violation by an ego vehicle, based on the lane speed limit in the map annotations. Metric Description Units Duration Duration of speed limit violation event time Max speed Maximum speed of ego vehicle during event. m/s SpeedLimit The ego lane's annotated speed limit of the HD map m/s","title":"SpeedViolation"},{"location":"user-interface/web/test-results-visualization/#egostuck","text":"An ego stuck event by the ego vehicle, in which the ego travels a threshold distance and then stops moving for a threshold time","title":"EgoStuck"},{"location":"user-interface/web/test-results-visualization/#suddenbrake","text":"A sudden braking event by the ego vehicle, in which it exceeds a threshold deceleration value.","title":"SuddenBrake"},{"location":"user-interface/web/test-results-visualization/#suddensteer","text":"A sudden steering event by the ego vehicle, in which absolute magnitude of steering angle exceeds a threshold value.","title":"SuddenSteer"},{"location":"user-interface/web/test-results-visualization/#lowfps","text":"A low simulation performance to indicate that results may not be accurate due to sufficiently low framerate during simulation on the cluster.","title":"LowFPS"},{"location":"user-interface/web/test-results/","text":"Test Results The Test Results contains the full collection of test results generated from running test cases. You can view individual test reports of executed test cases. Test reports contain detailed information about the test case, including the success/failure result, important statistics for each ego vehicle, interesting events that happened, and a video of the ego vehicle's drive during the test case. Test Results top # On the left-hand side of the web user interface, clicking \"Test Results\" brings up the Test Results page. This page contains a table of all executed test cases which enabled test report generation. For each test case, you can see when the report was created, the name of the test case for the report, an evaluated test case result, and a button to view the detailed report. In order to create and view a test case report, make sure to enable the \"Create Test Case report\" radio button when creating a simulation . Test Cases must have test report generation enabled when they are executed in order to be shown in Test Results. For video recording, the Video Recording sensor must also be included. Please see the sensor configurations page for the JSON. Each test result will show the evaluated result, with either \"Success\", \"Failed\", or \"In Progress\". You can view each test report for additional details or messages about the reason for the evaluation. Viewing a Test Report top # To view a detailed test report, click the \"View\" button on the right for a given test result. The top bar indicates the Test Report name, the associated simulation, and buttons to expand the starting simulation configuration details, and delete the report. The \"Iterations\" section displays all iterations for this test report. For test cases run with a Python API script, one test case can contain many iterations, each with its own result, statistics, and events. The highlighted box indicates the selected iteration. Simulation evaluation top # The test report indicates the evaluation status, or result, of the executed simulation. The below table specifies the conditions and criteria for a simulation result. In the future, additional criteria as well as user-customizable criteria will be supported. Result Cause/Criteria SUCCESS The test case did not result in FAILED or ERROR FAILED The test case involved at least one failure event callback ERROR The test case resulted in an error and failed to execute properly The \"Start Time\" indicates the cluster's system date and time when the test case was started/stopped. For test cases with a distributed cluster, the master node's system time is used. The \"Simulation time duration\" indicates the total duration of time of the test case in simulation , excluding paused time. Note that this can be different than the \"Real time duration\", which measures how much total time passed in the real world during execution. In the vehicle section, for each ego vehicle in the test case, you can view the vehicle bridge type, bridge IP and port location. If you included the Video Recording sensor in at least one ego vehicle, you can see the file location where the recorded video of the simulation has been stored. The following section, Sensors, exists for every ego vehicle involved in the test case. In test cases involving multiple ego vehicles, you will be able to see sensor statistics and callback event information for each ego vehicle. Sensors: Statistics top # The Callbacks section gives notable information about the ego vehicle during the test case run. Metric Description Units Distance travelled Total distance traveled, as measured by wheel odometry km Average speed The average speed of the ego vehicle km/hr Max speed The maximum speed of the ego vehicle km/hr Min speed The minimum speed of the ego vehicle km/hr Max longitudinal acceleration Maximum instantaneous acceleration along the vehicle's longitudinal axis m/s 2 Max lateral acceleration Maximum instantaneous acceleration along the vehicle's lateral (perpendicular to longitudinal) axis m/s 2 Max longitudinal jerk Maximum instantaneous jerk along the vehicle's longitudinal axis m/s 3 Max lateral jerk Maximum instantaneous jerk along the vehicle's lateral (perpendicular to longitudinal) axis m/s 3 Callbacks top # Callback events are categorized as one-time events during a test case that either affect the ego vehicle, or that the ego vehicle causes in the environment. The following types of events are supported: Collision involving ego (EgoCollision) Speed limit violation (SpeedViolation) Sudden braking (SuddenBrake) Sudden steer (SuddenSteer) Low simulation performance (LowFPS) The occurrence of at least one of these events will cause a simulation to be evaluated as \"Failed\". For each callback event, the returned information is reported, including the simulation time at which the event occurred. Selecting the eye icon will cause the visualization to seek to the time of the event in order to view the state of each sensor at that time. EgoCollision top # A collision involving an ego vehicle. Field Description Units OtherType Type of agent with which ego collided NPC|Pedestrian|Obstacle EgoVelocity Velocity vector of ego at time at collision time x, y, z in m/s OtherVelocity Velocity vector of agent/object at collision time km/hr EgoCollisionTotal Number of ego collisions in the event SpeedViolation top # A speed limit violation by an ego vehicle, based on the lane speed limit in the map annotations. Metric Description Units Duration Duration of speed limit violation event time Max speed Maximum speed of ego vehicle during event. m/s SpeedLimit The ego lane's annotated speed limit of the HD map m/s EgoStuck top # An ego stuck event by the ego vehicle, in which the ego travels a threshold distance and then stops moving for a threshold time SuddenBrake top # A sudden braking event by the ego vehicle, in which it exceeds a threshold deceleration value. SuddenSteer top # A sudden steering event by the ego vehicle, in which absolute magnitude of steering angle exceeds a threshold value. LowFPS top # A low simulation performance to indicate that results may not be accurate due to sufficiently low framerate during simulation on the cluster.","title":"Test Results"},{"location":"user-interface/web/test-results/#test-results","text":"On the left-hand side of the web user interface, clicking \"Test Results\" brings up the Test Results page. This page contains a table of all executed test cases which enabled test report generation. For each test case, you can see when the report was created, the name of the test case for the report, an evaluated test case result, and a button to view the detailed report. In order to create and view a test case report, make sure to enable the \"Create Test Case report\" radio button when creating a simulation . Test Cases must have test report generation enabled when they are executed in order to be shown in Test Results. For video recording, the Video Recording sensor must also be included. Please see the sensor configurations page for the JSON. Each test result will show the evaluated result, with either \"Success\", \"Failed\", or \"In Progress\". You can view each test report for additional details or messages about the reason for the evaluation.","title":"Test Results"},{"location":"user-interface/web/test-results/#viewing-a-test-report","text":"To view a detailed test report, click the \"View\" button on the right for a given test result. The top bar indicates the Test Report name, the associated simulation, and buttons to expand the starting simulation configuration details, and delete the report. The \"Iterations\" section displays all iterations for this test report. For test cases run with a Python API script, one test case can contain many iterations, each with its own result, statistics, and events. The highlighted box indicates the selected iteration.","title":"Viewing a Test Report"},{"location":"user-interface/web/test-results/#simulation-evaluation","text":"The test report indicates the evaluation status, or result, of the executed simulation. The below table specifies the conditions and criteria for a simulation result. In the future, additional criteria as well as user-customizable criteria will be supported. Result Cause/Criteria SUCCESS The test case did not result in FAILED or ERROR FAILED The test case involved at least one failure event callback ERROR The test case resulted in an error and failed to execute properly The \"Start Time\" indicates the cluster's system date and time when the test case was started/stopped. For test cases with a distributed cluster, the master node's system time is used. The \"Simulation time duration\" indicates the total duration of time of the test case in simulation , excluding paused time. Note that this can be different than the \"Real time duration\", which measures how much total time passed in the real world during execution. In the vehicle section, for each ego vehicle in the test case, you can view the vehicle bridge type, bridge IP and port location. If you included the Video Recording sensor in at least one ego vehicle, you can see the file location where the recorded video of the simulation has been stored. The following section, Sensors, exists for every ego vehicle involved in the test case. In test cases involving multiple ego vehicles, you will be able to see sensor statistics and callback event information for each ego vehicle.","title":"Simulation evaluation"},{"location":"user-interface/web/test-results/#sensors-statistics","text":"The Callbacks section gives notable information about the ego vehicle during the test case run. Metric Description Units Distance travelled Total distance traveled, as measured by wheel odometry km Average speed The average speed of the ego vehicle km/hr Max speed The maximum speed of the ego vehicle km/hr Min speed The minimum speed of the ego vehicle km/hr Max longitudinal acceleration Maximum instantaneous acceleration along the vehicle's longitudinal axis m/s 2 Max lateral acceleration Maximum instantaneous acceleration along the vehicle's lateral (perpendicular to longitudinal) axis m/s 2 Max longitudinal jerk Maximum instantaneous jerk along the vehicle's longitudinal axis m/s 3 Max lateral jerk Maximum instantaneous jerk along the vehicle's lateral (perpendicular to longitudinal) axis m/s 3","title":"Sensors: Statistics"},{"location":"user-interface/web/test-results/#callbacks","text":"Callback events are categorized as one-time events during a test case that either affect the ego vehicle, or that the ego vehicle causes in the environment. The following types of events are supported: Collision involving ego (EgoCollision) Speed limit violation (SpeedViolation) Sudden braking (SuddenBrake) Sudden steer (SuddenSteer) Low simulation performance (LowFPS) The occurrence of at least one of these events will cause a simulation to be evaluated as \"Failed\". For each callback event, the returned information is reported, including the simulation time at which the event occurred. Selecting the eye icon will cause the visualization to seek to the time of the event in order to view the state of each sensor at that time.","title":"Visualization: Callbacks"},{"location":"user-interface/web/test-results/#egocollision","text":"A collision involving an ego vehicle. Field Description Units OtherType Type of agent with which ego collided NPC|Pedestrian|Obstacle EgoVelocity Velocity vector of ego at time at collision time x, y, z in m/s OtherVelocity Velocity vector of agent/object at collision time km/hr EgoCollisionTotal Number of ego collisions in the event","title":"EgoCollision"},{"location":"user-interface/web/test-results/#speedviolation","text":"A speed limit violation by an ego vehicle, based on the lane speed limit in the map annotations. Metric Description Units Duration Duration of speed limit violation event time Max speed Maximum speed of ego vehicle during event. m/s SpeedLimit The ego lane's annotated speed limit of the HD map m/s","title":"SpeedViolation"},{"location":"user-interface/web/test-results/#egostuck","text":"An ego stuck event by the ego vehicle, in which the ego travels a threshold distance and then stops moving for a threshold time","title":"EgoStuck"},{"location":"user-interface/web/test-results/#suddenbrake","text":"A sudden braking event by the ego vehicle, in which it exceeds a threshold deceleration value.","title":"SuddenBrake"},{"location":"user-interface/web/test-results/#suddensteer","text":"A sudden steering event by the ego vehicle, in which absolute magnitude of steering angle exceeds a threshold value.","title":"SuddenSteer"},{"location":"user-interface/web/test-results/#lowfps","text":"A low simulation performance to indicate that results may not be accurate due to sufficiently low framerate during simulation on the cluster.","title":"LowFPS"}]}